
PT J
AU Brito, LC
   da Silva, MB
   Duarte, MAV
AF Brito, Lucas Costa
   da Silva, Marcio Bacci
   Viana Duarte, Marcus Antonio
TI Identification of cutting tool wear condition in turning using self-organizing map trained with imbalanced data
SO JOURNAL OF INTELLIGENT MANUFACTURING
LA English
DT Article
DE Tool wear monitoring; Self-organizing map neural network; Imbalanced data; Unsupervised learning; Vibration measurement
ID support vector machine; classification; algorithm; svm; features; system; signal
AB One of the most important parameters in machining process is tool wear. Thus, monitoring the wear of cutting tools is essential to ensure product quality, increase productivity, reduce environmental impact and avoid catastrophic damages. As wear is related to the vibrations of the process, the vibration signal is commonly used to monitor the process non-intrusively. Traditional wear monitoring techniques present a number of problems such as: the difficulty of identifying vibration features sensitive to wear evolution, the specialist requirement for supervising the model training and an endless series of tests to work with balanced data. To overcome these difficulties, this paper aims to propose a new approach in the application of unsupervised artificial intelligence technique with imbalanced data to identify the cutting tool wear condition during the turning process. The methodology will allow industrial applications since no supervision is required in the model training when machining condition is changed. From vibration signals collected during each tool pass, a self-organizing map model was used to identify the ideal moment of tool change. The classifier used was compared to benchmark supervised methods (weighted k-nearest neighbor and support vector machine). Imbalanced data sets were used to simulate the industrial reality. Tool tests were performed under different wear conditions and changing the cutting parameters. The results showed that it is possible to predict the cutting tool wear condition with a self-organizing map neural for imbalanced data, using only the vibration signal with up to 92% accuracy.
C1 [Brito, Lucas Costa; da Silva, Marcio Bacci; Viana Duarte, Marcus Antonio] Fed Univ Uberlandia UFU, Sch Mech Engn, Av Joao Naves de Avila 2121,Campus Santa Monica, Uberlandia, MG, Brazil.
C3 Universidade Federal de Uberlandia
RP Brito, LC (corresponding author), Fed Univ Uberlandia UFU, Sch Mech Engn, Av Joao Naves de Avila 2121,Campus Santa Monica, Uberlandia, MG, Brazil.
EM brito.lcb@gmail.com
FU Brazilian research funding agency CNPq (National Council for Scientific and Technological Development); Brazilian research funding agency CAPES (Federal Agency for the Support and Improvement of Higher Education); Brazilian research funding agency FAPEMIG (Minas Gerais State Research Foundation)
CR Arabmakki E, 2017, NEUROCOMPUTING, V262, P120, DOI 10.1016/j.neucom.2016.11.088
   Burnap P, 2018, COMPUT SECUR, V73, P399, DOI 10.1016/j.cose.2017.11.016
   Cai Q, 2014, NEUROCOMPUTING, V133, P258, DOI 10.1016/j.neucom.2013.11.010
   Changhao Xia, 2018, JOURNAL OF ELECTRICAL SYSTEMS AND INFORMATION TECHNOLOGY, V5, P681, DOI 10.1016/j.jesit.2017.05.008
   Demircan S, 2018, NEURAL COMPUT APPL, V29, P59, DOI 10.1007/s00521-016-2712-y
   Dudani S. A., 1976, IEEE TRANSACTIONS ON SYSTEMS, V0, P0
   Fix E., 1951, PROJECT 21 49 004, V0, P0
   Gajate A, 2012, J INTELL MANUF, V23, P869, DOI 10.1007/s10845-010-0443-y
   GRUBBS FE, 1969, TECHNOMETRICS, V11, P1, DOI 10.2307/1266761
   Hassan M, 2018, PROC CIRP, V72, P1451, DOI 10.1016/j.procir.2018.03.201
   Huang ZW, 2020, J INTELL MANUF, V31, P953, DOI 10.1007/s10845-019-01488-7
   Jain DK, 2018, J COMPUT SCI-NETH, V25, P252, DOI 10.1016/j.jocs.2017.07.016
   Jurkovic Z, 2018, J INTELL MANUF, V29, P1683, DOI 10.1007/s10845-016-1206-1
   Kannatey-Asibu E, 2017, MECH SYST SIGNAL PR, V85, P651, DOI 10.1016/j.ymssp.2016.08.035
   Kohonen T, 1995, SELF ORG MAPS, V0, P0
   Kong DD, 2019, MECH SYST SIGNAL PR, V127, P573, DOI 10.1016/j.ymssp.2019.03.023
   Lee WJ, 2019, PROC CIRP, V80, P506, DOI 10.1016/j.procir.2018.12.019
   Li ZF, 2018, COMPUT IND ENG, V116, P37, DOI 10.1016/j.cie.2017.12.002
   Liu CQ, 2018, INT J ADV MANUF TECH, V97, P229, DOI 10.1007/s00170-018-1916-y
   Liu M, 2004, J MATER PROCESS TECH, V150, P234, DOI 10.1016/j.jmatprotec.2004.02.038
   Liu YQ, 2018, MULTIMED TOOLS APPL, V77, P209, DOI 10.1007/s11042-016-4265-6
   Lokesh S, 2019, NEURAL COMPUT APPL, V31, P1521, DOI 10.1007/s00521-018-3466-5
   da Silva RHL, 2016, MACH SCI TECHNOL, V20, P386, DOI 10.1080/10910344.2016.1191026
   Lu MC, 2013, INT J ADV MANUF TECH, V66, P1785, DOI 10.1007/s00170-012-4458-8
   Lu SY, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, V0, P826
   Meng X, 2019, COMPUT ELECTR ENG, V77, P120, DOI 10.1016/j.compeleceng.2019.05.013
   Mikaeil R, 2018, GEOTECH GEOL ENG, V36, P1309, DOI 10.1007/s10706-017-0394-6
   Mikolajczyk T, 2018, MECH SYST SIGNAL PR, V104, P503, DOI 10.1016/j.ymssp.2017.11.022
   Mutheneni Srinivasa Rao, 2018, PARASITE EPIDEMIOL CONTROL, V3, P52, DOI 10.1016/j.parepi.2016.11.001
   Nguwi YY, 2010, EXPERT SYST APPL, V37, P8303, DOI 10.1016/j.eswa.2010.05.054
   Nouni M, 2015, INT J MACH TOOL MANU, V89, P1, DOI 10.1016/j.ijmachtools.2014.10.011
   Pandiyan V, 2018, J MANUF PROCESS, V31, P199, DOI 10.1016/j.jmapro.2017.11.014
   Prasad BS, 2017, ENG SCI TECHNOL, V20, P197, DOI 10.1016/j.jestch.2016.06.011
   Rizal M, 2017, WEAR, V376, P1759, DOI 10.1016/j.wear.2017.02.017
   Rmili W, 2016, MEASUREMENT, V77, P117, DOI 10.1016/j.measurement.2015.09.010
   Santhanam T, 2015, PROCEDIA COMPUT SCI, V47, P76, DOI 10.1016/j.procs.2015.03.185
   Sevilla-Camacho PY, 2015, INT J ADV MANUF TECH, V81, P1187, DOI 10.1007/s00170-015-7302-0
   Sevilla-Camacho PY, 2015, MEASUREMENT, V64, P81, DOI 10.1016/j.measurement.2014.12.037
   Sharif M, 2020, PATTERN ANAL APPL, V23, P281, DOI 10.1007/s10044-019-00789-0
   Siddhpura A, 2013, INT J ADV MANUF TECH, V65, P371, DOI 10.1007/s00170-012-4177-1
   Sun HT, 2019, OPTIK, V184, P214, DOI 10.1016/j.ijleo.2019.02.126
   Trent E.M., 2000, METAL CUTTING, V0, P0
   Unler A, 2011, INFORM SCIENCES, V181, P4625, DOI 10.1016/j.ins.2010.05.037
   Vapnik V, 1997, ADV NEUR IN, V9, P281
   Wang C, 2018, MEASUREMENT, V117, P312, DOI 10.1016/j.measurement.2017.12.015
   Wang GF, 2019, J INTELL MANUF, V30, P113, DOI 10.1007/s10845-016-1235-9
   Wu XY, 2018, MULTIMED TOOLS APPL, V77, P3745, DOI 10.1007/s11042-016-3931-z
   Xie ZY, 2019, INT J ADV MANUF TECH, V100, P3197, DOI 10.1007/s00170-018-2926-5
   Yen CL, 2013, MECH SYST SIGNAL PR, V34, P353, DOI 10.1016/j.ymssp.2012.05.001
   Zendehboudi A, 2018, J CLEAN PROD, V199, P272, DOI 10.1016/j.jclepro.2018.07.164
   Zhou YQ, 2018, INT J ADV MANUF TECH, V96, P2509, DOI 10.1007/s00170-018-1768-5
NR 51
TC 14
Z9 14
U1 7
U2 28
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0956-5515
EI 1572-8145
J9 J INTELL MANUF
JI J. Intell. Manuf.
PD JAN 15
PY 2021
VL 32
IS 1
BP 127
EP 140
DI 10.1007/s10845-020-01564-3
EA AUG 2020
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Manufacturing
SC Computer Science; Engineering
GA PT8BE
UT WOS:000559403100001
DA 2023-04-26
ER

PT J
AU Ibrahim, MR
   Haworth, J
   Cheng, T
AF Ibrahim, Mohamed R.
   Haworth, James
   Cheng, Tao
TI URBAN-i: From urban scenes to mapping slums, transport modes, and pedestrians in cities using deep learning and computer vision
SO ENVIRONMENT AND PLANNING B-URBAN ANALYTICS AND CITY SCIENCE
LA English
DT Article
DE Computer vision; deep learning; convolutional neural networks; object-based detection; mapping slums; urban modelling; cities
ID networks; improve; size
AB In recent years, deep learning and computer vision have been applied to solve complex problems across many domains. In urban studies, these technologies have been instrumental in the development of smart cities and autonomous vehicles. However, a knowledge gap is present when it comes to informal urban regions in less developed countries. How can deep learning and artificial intelligence untangle the complexities of informality to advance urban modelling? In this paper, we introduce a framework for multipurpose realistic-dynamic urban modelling using deep convolutional neural networks. The purpose of the framework is twofold: (1) to sense and detect informality and slums in urban scenes from aerial and street-level images and (2) to detect pedestrian and transport modes. The model has been trained on images of urban scenes in cities across the globe. The framework shows strong validation performance in the identification of planned and unplanned regions, despite broad variations in the classified images. The algorithms of the URBAN-i model are coded in Python and the trained models can be applied to images of any urban setting, including informal settlements and slum regions.
C1 [Ibrahim, Mohamed R.; Haworth, James] Univ Coll London UCL, SpaceTimeLab, Dept Civil Environm & Geomat Engineer, London, England.
   [Cheng, Tao] Univ Coll London UCL, Geoinformat, London, England.
C3 University of London; University College London; University of London; University College London
RP Ibrahim, MR (corresponding author), Univ Coll London UCL, Dept Civil Environm & Geomat Engn, Room 1-02,Chadwick Bldg,Gower St, London WC1E 6BT, England.
EM mohamed.ibrahim.17@ucl.ac.uk
FU EPSRC [EP/J004197/1] Funding Source: UKRI
CR [Anonymous], 2015, LEARNING REPRESENTAT, V0, P0
   [Anonymous], 1900, DOI 10.1038/NATURE14539, V0, P0
   [Anonymous], 2014, PROC IEEE C COMPUT V, V0, P0
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Batty M, 2005, ENVIRON PLANN A, V37, P1373, DOI 10.1068/a3784
   Batty M, 1996, ENVIRON PLANN A, V28, P1745, DOI 10.1068/a281745
   Batty M, 1997, J AM PLANN ASSOC, V63, P266, DOI 10.1080/01944369708975918
   Batty M, 1997, ENVIRON PLANN B, V24, P159
   Batty M., 1984, PSEUDO DYNAMIC URBAN, V0, P0
   Batty M., 2001, CYBERGEO, V0, P0, DOI DOI 10.4000/cybergeo.1035
   Batty M., 1994, FRACTAL CITIES GEOME, V0, P0
   Batty M, 2008, SCIENCE, V319, P769, DOI 10.1126/science.1151419
   Bettencourt L, 2010, NATURE, V467, P912, DOI 10.1038/467912a
   Bettencourt LMA, 2013, SCIENCE, V340, P1438, DOI 10.1126/science.1235823
   Brown E, 2017, PYTORCH IMPLEMENTATI, V0, P0
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, V0, PP1, DOI 10.1109/NANOARCH.2017.8053709
   Christian S., 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/CVPR.2015.7298594
   Crandall D., 2009, P 18 INT C WORLD WID, V0, PP761, DOI 10.1145/1526709.1526812
   Dahl GE, 2013, INT CONF ACOUST SPEE, V0, PP8609, DOI 10.1109/ICASSP.2013.6639346
   de Almeida C. M., 2003, COMPUTERS, V0, P0
   Demir I, 2018, IEEE COMPUT SOC CONF, V0, PP172, DOI 10.1109/CVPRW.2018.00031
   Dubey A, 2016, LECT NOTES COMPUT SC, V9905, P196, DOI 10.1007/978-3-319-46448-0_12
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Frankhauser P., 1998, POPUL ENGL SEL, V0, P205
   Friesen J, 2018, HABITAT INT, V73, P79, DOI 10.1016/j.habitatint.2018.02.002
   Gallagher Andrew, 2009, 2009 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPR WORKSHOPS), V0, PP55, DOI 10.1109/CVPR.2009.5204168
   Gamba P., 2007, INT ARCH PHOTOGRAMME, V36, P0
   Glorot X., 2011, P 14 INT C ART INT S, V0, P315
   Garcia CG, 2017, FUTURE GENER COMP SY, V76, P301, DOI 10.1016/j.future.2016.12.033
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Hays J., 2015, LARGE SCALE IMAGE GE, V0, PP41, DOI 10.1007/978-3-319-09861-6_3
   He K., 2015, PROC CVPR IEEE, V5, P6
   Heppenstall AJJ., 2012, AGENT BASED MODELS G, V164, P1, DOI 10.1007/978-90-481-8927-4
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G, 2017, PROC CVPR IEEE, V0, PP2261, DOI 10.1109/CVPR.2017.243
   Isalgue A, 2007, PHYSICA A, V382, P643, DOI 10.1016/j.physa.2007.04.019
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Kuffer M, 2018, ISPRS INT J GEO-INF, V7, P0, DOI 10.3390/ijgi7110428
   Kuffer M, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8060455
   LAW S, 2018, ARXIV180707155CSECON, V0, P0
   Li X, 2017, IEEE I CONF COMP VIS, V0, PP784, DOI 10.1109/ICCV.2017.91
   Lin GS, 2017, PROC CVPR IEEE, V0, PP5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lynch K., 2008, IMAGE CITY, V33rd ed., P0
   Mahabir R, 2018, URBAN SCI, V2, P0, DOI 10.3390/urbansci2010008
   Mboga N, 2017, INT GEOSCI REMOTE SE, V0, P5169
   Murcio R, 2015, PHYS REV E, V92, P0, DOI 10.1103/PhysRevE.92.062130
   Naik N, 2017, P NATL ACAD SCI USA, V114, P7571, DOI 10.1073/pnas.1619003114
   Naik N, 2016, AM ECON REV, V106, P128, DOI 10.1257/aer.p20161030
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Peng C, 2017, PROC CVPR IEEE, V0, PP1743, DOI 10.1109/CVPR.2017.189
   Persello C, 2017, IEEE GEOSCI REMOTE S, V14, P2325, DOI 10.1109/LGRS.2017.2763738
   Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), V0, PP1, DOI 10.1109/ICPHM.2017.7998297
   Quack T., 2008, P INT C CONT BAS IM, V0, PP47, DOI 10.1145/1386352.1386363
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salesses P, 2013, PLOS ONE, V8, P0, DOI 10.1371/journal.pone.0068400
   Sandler E., 2011, GET LAT LON EXIF PIL, V0, P0
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tarigan J, 2017, PROCEDIA COMPUT SCI, V116, P365, DOI 10.1016/j.procs.2017.10.068
   UN-Habitat, 2010, CHALL SLUMS GLOB REP, V0, P0
   Wang L, 2018, SENSORS-BASEL, V18, P0, DOI 10.3390/s18030769
   Yu F., 2015, 1511 ARXIV, V0, P0
   Zhou BL, 2014, LECT NOTES COMPUT SC, V8691, P519, DOI 10.1007/978-3-319-10578-9_34
NR 69
TC 13
Z9 13
U1 7
U2 27
PU SAGE PUBLICATIONS LTD
PI LONDON
PA 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN 2399-8083
EI 2399-8091
J9 ENVIRON PLAN B-URBAN
JI Env. Plan. B-Urban Anal. City Sci.
PD JAN 15
PY 2021
VL 48
IS 1
BP 76
EP 93
DI 10.1177/2399808319846517
PG 18
WC Environmental Studies; Geography; Regional & Urban Planning; Urban Studies
SC Environmental Sciences & Ecology; Geography; Public Administration; Urban Studies
GA PY6EO
UT WOS:000612136400006
DA 2023-04-26
ER

PT J
AU Luo, H
   He, BA
   Guo, RZ
   Wang, WX
   Kuai, X
   Xia, BL
   Wan, Y
   Ma, D
   Xie, LF
AF Luo, Heng
   He, Biao
   Guo, Renzhong
   Wang, Weixi
   Kuai, Xi
   Xia, Bilu
   Wan, Yuan
   Ma, Ding
   Xie, Linfu
TI Urban Building Extraction and Modeling Using GF-7 DLC and MUX Images
SO REMOTE SENSING
LA English
DT Article
DE building extraction; building modeling; Gaofen-7 image; deep learning; digital surface model
ID neural-networks; random forest; generation
AB Urban modeling and visualization are highly useful in the development of smart cities. Buildings are the most prominent features in the urban environment, and are necessary for urban decision support; thus, buildings should be modeled effectively and efficiently in three dimensions (3D). In this study, with the help of Gaofen-7 (GF-7) high-resolution stereo mapping satellite double-line camera (DLC) images and multispectral (MUX) images, the boundary of a building is segmented via a multilevel features fusion network (MFFN). A digital surface model (DSM) is generated to obtain the elevation of buildings. The building vector with height information is processed using a 3D modeling tool to create a white building model. The building model, DSM, and multispectral fused image are then imported into the Unreal Engine 4 (UE4) to complete the urban scene level, vividly rendered with environmental effects for urban visualization. The results of this study show that high accuracy of 95.29% is achieved in building extraction using our proposed method. Based on the extracted building vector and elevation information from the DSM, building 3D models can be efficiently created in Level of Details 1 (LOD1). Finally, the urban scene is produced for realistic 3D visualization. This study shows that high-resolution stereo mapping satellite images are useful in 3D modeling for urban buildings and can support the generation and visualization of urban scenes in a large area for different applications.
C1 [Luo, Heng; He, Biao; Guo, Renzhong; Wang, Weixi; Kuai, Xi; Ma, Ding; Xie, Linfu] Shenzhen Univ, Sch Architecture & Urban Planning, Res Inst Smart Cities, Shenzhen 518060, Peoples R China.
   [Luo, Heng; He, Biao; Guo, Renzhong; Wang, Weixi; Kuai, Xi; Wan, Yuan; Ma, Ding; Xie, Linfu] Minist Nat Resources, Key Lab Urban Land Resources Monitoring & Simulat, Shenzhen 518034, Peoples R China.
   [Luo, Heng] Guangxi Zhuang Autonomous Reg Inst Nat Resources, Nanning 530023, Peoples R China.
   [Guo, Renzhong; Wang, Weixi; Kuai, Xi; Ma, Ding; Xie, Linfu] Guangdong Hong Kong Macau Joint Lab Smart Cities, Shenzhen 518060, Peoples R China.
   [Xia, Bilu] Guangxi Vocat & Tech Coll Commun, Traff Informat Engn Inst, Nanning 530023, Peoples R China.
   [Wan, Yuan] Hubei Normal Univ, Coll Urban & Environm Sci, Huangshi 435002, Hubei, Peoples R China.
C3 Shenzhen University; Ministry of Natural Resources of the People's Republic of China; Hubei Normal University
RP He, BA (corresponding author), Shenzhen Univ, Sch Architecture & Urban Planning, Res Inst Smart Cities, Shenzhen 518060, Peoples R China.; He, BA (corresponding author), Minist Nat Resources, Key Lab Urban Land Resources Monitoring & Simulat, Shenzhen 518034, Peoples R China.
EM luoheng@szu.edu.cn; hebiao@szu.edu.cn; guorz@szu.edu.cn; wangwx@szu.edu.cn; kuaixi@szu.edu.cn; rachelpipi@126.com; wanyuan14@whu.edu.cn; dingma@szu.edu.cn; linfuxie@szu.edu.cn
FU National Natural Science Foundation of China [41971354, 42001331]; National Key Research and Development Program of China [2019YFB2103104]; Guangdong Science and Technology Strategic Innovation Fund (the Guangdong-Hong Kong-Macau Joint Laboratory Program) [2020B1212030009]; Open Fund of Key Laboratory of Urban Land Resource Monitoring and Simulation, Ministry of Land and Resource [KF-2018-03-031]; Ministry of Natural Resources of the People's Republic of China (MNR) [42-Y30B04-9001-19/21]; Guangxi Innovative Development Grand Program [GuikeAA18118038]; High-Resolution Remote Sensing Surveying Application Demonstration System of the Land Satellite Remote Sensing Application Center (LASAC)
CR Alshehhi R, 2017, ISPRS J PHOTOGRAMM, V130, P139, DOI 10.1016/j.isprsjprs.2017.05.002
   Batty M, 2018, ENVIRON PLAN B-URBAN, V45, P817, DOI 10.1177/2399808318796416
   Bruzzone L, 2000, IEEE T GEOSCI REMOTE, V38, P1171, DOI 10.1109/36.843009
   Chen Liang- Chieh, 2018, STUDY INFLUENCE EXTE, V0, P0
   Dembski F, 2020, SUSTAINABILITY-BASEL, V12, P0, DOI 10.3390/su12062307
   Dowman I., 2000, P 26 ANN C REM SENS, V0, P0
   Feng WQ, 2019, INT GEOSCI REMOTE SE, V0, PP52, DOI 10.1109/IGARSS.2019.8899163
   Friedl MA, 1997, REMOTE SENS ENVIRON, V61, P399, DOI 10.1016/S0034-4257(97)00049-7
   Fu G, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9050498
   Grigillo D, 2012, INT J REMOTE SENS, V33, P5149, DOI 10.1080/01431161.2012.659356
   Haala N., 2008, P 21 ISPRS C BEIJ CH, V0, P0
   Haala N, 2010, ISPRS J PHOTOGRAMM, V65, P570, DOI 10.1016/j.isprsjprs.2010.09.006
   Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Huang J, 2019, NANOCELLULOSE, V0, P0, DOI DOI 10.1002/9783527807437.CH11
   Huang ZM, 2016, INT GEOSCI REMOTE SE, V0, PP1835, DOI 10.1109/IGARSS.2016.7729471
   Lee DS, 2003, PHOTOGRAMM ENG REM S, V69, P143, DOI 10.14358/PERS.69.2.143
   Lek S, 1999, ECOL MODEL, V120, P65, DOI 10.1016/S0304-3800(99)00092-7
   Li L, 2014, REMOTE SENS-BASEL, V6, P4409, DOI 10.3390/rs6054409
   Liu PH, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11070830
   Liu T, 2018, GISCI REMOTE SENS, V55, P243, DOI 10.1080/15481603.2018.1426091
   Mnih V., 2013, ARXIV, V0, P0
   Mountrakis G, 2011, ISPRS J PHOTOGRAMM, V66, P247, DOI 10.1016/j.isprsjprs.2010.11.001
   Pal M, 2005, INT J REMOTE SENS, V26, P217, DOI 10.1080/01431160412331269698
   Shahrabi B., 2002, THESIS U STUTTGART S, V0, P0
   Shiramizu K, 2017, POLAR SCI, V14, P30, DOI 10.1016/j.polar.2017.10.002
   Sohn G., 2002, INT ARCH PHOTOGRAMM, V34, P336
   Sohn G, 2007, ISPRS J PHOTOGRAMM, V62, P43, DOI 10.1016/j.isprsjprs.2007.01.001
   Sun K, 2019, PROC CVPR IEEE, V0, PP5686, DOI 10.1109/CVPR.2019.00584
   [徐胜军 Xu Shengjun], 2020, 光学精密工程 OPTICS AND PRECISION ENGINEERING, V28, P1588
   [唐新明 Tang Xinming], 2021, 测绘学报 ACTA GEODETICA ET CARTOGRAPHICA SINICA, V50, P384
   [唐新明 Tang Xinming], 2012, 测绘学报 ACTA GEODETICA ET CARTOGRAPHICA SINICA, V41, P191
   Thomson C, 2015, REMOTE SENS-BASEL, V7, P11753, DOI 10.3390/rs70911753
   [王长杰 Wang Changjie], 2020, 航天返回与遥感 SPACECRAFT RECOVERY & REMOTE SENSING, V41, P29
   Xiong XH, 2013, AUTOMAT CONSTR, V31, P325, DOI 10.1016/j.autcon.2012.10.006
   Yang H, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10111768
   [杨居奎 Yang Jukui], 2020, 航天器工程 SPACECRAFT ENGINEERING, V29, P61
   Yi YN, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11151774
   Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798
NR 38
TC 7
Z9 7
U1 5
U2 39
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD SEP 15
PY 2021
VL 13
IS 17
BP 
EP 
DI 10.3390/rs13173414
PG 22
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA UO1SA
UT WOS:000694480600001
DA 2023-04-26
ER

PT J
AU Papakonstantinou, A
   Batsaris, M
   Spondylidis, S
   Topouzelis, K
AF Papakonstantinou, Apostolos
   Batsaris, Marios
   Spondylidis, Spyros
   Topouzelis, Konstantinos
TI A Citizen Science Unmanned Aerial System Data Acquisition Protocol and Deep Learning Techniques for the Automatic Detection and Mapping of Marine Litter Concentrations in the Coastal Zone
SO DRONES
LA English
DT Article
DE unmanned aerial systems; marine litter; deep learning; convolutional neural networks; computer vision; marine litter detection
ID beach litter; satellite; pollution; plastics
AB Marine litter (ML) accumulation in the coastal zone has been recognized as a major problem in our time, as it can dramatically affect the environment, marine ecosystems, and coastal communities. Existing monitoring methods fail to respond to the spatiotemporal changes and dynamics of ML concentrations. Recent works showed that unmanned aerial systems (UAS), along with computer vision methods, provide a feasible alternative for ML monitoring. In this context, we proposed a citizen science UAS data acquisition and annotation protocol combined with deep learning techniques for the automatic detection and mapping of ML concentrations in the coastal zone. Five convolutional neural networks (CNNs) were trained to classify UAS image tiles into two classes: (a) litter and (b) no litter. Testing the CCNs' generalization ability to an unseen dataset, we found that the VVG19 CNN returned an overall accuracy of 77.6% and an f-score of 77.42%. ML density maps were created using the automated classification results. They were compared with those produced by a manual screening classification proving our approach's geographical transferability to new and unknown beaches. Although ML recognition is still a challenging task, this study provides evidence about the feasibility of using a citizen science UAS-based monitoring method in combination with deep learning techniques for the quantification of the ML load in the coastal zone using density maps.
C1 [Papakonstantinou, Apostolos; Spondylidis, Spyros; Topouzelis, Konstantinos] Univ Aegean, Dept Marine Sci, Mitilini 81100, Greece.
   [Batsaris, Marios] Univ Aegean, Geog Dept, Mitilini 81100, Greece.
C3 University of Aegean; University of Aegean
RP Papakonstantinou, A (corresponding author), Univ Aegean, Dept Marine Sci, Mitilini 81100, Greece.
EM apapak@aegean.gr; m.batsaris@aegean.gr; sspo@aegean.gr; topouzelis@marine.aegean.gr
FU European Union (European Social FundESF) through the Operational Programme "Human Resources Development, Education and Lifelong Learning" [MIS-5033021]
CR Abadi M., 2016, TENSORFLOW LARGE SCA, V0, P0
   Abadi M, 2016, PROCEEDINGS OF OSDI16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, V0, P265
   Acosta J., 2012, SEAFLOOR GEOMORPHOLO, V0, Pxxxi xlv
   Andriolo U, 2020, SCI TOTAL ENVIRON, V736, P0, DOI 10.1016/j.scitotenv.2020.139632
   [Anonymous], 2017, G20 ANN G20 LEAD DEC, V0, P0
   Bao ZC, 2018, MAR POLLUT BULL, V137, P388, DOI 10.1016/j.marpolbul.2018.08.009
   Benassai G, 2017, NAT HAZARD EARTH SYS, V17, P1493, DOI 10.5194/nhess-17-1493-2017
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Cheshire A., 2009, IOC TECHNICAL SERIES, V83, P0
   Chircop A, 2018, OCEAN YEARB, V32, P752, DOI 10.1163/22116001-03201028
   Cicin-Sain B., 2015, UN CHRON, V51, P32, DOI 10.18356/8fcfd5a1-en
   Costanzo LG, 2020, J MAR SCI ENG, V8, P0, DOI 10.3390/jmse8090656
   Cox J, 2015, COMPUT SCI ENG, V17, P28, DOI 10.1109/MCSE.2015.65
   Deidun A, 2018, MAR POLLUT BULL, V131, P212, DOI 10.1016/j.marpolbul.2018.04.033
   Doukari M, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11161913
   Duarte D., 2020, ISPRS ANN PHOTOGRAMM, V3-2020, P439, DOI 10.5194/isprs-annals-v-3-2020-439-2020.-2020
   Fallati L, 2019, SCI TOTAL ENVIRON, V693, P0, DOI 10.1016/j.scitotenv.2019.133581
   Ferrari R, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8020113
   Fortson L, 2012, CH CRC DATA MIN KNOW, V0, P213
   Galgani F, 2013, ICES J MAR SCI, V70, P1055, DOI 10.1093/icesjms/fst122
   Galgani F., 2019, REP STUD GESAMP, V0, P130
   Galgani F, 2015, FRONT MAR SCI, V2, P0, DOI 10.3389/fmars.2015.00087
   Geraeds M, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11172045
   Goncalves G, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12162599
   Goncalves G, 2020, MAR POLLUT BULL, V155, P0, DOI 10.1016/j.marpolbul.2020.111158
   Goncalves G, 2020, SCI TOTAL ENVIRON, V706, P0, DOI 10.1016/j.scitotenv.2019.135742
   Haarr ML, 2019, MAR POLLUT BULL, V139, P117, DOI 10.1016/j.marpolbul.2018.12.025
   Haseler M, 2018, J COAST CONSERV, V22, P27, DOI 10.1007/s11852-017-0497-5
   He K., 2016, P IEEE C COMP VIS PA, V2016, P1512.03385, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, V0, PP2261, DOI 10.1109/CVPR.2017.243
   Husson E, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9030247
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Kylili K, 2020, ENVIRON SCI POLLUT R, V27, P42631, DOI 10.1007/s11356-020-10105-7
   Kylili K, 2019, ENVIRON SCI POLLUT R, V26, P17091, DOI 10.1007/s11356-019-05148-4
   Lo HS, 2020, MAR POLLUT BULL, V151, P0, DOI 10.1016/j.marpolbul.2019.110823
   Lohr A, 2017, CURR OPIN ENV SUST, V28, P90, DOI 10.1016/j.cosust.2017.08.009
   Maes T, 2019, MAR POLLUT BULL, V146, P274, DOI 10.1016/j.marpolbul.2019.06.019
   Martin C, 2018, MAR POLLUT BULL, V131, P662, DOI 10.1016/j.marpolbul.2018.04.045
   Maximenko N, 2019, FRONT MAR SCI, V6, P0, DOI 10.3389/fmars.2019.00447
   Morseletto P, 2020, MAR POLICY, V117, P0, DOI 10.1016/j.marpol.2020.103956
   Munari C, 2016, WASTE MANAGE, V49, P483, DOI 10.1016/j.wasman.2015.12.010
   Mury A, 2020, DRONES-BASEL, V4, P0, DOI 10.3390/drones4020025
   Nazerdeylami A, 2021, OCEAN COAST MANAGE, V200, P0, DOI 10.1016/j.ocecoaman.2020.105478
   Painting SJ, 2020, OCEAN SCI, V16, P235, DOI 10.5194/os-16-235-2020
   Papachristopoulou I, 2020, MAR POLLUT BULL, V150, P0, DOI 10.1016/j.marpolbul.2019.110684
   Papakonstantinou A., 2017, P 5 INT C REM SENS G, V0, P0
   Papakonstantinou A., 2020, HABITAT MAPPING USIN, V12, P554, DOI 10.3390/rs12030554
   Papathanassiou A., 2019, IEEE 5G TECH FOCUS, V1, P1
   REES G, 1995, MAR POLLUT BULL, V30, P103, DOI 10.1016/0025-326X(94)00192-C
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Riihimaki H, 2019, REMOTE SENS ENVIRON, V224, P119, DOI 10.1016/j.rse.2019.01.030
   Rios N, 2018, MAR POLLUT BULL, V133, P304, DOI 10.1016/j.marpolbul.2018.05.038
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schulz M, 2017, MAR POLLUT BULL, V122, P166, DOI 10.1016/j.marpolbul.2017.06.045
   Schulz M, 2015, MAR ENVIRON RES, V109, P21, DOI 10.1016/j.marenvres.2015.04.007
   Simonyan K, 2015, ARXIV, V0, P0
   Simpson R, 2014, WWW14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, V0, PP1049, DOI 10.1145/2567948.2579215
   Smail EA, 2019, J OPER OCEANOGR, V12, PS1, DOI 10.1080/1755876X.2019.1634959
   Taddia Y, 2020, DRONES-BASEL, V4, P0, DOI 10.3390/drones4020009
   Topouzelis K., 2016, C PAP, V0, P81100
   Topouzelis K, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12122013
   Topouzelis K, 2019, INT J APPL EARTH OBS, V79, P175, DOI 10.1016/j.jag.2019.03.011
   United Nations Environmental Programme (UNEP), 2014, PLAST DEBR OC UNEP Y, V0, P0
   Valavanidis A., 2012, SCI ADV ENV TOXICOLO, V0, P1
   Velegrakis A., 2016, P COMM INT EXPL SCI, V0, P0
   Vikas M, 2015, AQUAT PR, V4, P381, DOI 10.1016/j.aqpro.2015.02.051
   Vlachogianni T., 2016, JRC TECHNICAL REPORT, V0, P0
   Wenneker B., 2010, GUIDELINE MONITORING, V0, P0, DOI DOI 10.25607/OBP-968
   Xanthos D, 2017, MAR POLLUT BULL, V118, P17, DOI 10.1016/j.marpolbul.2017.02.048
NR 72
TC 30
Z9 30
U1 3
U2 17
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2504-446X
J9 DRONES-BASEL
JI Drones-Basel
PD MAR 15
PY 2021
VL 5
IS 1
BP 
EP 
DI 10.3390/drones5010006
PG 20
WC Remote Sensing
SC Remote Sensing
GA RC9HK
UT WOS:000633102700001
DA 2023-04-26
ER

PT J
AU Konapala, G
   Kumar, SV
   Ahmad, SK
AF Konapala, Goutam
   Kumar, Sujay, V
   Ahmad, Shahryar Khalique
TI Exploring Sentinel-1 and Sentinel-2 diversity for flood inundation mapping using deep learning
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Flood inundation mapping; Deep learning; Segmentation; Hydrology
ID water index ndwi; surface-water; time-series; sar; resolution; satellite; dem; delineation; imagery; areas
AB Identification of flood water extent from satellite images has historically relied on either synthetic aperture radar (SAR) or multi-spectral (MS) imagery. MS sensors are limited to cloud free conditions, whereas SAR imagery is plagued by noise-like speckle. Prior studies that use combinations of MS and SAR data to overcome individual limitations of these sensors have not fully examined sensitivity of flood mapping performance to different combinations of SAR and MS derived spectral indices or band transformations in color space. This study explores the use of diverse bands of Sentinel 2 (S2) through well-established water indices and Sentinel 1 (S1) derived SAR imagery along with their combinations to assess their capability for generating accurate flood inundation maps. The robustness in performance of S-1 and S-2 band combinations was evaluated using 446 hand labeled flood inundation images spanning across 11 flood events from Sen1Floods11 dataset which are highly diverse in terms of land cover as well as location. A modified K-fold cross validation approach is used to evaluate the performance of 32 combinations of S1 and S2 bands using a fully connected deep convolutional neural network known as U-Net. Our results indicated that usage of elevation information has improved the capability of S1 imagery to produce more accurate flood inundation maps. Compared to a median F1 score of 0.62 when using only S1 bands, the combined use of S1 and elevation information led to an improved median F1 score of 0.73. Water extraction indices based on S2 bands have a statistically significant superior performance in comparison to S1. Among all the band combinations, HSV (Hue, Saturation, Value) transformation of S2 bands provides a median F1 score of 0.9, outperforming the commonly used water spectral indices owing to HSV's transformation's superior contrast distinguishing abilities. Additionally, U-Net algorithm was able to learn the relationship between raw S2 based water extraction indices and their corresponding raw S2 bands, but not of HSV owing to relatively complex computation involved in the latter. Results of the paper establishes important benchmarks for the extension of S1 and S2 data-based flood inundation mapping efforts over large spatial extents.
C1 [Konapala, Goutam; Kumar, Sujay, V; Ahmad, Shahryar Khalique] NASA, Hydrol Sci Lab, Goddard Space Flight Ctr, Greenbelt, MD 20771 USA.
   [Konapala, Goutam] Univ Space Res Assoc, Greenbelt, MD USA.
   [Ahmad, Shahryar Khalique] Sci Applicat Int Corp, Greenbelt, MD USA.
C3 National Aeronautics & Space Administration (NASA); NASA Goddard Space Flight Center; Universities Space Research Association (USRA); Science Applications International Corporation (SAIC)
RP Konapala, G (corresponding author), NASA, Hydrol Sci Lab, Goddard Space Flight Ctr, Greenbelt, MD 20771 USA.
EM goutam.konapala@nasa.gov
FU NASA Earth Science Technology Office
CR BARTON IJ, 1989, REMOTE SENS ENVIRON, V30, P89, DOI 10.1016/0034-4257(89)90050-3
   Betbeder J, 2014, J APPL REMOTE SENS, V8, P0, DOI 10.1117/1.JRS.8.083648
   Binh PD, 2017, WATER-SUI, V9, P0, DOI 10.3390/w9060366
   Bioresita F, 2019, INT J REMOTE SENS, V40, P9026, DOI 10.1080/01431161.2019.1624869
   Bonafilia D, 2020, IEEE COMPUT SOC CONF, V0, PP835, DOI 10.1109/CVPRW50498.2020.00113
   Boschetti M, 2014, PLOS ONE, V9, P0, DOI 10.1371/journal.pone.0088741
   Clement MA, 2018, J FLOOD RISK MANAG, V11, P152, DOI 10.1111/jfr3.12303
   Colson D, 2018, INT J APPL EARTH OBS, V73, P262, DOI 10.1016/j.jag.2018.06.011
   DeVries B, 2020, REMOTE SENS ENVIRON, V240, P0, DOI 10.1016/j.rse.2020.111664
   Du GT, 2020, J IMAGING SCI TECHN, V64, P0, DOI 10.2352/J.ImagingSci.Technol.2020.64.2.020508
   Dusseux P, 2014, REMOTE SENS-BASEL, V6, P6163, DOI 10.3390/rs6076163
   Fereshtehpour M, 2018, WATER RESOUR RES, V54, P4965, DOI 10.1029/2017WR022318
   Feyisa GL, 2014, REMOTE SENS ENVIRON, V140, P23, DOI 10.1016/j.rse.2013.08.029
   Gao Q, 2017, SENSORS-BASEL, V17, P0, DOI 10.3390/s17091966
   Gebrehiwot A, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19071486
   Gevaert CM, 2015, IEEE J-STARS, V8, P3140, DOI 10.1109/JSTARS.2015.2406339
   Goffi A, 2020, INT J APPL EARTH OBS, V84, P0, DOI 10.1016/j.jag.2019.101951
   Haile A.T., 2005, P ISPRS WG III3 III4, VVolume 3, P12
   Huang C, 2018, REV GEOPHYS, V56, P333, DOI 10.1029/2018RG000598
   Iannelli GC, 2018, INT GEOSCI REMOTE SE, V0, P8209
   Ienco D, 2019, ISPRS J PHOTOGRAMM, V158, P11, DOI 10.1016/j.isprsjprs.2019.09.016
   Irwin K, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9090890
   Jain P, 2020, PROCEEDINGS OF THE 35TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING (SAC20), V0, PP617, DOI 10.1145/3341105.3374023
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM14), V0, PP675, DOI 10.1145/2647868.2654889
   King DB, 2015, ACS SYM SER, V1214, P1
   Klein I, 2017, REMOTE SENS ENVIRON, V198, P345, DOI 10.1016/j.rse.2017.06.045
   Li SM, 2013, INT J REMOTE SENS, V34, P5487, DOI 10.1080/01431161.2013.792969
   Li YQ, 2018, IMMS 2019: 2019 2ND INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT AND MANAGEMENT SCIENCES, V0, PP123, DOI 10.1145/3357292.3357320
   Liang JY, 2020, ISPRS J PHOTOGRAMM, V159, P53, DOI 10.1016/j.isprsjprs.2019.10.017
   Manakos I, 2020, EUR J REMOTE SENS, V53, P53, DOI 10.1080/22797254.2019.1596757
   Manfreda S, 2015, NAT HAZARDS, V79, P735, DOI 10.1007/s11069-015-1869-5
   Martinis S, 2013, REMOTE SENS-BASEL, V5, P5598, DOI 10.3390/rs5115598
   Mason DC, 2014, INT J APPL EARTH OBS, V28, P150, DOI 10.1016/j.jag.2013.12.002
   Mateo-Garcia G, 2021, SCI REP-UK, V11, P0, DOI 10.1038/s41598-021-86650-z
   Matgen P, 2007, INT J APPL EARTH OBS, V9, P247, DOI 10.1016/j.jag.2006.03.003
   Matgen P, 2011, PHYS CHEM EARTH, V36, P241, DOI 10.1016/j.pce.2010.12.009
   McFeeters SK, 1996, INT J REMOTE SENS, V17, P1425, DOI 10.1080/01431169608948714
   McNairn H, 2009, ISPRS J PHOTOGRAMM, V64, P434, DOI 10.1016/j.isprsjprs.2008.07.006
   Mosavi A, 2018, WATER-SUI, V10, P0, DOI 10.3390/w10111536
   Musa ZN, 2015, HYDROL EARTH SYST SC, V19, P3755, DOI 10.5194/hess-19-3755-2015
   Nemni E, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12162532
   Oberstadler R, 1997, HYDROL PROCESS, V11, P1415, DOI 10.1002/(SICI)1099-1085(199708)11:10&lt;1415::AID-HYP532&gt;3.0.CO;2-2
   Pekel JF, 2014, REMOTE SENS ENVIRON, V140, P704, DOI 10.1016/j.rse.2013.10.008
   Pekel JF, 2016, NATURE, V540, P418, DOI 10.1038/nature20584
   Peng B, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11212492
   Potnis AV, 2019, INT GEOSCI REMOTE SE, V0, PP9741, DOI 10.1109/IGARSS.2019.8900250
   Rajah P., 2018, REMOTE SENS APPL SOC, V10, P198, DOI 10.1016/J.RSASE.2018.04.007
   Rambour C., 2020, INT ARCH PHOTOGRAMM, V0, PP1343, DOI 10.5194/ISPRS-ARCHIVES-XLIII-B2-2020-1343
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saksena S, 2015, J HYDROL, V530, P180, DOI 10.1016/j.jhydrol.2015.09.069
   Samela C, 2016, J HYDROL ENG, V21, P0, DOI 10.1061/(ASCE)HE.1943-5584.0001272
   Schmitt M, 2020, PFG-J PHOTOGRAMM REM, V88, P271, DOI 10.1007/s41064-020-00111-2
   Schratz P, 2019, ECOL MODEL, V406, P109, DOI 10.1016/j.ecolmodel.2019.06.002
   Shen XY, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11070879
   Shen XY, 2019, REMOTE SENS ENVIRON, V221, P302, DOI 10.1016/j.rse.2018.11.008
   Slagter B, 2020, INT J APPL EARTH OBS, V86, P0, DOI 10.1016/j.jag.2019.102009
   Smith, 1978, COMPUTER GRAPHICS, V12, P12, DOI 10.1145/965139.807361
   Soergel U, 2003, 2ND GRSS/ISPRS JOINT WORKSHOP ON REMOTE SENSING AND DATA FUSION OVER URBAN AREAS, V0, PP120, DOI 10.1109/DFUA.2003.1219970
   Twele A, 2016, INT J REMOTE SENS, V37, P2990, DOI 10.1080/01431161.2016.1192304
   Xu HQ, 2006, INT J REMOTE SENS, V27, P3025, DOI 10.1080/01431160600589179
   Yang LP, 2011, INT J REMOTE SENS, V32, P3875, DOI 10.1080/01431161003786016
   Zheng X, 2018, WATER RESOUR RES, V54, P10013, DOI 10.1029/2018WR023457
NR 62
TC 18
Z9 18
U1 11
U2 54
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD OCT 15
PY 2021
VL 180
IS 
BP 163
EP 173
DI 10.1016/j.isprsjprs.2021.08.016
EA AUG 2021
PG 11
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA US0ZU
UT WOS:000697167200011
DA 2023-04-26
ER
