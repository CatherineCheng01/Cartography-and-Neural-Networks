
PT J
AU Abd Mubin, N
   Nadarajoo, E
   Shafri, HZM
   Hamedianfar, A
AF Abd Mubin, Nurulain
   Nadarajoo, Eiswary
   Shafri, Helmi Zulhaidi Mohd
   Hamedianfar, Alireza
TI Young and mature oil palm tree detection and counting using convolutional neural network deep learning method
SO INTERNATIONAL JOURNAL OF REMOTE SENSING
LA English
DT Article
ID primary productivity; palsar; gross
AB Detection and counting of oil palm are important in oil palm plantation management. In this article, we use a deep learning approach to predict and count oil palms in satellite imagery. Previous oil palm detections commonly focus on detecting oil palm trees that do not have overlapping crowns. Besides this, there is a lack of research that builds separate detection system for young and mature oil palm, utilizing deep learning approach for oil palm detection and combining geographic information system (GIS) with deep learning approach. This research attempts to fill this gap by utilizing two different convolution neural networks (CNNs) to detect young and mature oil palm separately and uses GIS during data processing and result storage process. The initial architecture developed is based on a CNN called LeNet. The training process reduces loss using adaptive gradient algorithm with a mini batch of size 20 for all the training sets used. Then, we exported prediction results to GIS software and created oil palm prediction map for mature and young oil palm. Based on the proposed method, the overall accuracies for young and mature oil palm are 95.11% and 92.96%, respectively. Overall, the classifier performs well on previously unseen datasets, and isable to accurately detect oil palm from background, including plant shadows and other plants.
C1 [Abd Mubin, Nurulain; Nadarajoo, Eiswary; Shafri, Helmi Zulhaidi Mohd] Univ Putra Malaysia, Fac Engn, Dept Civil Engn, Serdang 43400, Selangor, Malaysia.
   [Shafri, Helmi Zulhaidi Mohd] Univ Putra Malaysia, Fac Engn, GISRC, Serdang, Malaysia.
   [Hamedianfar, Alireza] Islamic Azad Univ, Estahban Branch, Dept Surveying Engn, Estahban, Iran.
C3 Universiti Putra Malaysia; Universiti Putra Malaysia; Islamic Azad University
RP Shafri, HZM (corresponding author), Univ Putra Malaysia, Fac Engn, Dept Civil Engn, Serdang 43400, Selangor, Malaysia.
EM helmi@upm.edu.my
FU Nvidia; Universiti Putra Malaysia [UPM/700-1/2/GPB/2017/9543100]
CR [Anonymous], 2016, GLOBAL PALM OIL PROD, V0, P0
   Britz D., 2017, UNDERSTANDING CONVOL, V0, P0
   Cheang E.K., 2017, USING CONVOLUTIONAL, V0, P0
   Cheng YQ, 2016, INT J REMOTE SENS, V37, P5431, DOI 10.1080/01431161.2016.1241448
   Chong KL, 2017, GEO-SPAT INF SCI, V20, P184, DOI 10.1080/10095020.2017.1337317
   Cracknell AP, 2015, INT J REMOTE SENS, V36, P262, DOI 10.1080/01431161.2014.995278
   Cracknell AP, 2013, INT J REMOTE SENS, V34, P7400, DOI 10.1080/01431161.2013.820367
   Kanniah K. D., 2012, GEOSC REM SENS S IGA, V0, P0
   Ke YH, 2011, INT J REMOTE SENS, V32, P4725, DOI 10.1080/01431161.2010.494184
   Kiama J., 2014, IOP C SER EARTH ENV, V0, P0
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li WJ, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9010022
   Mansur M.A., 2014, RES J, V1, P0
   Ng S. K., 1972, OIL PALM ITS CULTURE, V0, P0
   Rizeei HM, 2018, J SENSORS, V2018, P0, DOI 10.1155/2018/2536327
   Shafri HZM, 2011, INT J REMOTE SENS, V32, P2095, DOI 10.1080/01431161003662928
   Srestasathiern P, 2014, REMOTE SENS-BASEL, V6, P9749, DOI 10.3390/rs6109749
   Tan KP, 2013, INT J REMOTE SENS, V34, P7424, DOI 10.1080/01431161.2013.822601
   Tan KP, 2012, PROG PHYS GEOG, V36, P655, DOI 10.1177/0309133312452187
   Wong-In T., 2015, IND ENG MANAGEMENT S, V0, P403
   Zakharova M, 2017, THESIS, V0, P0
NR 21
TC 66
Z9 69
U1 2
U2 83
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 0143-1161
EI 1366-5901
J9 INT J REMOTE SENS
JI Int. J. Remote Sens.
PD OCT 2
PY 2019
VL 40
IS 19
BP 7500
EP 7515
DI 10.1080/01431161.2019.1569282
PG 16
WC Remote Sensing; Imaging Science & Photographic Technology
SC Remote Sensing; Imaging Science & Photographic Technology
GA ID8SI
UT WOS:000471955100012
DA 2023-04-26
ER

PT J
AU Song, H
   Baek, MS
   Sung, M
AF Song, Ha Yoon
   Baek, Moo Sang
   Sung, Minsuk
TI Generating Human Mobility Route Based on Generative Adversarial Network
SO PROCEEDINGS OF THE 2019 FEDERATED CONFERENCE ON COMPUTER SCIENCE AND INFORMATION SYSTEMS (FEDCSIS)
LA English
DT Proceedings Paper
ID prediction
AB Recently, many researches on human mobility are aiming to suggest the personal customized solution in the diverse field, usually by academia and industry. Combined with deep learning methods, it is able to predict and generate novel routes of objects from the mobility data including the given past trends. In this work, Generative Adversarial Network (GAN) model is introduced for creating individual mobility routes based on sets of accumulated personal mobility data. The mobility data had been collected by use of geopositioning system and personal mobile devices. GAN has Discriminator and Generator which are composed of neural networks, and can train and extract geopositionig information. A sequence of longitude and latitude can be geographically mapped, and matrices including all these information can be handled by GAN. The GAN-based model successfully handled individual mobility routes in this way. Consequently, our model can generate and suggest unexplored routes from the existing sets of personal geolocation data.
C1 [Song, Ha Yoon; Sung, Minsuk] Hongik Univ, Dept Comp Engn, Seoul, South Korea.
   [Baek, Moo Sang] Hongik Univ, Res Inst Sci & Technol, Seoul, South Korea.
C3 Hongik University; Hongik University
RP Song, H (corresponding author), Hongik Univ, Dept Comp Engn, Seoul, South Korea.
EM hayoon@hongik.ac.kr; moosangbaek@gmail.com; mssung94@mail.hongik.ac.kr
FU National Research Foundation of Korea (NRF) - Korea government (MEST) [NRF-2019R1F1A1056123]
CR Alzantot Moustafa, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS WORKSHOPS (PERCOM WORKSHOPS), V0, PP188, DOI 10.1109/PERCOMW.2017.7917555
   Baratchi M, 2014, UBICOMP14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, V0, PP401, DOI 10.1145/2632048.2636068
   Choi Y, 2018, PROC CVPR IEEE, V0, PP8789, DOI 10.1109/CVPR.2018.00916
   Featherstone B, 2018, CAPABILITY-PROMOTING POLICIES: ENHANCING INDIVIDUAL AND SOCIAL DEVELOPMENT, V0, P183
   Fedus W, 2017, MANY PATHS EQUILIBRI, V0, P0
   Giannotti F, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, V0, P330
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gorawski M, 2010, LECT NOTES ARTIF INT, V6076, P187, DOI 10.1007/978-3-642-13769-3_23
   Jeung HY, 2008, PROC INT CONF DATA, V0, PP70, DOI 10.1109/ICDE.2008.4497415
   Lee JW, 2004, J SYST SOFTWARE, V73, P481, DOI 10.1016/j.jss.2003.09.021
   Molano-Mazon Manuel, 2018, ARXIV180300338, V0, P0
   Monreale A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, V0, P637
   Morzy M, 2007, LECT NOTES ARTIF INT, V4571, P667
   Morzy M, 2006, LECT NOTES COMPUT SC, V4263, P583
   Nhan VTH, 2006, LECT NOTES CONTR INF, V344, P875
   Pfoser D., 2000, P VLDB, V2000, P395
   Radford A., 2015, 4 INT C LEARNING REP, V0, P0
   Reinecke P., 2012, 2012 9 INT C QUANT E, V0, P0, DOI DOI 10.1109/qest.2012.29
   Shi N, 2010, 2010 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY AND SECURITY INFORMATICS (IITSI 2010), V0, PP63, DOI 10.1109/IITSI.2010.74
   Song HY, 2015, PROCEDIA COMPUT SCI, V63, P142, DOI 10.1016/j.procs.2015.08.324
   Sudo A, 2016, 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016), V0, P0, DOI DOI 10.1145/2996913.2997000
   Ying J. C., 2011, ACM SIGSPATIAL INT C, V0, PP34, DOI 10.1145/2093973.2093980
NR 22
TC 5
Z9 5
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2325-0348
EI 
J9 FED CONF COMPUT SCI
PD JUN 15
PY 2019
VL 0
IS 
BP 91
EP 98
DI 10.15439/2019F320
PG 8
WC Computer Science, Information Systems; Computer Science, Theory & Methods
SC Computer Science
GA BQ4OU
UT WOS:000591782800016
DA 2023-04-26
ER

PT J
AU Wagatsuma, N
   Hidaka, A
   Tamura, H
AF Wagatsuma, Nobuhiko
   Hidaka, Akinori
   Tamura, Hiroshi
TI The Correspondence Between Monkey Visual Areas and Layers in DCNN Saliency Map Model for Representations of Natural Images
SO I-PERCEPTION
LA English
DT Meeting Abstract
DE Deep Convolutional Neural Networks; Saliency Map; Visual Areas
C1 [Wagatsuma, Nobuhiko] Toho Univ, Fac Sci, Tokyo, Japan.
   [Hidaka, Akinori] Tokyo Denki Univ, Sch Sci & Engn, Tokyo, Japan.
   [Tamura, Hiroshi] Osaka Univ, Grad Sch Frontier Biosci, Suita, Osaka, Japan.
   [Tamura, Hiroshi] Ctr Informat & Neural Networks CiNet, Suita, Osaka, Japan.
C3 Toho University; Tokyo Denki University; Osaka University; National Institute of Information & Communications Technology (NICT) - Japan
FU KAKENHI, Japan [17K12704]; Grants-in-Aid for Scientific Research [17K12704] Funding Source: KAKEN
NR 0
TC 0
Z9 0
U1 0
U2 0
PU SAGE PUBLICATIONS LTD
PI LONDON
PA 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN 2041-6695
EI 
J9 I-PERCEPTION
JI I-Perception
PD SEP 15
PY 2019
VL 10
IS 
BP 89
EP 89
DI 
PG 1
WC Psychology, Experimental
SC Psychology
GA JE2OZ
UT WOS:000490535400169
DA 2023-04-26
ER

PT J
AU Handan-Nader, C
   Ho, DE
AF Handan-Nader, Cassandra
   Ho, Daniel E.
TI Deep learning to map concentrated animal feeding operations
SO NATURE SUSTAINABILITY
LA English
DT Article
AB Enforcement of environmental law depends critically on permitting and monitoring intensive animal agricultural facilities, known in the United States as 'concentrated animal feeding operations' (CAFOs). The current legal landscape in the United States has made it difficult for government agencies, environmental groups and the public to know where such facilities are located. Numerous groups have, as a result, conducted manual, resource-intensive enumerations based on maps or ground investigation to identify facilities. Here we show that applying a deep convolutional neural network to high-resolution satellite images offers an effective, highly accurate and lower cost approach to detecting CAFO locations. In North Carolina, the algorithm is able to detect 589 additional poultry CAFOs, representing an increase of 15% from the baseline that was detected through manual enumeration. We show how the approach scales over geography and time, and can inform compliance and monitoring priorities.
C1 [Handan-Nader, Cassandra; Ho, Daniel E.] Stanford Univ, Stanford Law Sch, Stanford, CA 94305 USA.
   [Handan-Nader, Cassandra; Ho, Daniel E.] Stanford Univ, Dept Polit Sci, Stanford, CA 94305 USA.
   [Ho, Daniel E.] Stanford Inst Econ Policy Res, Stanford, CA 94305 USA.
C3 Stanford University; Stanford University; Stanford University
RP Ho, DE (corresponding author), Stanford Univ, Stanford Law Sch, Stanford, CA 94305 USA.; Ho, DE (corresponding author), Stanford Univ, Dept Polit Sci, Stanford, CA 94305 USA.; Ho, DE (corresponding author), Stanford Inst Econ Policy Res, Stanford, CA 94305 USA.
EM dho@law.stanford.edu
FU GRACE Communications Foundation; Stanford Institute for Economic and Policy Research
CR Amstrup S. C., 2010, HDB CAPTURE RECAPTUR, V0, P0
   [Anonymous], 2018, STOPP CAFO POLL, V0, P0
   [Anonymous], 2018, WHAT ARE TECHNICAL S, V0, P0
   [Anonymous], 2017, 2017 ANN REP WORK PL, V0, P0
   [Anonymous], 2017, INT FARM ANIMAL WILD, V0, P0
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Brown C. R., 2011, TEMPLE J SCI TECHNOL, V30, P175
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Casey Joan A, 2015, CURR ENVIRON HEALTH REP, V2, P259, DOI 10.1007/s40572-015-0061-0
   Conerly O., 2013, 820R13002 EPA, V0, P0
   Copeland C., 2010, CRSRL31851, V0, P0
   Dimitri C., 2005, EC INFORM B, V0, P0
   Environmental Protection Agency, 2011, FED REGISTER, V76, P65431
   Environmental Protection Agency National pollutant discharge elimination system (NPDES), 2012, FED REGISTER, V77, P42679
   Environmental Protection Agency National pollutant discharge elimination system permit regulation and effluent limitation guidelines and standards for concentrated animal feeding operations (CAFOs), 2003, FED REGISTER, V68, P7176
   Formuzis A., 2016, FIELDS FILTH LANDMAR, V0, P0
   Graham JP, 2010, J WATER HEALTH, V8, P646, DOI 10.2166/wh.2010.075
   Hribar Carrie, 2010, UNDERSTANDING CONCEN, V0, P0
   Hurst N., 2017, SMITHSONIAN, V0, P0
   Ilea RC, 2009, J AGR ENVIRON ETHIC, V22, P153, DOI 10.1007/s10806-008-9136-3
   Jerger Scott, 2004, STAN ENV LJ, V23, P91
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   MacDonald J. M., 2004, AER837 USDA, V0, P0
   Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
   McVey M. J., 2003, SAEA 2003 ANN M, V0, P0
   Newcombe T., 2018, GOVT TECHNOLOGY, V0, P0
   Oquab M, 2014, PROC CVPR IEEE, V0, PP1717, DOI 10.1109/CVPR.2014.222
   Osburn CL, 2016, ENVIRON SCI TECHNOL, V50, P8473, DOI 10.1021/acs.est.6b00053
   Patt H., 2017, COMP PAN P2O5 PRODUC, V0, P0
   PCloudy, 2015, PCLOUDY DEV FARM, V0, P0
   Peterka A., 2013, E E DAILY, V0, P0
   Pew Commission on Industrial Farm Animal Production, 2007, PUTT MEAT TABL IND F, V0, P0
   Rogers S., 2005, DETECTING MITIGATING, V0, P0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito T, 2015, PLOS ONE, V10, P0, DOI 10.1371/journal.pone.0118432
   Scoles Sarah, 2017, WIRED, V0, P0
   Simard PY, 2003, PROC INT CONF DOC, V0, P958
   Simonyan Karen, 2014, 3 INT C LEARN REPR, V0, P0
   Smith LN, 2017, IEEE WINT CONF APPL, V0, PP464, DOI 10.1109/WACV.2017.58
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2016, PROC CVPR IEEE, V0, PP2818, DOI 10.1109/CVPR.2016.308
   *US GOV ACC OFF, 2008, GAO08944, V0, P0
   Walton Brett, 2016, CIRCLE OF BLUE, V0, P0
   Zhou B, 2016, PROC CVPR IEEE, V0, PP2921, DOI 10.1109/CVPR.2016.319
NR 45
TC 23
Z9 24
U1 4
U2 28
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2398-9629
EI 
J9 NAT SUSTAIN
JI Nat. Sustain.
PD APR 15
PY 2019
VL 2
IS 4
BP 298
EP 306
DI 10.1038/s41893-019-0246-x
PG 9
WC Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies
SC Science & Technology - Other Topics; Environmental Sciences & Ecology
GA HS5QJ
UT WOS:000463925700016
DA 2023-04-26
ER

PT J
AU Mohammadimanesh, F
   Salehi, B
   Mandianpari, M
   Gill, E
   Molinier, M
AF Mohammadimanesh, Fariba
   Salehi, Bahram
   Mandianpari, Masoud
   Gill, Eric
   Molinier, Matthieu
TI A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Deep learning; Land cover; Wetland; Convolutional Neural Network (CNN); Fully Convolutional Network (FCN); Encoder-decoder; Polarimetric Synthetic Aperture Radar (PolSAR)
ID classification
AB Despite the application of state-of-the-art fully Convolutional Neural Networks (CNNs) for semantic segmentation of very high-resolution optical imagery, their capacity has not yet been thoroughly examined for the classification of Synthetic Aperture Radar (SAR) images. The presence of speckle noise, the absence of efficient feature expression, and the limited availability of labelled SAR samples have hindered the application of the state-of-the-art CNNs for the classification of SAR imagery. This is of great concern for mapping complex land cover ecosystems, such as wetlands, where backscattering/spectrally similar signatures of land cover units further complicate the matter. Accordingly, we propose a new Fully Convolutional Network (FCN) architecture that can be trained in an end-to-end scheme and is specifically designed for the classification of wetland complexes using polarimetric SAR (PoISAR) imagery. The proposed architecture follows an encoder-decoder paradigm, wherein the input data are fed into a stack of convolutional filters (encoder) to extract high-level abstract features and a stack of transposed convolutional filters (decoder) to gradually up-sample the low resolution output to the spatial resolution of the original input image. The proposed network also benefits from recent advances in CNN designs, namely the addition of inception modules and skip connections with residual units. The former component improves multi-scale inference and enriches contextual information, while the latter contributes to the recovery of more detailed information and simplifies optimization. Moreover, an in-depth investigation of the learned features via opening the black box demonstrates that convolutional filters extract discriminative polarimetric features, thus mitigating the limitation of the feature engineering design in PoISAR image processing. Experimental results from full polarimetric RADARSAT-2 imagery illustrate that the proposed network outperforms the conventional random forest classifier and the state-of-the-art FCNs, such as FCN-32s, FCN-16s, FCN-8s, and SegNet, both visually and numerically for wetland mapping.
C1 [Mohammadimanesh, Fariba; Mandianpari, Masoud] C CORE, Capt Robert A Bartlett Bldg,1 Morrissey Rd, St John, NF A1B 3X5, Canada.
   [Mohammadimanesh, Fariba; Mandianpari, Masoud; Gill, Eric] Mem Univ Newfoundland, Dept Elect & Comp Engn, St John, NF A1C 5S7, Canada.
   [Salehi, Bahram] SUNY Coll Environm Sci & Forestry, Environm Resources Engn, Syracuse, NY 13210 USA.
   [Molinier, Matthieu] VTT Tech Res Ctr Finland Ltd, Oulu, Finland.
C3 Memorial University Newfoundland; State University of New York (SUNY) System; State University of New York (SUNY) College of Environmental Science & Forestry; VTT Technical Research Center Finland
RP Mohammadimanesh, F (corresponding author), C CORE, Capt Robert A Bartlett Bldg,1 Morrissey Rd, St John, NF A1B 3X5, Canada.
EM f.mohammadimanesh@mun.ca
FU Government of Canada through the federal Department of Environment and Climate Change, Natural Sciences and Engineering Research Council of Canada [NSERC RGPIN-2015-05027]; Research and Development Corporation of Newfoundland and Labrador [RDC-5404-2108-101]; VTT Substance Node on Deep Learning Applications; Ducks Unlimited Canada; Government of Newfoundland and Labrador Department of Environment and Conservation; Nature Conservancy Canada
CR Anwer RM, 2018, ISPRS J PHOTOGRAMM, V138, P74, DOI 10.1016/j.isprsjprs.2018.01.023
   Badrinarayanan V, 2015, ARXIV150507293, V0, P0, DOI DOI 10.1109/TPAMI.2016.2644615
   Breiman L., 2001, MACHINE LEARNING, V45, P5, DOI 10.1023/A:1010933404324
   Chen GZ, 2018, IEEE J-STARS, V11, P1633, DOI 10.1109/JSTARS.2018.2810320
   Chen SW, 2018, IEEE GEOSCI REMOTE S, V15, P627, DOI 10.1109/LGRS.2018.2799877
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen YY, 2014, REMOTE SENS-BASEL, V6, P12575, DOI 10.3390/rs61212575
   Christian S., 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/CVPR.2015.7298594
   Dettwiler M, 2008, RADARSAT 2 PRODUCT F, V0, P0
   Fu G, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9050498
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, V0, P1
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   Henry C., 2018, ARXIV180201445, V0, P0
   Huang B, 2018, REMOTE SENS ENVIRON, V214, P73, DOI 10.1016/j.rse.2018.04.050
   Huang X, 2013, IEEE T GEOSCI REMOTE, V51, P257, DOI 10.1109/TGRS.2012.2202912
   Kampffmeyer M, 2016, IEEE COMPUT SOC CONF, V0, PP680, DOI 10.1109/CVPRW.2016.90
   Lardeux C, 2009, IEEE T GEOSCI REMOTE, V47, P4143, DOI 10.1109/TGRS.2009.2023908
   Lee JS, 2009, OPT SCI ENG-CRC, V0, P1
   Li YY, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10121984
   Liu HY, 2017, IEEE J-STARS, V10, P1456, DOI 10.1109/JSTARS.2016.2618891
   Liu S, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10091339
   Liu Y, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9060522
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lonnqvist A, 2010, IEEE T GEOSCI REMOTE, V48, P3652, DOI 10.1109/TGRS.2010.2048115
   Ma XR, 2018, IEEE T GEOSCI REMOTE, V56, P4781, DOI 10.1109/TGRS.2018.2837142
   Mahdianpari M, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11010043
   Mahdianpari M, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10071119
   Mahdianpari M, 2017, CAN J REMOTE SENS, V43, P468, DOI 10.1080/07038992.2017.1381550
   Mahdianpari M, 2017, CAN J REMOTE SENS, V43, P485, DOI 10.1080/07038992.2017.1381549
   Mandianpari M, 2017, ISPRS J PHOTOGRAMM, V130, P13, DOI 10.1016/j.isprsjprs.2017.05.010
   Marmanis D, 2018, ISPRS J PHOTOGRAMM, V135, P158, DOI 10.1016/j.isprsjprs.2017.11.009
   McInnes Leland, 2020, ARXIV, V0, P0, DOI DOI 10.48550/arXiv:1802.03426
   Mnih V., 2013, MACHINE LEARNING AER, V0, P0
   Mohammadimanesh F, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11050516
   Mohammadimanesh F, 2018, CAN J REMOTE SENS, V44, P247, DOI 10.1080/07038992.2018.1477680
   Mohammadimanesh F, 2018, INT J APPL EARTH OBS, V73, P450, DOI 10.1016/j.jag.2018.06.005
   Mohammadimanesh F, 2018, ISPRS J PHOTOGRAMM, V142, P78, DOI 10.1016/j.isprsjprs.2018.05.009
   Mou LC, 2018, IEEE T GEOSCI REMOTE, V56, P391, DOI 10.1109/TGRS.2017.2748160
   Paisitkriangkrai Sakrapee, 2015, 2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW), V0, PP36, DOI 10.1109/CVPRW.2015.7301381
   Pan XR, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10050743
   Rodriguez-Galiano VF, 2012, ISPRS J PHOTOGRAMM, V67, P93, DOI 10.1016/j.isprsjprs.2011.11.002
   Simonyan K, 2015, ARXIV, V0, P0
   Tao CS, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9070660
   Volpi M, 2017, IEEE T GEOSCI REMOTE, V55, P881, DOI 10.1109/TGRS.2016.2616585
   Wang Y, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10020342
   Wu W., 2019, IEEE GEOSCI REMOTE S, V0, P0
   Zhang C, 2018, ISPRS J PHOTOGRAMM, V140, P133, DOI 10.1016/j.isprsjprs.2017.07.014
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
   Zhang LP, 2013, IEEE T GEOSCI REMOTE, V51, P242, DOI 10.1109/TGRS.2012.2197860
   Zhang ZM, 2017, IEEE T GEOSCI REMOTE, V55, P7177, DOI 10.1109/TGRS.2017.2743222
   Zhao WZ, 2017, ISPRS J PHOTOGRAMM, V132, P48, DOI 10.1016/j.isprsjprs.2017.08.011
   Zhou Y, 2016, IEEE GEOSCI REMOTE S, V13, P1935, DOI 10.1109/LGRS.2016.2618840
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
NR 53
TC 114
Z9 116
U1 18
U2 198
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD MAY 15
PY 2019
VL 151
IS 
BP 223
EP 236
DI 10.1016/j.isprsjprs.2019.03.015
PG 14
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA IA1GC
UT WOS:000469306300016
DA 2023-04-26
ER
