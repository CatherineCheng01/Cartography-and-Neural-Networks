
PT J
AU Li, X
   Lei, L
   Sun, YL
   Li, M
   Kuang, GY
AF Li, Xiao
   Lei, Lin
   Sun, Yuli
   Li, Ming
   Kuang, Gangyao
TI Multimodal Bilinear Fusion Network With Second-Order Attention-Based Channel Selection for Land Cover Classification
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Attention mechanism; bilinear pooling model; convolutional neural network (CNN); feature fusion; land cover classification; multimodal learning
ID impervious surface; deep; model
AB As two different tools for earth observation, the optical and synthetic aperture radar (SAR) images can provide complementary information of the same land types for better land cover classification. However, because of the different imaging mechanisms of optical and SAR images, how to efficiently exploit the complementary information becomes an interesting and challenging problem. In this article, we propose a novel multimodal bilinear fusion network (MBFNet), which is used to fuse the optical and SAR features for land cover classification. The MBFNet consists of three components: the feature extractor, the second-order attention-based channel selection module (SACSM), and the bilinear fusion module. First, in order to avoid the network parameters tempting to ingratiate dominant modality, the pseudo-siamese convolutional neural network (CNN) is taken as the feature extractor to extract deep semantic feature maps of optical and SAR images, respectively. Then, the SACSM is embedded into each stream, and the fine channel-attention maps with second-order statistics are obtained by bilinear integrating the global average-pooling and global max-pooling information. The SACSM can not only automatically highlight the important channels of feature maps to improve the representation power of networks, but also uses the channel selection mechanism to reconfigure compact feature maps with better discrimination. Finally, the bilinear pooling is used as the feature-level fusion method, which establishes the second-order association between two compact feature maps of the optical and SAR streams to obtain the low-dimension bilinear fusion features for land cover classification. Experimental results on three broad coregistered optical and SAR datasets demonstrate that our method achieves more effective land cover classification performance than the state-of-the-art methods.
C1 [Li, Xiao; Lei, Lin; Sun, Yuli; Li, Ming; Kuang, Gangyao] Natl Univ Def Technol, State Key Lab Complex Electromagnet Environm Effe, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China
RP Lei, L (corresponding author), Natl Univ Def Technol, State Key Lab Complex Electromagnet Environm Effe, Changsha 410073, Peoples R China.
EM lxcherishm@126.com; alaleilin@163.com; sunyuli@mail.ustc.edu.cn; liming17@nudt.edu.cn; kuangyeats@hotmail.com
FU National Natural Science Foundation of China [61971426]
CR [Anonymous], 2012, P INT C MACH LEARN W, V0, P0
   Audebert N, 2018, ISPRS J PHOTOGRAMM, V140, P20, DOI 10.1016/j.isprsjprs.2017.11.011
   Bahdanau D, 2016, ARXIV, V0, P0
   Ben-younes H, 2017, IEEE I CONF COMP VIS, V0, PP2631, DOI 10.1109/ICCV.2017.285
   Benedetti P, 2018, IEEE J-STARS, V11, P4939, DOI 10.1109/JSTARS.2018.2876357
   Cen MB, 2018, IEEE IMAGE PROC, V0, PP3718, DOI 10.1109/ICIP.2018.8451102
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, V0, PP3640, DOI 10.1109/CVPR.2016.396
   Deng J, 2009, PROC CVPR IEEE, V0, PP248, DOI 10.1109/CVPRW.2009.5206848
   Eitel A, 2015, IEEE INT C INT ROBOT, V0, PP681, DOI 10.1109/IROS.2015.7353446
   Fukui Akira, 2016, P C EMP METH NAT LAN, V0, P0, DOI DOI 10.18653/v1/d16-1044
   Gao Y, 2016, PROC CVPR IEEE, V0, PP317, DOI 10.1109/CVPR.2016.41
   Gardner MW, 1998, ATMOS ENVIRON, V32, P2627, DOI 10.1016/S1352-2310(97)00447-0
   Haklay M, 2008, IEEE PERVAS COMPUT, V7, P12, DOI 10.1109/MPRV.2008.80
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He Kaiming, 2016, PROC CVPR IEEE, V0, P630
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Hughes LH, 2018, IEEE GEOSCI REMOTE S, V15, P784, DOI 10.1109/LGRS.2018.2799232
   Hughes LH, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10101552
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jegou H., 2019, P C COMP VIS PATT RE, V0, PP1169, DOI 10.1109/CVPR.2009.5206609
   Jegou H, 2010, PROC CVPR IEEE, V0, PP3304, DOI 10.1109/CVPR.2010.5540039
   Kim J.-H., 2016, ARXIV161004325, V0, P0, DOI DOI 10.48550/arXiv.1610.04325
   Kingma D. P., 2014, P INT C MACH LEARN W, V0, P0
   Kong S, 2017, PROC CVPR IEEE, V0, PP7025, DOI 10.1109/CVPR.2017.743
   Krizhevsky Alex, 2017, COMMUNICATIONS OF THE ACM, V60, P84, DOI 10.1145/3065386
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128
   Langner A, 2014, REMOTE SENS ENVIRON, V143, P122, DOI 10.1016/j.rse.2013.12.012
   Li H, 2018, ARXIV180510180, V0, P0
   Lin TY, 2015, IEEE I CONF COMP VIS, V0, PP1449, DOI 10.1109/ICCV.2015.170
   Liu DYH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), V0, P4146
   Liu Y, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), V0, P1070
   Lu DS, 2006, REMOTE SENS ENVIRON, V102, P146, DOI 10.1016/j.rse.2006.02.010
   Lu DS, 2004, PHOTOGRAMM ENG REM S, V70, P723, DOI 10.14358/PERS.70.6.723
   Ngiam J, 2011, IEEE INT C MACH LEAR, V0, PP689, DOI 10.5555/3104482.3104569
   Park E, 2016, IEEE WINT CONF APPL, V0, P0
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Quan SN, 2018, IEEE T GEOSCI REMOTE, V56, P4714, DOI 10.1109/TGRS.2018.2835513
   Ren ZL, 2018, IEEE J-STARS, V11, P3113, DOI 10.1109/JSTARS.2018.2851023
   Rudner TGJ, 2019, AAAI CONF ARTIF INTE, V0, P702
   Schilling H, 2018, IEEE J-STARS, V11, P4299, DOI 10.1109/JSTARS.2018.2825099
   Sohn K., 2014, ADV NEURAL INFORM PR, V2, P2141
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Tuia D, 2014, IEEE T GEOSCI REMOTE, V52, P7708, DOI 10.1109/TGRS.2014.2317499
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang F, 2017, PROC CVPR IEEE, V0, PP6450, DOI 10.1109/CVPR.2017.683
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiang YM, 2018, IEEE T GEOSCI REMOTE, V56, P3078, DOI 10.1109/TGRS.2018.2790483
   Xu L., 2019, REMOTE SENS, V11, P723
   Xu XD, 2018, IEEE T GEOSCI REMOTE, V56, P937, DOI 10.1109/TGRS.2017.2756851
   Yu Z, 2017, IEEE I CONF COMP VIS, V0, PP1839, DOI 10.1109/ICCV.2017.202
   ZAGORUYKO S, 2015, PROC CVPR IEEE, V0, PP4353, DOI 10.1109/CVPR.2015.7299064
   Zhang HS, 2019, IEEE J-STARS, V12, P2374, DOI 10.1109/JSTARS.2019.2915277
   Zhang XN, 2018, PROC CVPR IEEE, V0, PP714, DOI 10.1109/CVPR.2018.00081
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
NR 58
TC 22
Z9 23
U1 7
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2020
VL 13
IS 
BP 1011
EP 1026
DI 10.1109/JSTARS.2020.2975252
PG 16
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA KX9QF
UT WOS:000522209200008
DA 2023-04-26
ER

PT J
AU Han, ZM
   Dian, YY
   Xia, H
   Zhou, JJ
   Jian, YF
   Yao, CH
   Wang, X
   Li, Y
AF Han, Zemin
   Dian, Yuanyong
   Xia, Hao
   Zhou, Jingjing
   Jian, Yongfeng
   Yao, Chonghuai
   Wang, Xiong
   Li, Yuan
TI Comparing Fully Deep Convolutional Neural Networks for Land Cover Classification with High-Spatial-Resolution Gaofen-2 Images
SO ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION
LA English
DT Article
DE deep learning; classification; land cover; full convolutional network; Unet; Segnet; FCN
ID semantic segmentation; metaanalysis; cnn
AB Land cover is an important variable of the terrestrial ecosystem that provides information for natural resources management, urban sprawl detection, and environment research. To classify land cover with high-spatial-resolution multispectral remote sensing imagery is a difficult problem due to heterogeneous spectral values of the same object on the ground. Fully convolutional networks (FCNs) are a state-of-the-art method that has been increasingly used in image segmentation and classification. However, a systematic quantitative comparison of FCNs on high-spatial-multispectral remote imagery was not yet performed. In this paper, we adopted the three FCNs (FCN-8s, Segnet, and Unet) for Gaofen-2 (GF2) satellite imagery classification. Two scenes of GF2 with a total of 3329 polygon samples were used in the study area and a systematic quantitative comparison of FCNs was conducted with red, green, blue (RGB) and RGB+near infrared (NIR) inputs for GF2 satellite imagery. The results showed that: (1) The FCN methods perform well in land cover classification with GF2 imagery, and yet, different FCNs architectures exhibited different results in mapping accuracy. The FCN-8s model performed best among the Segnet and Unet architectures due to the multiscale feature channels in the upsampling stage. Averaged across the models, the overall accuracy (OA) andKappacoefficient (Kappa) were 5% and 0.06 higher, respectively, in FCN-8s when compared with the other two models. (2) High-spatial-resolution remote sensing imagery with RGB+NIR bands performed better than RGB input at mapping land cover, and yet the advantage was limited; theOAandKappaonly increased an average of 0.4% and 0.01 in the RGB+NIR bands. (3) The GF2 imagery provided an encouraging result in estimating land cover based on the FCN-8s method, which can be exploited for large-scale land cover mapping in the future.
C1 [Han, Zemin; Dian, Yuanyong; Zhou, Jingjing; Jian, Yongfeng; Yao, Chonghuai; Wang, Xiong; Li, Yuan] Huazhong Agr Univ, Coll Hort & Forestry Sci, Wuhan 430070, Peoples R China.
   [Dian, Yuanyong; Zhou, Jingjing; Yao, Chonghuai] Huazhong Agr Univ, Hubei Engn Technol Res Ctr Forestry Informat, Wuhan 430070, Peoples R China.
   [Dian, Yuanyong] Minist Agr, Key Lab Urban Agr Cent China, Wuhan 430070, Peoples R China.
   [Xia, Hao] Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100101, Peoples R China.
C3 Huazhong Agricultural University; Huazhong Agricultural University; Ministry of Agriculture & Rural Affairs; Chinese Academy of Sciences
RP Dian, YY (corresponding author), Huazhong Agr Univ, Coll Hort & Forestry Sci, Wuhan 430070, Peoples R China.; Dian, YY (corresponding author), Huazhong Agr Univ, Hubei Engn Technol Res Ctr Forestry Informat, Wuhan 430070, Peoples R China.; Dian, YY (corresponding author), Minist Agr, Key Lab Urban Agr Cent China, Wuhan 430070, Peoples R China.
EM HZM@webmail.hzau.edu.cn; dianyuanyong@mail.hzau.edu.cn; xiahao@radi.ac.cn; hupodingxiangyu@mail.hzau.edu.cn; JYongFeng@webmail.hzau.edu.cn; yao_chonghuai@mail.hzau.edu.cn; wangxiong@webmail.hzau.edu.cn; liyuan994@webmail.hzau.edu.cn
FU National Key Research and Development Program [2017YFC0821900]; National Natural Science Foundation of China [51778263]
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen J, 2015, ISPRS J PHOTOGRAMM, V103, P7, DOI 10.1016/j.isprsjprs.2014.09.002
   CHENG YF, 2017, SENSORS BASEL, V0017, P0
   Gong P, 2019, SCI BULL, V64, P370, DOI 10.1016/j.scib.2019.03.002
   Griffiths P, 2013, IEEE J-STARS, V6, P2088, DOI 10.1109/JSTARS.2012.2228167
   Hauser LT, 2017, APPL GEOGR, V86, P197, DOI 10.1016/j.apgeog.2017.06.019
   Heydari SS, 2019, ISPRS J PHOTOGRAMM, V152, P192, DOI 10.1016/j.isprsjprs.2019.04.016
   Jin BX, 2019, J INDIAN SOC REMOTE, V47, P951, DOI 10.1007/s12524-019-00945-3
   Karakizi C, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10081214
   Kemker R, 2018, ISPRS J PHOTOGRAMM, V145, P60, DOI 10.1016/j.isprsjprs.2018.04.014
   Khan N, 2019, NEUROCOMPUTING, V357, P36, DOI 10.1016/j.neucom.2019.05.024
   Kroupi E, 2019, J APPL REMOTE SENS, V13, P0, DOI 10.1117/1.JRS.13.024525
   Liu YC, 2018, ISPRS J PHOTOGRAMM, V145, P78, DOI 10.1016/j.isprsjprs.2017.12.007
   Lobser SE, 2007, INT J REMOTE SENS, V28, P5079, DOI 10.1080/01431160701253303
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015
   Mohammadimanesh F, 2019, ISPRS J PHOTOGRAMM, V151, P223, DOI 10.1016/j.isprsjprs.2019.03.015
   Preidl S, 2020, REMOTE SENS ENVIRON, V240, P0, DOI 10.1016/j.rse.2020.111673
   Ratajczak R, 2019, IEEE T IMAGE PROCESS, V28, P3357, DOI 10.1109/TIP.2019.2896492
   Ronneberger O., 2015, P MED IM COMP COMP A, V0, P234
   Sherrah J., 2016, ARXIV160602585CS, V0, P0
   Stivaktakis R, 2019, IEEE GEOSCI REMOTE S, V16, P1031, DOI 10.1109/LGRS.2019.2893306
   Tong X.-Y., 2018, P IGARSS 2018 2018 I, V0, P0
   Wulder MA, 2020, FORESTRY, V93, P331, DOI 10.1093/forestry/cpaa006
   ZHANG C, 2018, REMOTE SENS ENVIRON, V0216, P00057, DOI 10.1016/J.RSE.2018.06.034
   Zhang C, 2018, IEEE T GEOSCI REMOTE, V56, P4507, DOI 10.1109/TGRS.2018.2822783
   Zhang C, 2018, ISPRS J PHOTOGRAMM, V140, P133, DOI 10.1016/j.isprsjprs.2017.07.014
   Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
NR 29
TC 15
Z9 15
U1 9
U2 56
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2220-9964
J9 ISPRS INT J GEO-INF
JI ISPRS Int. J. Geo-Inf.
PD AUG 15
PY 2020
VL 9
IS 8
BP 
EP 
DI 10.3390/ijgi9080478
PG 13
WC Computer Science, Information Systems; Geography, Physical; Remote Sensing
SC Computer Science; Physical Geography; Remote Sensing
GA NI1YS
UT WOS:000565153800001
DA 2023-04-26
ER

PT J
AU Fang, DB
   Zhang, CH
AF Fang, Dingbang
   Zhang, Chenhao
TI Multi-Feature Learning by Joint Training for Handwritten Formula Symbol Recognition
SO IEEE ACCESS
LA English
DT Article
DE Feature extraction; Trajectory; Handwriting recognition; Data mining; Character recognition; Training; Convolutional neural networks; Artificial neural network; multi-feature; joint training; discriminative feature
ID competition
AB Given the similarity of handwritten formula symbols and various handwriting styles, this paper proposes a squeeze-extracted multi-feature convolution neural network (SE-MCNN) to improve the recognition rate of handwritten formula symbols. The system proposed in this paper integrates the eight-directional feature of the original sequence in the convolutional layer, which significantly compensates for the lost dynamic trajectory information in the handwritten formula symbol. Meanwhile, the joint loss is constructed to improve the discriminability of features in the way of supervised learning, which enlarges the inter-class difference and decreases inner-class similarity. The standard mathematical formula symbol library provided by the Competition Organization on Recognition of Online Handwritten Mathematical Expression (CROHME) is used to verify the effectiveness of the proposed algorithm. Experiments show that the proposed SE-MCNN approach outperforms the state-of-the-art methods even at the condition of without using the data augmentation.
C1 [Fang, Dingbang] Huaqiao Univ, Sch Informat Sci & Engn, Xiamen 361021, Peoples R China.
   [Zhang, Chenhao] Cent China Normal Univ, Sch Comp, Wuhan 430079, Peoples R China.
C3 Huaqiao University; Central China Normal University
RP Zhang, CH (corresponding author), Cent China Normal Univ, Sch Comp, Wuhan 430079, Peoples R China.
EM zhangchenhao@mails.ccnu.edu.cn
FU Graduate Student Scienti~c Research Innovation Project Foundation of Huaqiao University
CR Alvaro F, 2014, INT C PATT RECOG, V0, PP2944, DOI 10.1109/ICPR.2014.507
   Sanchez JA, 2019, PATTERN RECOGN, V94, P122, DOI 10.1016/j.patcog.2019.05.025
   [Anonymous], 1946, J I ELECT ENG PART 3, V0, P0, DOI DOI 10.1049/JI-3-2.1946.0074
   Bai ZL, 2005, PROC INT CONF DOC, V0, P262
   BELAID A, 1984, IEEE T PATTERN ANAL, V6, P105, DOI 10.1109/TPAMI.1984.4767483
   Blostein D., 1996, HDB CHARACTER RECOGN, V0, PP557, DOI 10.1142/9789812830968_0021
   Cheng Y., 2019, ARXIV190109890, V0, P0
   Christian S., 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/CVPR.2015.7298594
   Davila K, 2014, INT CONF FRONT HAND, V0, PP323, DOI 10.1109/ICFHR.2014.61
   Goodfellow I. J., 2013, P INT C MACH LEARN, V0, P0
   GOVINDAN VK, 1990, PATTERN RECOGN, V23, P671, DOI 10.1016/0031-3203(90)90091-X
   Nguyen HD, 2016, IEICE T INF SYST, VE99D, P3110, DOI 10.1587/transinf.2016EDP7102
   He KM, 2015, IEEE I CONF COMP VIS, V0, PP1026, DOI 10.1109/ICCV.2015.123
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Hu L, 2013, PROC INT CONF DOC, V0, PP1180, DOI 10.1109/ICDAR.2013.239
   Hu L, 2011, PROC INT CONF DOC, V0, PP457, DOI 10.1109/ICDAR.2011.98
   Huang BQ, 2007, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS, V0, PP793, DOI 10.1109/ISDA.2007.31
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Julca-Aguilar FD, 2018, 2018 13TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS), V0, PP151, DOI 10.1109/DAS.2018.79
   Kam-Fai Chan, 2000, INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION, V3, P3, DOI 10.1007/PL00013549
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ma BD, 2019, IEEE ACCESS, V7, P59613, DOI 10.1109/ACCESS.2019.2915368
   Mouchere H, 2014, INT CONF FRONT HAND, V0, PP791, DOI 10.1109/ICFHR.2014.138
   Mouchere H, 2011, PROC INT CONF DOC, V0, PP1497, DOI 10.1109/ICDAR.2011.297
   Nazemi A., 2019, ARXIV191007395, V0, P0
   Ramadhan I, 2016, 2016 4TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (ICOICT), V0, P0
   Simonyan K, 2015, ARXIV, V0, P0
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vuong BQ, 2010, EXPERT SYST APPL, V37, P886, DOI 10.1016/j.eswa.2009.05.091
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Yang WX, 2015, PROC INT CONF DOC, V0, PP551, DOI 10.1109/ICDAR.2015.7333822
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhong ZY, 2015, PROC INT CONF DOC, V0, PP846, DOI 10.1109/ICDAR.2015.7333881
NR 40
TC 6
Z9 6
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
EI 
J9 IEEE ACCESS
JI IEEE Access
PD JUN 15
PY 2020
VL 8
IS 
BP 48101
EP 48109
DI 10.1109/ACCESS.2020.2979346
PG 9
WC Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA LB5QD
UT WOS:000524689500008
DA 2023-04-26
ER

PT J
AU Frolov, A
   Bobrov, P
   Biryukova, E
   Isaev, M
   Kerechanin, Y
   Bobrov, D
   Lekin, A
AF Frolov, Alexander
   Bobrov, Pavel
   Biryukova, Elena
   Isaev, Mikhail
   Kerechanin, Yaroslav
   Bobrov, Dmitry
   Lekin, Alexander
TI Using Multiple Decomposition Methods and Cluster Analysis to Find and Categorize Typical Patterns of EEG Activity in Motor Imagery Brain-Computer Interface Experiments
SO FRONTIERS IN ROBOTICS AND AI
LA English
DT Article
DE brain-computer interface; motor imagery; blind source separation; independent component analysis; common spatial patterns; cluster analysis; EEG pattern extraction
ID blind source separation; single-trial eeg; neural-network; lateralization; algorithm; relevant; ica
AB In this study, the sources of EEG activity in motor imagery brain-computer interface (BCI) control experiments were investigated. Sixteen linear decomposition methods for EEG source separation were compared according to different criteria. The criteria were mutual information reduction between the source activities and physiological plausibility. The latter was tested by estimating the dipolarity of the source topographic maps, i.e., the accuracy of approximating the map by potential distribution from a single current dipole, as well as by the specificity of the source activity for different motor imagery tasks. The decomposition methods were also compared according to the number of shared components found. The results indicate that most of the dipolar components are found by the Independent Component Analysis Methods AMICA and PWCICA, which also provided the highest information reduction. These two methods also found the most task-specific EEG patterns of the blind source separation algorithms used. They are outperformed only by non-blind Common Spatial Pattern methods in terms of pattern specificity. The components found by all of the methods were clustered using the Attractor Neural Network with Increasing Activity. The results of the cluster analysis revealed the most frequent patterns of electrical activity occurring in the experiments. The patterns reflect blinking, eye movements, sensorimotor rhythm suppression during the motor imagery, and activations in the precuneus, supplementary motor area, and premotor areas of both hemispheres. Overall, multi-method decomposition with subsequent clustering and task-specificity estimation is a viable and informative procedure for processing the recordings of electrophysiological experiments.
C1 [Frolov, Alexander; Bobrov, Pavel; Biryukova, Elena; Isaev, Mikhail; Kerechanin, Yaroslav; Bobrov, Dmitry; Lekin, Alexander] Pirogov Russian Natl Res Med Univ, Res Inst Translat Med, Moscow, Russia.
   [Frolov, Alexander; Bobrov, Pavel; Biryukova, Elena; Isaev, Mikhail; Kerechanin, Yaroslav] Russian Acad Sci, Inst Higher Nervous Act & Neurophysiol, Moscow, Russia.
C3 Pirogov Russian National Research Medical University; Institute of Higher Nervous Activity & Neurophysiology of RAS; Russian Academy of Sciences
RP Bobrov, P (corresponding author), Pirogov Russian Natl Res Med Univ, Res Inst Translat Med, Moscow, Russia.; Bobrov, P (corresponding author), Russian Acad Sci, Inst Higher Nervous Act & Neurophysiol, Moscow, Russia.
EM p-bobrov@yandex.ru
FU Russian Ministry of Education and Science [RFMEFI60519X0184]
CR Ang KK, 2015, CLIN EEG NEUROSCI, V46, P310, DOI 10.1177/1550059414522229
   Ang KK, 2011, CLIN EEG NEUROSCI, V42, P253, DOI 10.1177/155005941104200411
   Bai ZF, 2020, J NEUROENG REHABIL, V17, P0, DOI 10.1186/s12984-020-00686-2
   Ball K, 2016, COMPUT INTEL NEUROSC, V2016, P0, DOI 10.1155/2016/9754813
   Bashashati A, 2007, J NEURAL ENG, V4, PR32, DOI 10.1088/1741-2560/4/2/R03
   Bashashati H, 2015, PLOS ONE, V10, P0, DOI 10.1371/journal.pone.0129435
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307
   Bingham E, 2000, INT J NEURAL SYST, V10, P1, DOI 10.1142/S0129065700000028
   Bobrov P. D., 2016, HUMAN PHYSIOLOGY, V42, P241, DOI 10.1134/S036211971603004X
   Bobrov P, 2014, ADV INTELL SYST, V303, P183, DOI 10.1007/978-3-319-08156-4_19
   Cichocki A., 2002, ADAPTIVE BLIND SIGNA, V0, P0
   DELFOSSE N, 1995, SIGNAL PROCESS, V45, P59, DOI 10.1016/0165-1684(95)00042-C
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Delorme A, 2012, PLOS ONE, V7, P0, DOI 10.1371/journal.pone.0030135
   Dharmaprani D, 2016, IEEE ENG MED BIO, V0, PP825, DOI 10.1109/EMBC.2016.7590828
   Fonov V, 2011, NEUROIMAGE, V54, P313, DOI 10.1016/j.neuroimage.2010.07.033
   Fonov VS., 2009, NEUROIMAGE, V0, PPS102, DOI 10.1016/S1053-8119(09)70884-5
   Frolov A. A., 2017, HUMAN PHYSIOLOGY, V43, P501, DOI 10.1134/S036211971705005X
   Frolov A, 2012, NEURAL NETW WORLD, V22, P21, DOI 10.14311/NNW.2012.22.002
   Frolov AA, 2007, IEEE T NEURAL NETWOR, V18, P698, DOI 10.1109/TNN.2007.891664
   Frolov AA, 2017, FRONT NEUROSCI-SWITZ, V11, P0, DOI 10.3389/fnins.2017.00400
   Guillot A., 2014, ADV BRAIN NEUROIMAGI, V0, PP433, DOI 10.5772/58270
   Hetu S, 2013, NEUROSCI BIOBEHAV R, V37, P930, DOI 10.1016/j.neubiorev.2013.03.017
   Holler Y, 2013, PLOS ONE, V8, P0, DOI 10.1371/journal.pone.0080479
   Hyvarinen A, 2001, IEEE T NEURAL NETWOR, V12, P1471, DOI 10.1109/72.963782
   Hyvarinen A., 2004, INDEPENDENT COMPONEN, V46, P0
   Kachenoura A, 2008, IEEE SIGNAL PROC MAG, V25, P57, DOI 10.1109/MSP.2008.4408442
   Kobler R., 2019, P 8 GRAZ BRAIN COMP, V0, P100
   Lee F., 2005, P 10 COMP VIS WINT W, V0, P195
   Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719
   Lotte F, 2018, J NEURAL ENG, V15, P0, DOI 10.1088/1741-2552/aab2f2
   Mane R, 2019, IEEE T NEUR SYS REH, V27, P1654, DOI 10.1109/TNSRE.2019.2924742
   Nam CS, 2011, CLIN NEUROPHYSIOL, V122, P567, DOI 10.1016/j.clinph.2010.08.002
   Ono Takashi, 2014, FRONT NEUROENG, V7, P19, DOI 10.3389/fneng.2014.00019
   Palmer JA, 2008, INT CONF ACOUST SPEE, V0, PP1805, DOI 10.1109/ICASSP.2008.4517982
   Palmer J.A., 2012, TECH REP, V0, P0
   Palmer JA, 2006, LECT NOTES COMPUT SC, V3889, P854
   Ramos-Murguialday A, 2013, ANN NEUROL, V74, P100, DOI 10.1002/ana.23879
   Ramoser H, 2000, IEEE T REHABIL ENG, V8, P441, DOI 10.1109/86.895946
   Rizzolatti G, 2014, PHYSIOL REV, V94, P655, DOI 10.1152/physrev.00009.2013
   Sarin Mohit, 2020, 2020 7TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), V0, PP1054, DOI 10.1109/SPIN48934.2020.9071360
   Schomer, 2011, NIEDERMEYERS ELECTRO, V0, P0
   Tadel F, 2011, COMPUT INTEL NEUROSC, V2011, P0, DOI 10.1155/2011/879716
   Vasilyev AN, 2016, ZH VYSSH NERV DEYAT+, V66, P302, DOI 10.7868/S0044467716030126
   VIEIRA S, 2010, IEEE INT CONF FUZZY, V0, PP1, DOI 10.1109/FUZZY.2010.5584447
   Wu XP, 2020, IEEE J BIOMED HEALTH, V24, P775, DOI 10.1109/JBHI.2019.2922976
   Zhou BY, 2016, PLOS ONE, V11, P0, DOI 10.1371/journal.pone.0162657
   Ziehe A, 2004, J MACH LEARN RES, V5, P777
NR 49
TC 3
Z9 3
U1 0
U2 3
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 2296-9144
EI 
J9 FRONT ROBOT AI
JI Front. Robot. AI
PD JUL 30
PY 2020
VL 7
IS 
BP 
EP 
DI 10.3389/frobt.2020.00088
PG 13
WC Robotics
SC Robotics
GA NC9LA
UT WOS:000561530700001
PM 33501255
DA 2023-04-26
ER

PT J
AU Mikheeva, TI
   Mikheev, SV
   Chekina, EV
   Tikhonov, AN
AF Mikheeva, T., I
   Mikheev, S., V
   Chekina, E., V
   Tikhonov, A. N.
TI Digital Transformation of Network-Centric Geo-Visualization Transport Infrastructure
SO PROCEEDINGS OF THE 8TH SCIENTIFIC CONFERENCE ON INFORMATION TECHNOLOGIES FOR INTELLIGENT DECISION MAKING SUPPORT (ITIDS 2020)
LA English
DT Proceedings Paper
DE Intelligent transport geographic information system; "ITSGIS"; network-centric management
AB The article describes the digital transformation of geo-visualization of network-centric management in the environment of the intelligent transport geographic information system ITSGIS with subsequent visualization and deployment of geo-objects of the transport infrastructure on an electronic map. The task of network-centric management of transport objects and processes is considered as a digital transformation with visualization of geo objects of the transport infrastructure. Methods of managing transport objects, processes, and a methodology for conducting a simulation experiment within the framework of a designed decision support system for managing transport infrastructure in local areas of coordinated management are considered. The described intellectual transport geographic information system "ITSGIS" is distinguished by the availability of developed means of supporting the simulation environment, which provide ease of modification and expansion of the range of research tasks based on patterns and neural networks. ITSGIS is based on the digital transformation of geo-visualization of network-centric management, on modern information technology, which combines the possibility of interaction of various geo objects with a database, including transport infrastructure, with data visualization on thematic layers of an interactive geographic electronic map. Certification of the transport network allows you to show in detail the scheme of the settlement, the coverage of roads, the length and status to which this road belongs.
C1 [Mikheeva, T., I; Mikheev, S., V; Chekina, E., V; Tikhonov, A. N.] Samara Natl Res Univ, Samara, Russia.
   [Mikheeva, T., I] Samara State Tech Univ, Samara, Russia.
   [Mikheev, S., V; Tikhonov, A. N.] IntelTrans Grp Co, Samara, Russia.
C3 Samara National Research University; Samara State Technical University
RP Mikheeva, TI (corresponding author), Samara Natl Res Univ, Samara, Russia.; Mikheeva, TI (corresponding author), Samara State Tech Univ, Samara, Russia.
EM Mikheevati@gmail.com; Mikheevati@gmail.com; ev-chekina@yandex.ru; tikhonovAN@gmail.com
CR Betelin V. B., 2013, INFORM TECHNOLOGIES, V3, P125
   Cascetta E, 2009, TRANSPORTATION SYSTE, V0, P185
   Fedoseev AA, 2017, PROCEDIA ENGINEER, V201, P363, DOI 10.1016/j.proeng.2017.09.649
   Golovnin O.K, 2019, INT MULT IND ENG MOD INT MULT IND ENG MOD, V0, P271
   Kudinov A.V, 2000, GEOINFORMATICS 2000, V0, P240
   Kurzhansky A.B., 2010, T MIPT, V2, P100
   Mikheeva T. I., 2017, 5 IEEE INT C MOD TEC 5 IEEE INT C MOD TEC, V0, P258
   Prangishvili I.V, 2003, ENTROPIC OTHER SYSTE, V0, P0
   Rokitsky R.B, 2000, CYBERNETICS SYSTEM A, V6, P27
   Saprykin O.N., 2019, V2416, V0, P95
   Saprykin O.N., 2015, P 1 INT C VEH TECHN P 1 INT C VEH TECHN, V0, P166
NR 11
TC 0
Z9 0
U1 0
U2 1
PU ATLANTIS PRESS
PI PARIS
PA 29 AVENUE LAVMIERE, PARIS, 75019, FRANCE
SN 1951-6851
EI 
J9 ADV INTEL SYS RES
PD JUN 15
PY 2020
VL 174
IS 
BP 200
EP 202
DI 
PG 3
WC Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Operations Research & Management Science; Mathematics, Applied
SC Computer Science; Operations Research & Management Science; Mathematics
GA BR9NH
UT WOS:000678794200038
DA 2023-04-26
ER
