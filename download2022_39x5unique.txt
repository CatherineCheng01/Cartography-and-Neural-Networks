
PT J
AU Cao, R
   Tu, W
   Cai, JX
   Zhao, TH
   Xiao, J
   Cao, JZ
   Gao, QL
   Su, HJ
AF Cao, Rui
   Tu, Wei
   Cai, Jixuan
   Zhao, Tianhong
   Xiao, Jie
   Cao, Jinzhou
   Gao, Qili
   Su, Hanjing
TI MACHINE LEARNING-BASED ECONOMIC DEVELOPMENT MAPPING FROM MULTI-SOURCE OPEN GEOSPATIAL DATA
SO XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION IV
LA English
DT Proceedings Paper
DE Economic Development; Remote Sensing; Geospatial Big Data; Data Fusion; Machine learning
AB Timely and accurate socioeconomic indicators are the prerequisite for smart social governance. For example, the level of economic development and the structure of population are important statistics for regional or national policy-making. However, the collection of these characteristics usually depends on demographic and social surveys, which are time- and labor-intensive. To address these issues, we propose a machine learning-based approach to estimate and map the economic development from multi-source open available geospatial data, including remote sensing imagery and OpenStreetMap road networks. Specifically, we first extract knowledge-based features from different data sources; then the multi-view graphs are constructed through different perspectives of spatial adjacency and feature similarity; and a multi-view graph neural network (MVGNN) model is built on them and trained in a self-supervised learning manner. Then, the handcrafted features and the learned graph representations are combined to estimate the regional economic development indicators via random forest models. Taking China's county-level gross domestic product (GDP) as an example, extensive experiments have been conducted and the results demonstrate the effectiveness of the proposed method, and the combination of the knowledge-based and learning-based features can significantly outperform baseline methods. Our proposed approach can advance the goal of acquiring timely and accurate socioeconomic variables through widely accessible geospatial data, which has the potential to extend to more social indicators and other geographic regions to support smart governance and policy-making in the future.
C1 [Cao, Rui] Hong Kong Polytech Univ, Dept Land Surveying & Geoinformat, Hong Kong, Peoples R China.
   [Cao, Rui] Hong Kong Polytech Univ, Smart Cities Res Inst, Hong Kong, Peoples R China.
   [Tu, Wei; Zhao, Tianhong; Xiao, Jie; Cao, Jinzhou; Gao, Qili] Shenzhen Univ, Guangdong Key Lab Urban Informat, Shenzhen, Peoples R China.
   [Tu, Wei; Zhao, Tianhong; Xiao, Jie; Cao, Jinzhou; Gao, Qili] Shenzhen Univ, Sch Architecture & Urban Planning, Shenzhen, Peoples R China.
   [Cai, Jixuan; Su, Hanjing] Tencent Inc, Shenzhen, Peoples R China.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University; Shenzhen University; Shenzhen University; Tencent
RP Cai, JX (corresponding author), Tencent Inc, Shenzhen, Peoples R China.
EM rucao@polyu.edu.hk; tuwei@szu.edu.cn; codyjxcai@tencent.com; zhaotianhong2016@szu.edu.cn; xiaojie2021@szu.edu.cn; caojz@szu.edu.cn; qlgao@szu.edu.cn; justinsu@tencent.com
FU Tencent WeChat Rhino Bird [JR-WXG-2021131]; National Natural Science Foundation of China [42101472, 42071360, 42001393]; Hong Kong Polytechnic University [BD41]; China Postdoctoral Science Foundation [2021M692163]
CR Abitbol JL, 2020, NAT MACH INTELL, V2, P684, DOI 10.1038/s42256-020-00243-5
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cao JZ, 2021, CITIES, V110, P0, DOI 10.1016/j.cities.2020.103077
   Cao R, 2020, ISPRS J PHOTOGRAMM, V163, P82, DOI 10.1016/j.isprsjprs.2020.02.014
   Cao R, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10101553
   Chen DS, 2022, INT J APPL EARTH OBS, V106, P0, DOI 10.1016/j.jag.2021.102661
   Chen Q, 2021, ENVIRON PLAN B-URBAN, V48, P1876, DOI 10.1177/2399808320951580
   Dong L, 2019, P NATL ACAD SCI USA, V116, P15447, DOI 10.1073/pnas.1903064116
   Elvidge CD, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13050922
   Gebru T, 2017, P NATL ACAD SCI USA, V114, P13108, DOI 10.1073/pnas.1700035114
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031
   Hamilton WL, 2017, ADV NEUR IN, V30, P0
   Huang Z., 2021, J CLEAN PROD, V0, P0
   Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894
   Kipf T. N., 2016, ARXIV, V0, P0
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu Y, 2015, ANN ASSOC AM GEOGR, V105, P512, DOI 10.1080/00045608.2015.1018773
   Lutkepohl H, 2012, EMPIR ECON, V42, P619, DOI 10.1007/s00181-010-0440-1
   Paszke A, 2019, ADV NEUR IN, V32, P0
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pitner G, 2020, INT EL DEVICES MEET, V0, P0, DOI DOI 10.1109/IEDM13553.2020.9371899
   Steele JE, 2017, J R SOC INTERFACE, V14, P0, DOI 10.1098/rsif.2016.0690
   Tomor Z, 2019, J URBAN TECHNOL, V26, P3, DOI 10.1080/10630732.2019.1651178
   [涂伟 Tu Wei], 2020, 武汉大学学报. 信息科学版 GEOMATICS AND INFORMATION SCIENCE OF WUHAN UNIVERSITY, V45, P1875
   Tu W, 2018, J TRANSP GEOGR, V69, P45, DOI 10.1016/j.jtrangeo.2018.04.013
   Velickovic P., 2018, ICLR, V0, P0, DOI DOI 10.17863/CAM.48429
   Wang MJ, 2020, ARXIV, V0, P0
   Yeh C, 2020, NAT COMMUN, V11, P0, DOI 10.1038/s41467-020-16185-w
NR 28
TC 0
Z9 0
U1 1
U2 1
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 2194-9042
EI 2194-9050
J9 ISPRS ANN PHOTO REM
PD JUN 15
PY 2022
VL 5-4
IS 
BP 259
EP 266
DI 10.5194/isprs-annals-V-4-2022-259-2022
PG 8
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BT8SF
UT WOS:000855205000032
DA 2023-04-26
ER

PT J
AU Soto, PJ
   Costa, GAOP
   Ortega, MX
   Bermudez, JD
   Feitosa, RQ
AF Soto, P. J.
   Costa, G. A. O. P.
   Ortega, M. X.
   Bermudez, J. D.
   Feitosa, R. Q.
TI DEFORESTATION DETECTION WITH WEAK SUPERVISED CONVOLUTIONAL NEURAL NETWORKS IN TROPICAL BIOMES
SO XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III
LA English
DT Proceedings Paper
DE Change detection; deep learning; domain adaptation; deforestation; weak supervision
ID change vector analysis; domain adaptation
AB Deep learning methods are known to demand large amounts of labeled samples for training. For remote sensing applications such as change detection, coping with that demand is expensive and time-consuming. This work aims at investigating a noisy-label-based weak supervised method in the context of a deforestation mapping application, characterized by a high class imbalance between the classes of interest, i.e., deforestation and no-deforestation. The study sites correspond to different regions in the Amazon and Brazilian Cerrado biomes. To mitigate the lack of ground-truth labeled training samples, we devised an unsupervised pseudo-labeling scheme based on the Change Vector Analysis technique. The experimental results indicate that the proposed approach can improve the accuracy of deforestation detection applications.
C1 [Soto, P. J.] IFREMER, PDG REM EEP LEP, F-29280 Plouzane, France.
   [Costa, G. A. O. P.] Rio de Janeiro State Univ UERJ, Dept Informat & Comp Sci, Rio De Janeiro, RJ, Brazil.
   [Ortega, M. X.; Bermudez, J. D.; Feitosa, R. Q.] Pontifical Catholic Univ Rio de Janeiro PUC Rio, Dept Elect Engn, Rio De Janeiro, RJ, Brazil.
C3 Ifremer; Universidade do Estado do Rio de Janeiro; Pontificia Universidade Catolica do Rio de Janeiro
RP Soto, PJ (corresponding author), IFREMER, PDG REM EEP LEP, F-29280 Plouzane, France.
EM pjsotove@ifremer.fr; gilson.costa@ime.uerj.br; mortega@aluno.puc-rio.br; bermudez@ele.puc-rio.br; raul@ele.puc-rio.br
CR Adarme MO, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12060910
   Almeida C. A., 2021, METHODOLOGY FOREST M, V0, P0
   Andrade R., 2020, INT ARCH PHOTOGRAMME, VXLIII, P1497
   Daudt RC, 2021, MACH LEARN, V0, P0, DOI DOI 10.1007/s10994-021-06008-4
   Daudt RC, 2018, IEEE IMAGE PROC, V0, PP4063, DOI 10.1109/ICIP.2018.8451652
   Daudt RC, 2018, INT GEOSCI REMOTE SE, V0, P2115
   Deng XQ, 2019, INT GEOSCI REMOTE SE, V0, PP4955, DOI 10.1109/IGARSS.2019.8900277
   IBGE, 2012, MAN TECN VEG BRAS MA, V0, P0
   Khan S. H., 2016, PREPRINT, V0, P0
   Malila W. A., 1980, SIXTH ANNUAL SYMPOSIUM ON MACHINE PROCESSING OF REMOTELY SENSED DATA AND SOIL INFORMATION SYSTEMS AND REMOTE SENSING AND SOIL SURVEY, V0, P326
   Noa J., 2021, ISPRS ANN PHOTOGRAMM, V3, P151
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Parente L, 2021, REMOTE SENS APPL, V21, P0, DOI 10.1016/j.rsase.2020.100444
   Pinheiro Maurano L. E., 2019, CIENCIA FLORESTAL 01, V29, P0
   Rosenfeld R, 2005, SEMISUPERVISED LEARN, V0, P0
   Saha S, 2021, IEEE GEOSCI REMOTE S, V18, P856, DOI 10.1109/LGRS.2020.2990284
   Saha S, 2019, IEEE T GEOSCI REMOTE, V57, P3677, DOI 10.1109/TGRS.2018.2886643
   Soto P.J., 2020, ISPRS ARCH, V0, PP1635, DOI 10.5194/isprs- archives-XLIII-B3-2020-1635-2020
   Vega PJS, 2021, ISPRS J PHOTOGRAMM, V181, P113, DOI 10.1016/j.isprsjprs.2021.08.026
   Thonfeld F, 2016, INT J APPL EARTH OBS, V50, P131, DOI 10.1016/j.jag.2016.03.009
   Torres DL, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13245084
   Tuia D, 2016, IEEE GEOSC REM SEN M, V4, P41, DOI 10.1109/MGRS.2016.2548504
   Weiss Karl, 2016, JOURNAL OF BIG DATA, V3, P0, DOI 10.1186/s40537-016-0043-6
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
NR 24
TC 0
Z9 0
U1 1
U2 2
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2022
VL 43-B3
IS 
BP 713
EP 719
DI 10.5194/isprs-archives-XLIII-B3-2022-713-2022
PG 7
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BT8VW
UT WOS:000855647800099
DA 2023-04-26
ER

PT J
AU Brim, A
   Flann, NS
AF Brim, Andrew
   Flann, Nicholas S.
TI Deep reinforcement learning stock market trading, utilizing a CNN with candlestick images
SO PLOS ONE
LA English
DT Article
ID coefficients
AB Billions of dollars are traded automatically in the stock market every day, including algorithms that use neural networks, but there are still questions regarding how neural networks trade. The black box nature of a neural network gives pause to entrusting it with valuable trading funds. A more recent technique for the study of neural networks, feature map visualizations, yields insight into how a neural network generates an output. Utilizing a Convolutional Neural Network (CNN) with candlestick images as input and feature map visualizations gives a unique opportunity to determine what in the input images is causing the neural network to output a certain action. In this study, a CNN is utilized within a Double Deep Q-Network (DDQN) to outperform the S&P 500 Index returns, and also analyze how the system trades. The DDQN is trained and tested on the 30 largest stocks in the S&P 500. Following training the CNN is used to generate feature map visualizations to determine where the neural network is placing its attention on the candlestick images. Results show that the DDQN is able to yield higher returns than the S&P 500 Index between January 2, 2020 and June 30, 2020. Results also show that the CNN is able to switch its attention from all the candles in a candlestick image to the more recent candles in the image, based on an event such as the coronavirus stock market crash of 2020.
C1 [Brim, Andrew; Flann, Nicholas S.] Utah State Univ, Dept Comp Sci, Logan, UT 84322 USA.
C3 Utah System of Higher Education; Utah State University
RP Brim, A (corresponding author), Utah State Univ, Dept Comp Sci, Logan, UT 84322 USA.
EM andrew.brim@usu.edu
CR [Anonymous], 1998, INTRO REINFORCEMENT, V0, P0
   [Anonymous], 1981, RANDOM WALK WALL STR, V0, P0
   Brim Andrew, 2020, 2020 10 ANN COMPUTIN, V0, P0
   BROCK W, 1992, J FINANC, V47, P1731, DOI 10.2307/2328994
   Brockman Greg, 2016, ARXIV, V0, P0
   Bui VH, 2020, IEEE T SMART GRID, V11, P457, DOI 10.1109/TSG.2019.2924025
   Carta S, 2021, EXPERT SYST APPL, V164, P0, DOI 10.1016/j.eswa.2020.113820
   Donahue J, 2014, PR MACH LEARN RES, V32, P0
   Dosovitskiy Alexey, 2016, P IEEE C COMPUTER VI, V0, P0
   FAMA EF, 1965, J BUS, V38, P34, DOI 10.1086/294743
   FAMA EF, 1970, J FINANC, V25, P383, DOI 10.2307/2325486
   Grn Felix, 2016, ARXIV PREPRINT ARXIV, V0, P0
   GUJARATI D, 1970, AM STAT, V24, P18, DOI 10.2307/2682446
   Han XF, 2019, APPL ENERG, V254, P0, DOI 10.1016/j.apenergy.2019.113708
   Hirschberg J, 2001, APPL ECON LETT, V8, P701, DOI 10.1080/13504850110042187
   Kim T, 2019, PLOS ONE, V14, P0, DOI 10.1371/journal.pone.0212320
   Li K, 2019, IEEE T VEH TECHNOL, V68, P12215, DOI 10.1109/TVT.2019.2945037
   Li YM, 2020, COMPUTING, V102, P1305, DOI 10.1007/s00607-019-00773-w
   Marshall BR, 2006, J BANK FINANC, V30, P2303, DOI 10.1016/j.jbankfin.2005.08.001
   Martin A., 2015, TENSORFLOW LARGE SCA, V0, P0
   Nguyen A., 2019, UNDERSTANDING NEURAL, V0, P55
   Nguyen Anh, 2016, ADV NEURAL INFORM PR, V0, P0
   Okuyama Takafumi, 2018, 2018 INT C INTELLIGE, V0, P0
   Olah C., 2017, DISTILL, V2, P0, DOI 10.23915/DISTILL.00007
   Sasaki Hikaru, 2017, 2017 56 ANN C SOC IN, V0, P0
   Selvin S, 2017, 2017 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, V0, P1643
   Shi Y, 2021, APPL SOFT COMPUT, V107, P0, DOI 10.1016/j.asoc.2021.107320
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K, 2015, ARXIV, V0, P0
   Skouras S, 2001, J ECON DYN CONTROL, V25, P213, DOI 10.1016/S0165-1889(99)00074-3
   Springenberg Jost, 2014, CORR, V0, P0
   Sutton RS, 2018, ADAPT COMPUT MACH LE, V0, P1
   Tsai Y.-H.H., 2019, ARXIV PREPRINT ARXIV, V0, P0
   Van Hasselt H., 2016, 30 AAAI C ARTIFICIAL, V0, P0
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Yang CL, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20010168
   Yu Wei, 2014, ARXIV PREPRINT ARXIV, V0, P0
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang QC, 2019, IEEE T SERV COMPUT, V12, P739, DOI 10.1109/TSC.2018.2867482
   Zhang Yi, 2018, 2018 IEEE INTELLIGEN, V0, P0
NR 40
TC 1
Z9 1
U1 8
U2 24
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
EI 
J9 PLOS ONE
JI PLoS One
PD FEB 18
PY 2022
VL 17
IS 2
BP 
EP 
DI 10.1371/journal.pone.0263181
PG 25
WC Multidisciplinary Sciences
SC Science & Technology - Other Topics
GA 1H3HL
UT WOS:000796435800012
PM 35180250
DA 2023-04-26
ER

PT J
AU Pittman, R
   Hu, B
AF Pittman, R.
   Hu, B.
TI IMPROVING THE BINARY CLASSIFICATION OF PEAT LOCALITIES FROM MULTI-SOURCE REMOTELY-SENSED DATA USING CNN
SO XXIV ISPRS CONGRESS: IMAGING TODAY, FORESEEING TOMORROW, COMMISSION III
LA English
DT Proceedings Paper
DE Digital Soil Mapping; Peat; Neural Network; Convolutional Neural Network; Logistic Regression; Wetlands
ID decomposition; wetlands
AB Neural networks were explored to achieve a binary classification for determining land corresponding to peat for a study area in the boreal forest of northern Ontario, Canada. Environmental covariates were employed as predictors and obtained from multiple sources, which included multispectral imagery, LiDAR, SAR, and aeromagnetic data. A dense neural network (DNN), as well as a convolutional neural network (CNN), were each implemented. Logistic regression, support vector machine (SVM) and random forest (RF) approaches were also modelled. Neighboring pixels surrounding the soil sampling sites were incorporated as input into the CNN, that permitted training on additional information that was not exploited by other methods. Preliminary results indicate that a CNN can attain improved accuracies for peat classification, when compared against other approaches.
C1 [Pittman, R.; Hu, B.] York Univ, Dept Earth & Space Sci & Engn, 4700 Keele St, N York, ON M3J 1P3, Canada.
C3 York University - Canada
RP Hu, B (corresponding author), York Univ, Dept Earth & Space Sci & Engn, 4700 Keele St, N York, ON M3J 1P3, Canada.
EM rpittman@yorku.ca; baoxin@yorku.ca
FU Ontario Ministry of Agriculture, Food and Rural Affairs (OMAFRA); Natural Sciences and Engineering Research Council (NSERC) of Canada
CR Abadi M, 2015, TENSORFLOW LARGE SCA, V0, P0
   Brungard CW, 2015, GEODERMA, V239, P68, DOI 10.1016/j.geoderma.2014.09.019
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Chollet F, 2015, KERAS, V0, P0
   Collobert R., 2011, PROC BIGLEARN NIPS W, V0, P1
   Conrad O, 2015, GEOSCI MODEL DEV, V8, P1991, DOI 10.5194/gmd-8-1991-2015
   ELABIDINE AZ, 1994, CAN J BOT, V72, P1511, DOI 10.1139/b94-186
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031
   Haynes KM, 2021, ECOHYDROLOGY, V14, P0, DOI 10.1002/eco.2273
   Heung B, 2016, GEODERMA, V265, P62, DOI 10.1016/j.geoderma.2015.11.014
   Kuhn M, 2008, J STAT SOFTW, V28, P1, DOI 10.18637/jss.v028.i05
   Lee H, 2019, COMMUN STAT APPL MET, V26, P591, DOI 10.29220/CSAM.2019.26.6.591
   Lucas R, 2010, IEEE J-STARS, V3, P576, DOI 10.1109/JSTARS.2010.2086436
   MALTERER TJ, 1992, SOIL SCI SOC AM J, V56, P1200, DOI 10.2136/sssaj1992.03615995005600040033x
   Minasny B, 2019, EARTH-SCI REV, V196, P0, DOI 10.1016/j.earscirev.2019.05.014
   Paloniemi J., 2018, FRI FIELD GUIDE, V0, P0
   Paszke A, 2019, ADV NEUR IN, V32, P0
   Rapinel S, 2019, J ENVIRON MANAGE, V247, P829, DOI 10.1016/j.jenvman.2019.06.098
   RStudio Team, 2020, RSTUDIO INT DEV R, V0, P0
   SAGA Development Team, 2020, SYSTEM AUTOMATED GEO, V0, P0
   Shimada M, 2014, REMOTE SENS ENVIRON, V155, P13, DOI 10.1016/j.rse.2014.04.014
   Sulla-Menashe D, 2018, ENVIRON RES LETT, V13, P0, DOI 10.1088/1748-9326/aa9b88
   Vitt DH, 2009, ECOSYSTEMS, V12, P360, DOI 10.1007/s10021-009-9228-6
   Whitcomb J, 2009, CAN J REMOTE SENS, V35, P54, DOI 10.5589/m08-080
NR 24
TC 0
Z9 0
U1 0
U2 0
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2022
VL 43-B3
IS 
BP 983
EP 988
DI 10.5194/isprs-archives-XLIII-B3-2022-983-2022
PG 6
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BT8VW
UT WOS:000855647800137
DA 2023-04-26
ER

PT J
AU Galodha, A
   Vashisht, R
   Nidamanuri, RR
   Ramiya, AM
AF Galodha, Abhinav
   Vashisht, Rahul
   Nidamanuri, R. R.
   Ramiya, A. M.
TI DEEP CONVOLUTION NEURAL NETWORKS WITH RESNET ARCHITECTURE FOR SPECTRAL-SPATIAL CLASSIFICATION OF DRONE BORNE AND GROUND BASED HIGH RESOLUTION HYPERSPECTRAL IMAGERY
SO XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II
LA English
DT Proceedings Paper
DE Unmanned Aerial Vehicle (UAV); Precision Agriculture; crop classification; Deep residual networks (ResNet); Hyperspectral image classification (HSI); Convolution Neural Networks (CNN)
AB Drones have been of vital importance in the fields of surveillance, mapping, and infrastructure inspection. Drones have played a vital role in acquiring high-resolution images and with the present need for precision farming, drones have helped in crop classification and monitoring various crop patterns. With the recent advancement in computational power and development of robust algorithms to carry out deep feature learning and neural network, based learning such techniques have regained prominence in contemporary research areas such as classification of common 2-D and 3-D images, object detection, etc. In our research, we propose a deep convolutional neural network architecture (CNN) for the classification of aerial images captured by drones and high-resolution Terrestrial Hyperspectral (THS or HSI) which includes 6-layers and with weights optimized along with the input layer, the convolutional layer, the max-pooling layer, the fully connected layer, softmax probability classifier, and the output layer. We have acquired THS (using Cubert-GmbH data) and drone agricultural data of seasonal crops sowed during the months of March-June for the year 2017. Crop patterns include Cabbage, Eggplant, and Tomato with varying nitrogen concentrations in the region of Bangalore, Southern India. To study the influence and impact of CNN, the ResNets model has been applied. ResNets model and architecture are combined with a deep learning network followed by a recurrent neural learning network model (RCNN). The HSI input layer with corresponding ground truth data for the region is fed into the ResNets model with a spectral and spatial residual network for the 7*7*139 input Hyperspectral Imagery (HSI) volume. The network includes two spectral and two spatial residual blocks. An average pooling layer and a fully connected layer transform into a 5*5*24 spectral-spatial feature volume further to a single output feature vector. At present we use an RMSProp optimizer for error loss minimization which when applied to the drone data was able to achieve an overall accuracy of 97.16%. Similarly, for cabbage, eggplant and tomato acquired through the same method we achieved overall accuracy at 87.619%, 89.25%, and 80.566% respectively in comparison to ground truth labels. Drones and ground-based datasets equipped with good computational techniques have become promising tools for improving the quality and efficiency of precision agriculture today.
C1 [Galodha, Abhinav; Vashisht, Rahul; Nidamanuri, R. R.; Ramiya, A. M.] Indian Inst Space Sci & Technol IIST, ISRO, Dept Earth & Space Sci, Dept Space,DoS, Trivandrum, Kerala, India.
C3 Department of Space (DoS), Government of India; Indian Institute of Space Science & Technology; Indian Space Research Organisation (ISRO)
RP Galodha, A (corresponding author), Indian Inst Space Sci & Technol IIST, ISRO, Dept Earth & Space Sci, Dept Space,DoS, Trivandrum, Kerala, India.
EM abhinavgalohda@gmail.com; rahulvashisht290@gmail.com; rao@iist.ac.in; ramiya@iist.ac.in
CR Acquarelli J., 2017, NETWORKS, V16, P21
   Bruzzone L, 2000, INT J REMOTE SENS, V21, P549, DOI 10.1080/014311600210740
   Chang C.-I., 2003, HYPERSPECTRAL IMAGIN, V0, P0
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Ding HY, 2020, ARAB J GEOSCI, V13, P0, DOI 10.1007/s12517-020-05487-4
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Grahn H., 2007, TECHNIQUES APPL HYPE, V0, P0
   Guo AJX, 2019, IEEE T GEOSCI REMOTE, V57, P1755, DOI 10.1109/TGRS.2018.2869004
   Ioffe S., 2015, ICML, V0, P0
   Jiang JJ, 2020, IEEE T COMPUT IMAG, V6, P1082, DOI 10.1109/TCI.2020.2996075
   Kuo BC, 2011, INT GEOSCI REMOTE SE, V0, PP3903, DOI 10.1109/IGARSS.2011.6050084
   Li Y, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9010067
   Myasnikov E, 2016, LECT NOTES COMPUT SC, V9730, P276, DOI 10.1007/978-3-319-41501-7_31
   Ozdemir A., 2020, J I ELECT COMPUTER, V2, P39, DOI 10.33969/JIEC.2020.21004
   Palsson B, 2018, IEEE ACCESS, V6, P25646, DOI 10.1109/ACCESS.2018.2818280
   Petrakos M, 2001, IEEE T GEOSCI REMOTE, V39, P2539, DOI 10.1109/36.964992
   Prasad S, 2008, IEEE GEOSCI REMOTE S, V5, P625, DOI 10.1109/LGRS.2008.2001282
   Yue J, 2015, REMOTE SENS LETT, V6, P468, DOI 10.1080/2150704X.2015.1047045
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
NR 19
TC 0
Z9 0
U1 3
U2 3
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2022
VL 43-B2
IS 
BP 577
EP 584
DI 10.5194/isprs-archives-XLIII-B2-2022-577-2022
PG 8
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BT8VT
UT WOS:000855635300080
DA 2023-04-26
ER
