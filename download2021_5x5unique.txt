
PT J
AU Lumnitz, S
   Devisscher, T
   Mayaud, JR
   Radic, V
   Coops, NC
   Griess, VC
AF Lumnitz, Stefanie
   Devisscher, Tahia
   Mayaud, Jerome R.
   Radic, Valentina
   Coops, Nicholas C.
   Griess, Verena C.
TI Mapping trees along urban street networks with deep learning and street-level imagery
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Deep learning; Instance segmentation; Monocular depth estimation; Street-level images; Urban forest management
ID view; health; point
AB Planning and managing urban forests for livable cities remains a challenge worldwide owing to sparse information on the spatial distribution, structure and composition of urban trees and forests. National and municipal sources of tree inventory remain limited due to a lack of detailed, consistent and frequent inventory assessments. Despite advancements in research on the automation of urban tree mapping using Light Detection and Ranging (LiDAR) or high-resolution satellite imagery, in practice most municipalities still perform labor-intensive field surveys to collect and update tree inventories. We present a robust, affordable and rapid method for creating tree inventories in any urban region where sufficient street-level imagery is readily available. Our approach is novel in that we use a Mask Regional Convolutional Neural Network (Mask R-CNN) to detect and locate separate tree instances from street-level imagery, thereby successfully creating shape masks around unique fuzzy urban objects like trees. The novelty of this method is enhanced by using monocular depth estimation and triangulation to estimate precise tree location, relying only on photographs and images taken from the street. Experiments across four cities show that our method is transferable to different image sources (Google Street View, Mapillary) and urban ecosystems. We successfully detect >70% of all public and private trees recorded in a ground-truth campaign across Metro Vancouver. The accuracy of geolocation is also promising. We automatically locate public and private trees with a mean error in the absolute position ranging from 4 to 6 m, which is comparable to ground-truth measurements in conventional manual urban tree inventory campaigns.
C1 [Lumnitz, Stefanie; Devisscher, Tahia; Coops, Nicholas C.] Univ British Columbia, Dept Forest Resources Management, Vancouver, BC, Canada.
   [Lumnitz, Stefanie] European Space Agcy, ESRIN, Frascati, Italy.
   [Mayaud, Jerome R.] Spare Labs Inc, Vancouver, BC, Canada.
   [Radic, Valentina] Univ British Columbia, Dept Earth Ocean Atmospher Sci, Vancouver, BC, Canada.
   [Griess, Verena C.] ETH, Dept Environm Syst Sci, Inst Terr Ecosyst, Zurich, Switzerland.
C3 University of British Columbia; European Space Agency; University of British Columbia; Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Lumnitz, S (corresponding author), Univ British Columbia, Dept Forest Resources Management, Vancouver, BC, Canada.
EM Stefanie.Lumnitz@gmail.com
FU University of British Columbia; Genome Canada; Genome British Columbia; Genome Quebec under the research project Biosurveillance of Alien Forest Enemies (bioSAFE) as part of the 2015 Large-Scale Applied Research Project competition in Natural Resources and the Environment: Sector Challenges - Genomic Solutions [10106]; Banting Postdoctoral Fellowship program [201709BPF-393653-294704]; Social Sciences and Humanities Research Council (SSHRC) of Canada
CR Abdulla W., 2017, GITHUB REPOSITORY, V0, P0
   Agarwal S, 2010, COMPUTER, V43, P40, DOI 10.1109/MC.2010.175
   Alberti M, 2003, BIOSCIENCE, V53, P1169, DOI 10.1641/0006-3568(2003)053[1169:IHIEOA]2.0.CO;2
   Alberti M., 2008, ADV URBAN ECOLOGY IN, V0, P0
   Alonzo M, 2014, REMOTE SENS ENVIRON, V148, P70, DOI 10.1016/j.rse.2014.03.018
   [Anonymous], 2017, P IEEE C COMP VIS PA, V0, P0, DOI DOI 10.1109/CVPR.2017.106
   Aval J, 2018, ISPRS J PHOTOGRAMM, V146, P197, DOI 10.1016/j.isprsjprs.2018.09.016
   Berland A, 2017, URBAN FOR URBAN GREE, V21, P11, DOI 10.1016/j.ufug.2016.11.006
   Bolei Z., 2017, COCO PLACES 2017 CHA, V0, P0
   Branson S, 2018, ISPRS J PHOTOGRAMM, V135, P13, DOI 10.1016/j.isprsjprs.2017.11.008
   Caesar H., 2016, ARXIV161203716, V0, P0
   Cai B.Y., 2018, TREEPEDIA 2 0 APPL D, V0, P0
   Cheng L, 2018, ISPRS J PHOTOGRAMM, V141, P72, DOI 10.1016/j.isprsjprs.2018.04.006
   Chollet F., 2017, DEEP LEARNING PYTHON, V0, P0
   Cordts M, 2016, PROC CVPR IEEE, V0, PP3213, DOI 10.1109/CVPR.2016.350
   Davis J, 2006, PROC 23 INT C MACH L, V0, PP233, DOI 10.1145/1143844.1143874
   Duarte F., 2017, HUM DIG REAL DES MOD, V0, PP59, DOI 10.1007/978-981-10-6611-5_6
   Falco G, 2017, SENSORS-BASEL, V17, P0, DOI 10.3390/s17020255
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C., 2016, ARXIV160903677, V0, P0
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, V0, P1
   He KM, 2017, IEEE I CONF COMP VIS, V0, PP2980, DOI 10.1109/TPAMI.2018.2844175
   Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Jang KM, 2020, ISPRS INT J GEO-INF, V9, P0, DOI 10.3390/ijgi9100586
   Kang YH, 2020, ANN GIS, V26, P261, DOI 10.1080/19475683.2020.1791954
   Ke YH, 2011, INT J REMOTE SENS, V32, P4725, DOI 10.1080/01431161.2010.494184
   Kelly M, 2007, COMPUT ENVIRON URBAN, V31, P689, DOI 10.1016/j.compenvurbsys.2006.10.002
   Kisantal M., 2019, ARXIV190207296, V0, P0
   Krylov VA, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10050661
   Laumer D, 2020, ISPRS J PHOTOGRAMM, V162, P125, DOI 10.1016/j.isprsjprs.2020.02.001
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lefevre S, 2017, P IEEE, V105, P1884, DOI 10.1109/JPROC.2017.2684300
   Li SN, 2016, ISPRS J PHOTOGRAMM, V115, P119, DOI 10.1016/j.isprsjprs.2015.10.012
   Li XJ, 2017, LECT NOTES GEOINF CA, V0, PP341, DOI 10.1007/978-3-319-57336-6_24
   Li XJ, 2018, LANDSCAPE URBAN PLAN, V169, P81, DOI 10.1016/j.landurbplan.2017.08.011
   Li XJ, 2015, URBAN FOR URBAN GREE, V14, P675, DOI 10.1016/j.ufug.2015.06.006
   Li X, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11101144
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015
   Michels J., 2005, P 22 INT C MACHINE L, V0, PP593, DOI 10.1145/1102351.1102426
   Nielsen P, 2014, LECT N PROD ENG, V0, PP17, DOI 10.1007/978-3-319-04271-8_2
   Nitoslawski SA, 2016, FORESTS, V7, P0, DOI 10.3390/f7060119
   Nowak DJ, 2014, ENVIRON POLLUT, V193, P119, DOI 10.1016/j.envpol.2014.05.028
   Padayachee AL, 2017, BIOL INVASIONS, V19, P3557, DOI 10.1007/s10530-017-1596-9
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Plowright AA, 2017, REMOTE SENS ENVIRON, V194, P391, DOI 10.1016/j.rse.2017.03.045
   Plowright AA, 2016, URBAN FOR URBAN GREE, V19, P140, DOI 10.1016/j.ufug.2016.06.026
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Seiferling I, 2017, LANDSCAPE URBAN PLAN, V165, P93, DOI 10.1016/j.landurbplan.2017.05.010
   Small C, 2001, INT J REMOTE SENS, V22, P1305, DOI 10.1080/01431160151144369
   Steele F., 2016, TECH REP, V0, P0
   Stewart I., 2010, URBAN CLIM, V0, P7
   Stubbings P, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11121395
   Tippetts B, 2016, J REAL-TIME IMAGE PR, V11, P5, DOI 10.1007/s11554-012-0313-2
   van den Bosch M, 2017, ENVIRON RES, V158, P373, DOI 10.1016/j.envres.2017.05.040
   Wegner JD, 2016, PROC CVPR IEEE, V0, PP6014, DOI 10.1109/CVPR.2016.647
   Yin DM, 2016, INT J REMOTE SENS, V37, P4521, DOI 10.1080/01431161.2016.1214302
   Zhang X, 2018, ISPRS J PHOTOGRAMM, V140, P77, DOI 10.1016/j.isprsjprs.2017.07.009
   Zhen Z, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8040333
NR 59
TC 31
Z9 31
U1 25
U2 68
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD MAY 15
PY 2021
VL 175
IS 
BP 144
EP 157
DI 10.1016/j.isprsjprs.2021.01.016
EA MAR 2021
PG 14
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA RT8HA
UT WOS:000644695700011
DA 2023-04-26
ER

PT J
AU Liu, YF
   Zhu, QG
   Cao, F
   Chen, JK
   Lu, G
AF Liu, Yifan
   Zhu, Qigang
   Cao, Feng
   Chen, Junke
   Lu, Gang
TI High-Resolution Remote Sensing Image Segmentation Framework Based on Attention Mechanism and Adaptive Weighting
SO ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION
LA English
DT Article
DE multi-scale convolutional; computer vision; semantic segmentation; remote sensing; neural network; ISPRS Vaihingen
ID extraction; network
AB Semantic segmentation has been widely used in the basic task of extracting information from images. Despite this progress, there are still two challenges: (1) it is difficult for a single-size receptive field to acquire sufficiently strong representational features, and (2) the traditional encoder-decoder structure directly integrates the shallow features with the deep features. However, due to the small number of network layers that shallow features pass through, the feature representation ability is weak, and noise information will be introduced to affect the segmentation performance. In this paper, an Adaptive Multi-Scale Module (AMSM) and Adaptive Fuse Module (AFM) are proposed to solve these two problems. AMSM adopts the idea of channel and spatial attention and adaptively fuses three-channel branches by setting branching structures with different void rates, and flexibly generates weights according to the content of the image. AFM uses deep feature maps to filter shallow feature maps and obtains the weight of deep and shallow feature maps to filter noise information in shallow feature maps effectively. Based on these two symmetrical modules, we have carried out extensive experiments. On the ISPRS Vaihingen dataset, the F1-score and Overall Accuracy (OA) reached 86.79% and 88.35%, respectively.
C1 [Liu, Yifan; Zhu, Qigang; Lu, Gang] Shandong Univ Sci & Technol, Dept Elect Engn & Informat Technol, Jinan 250031, Peoples R China.
   [Cao, Feng] Fujian Anta Logist Informat Technol Co Ltd, Quanzhou 362200, Peoples R China.
   [Chen, Junke] Shandong Univ Sci & Technol, Dept Finance & Econ, Jinan 250031, Peoples R China.
C3 Shandong University of Science & Technology; Shandong University of Science & Technology
RP Zhu, QG (corresponding author), Shandong Univ Sci & Technol, Dept Elect Engn & Informat Technol, Jinan 250031, Peoples R China.
EM 201803204418@sdust.edu.cn; skd992356@sdust.edu.cn; yingyu-liuyifan@yqsx.onexmail.com; 201903104102@sdust.edu.cn; 201903204415@sdust.edu.cn
FU Provincial and School-level Educational Reform Project Communication Engineering CDIO Talent Training [310301815]; Excellent Teaching Team Project [310302202]
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bilinski P, 2018, PROC CVPR IEEE, V0, PP6596, DOI 10.1109/CVPR.2018.00690
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, V0, PP1, DOI 10.1109/NANOARCH.2017.8053709
   Cheng BW, 2019, IEEE I CONF COMP VIS, V0, PP5217, DOI 10.1109/ICCV.2019.00532
   Cheng JY, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), V0, PP589, DOI 10.1109/ICAR.2017.8023671
   Dai JF, 2017, IEEE I CONF COMP VIS, V0, PP764, DOI 10.1109/ICCV.2017.89
   Guo H, 2019, PROC CVPR IEEE, V0, PP729, DOI 10.1109/CVPR.2019.00082
   He YX, 2023, TRANSPORTMETRICA A, V19, P0, DOI 10.1080/23249935.2022.2033348
   Kan K, 2021, RENEW ENERG, V168, P960, DOI 10.1016/j.renene.2020.12.103
   Ke T.W., 2018, P EUR C COMP VIS ECC, V0, P0
   Kumar BGV, 2016, PROC CVPR IEEE, V0, PP5385, DOI 10.1109/CVPR.2016.581
   Li B, 2019, IEEE I CONF COMP VIS, V0, PP8518, DOI 10.1109/ICCV.2019.00861
   Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37
   Lin GS, 2017, PROC CVPR IEEE, V0, PP5168, DOI 10.1109/CVPR.2017.549
   Liu SK, 2019, PROC CVPR IEEE, V0, PP1871, DOI 10.1109/CVPR.2019.00197
   Liu W, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11242912
   Liu YF, 2021, BASIC CLIN PHARMACOL, V128, P183
   Liu ZW, 2015, IEEE I CONF COMP VIS, V0, PP1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lu XK, 2019, PROC CVPR IEEE, V0, PP3618, DOI 10.1109/CVPR.2019.00374
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P7092, DOI 10.1109/TGRS.2017.2740362
   Matikainen L, 2011, REMOTE SENS-BASEL, V3, P1777, DOI 10.3390/rs3081777
   Mnih V., 2014, ARXIV, V0, P1406.6247
   Nassar AS, 2020, ISPRS INT J GEO-INF, V9, P0, DOI 10.3390/ijgi9110687
   Pan XG, 2018, AAAI CONF ARTIF INTE, V0, P7276
   Pan XR, 2018, SENSORS-BASEL, V18, P0, DOI 10.3390/s18113774
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruan T, 2019, AAAI CONF ARTIF INTE, V0, P4814
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Shi Y, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11222719
   Song A, 2020, ISPRS INT J GEO-INF, V9, P0, DOI 10.3390/ijgi9100601
   Volpi M, 2017, IEEE T GEOSCI REMOTE, V55, P881, DOI 10.1109/TGRS.2016.2616585
   Wen DW, 2017, IEEE J-STARS, V10, P1413, DOI 10.1109/JSTARS.2016.2645798
   Woo J., 2018, P EUR C COMP VIS ECC, V0, PP3, DOI 10.1007/978-3-030-01234-2_1
   Woo S, 2018, ADV NEUR IN, V31, P0
   Xu SB, 2018, IEEE T GEOSCI REMOTE, V56, P7369, DOI 10.1109/TGRS.2018.2850972
   Yang F, 2019, IEEE I CONF COMP VIS, V0, PP8310, DOI 10.1109/ICCV.2019.00840
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, V0, PP1857, DOI 10.1109/CVPR.2018.00199
   Zhang R, 2017, IEEE I CONF COMP VIS, V0, PP2050, DOI 10.1109/ICCV.2017.224
   Zhao HS, 2017, PROC CVPR IEEE, V0, PP6230, DOI 10.1109/CVPR.2017.660
   Zheng HL, 2019, PROC CVPR IEEE, V0, PP5007, DOI 10.1109/CVPR.2019.00515
   Zhou JC, 2019, IEEE PHOTONICS J, V11, P0, DOI 10.1109/JPHOT.2019.2950949
   Zhou K, 2021, ISPRS INT J GEO-INF, V10, P0, DOI 10.3390/ijgi10010039
NR 46
TC 12
Z9 14
U1 2
U2 30
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2220-9964
J9 ISPRS INT J GEO-INF
JI ISPRS Int. J. Geo-Inf.
PD APR 15
PY 2021
VL 10
IS 4
BP 
EP 
DI 10.3390/ijgi10040241
PG 18
WC Computer Science, Information Systems; Geography, Physical; Remote Sensing
SC Computer Science; Physical Geography; Remote Sensing
GA RR4MU
UT WOS:000643075200001
DA 2023-04-26
ER

PT J
AU Zeng, W
   Lin, CQ
   Lin, JC
   Jiang, JC
   Xia, JZ
   Turkay, C
   Chen, W
AF Zeng, Wei
   Lin, Chengqiao
   Lin, Juncong
   Jiang, Jincheng
   Xia, Jiazhi
   Turkay, Cagatay
   Chen, Wei
TI Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE MAUP; traffic prediction; deep learning; model diagnostic; visual analytics
ID aggregation; scales
AB Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.
C1 [Zeng, Wei; Jiang, Jincheng] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China.
   [Lin, Chengqiao; Lin, Juncong] Xiamen Univ, Xiamen, Peoples R China.
   [Xia, Jiazhi] Cent South Univ, Changsha, Peoples R China.
   [Turkay, Cagatay] Univ Warwick, Coventry, W Midlands, England.
   [Chen, Wei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Xiamen University; Central South University; University of Warwick; Zhejiang University
RP Lin, JC (corresponding author), Xiamen Univ, Xiamen, Peoples R China.
EM wei.zeng@siat.ac.cn; linchengqiao@xmu.edu.cn; jclin@xmu.edu.cn; jc.jiang@siat.ac.cn; xiajiazhi@csu.edu.cn; cagatay.turkay@warwick.ac.uk; chenwei@cad.zju.edu.cn
FU National Natural Science Foundation of China [61772456, U1609217, 61802388, 61702433, 61872389, 41701452]
CR Andrienko N, 2011, IEEE T VIS COMPUT GR, V17, P205, DOI 10.1109/TVCG.2010.44
   [Anonymous], 1954, INCORPORATED STAT, V0, P0
   ANSELIN L, 1995, GEOGR ANAL, V27, P93, DOI 10.1111/j.1538-4632.1995.tb00338.x
   Brunsdon C, 1996, GEOGR ANAL, V28, P281, DOI 10.1111/j.1538-4632.1996.tb00936.x
   Cao K, 2021, IEEE T IND INFORM, V17, P494, DOI 10.1109/TII.2020.2975897
   Correll M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), V0, P0, DOI DOI 10.1145/3173574.3174216
   Deng ZK, 2020, IEEE T VIS COMPUT GR, V26, P800, DOI 10.1109/TVCG.2019.2934670
   Duque JC, 2018, PLOS ONE, V13, P0, DOI 10.1371/journal.pone.0207377
   Gehlke CE, 1934, J AM STAT ASSOC, V29, P169, DOI 10.2307/2277827
   Goodwin S, 2016, IEEE T VIS COMPUT GR, V22, P599, DOI 10.1109/TVCG.2015.2467199
   Guo DS, 2009, IEEE T VIS COMPUT GR, V15, P1041, DOI 10.1109/TVCG.2009.143
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   Hohman F, 2019, IEEE T VIS COMPUT GR, V25, P2674, DOI 10.1109/TVCG.2018.2843369
   Huang Z., 2019, IEEE TVCG, V26, P2576
   Kwon BC, 2019, IEEE T VIS COMPUT GR, V25, P299, DOI 10.1109/TVCG.2018.2865027
   Lapuschkin S, 2019, NAT COMMUN, V10, P0, DOI 10.1038/s41467-019-08987-4
   Liu MC, 2017, IEEE T VIS COMPUT GR, V23, P91, DOI 10.1109/TVCG.2016.2598831
   Liu SX, 2017, VIS INFORM, V1, P48, DOI 10.1016/j.visinf.2017.01.006
   Lundberg SM, 2017, ADV NEUR IN, V30, P0
   Ming Y, 2017, IEEE CONF VIS ANAL, V0, PP13, DOI 10.1109/VAST.2017.8585721
   Moeckel R, 2015, ENVIRON PLANN B, V42, P888, DOI 10.1068/b130199p
   Moorthy C. K., 1988, TRANSPORT PLAN TECHN, V12, P45, DOI 10.1080/03081068808717359
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, V0, PP86, DOI 10.1109/CVPR.2017.17
   MORAN PAP, 1950, BIOMETRIKA, V37, P17, DOI 10.1093/biomet/37.1-2.17
   Nelson JK, 2017, CARTOGR GEOGR INF SC, V44, P35, DOI 10.1080/15230406.2015.1093431
   Openshaw S., 1984, CONCEPTS TECHNIQUES, V38, P41
   Park D, 2018, IEEE T VIS COMPUT GR, V24, P3032, DOI 10.1109/TVCG.2017.2785807
   Pena-Araya V, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI20), V0, P0, DOI DOI 10.1145/3313831.3376350
   Pezzotti N, 2018, IEEE T VIS COMPUT GR, V24, P98, DOI 10.1109/TVCG.2017.2744358
   Rodrigues N, 2018, IEEE T VIS COMPUT GR, V24, P616, DOI 10.1109/TVCG.2017.2744018
   Sedlmair M, 2014, IEEE T VIS COMPUT GR, V20, P2161, DOI 10.1109/TVCG.2014.2346321
   Shen QM, 2020, IEEE PAC VIS SYMP, V0, PP61, DOI 10.1109/PacificVis48177.2020.2785
   Shen QM, 2018, IEEE T VIS COMPUT GR, V24, P1004, DOI 10.1109/TVCG.2017.2744159
   Strobelt H, 2018, IEEE T VIS COMPUT GR, V24, P667, DOI 10.1109/TVCG.2017.2744158
   Turkay C, 2014, IEEE T VIS COMPUT GR, V20, P2033, DOI 10.1109/TVCG.2014.2346265
   Vaswani A, 2017, ADV NEUR IN, V30, P0
   Wang DD, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, V0, P0, DOI DOI 10.1145/3290605.3300831
   Wang P, 2012, SCI REP-UK, V2, P0, DOI 10.1038/srep01001
   Wang YZ, 2020, COMPUT GRAPH FORUM, V39, P405, DOI 10.1111/cgf.13882
   Weng D, 2019, IEEE T VIS COMPUT GR, V25, P459, DOI 10.1109/TVCG.2018.2865126
   Wexler J, 2020, IEEE T VIS COMPUT GR, V26, P56, DOI 10.1109/TVCG.2019.2934619
   Wilkinson L, 1999, AM STAT, V53, P276, DOI 10.2307/2686111
   Williams BM, 1998, TRANSPORT RES REC, V0, PP132, DOI 10.3141/1644-14
   Wu WC, 2017, IEEE PAC VIS SYMP, V0, PP91, DOI 10.1109/PACIFICVIS.2017.8031583
   Xu YY, 2014, IEEE T INTELL TRANSP, V15, P2457, DOI 10.1109/TITS.2014.2315794
   Yao HX, 2018, AAAI CONF ARTIF INTE, V0, P2588
   Yuan J, 2021, COMPUT VIS MEDIA, V7, P3, DOI 10.1007/s41095-020-0191-7
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng W, 2013, COMPUT GRAPH FORUM, V32, P271, DOI 10.1111/cgf.12114
   Zhang JW, 2016, COMPUT GRAPH FORUM, V35, P441, DOI 10.1111/cgf.12920
   Zhang JB, 2017, AAAI CONF ARTIF INTE, V0, P1655
   Zhang Y, 2016, COMPUT GRAPH FORUM, V35, P101, DOI 10.1111/cgf.12886
   Zhang YF, 2017, IEEE T VIS COMPUT GR, V23, P371, DOI 10.1109/TVCG.2016.2598541
   Zheng S, 2016, PROC CVPR IEEE, V0, PP4480, DOI 10.1109/CVPR.2016.485
NR 54
TC 15
Z9 16
U1 6
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 15
PY 2021
VL 27
IS 2
BP 839
EP 848
DI 10.1109/TVCG.2020.3030410
PG 10
WC Computer Science, Software Engineering
SC Computer Science
GA WF5FO
UT WOS:000706330100069
PM 33074818
DA 2023-04-26
ER

PT J
AU Liu, SY
   Cheng, J
   Liang, LK
   Bai, HW
   Dang, WL
AF Liu, Siyu
   Cheng, Jian
   Liang, Leikun
   Bai, Haiwei
   Dang, Wanli
TI Light-Weight Semantic Segmentation Network for UAV Remote Sensing Images
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Remote sensing; Semantics; Image segmentation; Task analysis; Unmanned aerial vehicles; Feature extraction; Convolution; Attention mechanism; light-weight network; remote sensing; semantic segmentation; unmanned aerial vehicle images
AB Semantic segmentation for unmanned aerial vehicle (UAV) remote sensing images has become one of the research focuses in the field of remote sensing at present, which could accurately analyze the ground objects and their relationships. However, conventional semantic segmentation methods based on deep learning require large-scale models that are not suitable for resource-constrained UAV remote sensing tasks. Therefore, it is important to construct a light-weight semantic segmentation method for UAV remote sensing images. With this motivation, we propose a light-weight neural network model with fewer parameters to solve the problem of semantic segmentation of UAV remote sensing images. The network adopts an encoder-decoder architecture. In the encoder, we build a light-weight convolutional neural network model with fewer channels of each layer to reduce the number of model parameters. Then, feature maps of different scales from the encoder are concatenated together after resizing to carry out the multiscale fusion. Moreover, we employ two attention modules to capture the global semantic information from the context and the correlation among channels in UAV remote sensing images. In the decoder part, the model obtains predictions of each pixel through the softmax function. We conducted experiments on the ISPRS Vaihingen dataset, UAVid dataset, and UDD6 dataset to verify the effectiveness of the light-weight network. Our method obtains quality semantic segmentation results evaluated on UAV remote sensing datasets with only 9 M parameters the model owns, which is competitive among popular methods with the same level of parameters.
C1 [Liu, Siyu; Cheng, Jian; Liang, Leikun; Bai, Haiwei; Dang, Wanli] Univ Elect Sci & Technol China, Chengdu 611731, Peoples R China.
   [Dang, Wanli] Second Res Inst Civil Aviat Adm China, Chengdu 610041, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Cheng, J (corresponding author), Univ Elect Sci & Technol China, Chengdu 611731, Peoples R China.
EM syliu@std.uestc.edu.cn; chengjian@uestc.edu.cn; liang_leikun@163.com; hwbaymax@std.uestc.edu.cn; dangwanli@caacsri.com
FU National Natural Science Foundation of China [62071104]; Sichuan Science and Technology Program [2020YFG0085, 2021YFG0328]; Intelligent TerminalKey Laboratory of Sichuan [SCITLAB-0017]
CR [Anonymous], 2017, PREPRINTS, V0, P0
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y, 2018, LECT NOTES COMPUT SC, V11256, P347, DOI 10.1007/978-3-030-03398-9_30
   Chowdhury T, 2020, IEEE INT CONF BIG DA, V0, PP3904, DOI 10.1109/BigData50022.2020.9377916
   Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23
   Erdelj M., 2016, 2016 INT C COMP NETW, V0, PP1, DOI 10.1109/ICCNC.2016.7440563
   Fu J, 2019, PROC CVPR IEEE, V0, PP3141, DOI 10.1109/CVPR.2019.00326
   Galarreta JF, 2015, NAT HAZARD EARTH SYS, V15, P1087, DOI 10.5194/nhess-15-1087-2015
   Hashemi-Beni L, 2021, IEEE J-STARS, V14, P2127, DOI 10.1109/JSTARS.2021.3051873
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   Howard Andrew G., 2017, PROC IEEE C COMPUT V, V0, P0
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Huang HY, 2018, IEEE J-STARS, V11, P2253, DOI 10.1109/JSTARS.2018.2830410
   Huang ZL, 2019, IEEE I CONF COMP VIS, V0, PP603, DOI 10.1109/ICCV.2019.00069
   Joseph R, 2018, ARXIV PREPRINT ARXIV, V0, P0
   Ke RM, 2019, IEEE T INTELL TRANSP, V20, P54, DOI 10.1109/TITS.2018.2797697
   Kerle N, 2020, ISPRS INT J GEO-INF, V9, P0, DOI 10.3390/ijgi9010014
   Khan MA, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10030458
   Li Hanchao, 2019, PROC CVPR IEEE, V0, PP9522, DOI 10.1109/CVPR.2019.00975
   Liu MJ, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20082238
   Liu SY, 2020, INT GEOSCI REMOTE SE, V0, PP2595, DOI 10.1109/IGARSS39084.2020.9324723
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lyu Y, 2020, ISPRS J PHOTOGRAMM, V165, P108, DOI 10.1016/j.isprsjprs.2020.05.009
   Marcos D, 2018, ISPRS J PHOTOGRAMM, V145, P96, DOI 10.1016/j.isprsjprs.2018.01.021
   Mou LC, 2019, PROC CVPR IEEE, V0, PP12408, DOI 10.1109/CVPR.2019.01270
   Nex F, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11030287
   Pan XG, 2018, AAAI CONF ARTIF INTE, V0, P7276
   Paszke A., 2016, ABS160602147 CORR, V0, P0
   Peng C, 2017, PROC CVPR IEEE, V0, PP1743, DOI 10.1109/CVPR.2017.189
   Quang N. T., 2015, P 6 INT S INF COMM T, V0, PP282, DOI 10.1145/2833258.2833272
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O., 2015, INT C MED IM COMP CO, V0, PP234, DOI 10.1007/978-3-319-24574-4_28
   Tan MX, 2019, PR MACH LEARN RES, V97, P0
   Teng SZ, 2021, INT J COMPUT VISION, V129, P719, DOI 10.1007/s11263-020-01402-2
   Tijtgat N, 2017, IEEE INT CONF COMP V, V0, PP2110, DOI 10.1109/ICCVW.2017.247
   Vaswani A, 2017, ADV NEUR IN, V30, P0
   Wang XL, 2018, PROC CVPR IEEE, V0, PP7794, DOI 10.1109/CVPR.2018.00813
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu Changqian, 2020, ARXIV200402147, V2, P8
   Yu F., 2015, 1511 ARXIV, V0, P0
   Zhang X, 2018, PROC CVPR IEEE, V0, PP6848, DOI 10.1109/CVPR.2018.00716
   Zhao HS, 2017, PROC CVPR IEEE, V0, PP6230, DOI 10.1109/CVPR.2017.660
   Zhu JS, 2018, IEEE J-STARS, V11, P4968, DOI 10.1109/JSTARS.2018.2879368
   Zhu XZ, 2019, IEEE I CONF COMP VIS, V0, PP6687, DOI 10.1109/ICCV.2019.00679
NR 46
TC 13
Z9 13
U1 24
U2 87
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2021
VL 14
IS 
BP 8287
EP 8296
DI 10.1109/JSTARS.2021.3104382
PG 10
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA UK8JJ
UT WOS:000692210900015
DA 2023-04-26
ER

PT J
AU Chalmers, C
   Fergus, P
   Montanez, CAC
   Longmore, SN
   Wich, SA
AF Chalmers, C.
   Fergus, P.
   Montanez, C. Aday Curbelo
   Longmore, Steven N.
   Wich, Serge A.
TI Video analysis for the detection of animals using convolutional neural networks and consumer-grade drones
SO JOURNAL OF UNMANNED VEHICLE SYSTEMS
LA English
DT Article
DE conservation; deep learning; convolutional neural networks; inferencing; drone technology
ID wildlife research; deep; cnn
AB Determining animal distribution and density is important in conservation. The process is both time-consuming and labour-intensive. Drones have been used to help mitigate human-intensive tasks by covering large geographical areas over a much shorter timescale. In this paper we investigate this idea further using a proof of concept to detect rhinos and cars from drone footage. The proof of concept utilises off-the-shelf technology and consumer-grade drone hardware. The study demonstrates the feasibility of using machine learning (ML) to automate routine conservation tasks, such as animal detection and tracking. The prototype has been developed using a DJI Mavic Pro 2 and tested over a global system for mobile communications (GSM) network. The Faster-RCNN Resnet 101 architecture is used for transfer learning. Inference is performed with a frame sampling technique to address the required trade-off between precision, processing speed, and live video feed synchronisation. Inference models are hosted on a web platform and video streams from the drone (using OcuSync) are transmitted to a real-time messaging protocol (RTMP) server for subsequent classification. During training, the best model achieves a mean average precision (mAP) of 0.83 intersection over union (@IOU) 0.50 and 0.69 @IOU 0.75, respectively. On testing the system in Knowsley Safari our prototype was able to achieve the following: sensitivity (Sen), 0.91 (0.869, 0.94); specificity (Spec), 0.78 (0.74, 0.82); and an accuracy (ACC), 0.84 (0.81, 0.87) when detecting rhinos, and Sen, 1.00 (1.00, 1.00); Spec, 1.00 (1.00, 1.00); and an ACC, 1.00 (1.00, 1.00) when detecting cars.
C1 [Chalmers, C.; Fergus, P.; Montanez, C. Aday Curbelo] Liverpool John Moores Univ, Sch Comp Sci, Liverpool L2 2QP, Merseyside, England.
   [Longmore, Steven N.] Liverpool John Moores Univ, Astrophys Res Inst, Liverpool L3 5RF, Merseyside, England.
   [Wich, Serge A.] Liverpool John Moores Univ, Sch Biol & Environm Sci, Liverpool L2 2QP, Merseyside, England.
C3 Liverpool John Moores University; Liverpool John Moores University; Liverpool John Moores University
RP Wich, SA (corresponding author), Liverpool John Moores Univ, Sch Biol & Environm Sci, Liverpool L2 2QP, Merseyside, England.
EM s.a.wich@ljmu.ac.uk
FU Research Council UK (RCUK) Science and Technology Facilities Council (STFC) [ST/R002673/1]
CR Agapito L., 1900, V8925, V0, P0
   [Anonymous], 2004, ADV DISTANCE SAMPLIN, V0, P0
   Ba J., 2013, ADV NEURAL INFORM PR, V0, P3084
   Banerjee DS, 2016, INT CONF CLOUD COMP, V0, PP144, DOI 10.1109/CloudCom.2016.33
   Bondi E, 2018, AAAI CONF ARTIF INTE, V0, P7741
   Bondi E, 2019, ARTIF INT SOC GOOD, V0, P77
   Buckland S.T., 2001, PI, V0, P0
   Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
   Christie KS, 2016, FRONT ECOL ENVIRON, V14, P242, DOI 10.1002/fee.1281
   Commercial Software Engineering (CSE) group at Microsoft, 2020, VOTT VIS OBJ TAGG TO, V0, P0
   Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
   Fang YF, 2016, PROCEDIA COMPUT SCI, V92, P13, DOI 10.1016/j.procs.2016.07.316
   Hazelwood K, 2018, INT S HIGH PERF COMP, V0, PP620, DOI 10.1109/HPCA.2018.00059
   Hensel M, 2017, ADV NEUR IN, V30, P0
   Jakobs S., 2019, ATZHEAVY DUTY WORLDW, V12, P44, DOI 10.1007/s41321-019-0024-8
   Kanai Sekitoshi, 2017, P 31 INT C NEURAL IN, V0, P435
   King DB, 2015, ACS SYM SER, V1214, P1
   Lamba A, 2019, CURR BIOL, V29, PR977, DOI 10.1016/j.cub.2019.08.016
   LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
   Lee J, 2017, 2017 FIRST IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC), V0, PP36, DOI 10.1109/IRC.2017.77
   Lim K, 2017, PLOS ONE, V12, P0, DOI 10.1371/journal.pone.0173317
   Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
   Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
   Martinez P, 2019, AUTOMAT CONSTR, V97, P151, DOI 10.1016/j.autcon.2018.10.021
   Maxwell S, 2016, NATURE, V536, P143, DOI 10.1038/536143a
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Peng JB, 2020, ISPRS J PHOTOGRAMM, V169, P364, DOI 10.1016/j.isprsjprs.2020.08.026
   Rampasek Ladislav, 2016, CELL SYST, V2, P12, DOI 10.1016/j.cels.2016.01.009
   Saria S, 2018, PLOS MED, V15, P0, DOI 10.1371/journal.pmed.1002721
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Talukdar J, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), V0, P78
   van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
   Wich SA, 2018, CONSERVATION DRONES: MAPPING AND MONITORING BIODIVERSITY, V0, PP1, DOI 10.1093/oso/9780198787617.001.0001
NR 35
TC 6
Z9 6
U1 2
U2 2
PU CANADIAN SCIENCE PUBLISHING
PI OTTAWA
PA 65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA
SN 2291-3467
EI 
J9 J UNMANNED VEH SYST
JI J. Unmanned Veh. Syst.
PD JUN 15
PY 2021
VL 9
IS 2
BP 112
EP 127
DI 10.1139/juvs-2020-0018
PG 16
WC Remote Sensing
SC Remote Sensing
GA UP5XS
UT WOS:000695453500003
DA 2023-04-26
ER
