
PT J
AU Sangani, DJ
   Thakker, RA
   Panchal, SD
   Gogineni, R
AF Sangani, Dhara J.
   Thakker, Rajesh A.
   Panchal, S. D.
   Gogineni, Rajesh
TI Pansharpening of Satellite Images with Convolutional Sparse Coding and Adaptive PCNN-Based Approach
SO JOURNAL OF THE INDIAN SOCIETY OF REMOTE SENSING
LA English
DT Article
DE Pansharpening; Non-subsampled shearlet transform; Convolutional sparse coding; Adaptive pulse couple neural network; High-resolution multispectral image
ID pan-sharpening method; contourlet transform; variational approach; fusion algorithm; model; representation
AB In remote sensing, Pansharpening process has great significance in many practical applications like map updating, hazard monitoring, target recognition and object classification. Satellite sensors capturing panchromatic and multispectral images with complementary characteristics due to tradeoff between IFOV (instantaneous field of view) and SNR (signal-to-noise ratio). Pansharpening is a process of combining PAN (panchromatic) image of high spatial resolution with MS (multispectral) image of high spectral resolution to get image of high spectral and spatial resolution. In Pansharpening, balancing between extraction of information and injection of information is crucial point; misbalancing can cause intensity distortion. Proposed method is a combination of CSC (convolution sparse coding) and adaptive PCNN (pulse coupled neural network) approach. NSST (non-sub-sampled shearlet transform) is used for band separation of PAN and MS image. CSC is used for fusing low pass sub-bands, and adaptive PCNN method is employed for fusing high pass sub-bands. Five datasets with different geographical areas like mountain, urban and vegetation area are used for experiment purpose. Visual results and quantitative index analysis reflect the superiority of proposed method in preserving spectral details in pansharpened image.
C1 [Sangani, Dhara J.] Vishwakarma Govt Engn Coll Chandkheda, Elect & Commun Dept, Gandhi Sagar 382424, Gujarat, India.
   [Thakker, Rajesh A.] Govt Engn Coll, Dept ECE, Rajkot, Gujarat, India.
   [Panchal, S. D.] Gujarat Technol Univ, Comp Engn, Ahmadabad, Gujarat, India.
   [Gogineni, Rajesh] Dhanekula Inst Engn & Technol, Dept Elect & Commun Engn, Vijayawada 521139, India.
C3 Gujarat Technological University
RP Sangani, DJ (corresponding author), Vishwakarma Govt Engn Coll Chandkheda, Elect & Commun Dept, Gandhi Sagar 382424, Gujarat, India.
EM dsangani1987@gmail.com
CR Aiazzi B, 2006, PHOTOGRAMM ENG REM S, V72, P591, DOI 10.14358/PERS.72.5.591
   Amro I, 2011, EURASIP J ADV SIG PR, V0, P0, DOI DOI 10.1186/1687-6180-2011-79
   Ballester C, 2006, INT J COMPUT VISION, V69, P43, DOI 10.1007/s11263-006-6852-x
   CARPER WJ, 1990, PHOTOGRAMM ENG REM S, V56, P459
   Chen CQ, 2018, REMOTE SENS LETT, V9, P170, DOI 10.1080/2150704X.2017.1410292
   Chen FR, 2012, PROCEDIA ENGINEER, V29, P2938, DOI 10.1016/j.proeng.2012.01.418
   Choi J, 2011, IEEE T GEOSCI REMOTE, V49, P295, DOI 10.1109/TGRS.2010.2051674
   Chu H, 2008, IEEE GEOSCI REMOTE S, V5, P653, DOI 10.1109/LGRS.2008.2002034
   Deng LJ, 2017, IEEE IMAGE PROC, V0, P535
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Duran J, 2017, ISPRS J PHOTOGRAMM, V125, P78, DOI 10.1016/j.isprsjprs.2016.12.013
   Fang FM, 2013, IEEE T IMAGE PROCESS, V22, P2822, DOI 10.1109/TIP.2013.2258355
   Fei RR, 2019, IEEE GEOSCI REMOTE S, V16, P1595, DOI 10.1109/LGRS.2019.2904526
   Fu XY, 2019, PROC CVPR IEEE, V0, PP10257, DOI 10.1109/CVPR.2019.01051
   Ganasala P, 2016, J DIGIT IMAGING, V29, P73, DOI 10.1007/s10278-015-9806-4
   Garzelli A, 2008, IEEE T GEOSCI REMOTE, V46, P228, DOI 10.1109/TGRS.2007.907604
   Ghahremani M, 2015, IEEE GEOSCI REMOTE S, V12, P502, DOI 10.1109/LGRS.2014.2347955
   Gogineni R, 2019, IEEE J-STARS, V12, P4024, DOI 10.1109/JSTARS.2019.2945815
   Gogineni R, 2018, ISPRS J PHOTOGRAMM, V146, P360, DOI 10.1016/j.isprsjprs.2018.10.009
   Huang W, 2015, IEEE GEOSCI REMOTE S, V12, P1037, DOI 10.1109/LGRS.2014.2376034
   Jiang YY, 2015, IEEE I CONF COMP VIS, V0, PP540, DOI 10.1109/ICCV.2015.69
   Joshi MV, 2006, IEEE T GEOSCI REMOTE, V44, P2549, DOI 10.1109/TGRS.2006.873340
   Laben C. A., 2000, US PATENT, V0, Patent No. 6011875
   Li ST, 2011, IEEE T GEOSCI REMOTE, V49, P738, DOI 10.1109/TGRS.2010.2067219
   Lolli S, 2017, IEEE GEOSCI REMOTE S, V14, P2255, DOI 10.1109/LGRS.2017.2761021
   Masi G, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8070594
   Moller M, 2012, SIAM J IMAGING SCI, V5, P150, DOI 10.1137/100810356
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Nunez J, 1999, IEEE T GEOSCI REMOTE, V37, P1204, DOI 10.1109/36.763274
   Ophir B, 2011, IEEE J-STSP, V5, P1014, DOI 10.1109/JSTSP.2011.2155032
   Otazu X, 2005, IEEE T GEOSCI REMOTE, V43, P2376, DOI 10.1109/TGRS.2005.856106
   Palsson F, 2014, IEEE GEOSCI REMOTE S, V11, P318, DOI 10.1109/LGRS.2013.2257669
   Panchal S, 2017, J INDIAN SOC REMOTE, V45, P385, DOI 10.1007/s12524-016-0608-z
   Rahmani S, 2010, IEEE GEOSCI REMOTE S, V7, P746, DOI 10.1109/LGRS.2010.2046715
   Sahu A, 2015, 2014 INTERNATIONAL CONFERENCE ON MEDICAL IMAGING, V0, P448
   Scarpa G, 2018, IEEE T GEOSCI REMOTE, V56, P5443, DOI 10.1109/TGRS.2018.2817393
   Shah VP, 2008, IEEE T GEOSCI REMOTE, V46, P1323, DOI 10.1109/TGRS.2008.916211
   Vicinanza MR, 2015, IEEE GEOSCI REMOTE S, V12, P180, DOI 10.1109/LGRS.2014.2331291
   Vivone G, 2015, IEEE T GEOSCI REMOTE, V53, P2565, DOI 10.1109/TGRS.2014.2361734
   Wang XH, 2019, IEEE ACCESS, V7, P52508, DOI 10.1109/ACCESS.2019.2910656
   YIN H, 2015, SIGNAL PROCESS, V0, P0
   Zhu XX, 2016, IEEE T GEOSCI REMOTE, V54, P2664, DOI 10.1109/TGRS.2015.2504261
   Zhu XX, 2013, IEEE T GEOSCI REMOTE, V51, P2827, DOI 10.1109/TGRS.2012.2213604
NR 43
TC 1
Z9 1
U1 3
U2 8
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0255-660X
EI 0974-3006
J9 J INDIAN SOC REMOTE
JI J. Indian Soc. Remote Sens.
PD DEC 15
PY 2021
VL 49
IS 12
BP 2989
EP 3004
DI 10.1007/s12524-021-01440-4
EA OCT 2021
PG 16
WC Environmental Sciences; Remote Sensing
SC Environmental Sciences & Ecology; Remote Sensing
GA XC3JP
UT WOS:000710066100001
DA 2023-04-26
ER

PT J
AU Mehltretter, M
   Heipke, C
AF Mehltretter, Max
   Heipke, Christian
TI Aleatoric uncertainty estimation for dense stereo matching via CNN-based cost volume analysis
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Depth reconstruction; Uncertainty quantification; Confidence; Deep learning
AB Motivated by the need to identify erroneous disparity estimates, various methods for the estimation of aleatoric uncertainty in the context of dense stereo matching have been presented in recent years. Especially, the introduction of deep learning based methods and the accompanying significant improvement in accuracy have greatly increased the popularity of this field. Despite this remarkable development, most of these methods rely on features learned from disparity maps only, neglecting the corresponding 3-dimensional cost volumes. However, conventional hand-crafted methods have already demonstrated that the additional information contained in such cost volumes are beneficial for the task of uncertainty estimation. In this paper, we combine the advantages of deep learning and cost volume based features and present a new Convolutional Neural Network (CNN) architecture to directly learn features for the task of aleatoric uncertainty estimation from volumetric 3D data. Furthermore, we discuss and apply three different uncertainty models to train our CNN without the need to provide ground truth for uncertainty. In an extensive evaluation on three datasets using three common dense stereo matching methods, we investigate the effects of these uncertainty models and demonstrate the generality and state-of-the-art accuracy of the proposed method.
C1 [Mehltretter, Max; Heipke, Christian] Leibniz Univ Hannover, Inst Photogrammetry & GeoInformat, Hannover, Germany.
C3 Leibniz University Hannover
RP Mehltretter, M (corresponding author), Leibniz Univ Hannover, Inst Photogrammetry & GeoInformat, Hannover, Germany.
EM mehltretter@ipi.uni-hannover.de; heipke@ipi.uni-hannover.de
FU German Research Foundation (DFG) as a part of the Research Training Group i.c.sens [GRK2159]; MOBILISE initiative of the Leibniz University Hannover; TU Braunschweig, Germany; NVIDIA Corporation
CR [Anonymous], 2010, P 13 INT C ARTIFICIA, V0, P0
   Batsos K, 2018, PROC CVPR IEEE, V0, PP2060, DOI 10.1109/CVPR.2018.00220
   Coenen M, 2019, IEEE INT CONF COMP V, V0, PP822, DOI 10.1109/ICCVW.2019.00110
   Fu Z., 2019, REPRESENTATIONS ANAL, V0, P69
   Geiger A, 2012, PROC CVPR IEEE, V0, PP3354, DOI 10.1109/CVPR.2012.6248074
   Hacking Ian., 1975, EMERGENCE PROBABILIT, V0, P0
   Haeusler R, 2013, PROC CVPR IEEE, V0, PP305, DOI 10.1109/CVPR.2013.46
   Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hollmann M., 2020, ISPRS ANN PHOTOGRAMM, VV-2-2020, P151
   Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46
   Johns E, 2016, PROC CVPR IEEE, V0, PP3813, DOI 10.1109/CVPR.2016.414
   Kang J., 2020, INT ARCH PHOTOGRAMME, V0, P135
   Kendall A., 2017, PROC 31 INT C NEURAL, V0, P5574
   Kendall A., 2017, THESIS, V0, P0
   Kendall A, 2017, IEEE I CONF COMP VIS, V0, PP66, DOI 10.1109/ICCV.2017.17
   Kim S, 2019, IEEE T IMAGE PROCESS, V28, P1299, DOI 10.1109/TIP.2018.2878325
   King DB, 2015, ACS SYM SER, V1214, P1
   Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020
   Li Y., 2016, ADV NEURAL INFORM PR, V0, P307
   Maturana D, 2015, IEEE INT C INT ROBOT, V0, PP922, DOI 10.1109/IROS.2015.7353481
   Mayer N, 2016, PROC CVPR IEEE, V0, PP4040, DOI 10.1109/CVPR.2016.438
   Mehltretter M., 2020, ISPRS ANN PHOTOGRAMM, VV-2-2020, P161
   Mehltretter M, 2019, IEEE INT CONF COMP V, V0, PP2070, DOI 10.1109/ICCVW.2019.00262
   Mehltretter Max, 2018, GERM C PATT REC, V0, P3
   Menze M, 2015, PROC CVPR IEEE, V0, PP3061, DOI 10.1109/CVPR.2015.7298925
   Nguyen U, 2020, ISPRS J PHOTOGRAMM, V166, P347, DOI 10.1016/j.isprsjprs.2020.05.002
   Park MG, 2015, PROC CVPR IEEE, V0, PP101, DOI 10.1109/CVPR.2015.7298605
   Poggi M., 2016, P BRIT MACH VIS C, V0, P0
   Poggi M., 2017, PROC IEEE C COMPUT V, V0, P76
   Poggi M, 2017, PROC CVPR IEEE, V0, PP4541, DOI 10.1109/CVPR.2017.483
   Poggi M, 2016, INT CONF 3D VISION, V0, PP138, DOI 10.1109/3DV.2016.22
   Poggi M, 2016, INT CONF 3D VISION, V0, PP509, DOI 10.1109/3DV.2016.61
   Qi CR, 2016, PROC CVPR IEEE, V0, PP5648, DOI 10.1109/CVPR.2016.609
   Riegler G, 2017, PROC CVPR IEEE, V0, PP6620, DOI 10.1109/CVPR.2017.701
   Riesch H., 2012, HDB RISK THEORY EPIS, V1, P88
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schonberger J.L., 2018, EUROPEAN C COMPUTER, V0, P739
   Seki A., 2016, BMVC, V0, P0
   Shaked A., 2017, CVPR, V0, P0
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Spyropoulos A, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, V0, PP73, DOI 10.1109/3DV.2015.16
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stucker C., 2020, P IEEE CVF C COMP VI, V0, P184
   Su H, 2015, IEEE I CONF COMP VIS, V0, PP945, DOI 10.1109/ICCV.2015.114
   Sullivan T.J, 2015, ORTHOGONAL POLYNOMIA, V0, P0
   Sun L, 2017, IEEE T IMAGE PROCESS, V26, P3331, DOI 10.1109/TIP.2017.2687101
   Tosi F., 2018, P IEEE EUR C COMP VI, V0, P319
   Tulyakov S, 2018, ADV NEUR IN, V31, P0
   van Asselt MBA, 2002, CLIMATIC CHANGE, V54, P75, DOI 10.1023/A:1015783803445
   Veld ROH, 2018, IEEE IMAGE PROC, V0, PP644, DOI 10.1109/ICIP.2018.8451500
   Wu ZR, 2015, PROC CVPR IEEE, V0, PP1912, DOI 10.1109/CVPR.2015.7298801
   Zabih R., 1994, COMPUTER VISION - ECCV 94. THIRD EUROPEAN CONFERENCE ON COMPUTER VISION. PROCEEDINGS. VOL.II, V0, PP151, DOI 10.1007/BFb0028345
   Zbontar J, 2016, J MACH LEARN RES, V17, P0
NR 53
TC 4
Z9 4
U1 1
U2 17
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD JAN 15
PY 2021
VL 171
IS 
BP 63
EP 75
DI 10.1016/j.isprsjprs.2020.11.003
PG 13
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA PN3UD
UT WOS:000604406500005
DA 2023-04-26
ER

PT J
AU Ahadit, AB
   Jatoth, RK
AF Ahadit, Alagesan Bhuvaneswari
   Jatoth, Ravi Kumar
TI A Novel Dual CNN Architecture with LogicMax for Facial Expression Recognition
SO JOURNAL OF INFORMATION SCIENCE AND ENGINEERING
LA English
DT Article
DE convolutional neural networks; transfer learning; facial action coding system; action units; Pearson correlation; data augmentation; dlib facial landmark predictor; vgg16; logicMax
AB Facial expressions convey important features for recognizing human emotions. It is a challenging task to classify accurate facial expressions due to high intra-class correlation. Conventional methods depend on the classification of handcrafted features like scale-invariant feature transform and local binary patterns to predict the emotion. In recent years, deep learning techniques are used to boost the accuracy of FER models. Although it has improved the accuracy in standard datasets, FER models have to consider problems like face occlusion and intra-class variance. In this paper, we have used two convolutional neural networks which have vgg16 architecture as a base network using transfer learning. This paper explains the method to tackle issues on classifying high intra-class correlated facial expressions through an in-depth investigation of the Facial Action Coding System (FACS) action units. We have used a novel LogicMax layer at the end of the model to boost the accuracy of the FER model. Classification metrics like Accuracy, Precision, Recall, and Fl score are calculated for evaluating the model performance on CK+ and JAFFE datasets. The model is tested using 10-fold cross-validation and the obtained classification accuracy rate of 98.62% and 94.86% on CK+ and JAFFE datasets respectively. The experimental results also include a feature map visualization of 64 convolutional filters of the two convolutional neural networks.
C1 [Ahadit, Alagesan Bhuvaneswari; Jatoth, Ravi Kumar] Natl Inst Technol, Dept Elect & Commun, Warangal 506004, Telangana, India.
C3 National Institute of Technology (NIT System); National Institute of Technology Warangal
RP Ahadit, AB (corresponding author), Natl Inst Technol, Dept Elect & Commun, Warangal 506004, Telangana, India.
EM ahadit.ab.ml@gmail.com; ravikumar@nitw.ac.in
CR Alisa, 2018, P S EL MECH APPL SCI, V0, P73
   Alphonse AS, 2018, MULTIMED TOOLS APPL, V77, P9455, DOI 10.1007/s11042-017-5141-8
   Bradski G., 2008, LEARNING OPENCV COMP, V0, P0
   Cai J, 2018, IEEE INT CONF AUTOMA, V0, PP302, DOI 10.1109/FG.2018.00051
   Christian S., 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/CVPR.2015.7298594
   CORRADO G, 2011, J MACHINE LEARNING R, V12, P2825
   Darwin C., 1872, P374, V0, P0
   Deng J, 2009, PROC CVPR IEEE, V0, PP248, DOI 10.1109/CVPRW.2009.5206848
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman R., 1997, WHAT FACE REVEALS BA, V0, P0
   Huang J, 2017, IEEE INT C INT ROBOT, V0, P3296
   Iwasaki M, 2016, SCI REP-UK, V6, P0, DOI 10.1038/srep22049
   Jung H, 2015, IEEE I CONF COMP VIS, V0, PP2983, DOI 10.1109/ICCV.2015.341
   Kahou SE, 2013, ICMI13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, V0, PP543, DOI 10.1145/2522848.2531745
   KAZEMI V, 2014, PROC CVPR IEEE, V0, PP1867, DOI 10.1109/CVPR.2014.241
   Kim BK, 2016, IEEE COMPUT SOC CONF, V0, PP1499, DOI 10.1109/CVPRW.2016.187
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Lucey P., 2010, PROC IEEE C COMPUT V, V0, PP94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M.J., 1998, PROC IEEE INT C AUTO, V0, PP14, DOI 10.5281/ZENODO.3451524
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   Petrantonakis PC, 2010, IEEE T INF TECHNOL B, V14, P186, DOI 10.1109/TITB.2009.2034649
   Schroff F, 2015, PROC CVPR IEEE, V0, PP815, DOI 10.1109/CVPR.2015.7298682
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Simonyan K, 2015, ARXIV, V0, P0
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, V0, P0, DOI DOI 10.5244/C.27.8
   Wang J, 2018, ENVIRON TECHNOL, V39, P3055, DOI 10.1080/09593330.2017.1371797
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   XIE S, 2017, ELECTRON LETT, V53, P235, DOI 10.1049/el.2016.4328
   Yadan Lv, 2014, 2014 INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP), V0, PP303, DOI 10.1109/SMARTCOMP.2014.7043872
   Yang HY, 2018, PROC CVPR IEEE, V0, PP2168, DOI 10.1109/CVPR.2018.00231
   Zhang FF, 2018, PROC CVPR IEEE, V0, PP3359, DOI 10.1109/CVPR.2018.00354
   Zhang ZL, 2018, ADV NEUR IN, V31, P0
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhi RC, 2011, IEEE T SYST MAN CY B, V41, P38, DOI 10.1109/TSMCB.2010.2044788
NR 35
TC 0
Z9 0
U1 2
U2 12
PU INST INFORMATION SCIENCE
PI TAIPEI
PA ACADEMIA SINICA, TAIPEI 115, TAIWAN
SN 1016-2364
EI 
J9 J INF SCI ENG
JI J. Inf. Sci. Eng.
PD JAN 15
PY 2021
VL 37
IS 1
BP 15
EP 39
DI 10.6688/JISE.202101_37(1).0002
PG 25
WC Computer Science, Information Systems
SC Computer Science
GA PS6PS
UT WOS:000608049100002
DA 2023-04-26
ER

PT J
AU Zhou, L
AF Zhou, Lin
TI Analysis of Psychological and Emotional Tendency Based on Brain Functional Imaging and Deep Learning
SO DISCRETE DYNAMICS IN NATURE AND SOCIETY
LA English
DT Article
ID eeg signal; recognition
AB When facing various pressures, human beings will have different degrees of bad psychological emotions, especially depression and anxiety. How to effectively obtain psychological data signals and use advanced intelligent technology to identify and make decisions is a research hotspot in psychology and computer science. Therefore, a personal emotional tendency analysis method based on brain functional imaging and deep learning is proposed. Firstly, the EEG forward model is established according to functional magnetic resonance imaging (fMRI), and the transfer matrix from the signal source at the cerebral cortex to the head surface electrode is obtained. Therefore, the activation results of fMRI emotional experiment can be mapped to the three-layer head model to obtain the EEG topographic map reflecting the degree of emotional correlation. Then, combining data enhancement (Mixup) with three-dimensional convolutional neural network (3D-CNN), an emotion-related EEG topographic map classification method based on M-3DCNN is proposed. Mixup is used to generate virtual data, the original data and virtual data are used to train the network together, the number of training samples is expanded, the overfitting phenomenon of 3D-CNN is alleviated, and 3D-CNN is used for feature extraction and classification. Experimental data analysis shows that, compared with traditional methods, the proposed method can retain emotion related EEG signals to a greater extent and obtain a higher accuracy of emotion five classifications under the same feature dimension.
C1 [Zhou, Lin] Shenyang Sport Univ, Students Affairs Div, Shenyang 110102, Peoples R China.
C3 Shenyang Sport University
RP Zhou, L (corresponding author), Shenyang Sport Univ, Students Affairs Div, Shenyang 110102, Peoples R China.
EM zhoul@syty.edu.cn
CR Albornoz EM, 2017, IEEE T AFFECT COMPUT, V8, P43, DOI 10.1109/TAFFC.2015.2503757
   Antipov G, 2017, PATTERN RECOGN, V72, P15, DOI 10.1016/j.patcog.2017.06.031
   Bao S, 2020, OPT REV, V27, P475, DOI 10.1007/s10043-020-00614-8
   Berggren S, 2018, DEV NEUROREHABIL, V21, P141, DOI 10.1080/17518423.2017.1305004
   Cai JH, 2020, IET COMPUT VIS, V14, P634, DOI 10.1049/iet-cvi.2020.0023
   Castillo JC, 2016, COGN COMPUT, V8, P357, DOI 10.1007/s12559-016-9383-y
   Chandra BS, 2019, IEEE T BIO-MED ENG, V66, P710, DOI 10.1109/TBME.2018.2854899
   Elngar AA, 2020, OPEN COMPUT SCI, V10, P17, DOI 10.1515/comp-2020-0003
   Guo WQ, 2016, CHIN CONT DECIS CONF, V0, PP131, DOI 10.1109/CCDC.2016.7530968
   Jenke R., 2017, IEEE T AFFECT COMPUT, V5, P339
   Kavakiotis I, 2017, COMPUT STRUCT BIOTEC, V15, P104, DOI 10.1016/j.csbj.2016.12.005
   Kaya H, 2017, IMAGE VISION COMPUT, V65, P66, DOI 10.1016/j.imavis.2017.01.012
   Kaya H, 2016, J MULTIMODAL USER IN, V10, P139, DOI 10.1007/s12193-015-0175-6
   Kim T, 2020, HUM-CENT COMPUT INFO, V10, P0, DOI 10.1186/s13673-020-00244-8
   Kohli M, 2017, AM J ROENTGENOL, V208, P754, DOI 10.2214/AJR.16.17224
   Lin XK, 2020, APPL INTELL, V50, P2105, DOI 10.1007/s10489-020-01641-3
   Liu SX, 2017, VIS INFORM, V1, P48, DOI 10.1016/j.visinf.2017.01.006
   Ludi Bai, 2020, PROCEDIA COMPUTER SCIENCE, V174, P364, DOI 10.1016/j.procs.2020.06.100
   Masruroh AH, 2019, PROCEDIA COMPUT SCI, V157, P552, DOI 10.1016/j.procs.2019.09.013
   Mitsukura Y., 2016, J SIGNAL PROCESSING, V20, P1, DOI 10.2299/jsp.20.1
   Pound MP, 2017, GIGASCIENCE, V6, P0, DOI 10.1093/gigascience/gix083
   Raissi M, 2018, J COMPUT PHYS, V357, P125, DOI 10.1016/j.jcp.2017.11.039
   Ravikumar S, 2021, J FIELD ROBOT, V38, P967, DOI 10.1002/rob.22020
   Schlegel K, 2016, BEHAV RES METHODS, V48, P1383, DOI 10.3758/s13428-015-0646-4
   Schuller BW, 2018, COMMUN ACM, V61, P90, DOI 10.1145/3129340
   Voyant C, 2017, RENEW ENERG, V105, P569, DOI 10.1016/j.renene.2016.12.095
   Wankhade SB, 2020, INT J UNCERTAIN FUZZ, V28, P153, DOI 10.1142/S0218488520500075
   Xu BH, 2018, IEEE T AFFECT COMPUT, V9, P255, DOI 10.1109/TAFFC.2016.2622690
   Yang RP, 2021, NEURAL NETWORKS, V142, P564, DOI 10.1016/j.neunet.2021.07.018
   Zhao L, 2020, MULTIMED TOOLS APPL, V79, P26683, DOI 10.1007/s11042-020-09259-w
NR 30
TC 1
Z9 1
U1 2
U2 8
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1026-0226
EI 1607-887X
J9 DISCRETE DYN NAT SOC
JI Discrete Dyn. Nat. Soc.
PD NOV 16
PY 2021
VL 2021
IS 
BP 
EP 
DI 10.1155/2021/1272502
PG 9
WC Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences
SC Mathematics; Science & Technology - Other Topics
GA XH4QT
UT WOS:000725421800005
DA 2023-04-26
ER

PT J
AU Xu, F
   Li, H
   Yao, HG
   An, MS
AF Xu, Fei
   Li, He
   Yao, Hongge
   An, MingShou
TI Detection method of tunnel lining voids based on guided anchoring mechanism
SO COMPUTERS & ELECTRICAL ENGINEERING
LA English
DT Article
DE Tunnel void disease; Neural network; Guide anchoring; GIoU
AB In tunnel construction engineering, the form of tunnel void diseases are complex and easily affected by the geographical environment. The traditional manual interpretation of image data has the characteristics of heavy workload, high probability of missing, and misjudgment. This paper constructs a convolution neural network that integrates the mechanism of guiding anchoring to detect tunnel voids. The network is composed of four parts: Feature extraction network extracts disease features from the enriched samples; Region proposal by guided anchoring join the generalized intersection over union (GIoU) evaluation criteria, and predict the shape of the anchor point through learning; The obtained feature maps are fixed in the region of interest pooling; Finally, the disease features are classified and bounding box regression. Compared with the existing target detection algorithm, the experimental results show that the improved network achieves an average classification accuracy of 92.74%, and the trained model has good generalization ability and robustness.
C1 [Xu, Fei; Li, He; Yao, Hongge; An, MingShou] Xian Technol Univ, Xian, Peoples R China.
   [Xu, Fei; Yao, Hongge; An, MingShou] State & Prov Joint Engn Lab Adv Network Monitorin, Xian, Peoples R China.
C3 Xi'an Technological University
RP Li, H (corresponding author), Xian Technol Univ, Xian, Peoples R China.
EM 1003294436@qq.com
FU Natural Science Project of Shaanxi Education Department [18JK0399]; fund of the State and Provincial Joint Engineering Lab of Advanced Network, Monitoring and Control, China [GSYSJ2018006]; Natural Science Foundation of Shaanxi Provincial Department of Education (CN) [19JK0396]
CR Abu Alhaija H, 2018, INT J COMPUT VISION, V126, P961, DOI 10.1007/s11263-018-1070-x
   Artagan SS, 2020, SURV GEOPHYS, V41, P447, DOI 10.1007/s10712-019-09544-w
   Byeon YH, 2017, 2017 6TH IIAI INTERNATIONAL CONGRESS ON ADVANCED APPLIED INFORMATICS (IIAI-AAI), V0, PP858, DOI 10.1109/IIAI-AAI.2017.196
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fawzi A, 2016, IEEE IMAGE PROC, V0, PP3688, DOI 10.1109/ICIP.2016.7533048
   Feng Yang, 2020, COMPUTER ENG APPL, V0, P1
   Giannakis I, 2019, IEEE T GEOSCI REMOTE, V57, P4417, DOI 10.1109/TGRS.2019.2891206
   He T, 2019, PROC CVPR IEEE, V0, PP558, DOI 10.1109/CVPR.2019.00065
   Khan RU, 2019, P 2019 2 INT C ALG C, V0, P78
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Lemley J, 2017, IEEE ACCESS, V5, P5858, DOI 10.1109/ACCESS.2017.2696121
   Lin TY, 2017, PROC CVPR IEEE, V0, PP936, DOI 10.1109/CVPR.2017.106
   Pham MT, 2018, INT GEOSCI REMOTE SE, V0, P6804
   Qi XQ, 2017, 2017 SECOND INTERNATIONAL CONFERENCE ON MECHANICAL, V0, P151, DOI 10.1109/ICMCCE.2017.49
   Redmon J, 2016, PROC CVPR IEEE, V0, PP779, DOI 10.1109/CVPR.2016.91
   Reichman D, 2017, PROC 9 INT WORKSHOP, V28, P1, DOI 10.1109/IWAGPR.2017.7996100
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, V0, PP658, DOI 10.1109/CVPR.2019.00075
   SimonYan K, 2014, VERY DEEP CONVOLUTIO, V34, P0
   Soldovieri F, 2017, IEEE J-STARS, V10, P562, DOI 10.1109/JSTARS.2016.2543840
   The Professional Standards Compilation Group of People s Republic of China, 2015, JTG H12 2015 TECHN S, V0, P0
   Wang JQ, 2019, PROC CVPR IEEE, V0, PP2960, DOI 10.1109/CVPR.2019.00308
   Xisto L., 2018, APPL COMPUT INFORM, V17, P296, DOI 10.1016/j.aci.2018.10.001
   Yao L, 2016, ROCK SOIL MECH, V37, P3627
   Yong L., 2019, CHINA RAILW SCI, V40, P72
NR 25
TC 1
Z9 1
U1 1
U2 8
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0045-7906
EI 1879-0755
J9 COMPUT ELECTR ENG
JI Comput. Electr. Eng.
PD OCT 15
PY 2021
VL 95
IS 
BP 
EP 
DI 10.1016/j.compeleceng.2021.107462
EA SEP 2021
PG 16
WC Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA WA1NU
UT WOS:000702661400010
DA 2023-04-26
ER
