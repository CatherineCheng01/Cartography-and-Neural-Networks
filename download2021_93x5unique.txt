
PT J
AU Xu, Y
   Yu, JT
   Li, WX
   Feng, JQ
AF Xu, Yao
   Yu, Jintong
   Li, Wenxue
   Feng, Jiqiang
TI Global asymptotic stability of fractional-order competitive neural networks with multiple time-varying-delay links
SO APPLIED MATHEMATICS AND COMPUTATION
LA English
DT Article
DE Competitive neural networks; Fractional derivatives; Global asymptotic stability; Time-varying delays; Multiple links
ID finite-time; exponential stability; group models; synchronization; dispersal
AB Competitive neural networks have become increasingly popular since this kind of neural networks can better describe the dynamics of cortical cognitive maps with unsupervised synaptic modifications. In this paper, we first propose fractional-order competitive neural networks with multiple time-varying-delay links and explore the global asymptotic stability of this class of neural networks. A novel and generalized integral inequality related to every upper bound of each time-varying delay is given. Moreover, based on Lyapunov method and graph theory, we obtain some sufficient conditions with the help of this integral inequality to guarantee the global asymptotic stability. The theoretical results offer a new perspective to show the close relationship between the stability criterion and the topological structure of networks. Finally, an illustrative numerical example is given to demonstrate the feasibility and effectiveness of the theoretical results. (C) 2020 Elsevier Inc. All rights reserved.
C1 [Xu, Yao; Yu, Jintong; Li, Wenxue] Harbin Inst Technol Weihai, Dept Math, Weihai 264209, Peoples R China.
   [Feng, Jiqiang] Shenzhen Univ, Coll Math & Stat, Shenzhen Key Lab Adv Machine Learning & Applicat, Shenzhen 518060, Peoples R China.
C3 Harbin Institute of Technology; Shenzhen University
RP Li, WX (corresponding author), Harbin Inst Technol Weihai, Dept Math, Weihai 264209, Peoples R China.
EM wenxuetg@hitwh.edu.cn
FU Shandong Province Natural Science Foundation [ZR2018MA005, ZR2018MA020, ZR2017MA008]; Key Project of Science and Technology of Weihai [2014DXGJMS08]; Innovation Technology Funding Project in Harbin Institute of Technology [HIT.NSRIF.201703]; Science and Technology Program of Shenzhen [JCYJ20170818091621856]; National Science Foundation of China [61872429]
CR Aguila-Camacho N, 2014, COMMUN NONLINEAR SCI, V19, P2951, DOI 10.1016/j.cnsns.2014.01.022
   Akhmet MU, 2010, NEURAL NETWORKS, V23, P805, DOI 10.1016/j.neunet.2010.05.006
   Arbi A, 2017, NEURAL PROCESS LETT, V46, P719, DOI 10.1007/s11063-017-9620-8
   Chen LP, 2019, NEURAL NETWORKS, V118, P289, DOI 10.1016/j.neunet.2019.07.006
   COHEN MA, 1983, IEEE T SYST MAN CYB, V13, P815, DOI 10.1109/TSMC.1983.6313075
   Duan L, 2014, NEUROCOMPUTING, V123, P318, DOI 10.1016/j.neucom.2013.07.026
   Gu HB, 2010, J FRANKLIN I, V347, P719, DOI 10.1016/j.jfranklin.2009.03.005
   Guo Y, 2019, APPL MATH COMPUT, V343, P114, DOI 10.1016/j.amc.2018.07.058
   Gupta M. M., 2004, STATIC DYNAMIC NEURA, V0, P0
   Halanay A., 1966, OSCILLATIONS TIME LA, V23, P0
   Huang LL, 2020, J COMPUT APPL MATH, V370, P0, DOI 10.1016/j.cam.2019.112633
   Katchinskiy N, 2016, SCI REP-UK, V6, P0, DOI 10.1038/srep20529
   Li HL, 2019, NEURAL NETWORKS, V118, P102, DOI 10.1016/j.neunet.2019.06.008
   Li MY, 2010, J DIFFER EQUATIONS, V248, P1, DOI 10.1016/j.jde.2009.09.003
   Li RX, 2017, APPL MATH COMPUT, V313, P37, DOI 10.1016/j.amc.2017.05.073
   Li S, 2020, J MATH ANAL APPL, V489, P0, DOI 10.1016/j.jmaa.2020.124150
   Li S, 2020, NONLINEAR ANAL-HYBRI, V35, P0, DOI 10.1016/j.nahs.2019.100819
   Li S, 2021, INT J CONTROL, V94, P7, DOI 10.1080/00207179.2019.1577562
   Liu PP, 2018, NEURAL NETWORKS, V108, P452, DOI 10.1016/j.neunet.2018.09.005
   Liu XM, 2018, NEUROCOMPUTING, V273, P357, DOI 10.1016/j.neucom.2017.07.047
   Liu Y, 2019, NONLINEAR ANAL-HYBRI, V33, P93, DOI 10.1016/j.nahs.2019.01.007
   Lu HT, 2005, NEURAL NETWORKS, V18, P243, DOI 10.1016/j.neunet.2004.11.009
   Meyer-Baese A, 2003, IEEE T NEURAL NETWOR, V14, P716, DOI 10.1109/TNN.2003.810594
   Peng HP, 2010, PHYS LETT A, V374, P2335, DOI 10.1016/j.physleta.2010.03.052
   Peng X, 2017, NEURAL NETWORKS, V94, P46, DOI 10.1016/j.neunet.2017.06.011
   Podlubny L., 1999, FRACTIONAL DIFFERENT, V0, P0
   Pratap A, 2019, J FRANKLIN I, V356, P2212, DOI 10.1016/j.jfranklin.2019.01.017
   Pratap A, 2018, NEUROCOMPUTING, V317, P110, DOI 10.1016/j.neucom.2018.08.016
   Su TT, 2016, DISCRETE CONT DYN-B, V21, P3655, DOI 10.3934/dcdsb.2016115
   Sun ZR, 2020, APPL MATH LETT, V99, P0, DOI 10.1016/j.aml.2019.07.013
   Wang H, 2015, NEUROCOMPUTING, V154, P15, DOI 10.1016/j.neucom.2014.12.031
   Wang PF, 2020, NONLINEAR ANAL-HYBRI, V38, P0, DOI 10.1016/j.nahs.2020.100916
   West D. B., 1996, INTRO GRAPH THEORY, V0, P0
   Wu GC, 2019, CHAOS, V29, P0, DOI 10.1063/1.5096645
   Wu GC, 2019, FRACT CALC APPL ANAL, V22, P180, DOI 10.1515/fca-2019-0012
   Wu GC, 2018, FRACT CALC APPL ANAL, V21, P354, DOI 10.1515/fca-2018-0021
   Wu YB, 2021, IEEE T SYST MAN CY-S, V51, P3251, DOI 10.1109/TSMC.2019.2920451
   Wu YB, 2020, IEEE T CYBERNETICS, V50, P2414, DOI 10.1109/TCYB.2019.2930579
   Xu Y, 2020, J APPL ANAL COMPUT, V10, P1, DOI 10.11948/20180051
   Xu Y, 2020, INT J CONTROL, V93, P505, DOI 10.1080/00207179.2018.1479538
   Zhang CM, 2020, PHYSICA A, V538, P0, DOI 10.1016/j.physa.2019.122827
   Zhang CM, 2015, NONLINEAR ANAL-HYBRI, V15, P37, DOI 10.1016/j.nahs.2014.07.003
   Zhou H, 2020, MATH METHOD APPL SCI, V43, P9557, DOI 10.1002/mma.6624
   Zhou YH, 2017, NEURAL COMPUT APPL, V28, P775, DOI 10.1007/s00521-015-2105-7
   Zou XL, 2020, COMMUN NONLINEAR SCI, V83, P0, DOI 10.1016/j.cnsns.2019.105136
NR 45
TC 33
Z9 33
U1 12
U2 136
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 0096-3003
EI 1873-5649
J9 APPL MATH COMPUT
JI Appl. Math. Comput.
PD JAN 15
PY 2021
VL 389
IS 
BP 
EP 
DI 10.1016/j.amc.2020.125498
PG 12
WC Mathematics, Applied
SC Mathematics
GA OC7RE
UT WOS:000579354200016
DA 2023-04-26
ER

PT J
AU Foroughi, F
   Chen, ZH
   Wang, JK
AF Foroughi, Farzin
   Chen, Zonghai
   Wang, Jikai
TI A CNN-Based System for Mobile Robot Navigation in Indoor Environments via Visual Localization with a Small Dataset
SO WORLD ELECTRIC VEHICLE JOURNAL
LA English
DT Article
DE mobile robot; mobile robot localization; convolutional neural network; cross-entropy loss function
AB Deep learning has made great advances in the field of image processing, which allows automotive devices to be more widely used in humans' daily lives than ever before. Nowadays, the mobile robot navigation system is among the hottest topics that researchers are trying to develop by adopting deep learning methods. In this paper, we present a system that allows the mobile robot to localize and navigate autonomously in the accessible areas of an indoor environment. The proposed system exploits the Convolutional Neural Network (CNN) model's advantage to extract data feature maps for image classification and visual localization, which attempts to precisely determine the location region of the mobile robot focusing on the topological maps of the real environment. The system attempts to precisely determine the location region of the mobile robot by integrating the CNN model and topological map of the robot workspace. A dataset with small numbers of images is acquired from the MYNT EYE camera. Furthermore, we introduce a new loss function to tackle the bounded generalization capability of the CNN model in small datasets. The proposed loss function not only considers the probability of the input data when it is allocated to its true class but also considers the probability of allocating the input data to other classes rather than its actual class. We investigate the capability of the proposed system by evaluating the empirical studies based on provided datasets. The results illustrate that the proposed system outperforms other state-of-the-art techniques in terms of accuracy and generalization capability.
C1 [Foroughi, Farzin; Chen, Zonghai; Wang, Jikai] Univ Sci & Technol China, Dept Automat, Hefei 230026, Peoples R China.
RP Wang, JK (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230026, Peoples R China.
EM foroughi@mail.ustc.edu.cn; chenzh@ustc.edu.cn; wangjk@ustc.edu.cm
FU National Natural Science Foundation of China [62103393, 91848111]; Chinese Academy of Science-The World Academy of Sciences (CAS-TWAS)
CR Agarap A F, 2018, ARXIV, V0, P0
   Ba J. L., 2016, ARXIV, V0, P0, DOI DOI 10.48550/ARXIV.1607.06450
   Deng J, 2009, PROC CVPR IEEE, V0, PP248, DOI 10.1109/CVPRW.2009.5206848
   Dourado CMJM, 2019, FUTURE GENER COMP SY, V100, P859, DOI 10.1016/j.future.2019.05.074
   Elbasiony R, 2014, 2014 13TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), V0, PP225, DOI 10.1109/ICMLA.2014.42
   Ferris B, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, V0, P2480
   Foroughi F., 2019, P 2019 INT C IND ENG, V0, P1
   Foroughi F., 2015, INT J COMPUT APPL, V116, P1
   Howard AG, 2017, ARXIV, V0, P0
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, V0, P1
   Guang XX, 2018, SENSORS-BASEL, V18, P0, DOI 10.3390/s18092952
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kendall A, 2015, IEEE I CONF COMP VIS, V0, PP2938, DOI 10.1109/ICCV.2015.336
   Kim H, 2015, SENSORS-BASEL, V15, P21636, DOI 10.3390/s150921636
   Konrad T, 2018, ANNU REV CONTROL, V46, P181, DOI 10.1016/j.arcontrol.2018.09.002
   Krizhevsky A., 2012, P ADV NEUR INF PROC, V25, P1097
   Kukacka J., 2017, ARXIV, V0, P0
   Lin SQ, 2021, IEEE ROBOT AUTOM LET, V6, P7041, DOI 10.1109/LRA.2021.3097242
   Morales Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON INTEGRATION TECHNOLOGY, V0, P519, DOI 10.1109/ICITECHNOLOGY.2007.4290370
   Onyekpe U, 2021, APPL SCI-BASEL, V11, P0, DOI 10.3390/app11031270
   Radwan N, 2018, IEEE ROBOT AUTOM LET, V3, P4407, DOI 10.1109/LRA.2018.2869640
   Raghavan AN, 2010, IEEE INT CONF ROBOT, V0, PP4391, DOI 10.1109/ROBOT.2010.5509232
   Ran T, 2021, ISA T, V109, P389, DOI 10.1016/j.isatra.2020.10.023
   Ren MY, 2017, ARXIV, V0, P0, DOI DOI 10.48550/arXiv.1611.04520
   Rodrigues M. L., 2012, 2012 BRAZILIAN ROBOTICS SYMPOSIUM AND LATIN AMERICAN ROBOTICS SYMPOSIUM (SBR-LARS 2012), V0, PP79, DOI 10.1109/SBR-LARS.2012.20
   Saravanan M, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON INDUSTRY 4.0, V0, P0
   Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662
   Sibai F.N., 2012, P 2012 INT C COMPUTE, V0, P1
   Simonyan K, 2015, ARXIV, V0, P0
   Song ZL, 2011, COMM COM INF SC, V164, P198
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wu H, 2018, ARTIF LIFE ROBOT, V23, P373, DOI 10.1007/s10015-018-0449-7
   Yang J, 2018, ALGORITHMS, V11, P0, DOI 10.3390/a11030028
NR 33
TC 4
Z9 4
U1 0
U2 0
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2032-6653
EI 
J9 WORLD ELECTR VEHIC J
JI World Electr. Vehicle J.
PD SEP 15
PY 2021
VL 12
IS 3
BP 
EP 
DI 10.3390/wevj12030134
PG 22
WC Engineering, Electrical & Electronic; Transportation Science & Technology
SC Engineering; Transportation
GA VM0QH
UT WOS:000937526300046
DA 2023-04-26
ER

PT J
AU Yang, BC
   Xiao, ZF
AF Yang, Bochen
   Xiao, Zhifeng
TI A Multi-Channel and Multi-Spatial Attention Convolutional Neural Network for Prostate Cancer ISUP Grading
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE prostate cancer; ISUP grading; channel attention; spatial attention; convolutional neural network; reduction ratio; kernel size
ID international society; gleason
AB Prostate cancer (PCa) is one of the most prevalent cancers worldwide. As the demand for prostate biopsies increases, a worldwide shortage and an uneven geographical distribution of proficient pathologists place a strain on the efficacy of pathological diagnosis. Deep learning (DL) is able to automatically extract features from whole-slide images of prostate biopsies annotated by skilled pathologists and to classify the severity of PCa. A whole-slide image of biopsies has many irrelevant features that weaken the performance of DL models. To enable DL models to focus more on cancerous tissues, we propose a Multi-Channel and Multi-Spatial (MCMS) Attention module that can be easily plugged into any backbone CNN to enhance feature extraction. Specifically, MCMS learns a channel attention vector to assign weights to channels in the feature map by pooling from multiple attention branches with different reduction ratios; similarly, it also learns a spatial attention matrix to focus on more relevant areas of the image, by pooling from multiple convolutional layers with different kernel sizes. The model is verified on the most extensive multi-center PCa dataset that consists of 11,000 H&E-stained histopathology whole-slide images. Experimental results demonstrate that an MCMS-assisted CNN can effectively boost prediction performance in accuracy (ACC) and quadratic weighted kappa (QWK), compared with prior studies. The proposed model and results can serve as a credible benchmark for future research in automated PCa grading.
C1 [Yang, Bochen] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Xiao, Zhifeng] Behrend Coll, Sch Engn, Penn State Erie, Erie, PA 16563 USA.
C3 Beijing University of Posts & Telecommunications; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University
RP Xiao, ZF (corresponding author), Behrend Coll, Sch Engn, Penn State Erie, Erie, PA 16563 USA.
EM bochenyang@outlook.com; zux2@psu.edu
CR [Anonymous], 2016, PROC CVPR IEEE, V0, P0, DOI DOI 10.1109/CVPR.2016.90
   Arvaniti E, 2018, SCI REP-UK, V8, P0, DOI 10.1038/s41598-018-30535-1
   Bray F, 2018, CA-CANCER J CLIN, V68, P394, DOI 10.3322/caac.21492
   Campanella G., 2018, 180506983 ARXIV, V0, P0
   Copeland AR, 2007, ARCH PATHOL LAB MED, V131, P1767
   Egevad L, 2019, VIRCHOWS ARCH, V474, P577, DOI 10.1007/s00428-019-02540-w
   Epstein JI, 2016, AM J SURG PATHOL, V40, P244, DOI 10.1097/PAS.0000000000000530
   Farjam R, 2007, CYTOM PART B-CLIN CY, V72B, P227, DOI 10.1002/cyto.b.20162
   Gorelick L, 2013, IEEE T MED IMAGING, V32, P1804, DOI 10.1109/TMI.2013.2265334
   Hou L, 2016, PROC CVPR IEEE, V0, PP2424, DOI 10.1109/CVPR.2016.266
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Hussain L, 2018, CANCER BIOMARK, V21, P393, DOI 10.3233/CBM-170643
   Ilse M, 2018, PR MACH LEARN RES, V80, P0
   Li J., 2019, 190513208 ARXIV, V0, P0
   Munir K, 2019, CANCERS, V11, P0, DOI 10.3390/cancers11091235
   Nagpal K, 2019, NPJ DIGIT MED, V2, P0, DOI 10.1038/s41746-019-0112-2
   Nguyen Kien, 2011, J PATHOL INFORM, V2, PS3, DOI 10.4103/2153-3539.92030
   Nirthika Rajendran, 2020, 2020 IEEE 15TH INTERNATIONAL CONFERENCE ON INDUSTRIAL AND INFORMATION SYSTEMS (ICIIS), V0, PP144, DOI 10.1109/ICIIS51140.2020.9342711
   Pinckaers H., 2020, 200603394 ARXIV, V0, P0
   Regnier-Coudert O, 2012, ARTIF INTELL MED, V55, P25, DOI 10.1016/j.artmed.2011.11.003
   Samaratunga H, 2016, SCAND J UROL, V50, P325, DOI 10.1080/21681805.2016.1201858
   Strom P, 2020, LANCET ONCOL, V21, P222, DOI 10.1016/S1470-2045(19)30738-7
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie H., 2020, 201114301 ARXIV, V0, P0
   Xu HM, 2020, IEEE ACM T COMPUT BI, V17, P1871, DOI 10.1109/TCBB.2019.2941195
   Zhang GK, 2019, IEEE ACCESS, V7, P131448, DOI 10.1109/ACCESS.2019.2939389
   Zhou B, 2016, PROC CVPR IEEE, V0, PP2921, DOI 10.1109/CVPR.2016.319
NR 27
TC 5
Z9 5
U1 0
U2 10
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD MAY 15
PY 2021
VL 11
IS 10
BP 
EP 
DI 10.3390/app11104321
PG 11
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied
SC Chemistry; Engineering; Materials Science; Physics
GA ST7JE
UT WOS:000662615200001
DA 2023-04-26
ER

PT J
AU Du, L
   Jin, ZL
   Chen, BW
   Chen, BW
   Gao, W
   Yang, J
   Shi, S
   Song, SL
   Wang, MM
   Gong, W
   Wang, W
AF Du, Lin
   Jin, Zhili
   Chen, Bowen
   Chen, Biwu
   Gao, Wei
   Yang, Jian
   Shi, Shuo
   Song, Shalei
   Wang, Mengmeng
   Gong, Wei
   Wang, Wei
TI Application of Hyperspectral LiDAR on 3-D Chlorophyll-Nitrogen Mapping of Rohdea Japonica in Laboratory
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Vegetation mapping; Three-dimensional displays; Silicon; Mirrors; Indexes; Laser radar; Estimation; Artificial neural network (ANN); broadband spectral index; Chl-N content mapping; hyperspectral LiDAR (HSL)
ID terrestrial laser scanner; backscatter intensity; vegetation index; remote estimation; broad-band; k-fold; leaf; reflectance; canopy; meter
AB Biochemicals, such as chlorophyll (Chl) and nitrogen (N), are closely related to photosynthesis process of vegetation. Their accurate estimation is an important topic in remote sensing of vegetation. Previous studies mainly focused on Chl-N content inversion in leaf and canopy level, and few cared about their 3-D distributions, which was also an important indicator for the growth status of vegetation (GSV). Hyperspectral LiDAR (HSL) is a novel active remote sensing technology, which has target-sensitive band with hyperspectra resolution. Its 3-D point cloud data simultaneously contains rich spectral and precise geometrical characteristics of the target. This work aims to apply HSL data on 3-D Chl-N content mapping in vegetation through constructing HSL-based spectral indices (SIs). Except for following the SI forms of previous works, the normalized differential vegetation index and ratio index (RI) with four broadbands in an HSL spectral space were successively proposed to invert Chl-N content for the whole vegetation based on the artificial neural network (ANN) method. These four broadbands were transformed based on the relative spectral response curve of detector and the feature weights (FWs) of multiwavelength, respectively. Results show that most HSL-based ANN models can accurately invert Chl-N content with a mean R-2 of >0.75, and some that fusing broadband data with convolution transformation, namely the FW-based RI, can even obtain a model R-2 of 0.84 for N content inversion. Thus, HSL can be efficiently applied to 3-D Chl-N content mapping of vegetation and has great potential in GSV monitoring.
C1 [Du, Lin; Wang, Mengmeng] China Univ Geosci, Fac Informat Engn, Sch Geog & Informat Engn, Wuhan 430074, Peoples R China.
   [Du, Lin; Wang, Mengmeng] Wuchang Univ Technol, Artificial Intelligence Sch, Wuhan 430223, Peoples R China.
   [Gao, Wei; Yang, Jian] China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Peoples R China.
   [Jin, Zhili; Wang, Wei] Cent South Univ, Sch Geosci & Infophys, Changsha 410083, Peoples R China.
   [Chen, Bowen; Chen, Biwu; Shi, Shuo; Gong, Wei] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
   [Song, Shalei] Chinese Acad Sci, Wuhan Inst Phys & Math, Key Lab Magnet Resonance & Atom & Mol Phys, Wuhan 430071, Peoples R China.
C3 China University of Geosciences; Wuchang University of Technology; China University of Geosciences; Central South University; Wuhan University; Chinese Academy of Sciences; Wuhan Institute of Physics & Mathematics, CAS
RP Gao, W (corresponding author), China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Peoples R China.
EM dulin@cug.edu.cn; jinzhili@csu.edn.cn; chenbowen1204@whu.edu.cn; cbw_think@whu.edu.cn; gaowei@cug.edu.cn; yangjian@cug.edu.cn; shishuo@whu.edu.cn; songshalei@gmail.com; wangmm@cug.edu.cn; weigong@whu.edu.cn; wangweicn@csu.edu.cn
FU National Key R&D Program of China [2018YFB0504500]; National Natural Science Foundation of China [41971307]; Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan) [CUG170662]
CR Asner GP, 2008, REMOTE SENS ENVIRON, V112, P3958, DOI 10.1016/j.rse.2008.07.003
   Barutcular C, 2016, FRESEN ENVIRON BULL, V25, P1258
   Beck PSA, 2006, REMOTE SENS ENVIRON, V100, P321, DOI 10.1016/j.rse.2005.10.021
   Bi KY, 2020, IEEE T GEOSCI REMOTE, V58, P8125, DOI 10.1109/TGRS.2020.2987436
   Broge NH, 2001, REMOTE SENS ENVIRON, V76, P156, DOI 10.1016/S0034-4257(00)00197-8
   Carrea D, 2016, ISPRS J PHOTOGRAMM, V113, P17, DOI 10.1016/j.isprsjprs.2015.12.004
   CHAPPELLE EW, 1992, REMOTE SENS ENVIRON, V39, P239, DOI 10.1016/0034-4257(92)90089-3
   Chen BW, 2019, OPT EXPRESS, V27, P24043, DOI 10.1364/OE.27.024043
   Danson FM, 2014, AGR FOREST METEOROL, V198, P7, DOI 10.1016/j.agrformet.2014.07.007
   Daughtry CST, 2000, REMOTE SENS ENVIRON, V74, P229, DOI 10.1016/S0034-4257(00)00113-9
   Delegido J, 2010, INT J APPL EARTH OBS, V12, P165, DOI 10.1016/j.jag.2010.02.003
   DICKSON RE, 1989, ANN SCI FOREST, V46, PS631, DOI 10.1051/forest:198905ART0142
   Du L, 2020, FRONT PLANT SCI, V11, P0, DOI 10.3389/fpls.2020.00533
   Du L, 2018, OPT LASER TECHNOL, V107, P372, DOI 10.1016/j.optlastec.2018.06.019
   Du L, 2016, INT J APPL EARTH OBS, V44, P136, DOI 10.1016/j.jag.2015.08.008
   Eitel JUH, 2016, REMOTE SENS ENVIRON, V186, P372, DOI 10.1016/j.rse.2016.08.018
   Eitel JUH, 2014, ISPRS J PHOTOGRAMM, V97, P229, DOI 10.1016/j.isprsjprs.2014.09.009
   Eitel JUH, 2014, FIELD CROP RES, V159, P21, DOI 10.1016/j.fcr.2014.01.008
   Eitel JUH, 2011, AGR FOREST METEOROL, V151, P1338, DOI 10.1016/j.agrformet.2011.05.015
   Eitel JUH, 2010, REMOTE SENS ENVIRON, V114, P2229, DOI 10.1016/j.rse.2010.04.025
   ELVIDGE CD, 1995, REMOTE SENS ENVIRON, V54, P38, DOI 10.1016/0034-4257(95)00132-K
   Erdle K, 2011, FIELD CROP RES, V124, P74, DOI 10.1016/j.fcr.2011.06.007
   Gastellu-Etchegorry JP, 2015, REMOTE SENS-BASEL, V7, P1667, DOI 10.3390/rs70201667
   Gaulton R, 2013, REMOTE SENS ENVIRON, V132, P32, DOI 10.1016/j.rse.2013.01.001
   Gitelson AA, 2005, GEOPHYS RES LETT, V32, P0, DOI 10.1029/2005GL022688
   Gitelson AA, 2006, GEOPHYS RES LETT, V33, P0, DOI 10.1029/2006GL026457
   Goel N., 1994, REMOTE SENSING REV, V10, P309, DOI 10.1080/02757259409532252
   Haboudane D, 2002, REMOTE SENS ENVIRON, V81, P416, DOI 10.1016/S0034-4257(02)00018-4
   Hakala T, 2012, OPT EXPRESS, V20, P7119, DOI 10.1364/OE.20.007119
   Hosoi F, 2012, ISPRS J PHOTOGRAMM, V74, P11, DOI 10.1016/j.isprsjprs.2012.08.001
   Hu PL, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12060919
   Hu YB, 2020, PLANT PHYSIOL BIOCH, V154, P530, DOI 10.1016/j.plaphy.2020.06.053
   Huang HG, 2013, REMOTE SENS ENVIRON, V132, P221, DOI 10.1016/j.rse.2013.01.013
   Huang R, 2005, IEEE GEOSCI REMOTE S, V2, P156, DOI 10.1109/LGRS.2005.844658
   Humair F, 2015, EUR J REMOTE SENS, V48, P541, DOI 10.5721/EuJRS20154831
   Hunt ER, 2014, AGRON J, V106, P931, DOI 10.2134/agronj13.0322
   Jacquemoud S, 2009, REMOTE SENS ENVIRON, V113, PS56, DOI 10.1016/j.rse.2008.01.026
   Jung Y, 2018, J NONPARAMETR STAT, V30, P197, DOI 10.1080/10485252.2017.1404598
   Kaasalainen S., 2014, INT ARCH PHOTOGRAMM, V40, P109
   Kaasalainen S, 2007, IEEE GEOSCI REMOTE S, V4, P211, DOI 10.1109/LGRS.2006.888848
   Kaasalainen S, 2018, INTERFACE FOCUS, V8, P0, DOI 10.1098/rsfs.2017.0033
   Kim M.S., 1994, P 6 INT S PHYS MEASU, V0, P0
   Koetz B, 2007, REMOTE SENS ENVIRON, V106, P449, DOI 10.1016/j.rse.2006.09.013
   Li W, 2016, OPT EXPRESS, V24, P4771, DOI 10.1364/OE.24.004771
   Li XL, 2013, OPT ENG, V52, P0, DOI 10.1117/1.OE.52.11.116110
   Lunetta RS, 2006, REMOTE SENS ENVIRON, V105, P142, DOI 10.1016/j.rse.2006.06.018
   Magney TS, 2014, NEW PHYTOL, V201, P344, DOI 10.1111/nph.12453
   Merton R.N., 1999, P 8 ANN JPL AIRB EAR, V0, P0
   Mkhabela MS, 2011, AGR FOREST METEOROL, V151, P385, DOI 10.1016/j.agrformet.2010.11.012
   Naus J, 2010, PHOTOSYNTH RES, V105, P265, DOI 10.1007/s11120-010-9587-z
   Nevalainen O., 2013, REMOTE SENS SPAT INF, V2, P205, DOI 10.5194/ISPRSANNALS-II-5-W2-205-2013
   Nevalainen O, 2014, AGR FOREST METEOROL, V198, P250, DOI 10.1016/j.agrformet.2014.08.018
   Ni-Meister W, 2001, IEEE T GEOSCI REMOTE, V39, P1943, DOI 10.1109/36.951085
   Olfs HW, 2005, J PLANT NUTR SOIL SC, V168, P414, DOI 10.1002/jpln.200520526
   Peng S, 1996, FIELD CROP RES, V47, P243, DOI 10.1016/0378-4290(96)00018-4
   Reyniers M, 2006, INT J REMOTE SENS, V27, P4159, DOI 10.1080/01431160600791650
   Schlemmer M, 2013, INT J APPL EARTH OBS, V25, P47, DOI 10.1016/j.jag.2013.04.003
   Shi S, 2015, IEEE GEOSCI REMOTE S, V12, P1421, DOI 10.1109/LGRS.2015.2405573
   Sims DA, 2002, REMOTE SENS ENVIRON, V81, P337, DOI 10.1016/S0034-4257(02)00010-X
   Takashima T, 2004, PLANT CELL ENVIRON, V27, P1047, DOI 10.1111/j.1365-3040.2004.01209.x
   Tan K, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9080853
   Thenkabail PS, 2000, REMOTE SENS ENVIRON, V71, P158, DOI 10.1016/S0034-4257(99)00067-X
   Uddling J, 2007, PHOTOSYNTH RES, V91, P37, DOI 10.1007/s11120-006-9077-5
   Vincini M, 2008, PRECIS AGRIC, V9, P303, DOI 10.1007/s11119-008-9075-z
   Wagner W, 2008, INT J REMOTE SENS, V29, P1433, DOI 10.1080/01431160701736398
   Wagner W, 2010, ISPRS J PHOTOGRAMM, V65, P505, DOI 10.1016/j.isprsjprs.2010.06.007
   Wang B, 2016, MITOCHONDRIAL DNA A, V27, P2913, DOI 10.3109/19401736.2015.1060436
   Wang GY, 2014, CROP SCI, V54, P817, DOI 10.2135/cropsci2013.03.0160
   Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
   Woodhouse IH, 2011, IEEE GEOSCI REMOTE S, V8, P839, DOI 10.1109/LGRS.2011.2113312
   Xiong DL, 2015, SCI REP-UK, V5, P0, DOI 10.1038/srep13389
   Yang J, 2020, OPT EXPRESS, V28, P18728, DOI 10.1364/OE.395478
   Yegnanarayana B, 2009, ARTIFICIAL NEURAL NE, V0, P201
   Yl QX, 2007, ENVIRON SCI TECHNOL, V41, P6770, DOI 10.1021/es070144e
   Zhang CS, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12172855
   Zhang ZJ, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10020298
   Zhu X, 2015, ISPRS J PHOTOGRAMM, V110, P14, DOI 10.1016/j.isprsjprs.2015.10.001
NR 77
TC 8
Z9 8
U1 10
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2021
VL 14
IS 
BP 9667
EP 9679
DI 10.1109/JSTARS.2021.3111295
PG 13
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA WD3DB
UT WOS:000704824700002
DA 2023-04-26
ER

PT J
AU Feizizadeh, B
   Alajujeh, KM
   Lakes, T
   Blaschke, T
   Omarzadeh, D
AF Feizizadeh, Bakhtiar
   Alajujeh, Keyvan Mohammadzade
   Lakes, Tobia
   Blaschke, Thomas
   Omarzadeh, Davoud
TI A comparison of the integrated fuzzy object-based deep learning approach and three machine learning techniques for land use/cover change monitoring and environmental impacts assessment
SO GISCIENCE & REMOTE SENSING
LA English
DT Article
DE Integrated approach; fuzzy object-base; deep learning CNN; machine learning algorithms; spatial uncertainty; land use; cover; change detection; urmia lake
ID convolutional neural-network; urmia lake basin; image-analysis; cover classification; random forest; optimization; segmentation; uncertainty; landslides; extraction
AB Recent improvements in the spatial, temporal, and spectral resolution of satellite images necessitate (semi-)automated classification and information extraction approaches. Therefore, we developed an integrated fuzzy object-based image analysis and deep learning (FOBIA-DL) approach for monitoring the land use/cover (LULC) and respective changes and compared it to three machine learning (ML) algorithms, namely the support vector machine (SVM), random forest (RF), and classification and regression tree (CART). We investigated LULC impacts on drought by analyzing Landsat satellite images from 1990 to 2020 for the Urmia Lake area in northern Iran. In the FOBIA-DL approach, following the initial segmentation steps, object features were identified for each LULC class. We then derived their respective attributes using fuzzy membership functions and deep convolutional neural networks (DCNNs), a deep learning method. The Fuzzy Synthetic Evaluation and Dempster-Shafer Theory (FSE-DST) also applied to validate and carryout the spatial uncertainties. Our results indicate that the FOBIA-DL, with an accuracy of 90.1% to 96.4% and a spatial certainty of 0.93 to 0.97, outperformed the other approaches, closely followed by the SVM. Our results also showed that the integration of Fuzzy-OBIA and DCNNs could improve the strength and robustness of the OBIA's decision rules, while the FSE-DST approach notably improved the spatial accuracy of the object-based classification maps. While object-based image analysis (OBIA) is already considered a paradigm shift in GIScience, the integration of OBIA with fuzzy and deep learning creates more flexibility and robust OBIA decision rules for image analysis and classification. This research integrated popular data-driven approaches and developed a novel methodology for image classification and spatial accuracy assessment. From the environmental perspective, the results of this research support lake restoration initiatives by decision-makers and authorities in applications such as drought mitigation, land use management and precision agriculture programs.
C1 [Feizizadeh, Bakhtiar; Alajujeh, Keyvan Mohammadzade; Omarzadeh, Davoud] Univ Tabriz, Dept Remote Sensing & GIS, Tabriz, Iran.
   [Feizizadeh, Bakhtiar; Lakes, Tobia] Humboldt Univ, Dept Geog, Lab Geoinformat Sci, Berlin, Germany.
   [Blaschke, Thomas] Univ Salzburg, Dept Geoinformat Z GIS, Salzburg, Austria.
C3 University of Tabriz; Humboldt University of Berlin; Salzburg University
RP Feizizadeh, B (corresponding author), Univ Tabriz, Dept Remote Sensing & GIS, Tabriz, Iran.; Feizizadeh, B (corresponding author), Humboldt Univ, Dept Geog, Lab Geoinformat Sci, Berlin, Germany.
EM Feizizadeh@Tabrizu.ac.ir
FU University of Tabriz [s818]; Alexander von Humboldt Foundation
CR Abedi Gheshlaghi H., 2021, GIS BASED ENSEMBLE M, V0, P0, DOI DOI 10.1007/s11069-021-04673-1
   AghaKouchak A, 2015, J GREAT LAKES RES, V41, P307, DOI 10.1016/j.jglr.2014.12.007
   Aksoy B, 2012, COMPUT GEOSCI-UK, V38, P87, DOI 10.1016/j.cageo.2011.05.010
   ALHASSAN V, 2019, NEURAL COMPUTING APP, V0, P0
   [Anonymous], 1994, REMOTE SENSING IMAGE, V0, P0
   Araki S, 2018, SCI TOTAL ENVIRON, V634, P1269, DOI 10.1016/j.scitotenv.2018.03.324
   ARONOFF S, 1985, PHOTOGRAMM ENG REM S, V51, P99
   BAATZ M., 2004, ECOGNITION PROFESSIO, V0, P0
   Balkanlou KR, 2020, SCI TOTAL ENVIRON, V716, P0, DOI 10.1016/j.scitotenv.2020.137100
   Baraldi P, 2010, RISK ANAL, V30, P1139, DOI 10.1111/j.1539-6924.2010.01416.x
   Betts MG, 2017, NATURE, V547, P441, DOI 10.1038/nature23285
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004
   Blaschke T., 2015, REMOTELY SENSED DATA, V0, P0
   Blaschke T, 2014, IEEE J-STARS, V7, P4806, DOI 10.1109/JSTARS.2014.2350036
   Blaschke T, 2014, ISPRS J PHOTOGRAMM, V87, P180, DOI 10.1016/j.isprsjprs.2013.09.014
   BUI DT, 2020, MATH PROBL ENG, V0, P0
   Cai YP, 2018, REMOTE SENS ENVIRON, V210, P35, DOI 10.1016/j.rse.2018.02.045
   Chartrand G, 2017, RADIOGRAPHICS, V37, P2113, DOI 10.1148/rg.2017170077
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Comber A, 2012, REMOTE SENS ENVIRON, V127, P237, DOI 10.1016/j.rse.2012.09.005
   Cresson R., 2018, IEEE GEOSCI REMOTE S, V16, P1
   Das M, 2016, IEEE GEOSCI REMOTE S, V13, P1984, DOI 10.1109/LGRS.2016.2619984
   Delju AH, 2013, THEOR APPL CLIMATOL, V111, P285, DOI 10.1007/s00704-012-0651-9
   Doyle C, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13030379
   Dragut L, 2010, INT J GEOGR INF SCI, V24, P859, DOI 10.1080/13658810903174803
   Dutta D, 2019, ENVIRON MONIT ASSESS, V191, P0, DOI 10.1007/s10661-019-7645-3
   Ebrahimy H, 2021, APPL SCI-BASEL, V11, P0, DOI 10.3390/app112110309
   Eisank C, 2014, GEOMORPHOLOGY, V214, P452, DOI 10.1016/j.geomorph.2014.02.028
   FEIZIZADEH B, 2021, J ENVIRON PLANN MAN, V0, P0
   Feizizadeh B, 2021, CATENA, V198, P0, DOI 10.1016/j.catena.2020.105073
   Feizizadeh B, 2019, CAN J REMOTE SENS, V45, P847, DOI 10.1080/07038992.2019.1704622
   Feizizadeh B, 2018, IEEE GEOSCI REMOTE S, V15, P18, DOI 10.1109/LGRS.2017.2763979
   Feizizadeh B, 2017, GEOMORPHOLOGY, V293, P240, DOI 10.1016/j.geomorph.2017.06.002
   Feizizadeh B, 2017, J ENVIRON PLANN MAN, V60, P2013, DOI 10.1080/09640568.2016.1269643
   Feizizadeh B, 2014, INT J GEOGR INF SCI, V28, P610, DOI 10.1080/13658816.2013.869821
   Foody G.M., 2006, 7 INT S SPAT ACC ASS, V0, PP18, DOI 10.1016/J.JHSB.2006.07.007
   Foody GM, 2002, REMOTE SENS ENVIRON, V80, P185, DOI 10.1016/S0034-4257(01)00295-4
   Garajeh MK, 2021, SCI TOTAL ENVIRON, V778, P0, DOI 10.1016/j.scitotenv.2021.146253
   Ghasemi M, 2021, EARTH SCI INFORM, V14, P1745, DOI 10.1007/s12145-021-00617-2
   Ghorbanzadeh O, 2021, EUR J REMOTE SENS, V54, P127, DOI 10.1080/22797254.2020.1759456
   Ghorbanzadeh O, 2019, J ECOTOURISM, V18, P261, DOI 10.1080/14724049.2019.1597876
   Ghorbanzadeh O, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11020196
   Guirado E, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9121220
   Hamidi-Razi H, 2019, J GREAT LAKES RES, V45, P87, DOI 10.1016/j.jglr.2018.10.002
   Henry CJ, 2019, INT J REMOTE SENS, V40, P4416, DOI 10.1080/01431161.2018.1563840
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hofmann P, 2011, INT J REMOTE SENS, V32, P7359, DOI 10.1080/01431161.2010.523727
   Huang B, 2018, REMOTE SENS ENVIRON, V214, P73, DOI 10.1016/j.rse.2018.04.050
   JANSSEN LLF, 1994, PHOTOGRAMM ENG REM S, V60, P419
   Kamran KV, 2021, APPL GEOMAT, V13, P837, DOI 10.1007/s12518-021-00393-0
   Kassouk Z, 2014, GEOMORPHOLOGY, V221, P18, DOI 10.1016/j.geomorph.2014.04.022
   Kucharczyk M, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12122012
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128
   Lang S., 2010, INT ARCH PHOTOGRAM R, V38, P4
   LEES B, 2006, APPL GIS, V2, P0
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu ZG, 2017, IEEE T SYST MAN CY-S, V47, P2783, DOI 10.1109/TSMC.2016.2622247
   Liu ZG, 2015, KNOWL-BASED SYST, V74, P119, DOI 10.1016/j.knosys.2014.11.013
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015
   Ma L, 2017, ISPRS INT J GEO-INF, V6, P0, DOI 10.3390/ijgi6020051
   Maboudi M, 2018, ISPRS J PHOTOGRAMM, V138, P151, DOI 10.1016/j.isprsjprs.2017.11.014
   Mardi AH, 2018, SCI TOTAL ENVIRON, V633, P42, DOI 10.1016/j.scitotenv.2018.03.148
   Marmanis D, 2016, IEEE GEOSCI REMOTE S, V13, P105, DOI 10.1109/LGRS.2015.2499239
   Mas JF, 2017, EUR J REMOTE SENS, V50, P626, DOI 10.1080/22797254.2017.1387505
   Maxwell AE, 2018, INT J REMOTE SENS, V39, P2784, DOI 10.1080/01431161.2018.1433343
   Moradpour H, 2022, GEOCARTO INT, V37, P1971, DOI 10.1080/10106049.2020.1810327
   Naboureh A, 2017, ARAB J GEOSCI, V10, P0, DOI 10.1007/s12517-017-3012-2
   Najafi P, 2018, INT J REMOTE SENS, V39, P6117, DOI 10.1080/01431161.2018.1454621
   Najafi P, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13050937
   Najafi P, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11212583
   Namatevs I., 2018, INFORM TECHNOL MANAG, V20, P40, DOI 10.1515/ITMS-2017-0007
   Hoan NT, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10121965
   Nitze I, 2015, INT J APPL EARTH OBS, V34, P136, DOI 10.1016/j.jag.2014.08.001
   OMARZADEH D, 2021, J ENVIRON PLANN MAN, V0, P0
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sarmento P, 2008, PROCEEDINGS OF THE 8TH INTERNATIONAL SYMPOSIUM ON SPATIAL ACCURACY ASSESSMENT IN NATURAL RESOURCES AND ENVIRONMENTAL SCIENCES, VOL I, P348
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sharma A, 2017, NEURAL NETWORKS, V95, P19, DOI 10.1016/j.neunet.2017.07.017
   Shokati B, 2019, J ENVIRON PLANN MAN, V62, P517, DOI 10.1080/09640568.2018.1427561
   Singh P, 2016, ADV INTELL SYST, V434, P551, DOI 10.1007/978-81-322-2752-6_54
   Sudmanns M, 2020, INT J DIGIT EARTH, V13, P832, DOI 10.1080/17538947.2019.1585976
   Talukdar S, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12071135
   Tong X.-Y., 2018, ARXIV180705713, V0, P0
   Tsagkatakis G, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19183929
   Vetrivel A, 2018, ISPRS J PHOTOGRAMM, V140, P45, DOI 10.1016/j.isprsjprs.2017.03.001
   Vieira S, 2017, NEUROSCI BIOBEHAV R, V74, P58, DOI 10.1016/j.neubiorev.2017.01.002
   Woznicki SA, 2019, SCI TOTAL ENVIRON, V647, P942, DOI 10.1016/j.scitotenv.2018.07.353
   Xia M, 2020, INT J REMOTE SENS, V41, P7779, DOI 10.1080/01431161.2020.1763511
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9
   Yu XR, 2017, GISCI REMOTE SENS, V54, P741, DOI 10.1080/15481603.2017.1323377
   Zhao WZ, 2017, ISPRS J PHOTOGRAMM, V132, P48, DOI 10.1016/j.isprsjprs.2017.08.011
   Zhou W, 2008, INT J REMOTE SENS, V29, P3119, DOI 10.1080/01431160701469065
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
NR 97
TC 11
Z9 11
U1 6
U2 27
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 1548-1603
EI 1943-7226
J9 GISCI REMOTE SENS
JI GISci. Remote Sens.
PD NOV 17
PY 2021
VL 58
IS 8
BP 1543
EP 1570
DI 10.1080/15481603.2021.2000350
EA DEC 2021
PG 28
WC Geography, Physical; Remote Sensing
SC Physical Geography; Remote Sensing
GA XN8NW
UT WOS:000724069000001
DA 2023-04-26
ER
