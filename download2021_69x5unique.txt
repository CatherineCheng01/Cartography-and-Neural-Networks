
PT J
AU Liu, B
   Yan, S
   Li, JQ
   Li, Y
   Lang, JL
   Qu, GZ
AF Liu, Bo
   Yan, Shuo
   Li, Jianqiang
   Li, Yong
   Lang, Jianlei
   Qu, Guangzhi
TI A Spatiotemporal Recurrent Neural Network for Prediction of Atmospheric PM2.5: A Case Study of Beijing
SO IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS
LA English
DT Article
DE Air quality; environment pollution; prediction; recurrent neural network (RNN); spatiotemporal sequences
ID air-quality; memory; model
AB With rapid industrial development, air pollution problems, especially in urban and metropolitan centers, have become a serious societal problem and require our immediate attention and comprehensive solutions to protect human and animal health and the environment. Because bad air quality brings prominent effects on our daily life, how to forecast future air quality accurately and tenuously has emerged as a priority for guaranteeing the quality of human life in many urban areas worldwide. Existing models usually neglect the influence of wind and do not consider both distance and similarity to select the most related stations, which can provide significant information in prediction. Therefore, we propose a Geographic Self-Organizing Map (GeoSOM) spatiotemporal gated recurrent unit (GRU) model, which clusters all the monitor stations into several clusters by geographical coordinates and time-series features. For each cluster, we build a GRU model and weighted different models with the Gaussian vector weights to predict the target sequence. The experimental results on real air quality data in Beijing validate the superiority of the proposed method over a number of state-of-the-art ones in metrics, such as R-2, mean relative error (MRE), and mean absolute error (MAE). The MAE, MRE, and R-2 are 16.1, 0.79, and 035 at the Gucheng station and 19.53, 0.82, and 036 at the Dongsi station.
C1 [Liu, Bo; Li, Jianqiang; Li, Yong] Beijing Univ Technol, Fac Informat Technol, Sch Software Engn, Beijing, Peoples R China.
   [Liu, Bo] Univ Auckland, Sch Comp Sci, Beijing 1010, Peoples R China.
   [Yan, Shuo] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Lang, Jianlei] Beijing Univ Technol, Coll Environm & Energy Engn, Key Lab Beijing Reg Air Pollut Control, Beijing 100124, Peoples R China.
   [Qu, Guangzhi] Oakland Univ, Comp Sci & Engn Dept, Rochester, MI 48309 USA.
C3 Beijing University of Technology; Chinese Academy of Sciences; Institute of Automation, CAS; Beijing University of Technology; Oakland University
RP Yan, S (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM boliu@bjut.edu.cn; yanshuok@163.com; lijianqiang@bjut.edu.cn; li.yong@bjut.edu.cn; jllang@bjut.edu.cn; gqu@oakland.edu
FU National Natural Science Foundation of China [61702021]; Beijing Natural Science Foundation [4174082]; China Scholarship Council
CR [Anonymous], 2000, QUANTITATIVE GEOGRAP, V0, P0
   Athira V., 2018, INT C COMPUTATIONAL, V132, P1394
   Bao G, 2020, IEEE-CAA J AUTOMATIC, V7, P96, DOI 10.1109/JAS.2019.1911828
   Bi J, 2019, IEEE T AUTOM SCI ENG, V16, P1763, DOI 10.1109/TASE.2019.2895801
   Bui T., 2018, ARXIV180407891, V0, P0
   Chaudhary V, 2018, UDM, V0, P0
   Chen L, 2019, IEEE-CAA J AUTOMATIC, V6, P236, DOI 10.1109/JAS.2018.7511186
   Deng M, 2013, SCI CHINA INFORM SCI, V56, P0, DOI 10.1007/s11432-011-4391-8
   Dey R, 2017, MIDWEST SYMP CIRCUIT, V0, PP1597, DOI 10.1109/MWSCAS.2017.8053243
   Donnelly A, 2015, ATMOS ENVIRON, V103, P53, DOI 10.1016/j.atmosenv.2014.12.011
   Du S., 2018, ARXIV181204783, V0, P0
   Feng F, 2018, LECT NOTES COMPUT SC, V10987, P349, DOI 10.1007/978-3-319-96890-2_29
   Gao SC, 2019, IEEE T NEUR NET LEAR, V30, P601, DOI 10.1109/TNNLS.2018.2846646
   Henriques R, 2010, SECOND INTERNATIONAL CONFERENCE ON ADVANCED GEOGRAPHIC INFORMATION SYSTEMS, V0, P0
   Henriques R, 2009, LECT NOTES COMPUT SC, V5592, P453, DOI 10.1007/978-3-642-02454-2_32
   Huang CJ, 2018, SENSORS-BASEL, V18, P0, DOI 10.3390/s18072220
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kolehmainen M, 2001, ATMOS ENVIRON, V35, P815, DOI 10.1016/S1352-2310(00)00385-X
   Lal B, 2012, ATMOS POLLUT RES, V3, P211, DOI 10.5094/APR.2012.023
   LeCun Y., 1995, HDB BRAIN THEORY NEU, V3361, P0, DOI 10.5555/303568.303704
   Li CD, 2019, IEEE-CAA J AUTOMATIC, V6, P1487, DOI 10.1109/JAS.2019.1911543
   Liang YX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, V0, P3428
   Liu DR, 2021, COMPUTING, V103, P75, DOI 10.1007/s00607-020-00849-y
   Nejadkoorki F, 2012, INT J ENVIRON RES, V6, P277
   Ong BT, 2014, IEEE INT CONF BIG DA, V0, PP760, DOI 10.1109/BigData.2014.7004302
   Pardo E, 2017, LECT NOTES COMPUT SC, V10338, P232, DOI 10.1007/978-3-319-59773-7_24
   Pascanu R., 2013, ARXIV12115063, V0, P1310
   Reddy V., 2018, ENVIRON SCI-TOKYO, V0, P0
   Russo A, 2013, ATMOS ENVIRON, V79, P822, DOI 10.1016/j.atmosenv.2013.07.072
   Shi ZC, 2019, ISPRS INT J GEO-INF, V8, P0, DOI 10.3390/ijgi8030112
   Soh PW, 2018, IEEE ACCESS, V6, P38186, DOI 10.1109/ACCESS.2018.2849820
   Sutskever I., 2014, ADV NEURAL INF PROCE, V2, P3104, DOI 10.48550/ARXIV.1409.3215
   Vardoulakis S, 2003, ATMOS ENVIRON, V37, P155, DOI 10.1016/S1352-2310(02)00857-9
   Vaswani A, 2017, ADV NEUR IN, V30, P0
   Wang B, 2018, LECT NOTES COMPUT SC, V11305, P93, DOI 10.1007/978-3-030-04221-9_9
   Wang F, 2017, IEEE T INTELL TRANSP, V18, P49, DOI 10.1109/TITS.2016.2521866
   Wang H., 2018, ARXIV180903964, V0, P0
   Wang JS, 2018, NEUROCOMPUTING, V314, P198, DOI 10.1016/j.neucom.2018.06.049
   Yang R., 2019, P 3 HIGH PERF COMP C, V0, P108
   Yi XW, 2018, KDD18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, V0, PP965, DOI 10.1145/3219819.3219822
   Zhang Y, 2012, ATMOS ENVIRON, V60, P632, DOI 10.1016/j.atmosenv.2012.06.031
   Zhao JC, 2019, CHEMOSPHERE, V220, P486, DOI 10.1016/j.chemosphere.2018.12.128
   Zhao XD, 2020, IEEE-CAA J AUTOMATIC, V7, P965, DOI 10.1109/JAS.2020.1003228
   Zheng Y, 2015, KDD15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, V0, PP2267, DOI 10.1145/2783258.2788573
   Zhu JY, 2017, IEEE T BIG DATA, V3, P307, DOI 10.1109/TBDATA.2017.2651898
NR 45
TC 5
Z9 5
U1 4
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-924X
EI 
J9 IEEE T COMPUT SOC SY
JI IEEE Trans. Comput. Soc. Syst.
PD JUN 15
PY 2021
VL 8
IS 3
BP 578
EP 588
DI 10.1109/TCSS.2021.3056410
PG 11
WC Computer Science, Cybernetics; Computer Science, Information Systems
SC Computer Science
GA SJ9CB
UT WOS:000655822700005
DA 2023-04-26
ER

PT J
AU Wagatsuma, N
   Hidaka, A
   Tamura, H
AF Wagatsuma, Nobuhiko
   Hidaka, Akinori
   Tamura, Hiroshi
TI Correspondence between monkey visual cortices and layers of a saliency map model based on a deep convolutional neural network for representations of natural images
SO ENEURO
LA English
DT Article
ID classical receptive-field; inferior temporal cortex; v1 mechanisms; contextual influences; contour integration; border ownership; areas v1; attention; organization; suppression
AB Attentional selection is a function that allocates the brain's computational resources to the most important part of a visual scene at a specific moment. Saliency map models have been proposed as computational models to predict attentional selection within a spatial location. Recent saliency map models based on deep convolutional neural networks (DCNNs) exhibit the highest performance for predicting the location of attentional selection and human gaze, which reflect overt attention. Trained DCNNs potentially provide insight into the perceptual mechanisms of biological visual systems. However, the relationship between artificial and neural representations used for determining attentional selection and gaze location remains unknown. To understand the mechanism underlying saliency map models based on DCNNs and the neural system of attentional selection, we investigated the correspondence between layers of a DCNN saliency map model and monkey visual areas for natural image representations. We compared the characteristics of the responses in each layer of the model with those of the neural representation in the primary visual (V1), intermediate visual (V4), and inferior temporal cortices. Regardless of the DCNN layer level, the characteristics of the responses were consistent with that of the neural representation in V1. We found marked peaks of correspondence between V1 and the early level and higher-intermediate-level layers of the model. These results provide insight into the mechanism of the trained DCNN saliency map model and suggest that the neural representations in V1 play an important role in computing the saliency that mediates attentional selection, which supports the V1 saliency hypothesis.
C1 [Wagatsuma, Nobuhiko] Toho Univ, Fac Sci, Miyama 2-2-1, Funabashi, Chiba 2748510, Japan.
   [Hidaka, Akinori] Tokyo Denki Univ, Sch Sci & Engn, Tokyo, Japan.
   [Tamura, Hiroshi] Osaka Univ, Grad Sch Frontiers Biosci, Suita, Osaka, Japan.
   [Tamura, Hiroshi] Ctr Informat & Neural Networks CiNet, Osaka, Japan.
C3 Toho University; Tokyo Denki University; Osaka University; National Institute of Information & Communications Technology (NICT) - Japan
RP Wagatsuma, N (corresponding author), Toho Univ, Fac Sci, Miyama 2-2-1, Funabashi, Chiba 2748510, Japan.
EM nwagatsuma@is.sci.toho-u.ac.jp
CR Adesnik H, 2012, NATURE, V490, P226, DOI 10.1038/nature11526
   ALLMAN J, 1985, ANNU REV NEUROSCI, V8, P407, DOI 10.1146/annurev.ne.08.030185.002203
   Borji A., 2015, P IEEE C COMP VIS PA, V0, P0
   Bruce NDB, 2009, J VISION, V9, P0, DOI 10.1167/9.3.5
   Bylinskii Z, 2015, VISION RES, V116, P165, DOI 10.1016/j.visres.2015.03.005
   Carrasco M, 2011, VISION RES, V51, P1484, DOI 10.1016/j.visres.2011.04.012
   Chen G, 2017, NEURON, V96, P1403, DOI 10.1016/j.neuron.2017.11.033
   Craft E, 2007, J NEUROPHYSIOL, V97, P4310, DOI 10.1152/jn.00203.2007
   Deco G, 2004, EUR J NEUROSCI, V20, P1089, DOI 10.1111/j.1460-9568.2004.03528.x
   Geirhos R., 2019, INT C LEARNING REPRE, V0, P0
   Goda N, 2014, J NEUROSCI, V34, P2660, DOI 10.1523/JNEUROSCI.2593-13.2014
   Green DM., 1966, SIGNAL DETECTION THE, V0, P0
   Haxby JV, 2011, NEURON, V72, P404, DOI 10.1016/j.neuron.2011.08.026
   Hiramatsu C, 2011, NEUROIMAGE, V57, P482, DOI 10.1016/j.neuroimage.2011.04.056
   Hu B, 2017, J COMPUT NEUROSCI, V43, P227, DOI 10.1007/s10827-017-0659-3
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Jiang M, 2015, PROC CVPR IEEE, V0, PP1072, DOI 10.1109/CVPR.2015.7298710
   Jingling L, 2008, PERCEPTION, V37, P197, DOI 10.1068/p5829
   Jones HE, 2002, J NEUROPHYSIOL, V88, P2796, DOI 10.1152/jn.00403.2001
   Jones HE, 2001, J NEUROPHYSIOL, V86, P2011, DOI 10.1152/jn.2001.86.4.2011
   Judd T, 2009, IEEE I CONF COMP VIS, V0, PP2106, DOI 10.1109/ICCV.2009.5459462
   Kaneko H, 1999, IEEE T BIO-MED ENG, V46, P280, DOI 10.1109/10.748981
   Kaneko H, 2007, IEEE T BIO-MED ENG, V54, P262, DOI 10.1109/TBME.2006.886934
   Kiani R, 2007, J NEUROPHYSIOL, V97, P4296, DOI 10.1152/jn.00024.2007
   Kiimmerer M., 2014, ARXIV14111045, V0, P0
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   KNIERIM JJ, 1992, J NEUROPHYSIOL, V67, P961, DOI 10.1152/jn.1992.67.4.961
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Koene AR, 2007, J VISION, V7, P0, DOI 10.1167/7.7.6
   Kriegeskorte N, 2008, NEURON, V60, P1126, DOI 10.1016/j.neuron.2008.10.043
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Kummerer M, 2017, IEEE I CONF COMP VIS, V0, PP4799, DOI 10.1109/ICCV.2017.513
   Le Q. V., 2012, P INT C MACH LEARN I, V0, P8595
   Lee DK, 1999, NAT NEUROSCI, V2, P375, DOI 10.1038/7286
   LI CY, 1994, VISION RES, V34, P2337, DOI 10.1016/0042-6989(94)90280-1
   Zhaoping L, 2019, CURR OPIN NEUROBIOL, V58, P1, DOI 10.1016/j.conb.2019.06.001
   Li ZP, 1999, P NATL ACAD SCI USA, V96, P10530, DOI 10.1073/pnas.96.18.10530
   Li ZP, 2002, TRENDS COGN SCI, V6, P9, DOI 10.1016/S1364-6613(00)01817-9
   Li ZP, 2000, ADV NEUR IN, V12, P136
   Li ZP, 1999, NETWORK-COMP NEURAL, V10, P187, DOI 10.1088/0954-898X/10/2/305
   Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Mahendran A, 2015, PROC CVPR IEEE, V0, PP5188, DOI 10.1109/CVPR.2015.7299155
   Martin AB, 2015, J NEUROSCI, V35, P6860, DOI 10.1523/JNEUROSCI.3590-14.2015
   Mihalas S, 2011, P NATL ACAD SCI USA, V108, P7583, DOI 10.1073/pnas.1014655108
   Nair V, 2010, ICML, V27, P807
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ozeki H, 2009, NEURON, V62, P578, DOI 10.1016/j.neuron.2009.03.028
   Pan J., 2017, CVPR SCEN UND WORKSH, V0, P0
   Pan JT, 2016, PROC CVPR IEEE, V0, PP598, DOI 10.1109/CVPR.2016.71
   Pasupathy A, 2001, J NEUROPHYSIOL, V86, P2505, DOI 10.1152/jn.2001.86.5.2505
   Poort J, 2016, CEREB CORTEX, V26, P3964, DOI 10.1093/cercor/bhw235
   Poort J, 2012, NEURON, V75, P143, DOI 10.1016/j.neuron.2012.04.032
   POSNER MI, 1980, Q J EXP PSYCHOL, V32, P3, DOI 10.1080/00335558008248231
   Pospisil DA, 2018, ELIFE, V7, P0, DOI 10.7554/eLife.38242
   Qiu FTT, 2007, NAT NEUROSCI, V10, P1492, DOI 10.1038/nn1989
   Rolls E. T., 2002, COMPUTATIONAL NEUROS, V0, P0
   Russell AF, 2014, VISION RES, V94, P1, DOI 10.1016/j.visres.2013.10.005
   Sakai K, 2006, J COGNITIVE NEUROSCI, V18, P562, DOI 10.1162/jocn.2006.18.4.562
   Sakai K, 2012, NEURAL NETWORKS, V33, P257, DOI 10.1016/j.neunet.2012.05.006
   Simonyan K, 2015, ARXIV, V0, P0
   Tamura H, 2001, CEREB CORTEX, V11, P384, DOI 10.1093/cercor/11.5.384
   Tamura H, 2016, NEURONS INFERIOR TEM, V0, P0
   Tamura H, 2014, J NEUROPHYSIOL, V111, P2589, DOI 10.1152/jn.00336.2013
   Tokui T, 2015, P WORKSH MACH LEARN, V0, P0
   Uejima T, 2020, FRONT COMPUT NEUROSC, V14, P0, DOI 10.3389/fncom.2020.541581
   Wagatsuma N, 2019, NEURAL NETWORKS, V110, P33, DOI 10.1016/j.neunet.2018.10.015
   Wagatsuma N, 2016, J NEUROPHYSIOL, V116, P1418, DOI 10.1152/jn.01142.2015
   WURTZ RH, 1969, J NEUROPHYSIOL, V32, P727, DOI 10.1152/jn.1969.32.5.727
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yan Y, 2018, P NATL ACAD SCI USA, V115, P10499, DOI 10.1073/pnas.1803854115
   Yang TX, 2018, PLOS ONE, V13, P0, DOI 10.1371/journal.pone.0203024
   Zeiler M.D., 2013, P EUR C COMP VIS, V0, P0, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhang XL, 2012, NEURON, V73, P183, DOI 10.1016/j.neuron.2011.10.035
   Zhaoping L, 2003, J PHYSIOL-PARIS, V97, P503, DOI 10.1016/j.jphysparis.2004.01.008
   Zhaoping L, 2014, UNDERSTANDING VISION, V0, P189
   Zhaoping L, 2015, PLOS COMPUT BIOL, V11, P0, DOI 10.1371/journal.pcbi.1004375
   Zhou H, 2000, J NEUROSCI, V20, P6594, DOI 10.1523/JNEUROSCI.20-17-06594.2000
NR 81
TC 4
Z9 4
U1 1
U2 12
PU SOC NEUROSCIENCE
PI WASHINGTON
PA 11 DUPONT CIRCLE, NW, STE 500, WASHINGTON, DC 20036 USA
SN 
EI 2373-2822
J9 ENEURO
JI eNeuro
PD JAN-FEB 15
PY 2021
VL 8
IS 1
BP 
EP 
DI 10.1523/ENEURO.0200-20.2020
PG 68
WC Neurosciences
SC Neurosciences & Neurology
GA UK2DH
UT WOS:000691785000002
PM 33234544
DA 2023-04-26
ER

PT J
AU Fyleris, T
   Krisciunas, A
   Gruzauskas, V
   Calneryte, D
AF Fyleris, Tautvydas
   Krisciunas, Andrius
   Gruzauskas, Valentas
   Calneryte, Dalia
TI Deep Learning Application for Urban Change Detection from Aerial Images
SO PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON GEOGRAPHICAL INFORMATION SYSTEMS THEORY, APPLICATIONS AND MANAGEMENT (GISTAM)
LA English
DT Proceedings Paper
DE Urban Change; Aerial Images; Deep Learning
ID semantic segmentation; satellite images; extraction; tracking
AB Urban growth estimation is an essential part of urban planning in order to ensure sustainable regional development. For such purpose, analysis of remote sensing data can be used. The difficulty in analysing a time series of remote sensing data lies in ensuring that the accuracy stays stable in different periods. In this publication, aerial images were analysed for three periods, which lasted for 9 years. The main issues arose due to the different quality of images, which lead to bias between periods. Consequently, this results in difficulties in interpreting whether the urban growth actually happened, or it was identified due to the incorrect segmentation of images. To overcome this issue, datasets were generated to train the convolutional neural network (CNN) and transfer learning technique has been applied. Finally, the results obtained with the created CNN of different periods enable to implement different approaches to detect, analyse and interpret urban changes for the policymakers and investors on different levels as a map, grid, or contour map.
C1 [Fyleris, Tautvydas] Kaunas Univ Technol, Fac Informat, Dept Software Engn, Kaunas, Lithuania.
   [Krisciunas, Andrius; Calneryte, Dalia] Kaunas Univ Technol, Fac Informat, Dept Appl Informat, Kaunas, Lithuania.
   [Gruzauskas, Valentas] Kaunas Univ Technol, Sch Econ & Business, Sustainable Management Res Grp, Kaunas, Lithuania.
C3 Kaunas University of Technology; Kaunas University of Technology; Kaunas University of Technology
RP Fyleris, T (corresponding author), Kaunas Univ Technol, Fac Informat, Dept Software Engn, Kaunas, Lithuania.
FU Research, Development and Innovation Fund of Kaunas University of Technology [PP91L/19]
CR Al-Ruzouq R, 2017, ANN GIS, V23, P183, DOI 10.1080/19475683.2017.1325935
   Albert A, 2017, KDD17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, V0, PP1357, DOI 10.1145/3097983.3098070
   Celik T, 2009, IEEE GEOSCI REMOTE S, V6, P772, DOI 10.1109/LGRS.2009.2025059
   Corbane C., 2020, ARXIV, V0, P0
   de Jong K. L., 2019, 2019 INT JOINT C NEU, V0, P1
   Donaldson D, 2016, J ECON PERSPECT, V30, P171, DOI 10.1257/jep.30.4.171
   Dornaika F, 2016, EXPERT SYST APPL, V58, P130, DOI 10.1016/j.eswa.2016.03.024
   Goyette N., 2012, 2012 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPR WORKSHOPS), V0, P0, DOI DOI 10.1109/CVPRW.2012.6238919
   Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894
   Kanagamalliga S, 2018, OPTIK, V157, P787, DOI 10.1016/j.ijleo.2017.11.181
   Krupinski M, 2019, PROC SPIE, V11176, P0, DOI 10.1117/12.2535547
   Langkvist M, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8040329
   Liu YS, 2020, IEEE T GEOSCI REMOTE, V58, P6699, DOI 10.1109/TGRS.2020.2979011
   Marmanis D, 2016, ISPRS ANN PHOTO REM, V3, P473, DOI 10.5194/isprsannals-III-3-473-2016
   Nahhas F. H., 2018, DEEP LEARNING APPROA, V2018, P0
   Shermeyer J, 2019, IEEE COMPUT SOC CONF, V0, PP1432, DOI 10.1109/CVPRW.2019.00184
   Suraj P. K., 2018, MONITORING DEV INDIC, V0, P1
   Vakalopoulou M, 2015, INT GEOSCI REMOTE SE, V0, PP1873, DOI 10.1109/IGARSS.2015.7326158
   Verbesselt J, 2012, REMOTE SENS ENVIRON, V123, P98, DOI 10.1016/j.rse.2012.02.022
   Verbesselt J, 2010, REMOTE SENS ENVIRON, V114, P106, DOI 10.1016/j.rse.2009.08.014
   Wang B, 2019, J VIS COMMUN IMAGE R, V58, P102, DOI 10.1016/j.jvcir.2018.11.014
   Wang J, 2015, INT J REMOTE SENS, V36, P3144, DOI 10.1080/01431161.2015.1054049
   Witwit W, 2017, J ELECTRON IMAGING, V26, P0, DOI 10.1117/1.JEI.26.2.023014
   Wu GM, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10030407
   Wurm M, 2019, ISPRS J PHOTOGRAMM, V150, P59, DOI 10.1016/j.isprsjprs.2019.02.006
   Xie M, 2016, AAAI CONF ARTIF INTE, V0, P3929
   Ye ZR, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11242970
NR 27
TC 3
Z9 3
U1 1
U2 3
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
SN 
EI 
J9 
PD JUN 15
PY 2021
VL 0
IS 
BP 15
EP 24
DI 10.5220/0010415700150024
PG 10
WC Computer Science, Information Systems; Geography, Physical
SC Computer Science; Physical Geography
GA BT3MY
UT WOS:000821066300001
DA 2023-04-26
ER

PT J
AU Bickel, VT
   Mandrake, L
   Doran, G
AF Bickel, Valentin T.
   Mandrake, Lukas
   Doran, Gary
TI Analyzing multi-domain learning for enhanced rockfall mapping in known and unknown planetary domains
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Rockfall; Transfer learning; Domain adaptation; Moon; Mars; Ceres
AB Rockfalls are small-scale mass wasting events that have been observed across the solar system. They provide valuable information about the endo- and exogenic activity of their host body, but are difficult to identify and map in satellite imagery, especially on global scales and in big data sets. Past work implemented convolutional neural networks to automate rockfall mapping on the Moon and Mars with the caveat of (1) achieving sub-optimal performance and (2) requiring substantial manual image labeling efforts. Mixing annotated image data from the Moon and Mars while keeping the total number of labels constant, we show that including a small number (10%) of rockfall labels from a foreign domain (e.g. Moon) during detector training can increase performance in the home domain (e.g. Mars) by up to 6% Average Precision (AP) in comparison to a purely home domain-trained detector. We additionally show that using a large number of foreign domain training examples (90%) in combination with a small number (10%) of home domain labels can be as powerful or more powerful as exclusively (100%) using home labels in the home domain. We further observe that rockfall detectors trained on multiple domains outperform single-domain trained detectors in completely unknown domains by up to 16% AP, using image data from Ceres and comet 67P. We conduct an experiment varying only image resolution on a single planetary body (Mars) to test whether the improvement was due to training on differing resolutions specifically and show that none of the improvement can be explained by this effect alone. This means that the benefits of multi-domain training mostly draw from either variations in lighting condition, differing physical appearance/backgrounds around the target of interest for generalization purposes, or both. Our findings have important applications such as machine learning-enabled science discovery in legacy and new planetary datasets.
C1 [Bickel, Valentin T.] MPI Solar Syst Res, Justus Von Liebig Weg 3, D-37077 Gottingen, Lower Saxony, Germany.
   [Bickel, Valentin T.; Mandrake, Lukas; Doran, Gary] NASA, Jet Prop Lab, 4800 Oak Grove Dr, Pasadena, CA 91109 USA.
C3 National Aeronautics & Space Administration (NASA); NASA Jet Propulsion Laboratory (JPL)
RP Bickel, VT (corresponding author), MPI Solar Syst Res, Justus Von Liebig Weg 3, D-37077 Gottingen, Lower Saxony, Germany.; Bickel, VT (corresponding author), NASA, Jet Prop Lab, 4800 Oak Grove Dr, Pasadena, CA 91109 USA.
EM bickel@mps.mpg.de
FU IFI programme of the German Academic Exchange Service (DAAD)
CR Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Benedix GK, 2020, EARTH SPACE SCI, V7, P0, DOI 10.1029/2019EA001005
   Bickel V., 2018, IEEE T GEOSCI ELECT, V57, P0
   Bickel V., 2021, FRONT REMOTE SENS, V0, P0
   Bickel VT, 2020, ICARUS, V348, P0, DOI 10.1016/j.icarus.2020.113850
   Bickel VT, 2019, J GEOPHYS RES-PLANET, V124, P1296, DOI 10.1029/2018JE005876
   Bickel VT, 2020, IEEE J-STARS, V13, P2831, DOI 10.1109/JSTARS.2020.2991588
   Bickel VT, 2020, NAT COMMUN, V11, P0, DOI 10.1038/s41467-020-16653-3
   Bilen H., 2017, ARXIV170107275, V0, P0
   Douillard A., 2018, OBJECT DETECTION DEE, V0, P0
   Duarte KD, 2019, J GEOPHYS RES-PLANET, V124, P3329, DOI 10.1029/2018JE005673
   Dundas CM, 2017, NAT GEOSCI, V10, P903, DOI 10.1038/s41561-017-0012-5
   Hovland H., 1973, MOON PLANETS, V6, P0
   Jackson P. T., 2018, STYLE AUGMENTATION D, V0, P83
   Jaeger PF, 2018, MACHINE LEARNING HLT, V0, P0
   Jung H, 2018, PLOS ONE, V13, P0, DOI 10.1371/journal.pone.0203355
   Lin TY, 2017, IEEE I CONF COMP VIS, V0, PP2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, V0, PP936, DOI 10.1109/CVPR.2017.106
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lucchetti A, 2019, GEOPHYS RES LETT, V46, P14336, DOI 10.1029/2019GL085132
   MALIN MC, 1992, J GEOPHYS RES-PLANET, V97, P16337, DOI 10.1029/92JE01343
   Malisiewicz T., 2011, BLAZING FAST NMS, V0, P0
   McEwen AS, 2007, J GEOPHYS RES-PLANET, V112, P0, DOI 10.1029/2005JE002605
   Nagle-McNaughton T, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12213607
   Needham DH, 2017, EARTH PLANET SC LETT, V478, P175, DOI 10.1016/j.epsl.2017.09.002
   Otto KA, 2013, J GEOPHYS RES-PLANET, V118, P2279, DOI 10.1002/2013JE004333
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Prakash N., 2021, SCI REP-UK, V0, P0
   Raymond CA, 2020, NAT ASTRON, V4, P741, DOI 10.1038/s41550-020-1168-2
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Robinson MS, 2010, SPACE SCI REV, V150, P81, DOI 10.1007/s11214-010-9634-2
   Russell P, 2008, GEOPHYS RES LETT, V35, P0, DOI 10.1029/2008GL035790
   Sargeant HM, 2020, J GEOPHYS RES-PLANET, V125, P0, DOI 10.1029/2019JE006157
   Sierks H, 2011, SPACE SCI REV, V163, P263, DOI 10.1007/s11214-011-9745-4
   Singh K. R., 2019, ESRI OBJECT DETECTIO, V0, P0
   Tesson PA, 2020, ICARUS, V342, P0, DOI 10.1016/j.icarus.2019.113503
   Hoang TM, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19020281
   Tranheden W., 2020, DACS DOMAIN ADAPTATI, V0, P0
   Tubiana C, 2015, ASTRON ASTROPHYS, V583, P0, DOI 10.1051/0004-6361/201525985
   Wang XD, 2019, PROC CVPR IEEE, V0, PP7281, DOI 10.1109/CVPR.2019.00746
   Weinstein BG, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11111309
   Wright J, 2020, EARTH PLANET SC LETT, V549, P0, DOI 10.1016/j.epsl.2020.116519
   Xiao ZY, 2013, EARTH PLANET SC LETT, V376, P1, DOI 10.1016/j.epsl.2013.06.015
NR 43
TC 1
Z9 1
U1 0
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD DEC 15
PY 2021
VL 182
IS 
BP 1
EP 13
DI 10.1016/j.isprsjprs.2021.09.018
EA OCT 2021
PG 13
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA WK6CD
UT WOS:000709811500001
DA 2023-04-26
ER

PT J
AU Zamboni, P
   Junior, JM
   Silva, JD
   Miyoshi, GT
   Matsubara, ET
   Nogueira, K
   Goncalves, WN
AF Zamboni, Pedro
   Junior, Jose Marcato
   Silva, Jonathan de Andrade
   Miyoshi, Gabriela Takahashi
   Matsubara, Edson Takashi
   Nogueira, Keiller
   Goncalves, Wesley Nunes
TI Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images
SO REMOTE SENSING
LA English
DT Article
DE object detection; convolutional neural network; remote sensing
AB Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 x 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications.
C1 [Zamboni, Pedro; Junior, Jose Marcato; Goncalves, Wesley Nunes] Univ Fed Mato Grosso do Sul, Fac Engn Architecture & Urbanism & Geog, BR-79070900 Campo Grande, MS, Brazil.
   [Silva, Jonathan de Andrade; Matsubara, Edson Takashi; Goncalves, Wesley Nunes] Univ Fed Mato Grosso do Sul, Fac Comp Sci, BR-79070900 Campo Grande, MS, Brazil.
   [Miyoshi, Gabriela Takahashi] Sao Paulo State Univ UNESP, Dept Cartog, BR-19060900 Presidente Prudente, Brazil.
   [Nogueira, Keiller] Univ Stirling, Fac Nat Sci, Comp Sci & Math Div, Stirling FK9 4LA, Scotland.
C3 Universidade Federal de Mato Grosso do Sul; Universidade Federal de Mato Grosso do Sul; Universidade Estadual Paulista; University of Stirling
RP Zamboni, P (corresponding author), Univ Fed Mato Grosso do Sul, Fac Engn Architecture & Urbanism & Geog, BR-79070900 Campo Grande, MS, Brazil.
EM pedro.zamboni@ufms.br; jose.marcato@ufms.br; jonathan.andrade@ufms.br; gabriela.t.miyoshi@unesp.br; edsontm@facom.ufms.br; keiller.nogueira@stir.ac.uk; wesley.goncalves@ufms.br
FU CNPq [p: 433783/2018-4, 303559/2019-5, 304052/2019-1]; CAPES Print [p: 88881.311850/2018-01]; Fundect [59/300.066/2015]
CR Abass K, 2020, INT J DISAST RISK RE, V51, P0, DOI 10.1016/j.ijdrr.2020.101915
   Ampatzidis Y, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11040410
   [Anonymous], 2010, CAMPO GRANDE URBAN A, V0, P0
   Arnpatzidis Y, 2019, COMPUT ELECTRON AGR, V164, P0, DOI 10.1016/j.compag.2019.104900
   Biffi LJ, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13010054
   Chen K., 2019, ARXIV190607155, V0, P0
   Chen XX, 2021, FORESTS, V12, P0, DOI 10.3390/f12020131
   Courtrai L, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12193152
   CRED and UNISDR, 2015, HUM COST WEATH REL D, V0, P0
   Csillik O, 2018, DRONES-BASEL, V2, P0, DOI 10.3390/drones2040039
   Culman M, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12213476
   dos Santos AA, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19163595
   Endreny TA, 2018, NAT COMMUN, V9, P0, DOI 10.1038/s41467-018-03622-0
   Fasihi H, 2020, J ENVIRON MANAGE, V260, P0, DOI 10.1016/j.jenvman.2020.110122
   Fassnacht FE, 2016, REMOTE SENS ENVIRON, V186, P64, DOI 10.1016/j.rse.2016.08.013
   Ghiasi G, 2019, PROC CVPR IEEE, V0, PP7029, DOI 10.1109/CVPR.2019.00720
   Gomes M, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20216070
   Hartling S, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19061284
   He KM, 2017, IEEE I CONF COMP VIS, V0, PP2980, DOI 10.1109/TPAMI.2018.2844175
   Heinz A, 2013, WORLD PSYCHIATRY, V12, P187, DOI 10.1002/wps.20056
   Intergov Panel Clim Chg, 2012, MANAGING THE RISKS OF EXTREME EVENTS AND DISASTERS TO ADVANCE CLIMATE CHANGE ADAPTATION, V0, PP1, DOI 10.1017/CBO9781139177245
   Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201
   Jiaqi Wang, 2020, COMPUTER VISION - ECCV 2020. 16TH EUROPEAN CONFERENCE. PROCEEDINGS. LECTURE NOTES IN COMPUTER SCIENCE (LNCS 12349), V0, PP403, DOI 10.1007/978-3-030-58548-8_24
   Kang Kim, 2020, COMPUTER VISION - ECCV 2020. 16TH EUROPEAN CONFERENCE. PROCEEDINGS. LECTURE NOTES IN COMPUTER SCIENCE (LNCS 12370), V0, PP355, DOI 10.1007/978-3-030-58595-2_22
   Ke JL, 2021, J CLEAN PROD, V295, P0, DOI 10.1016/j.jclepro.2021.126250
   Khomenko S, 2021, LANCET PLANET HEALTH, V5, PE121, DOI 10.1016/S2542-5196(20)30272-2
   Kong T., 2019, ARXIV190403797, V0, P0
   Li B., 2019, P AAAI C ART INT HON, V0, P0
   Li H, 2020, ENVIRON RES, V191, P0, DOI 10.1016/j.envres.2020.110214
   Li K, 2020, ISPRS J PHOTOGRAMM, V159, P296, DOI 10.1016/j.isprsjprs.2019.11.023
   Li WJ, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9010022
   Li X., 2020, ARXIV200604388, V0, P0
   Lin TY, 2017, PROC CVPR IEEE, V0, PP936, DOI 10.1109/CVPR.2017.106
   Torres DL, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20020563
   Lu X., 2020, MIMICDET BRIDGING GA, V0, P0
   Lumnitz S, 2021, ISPRS J PHOTOGRAMM, V175, P144, DOI 10.1016/j.isprsjprs.2021.01.016
   McDonald RI, 2020, NAT SUSTAIN, V3, P16, DOI 10.1038/s41893-019-0436-6
   Micikevicius P., 2017, ARXIV171003740, V0, P0
   Miyoshi GT, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12081294
   Nezami S, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12071070
   Nielsen Anders B., 2014, ARBORICULTURE & URBAN FORESTRY, V40, P96
   Oh S, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12182981
   Osco LP, 2021, ISPRS J PHOTOGRAMM, V174, P1, DOI 10.1016/j.isprsjprs.2021.01.024
   Padayachee AL, 2017, BIOL INVASIONS, V19, P3557, DOI 10.1007/s10530-017-1596-9
   Plesoianu AI, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12152426
   Qiao S., 2021, PROC IEEECVF C COMPU, V0, P10213
   Qiao S., 2019, ARXIV, V0, P0
   Redmon J, 2018, ARXIV, V0, P0, DOI DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Roslan Z., 2020, P 14 INT C UB INF MA, V0, PP1, DOI 10.1109/IMCOM48794.2020.9001817
   Roslan Z, 2021, INT CONF UBIQUIT INF, V0, P0, DOI DOI 10.1109/IMCOM51814.2021.9377360
   Roy S, 2012, URBAN FOR URBAN GREE, V11, P351, DOI 10.1016/j.ufug.2012.06.006
   Santos A, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20164450
   Stocker TF, 2014, CLIMATE CHANGE 2013: THE PHYSICAL SCIENCE BASIS, V0, PP1, DOI 10.1017/cbo9781107415324
   Wagner FH, 2018, ISPRS J PHOTOGRAMM, V145, P362, DOI 10.1016/j.isprsjprs.2018.09.013
   Wang J, 2019, IEEE I CONF COMP VIS, V0, PP8200, DOI 10.1109/ICCV.2019.00829
   Weinstein BG, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11111309
   Wu JT, 2020, COMPUT ELECTRON AGR, V174, P0, DOI 10.1016/j.compag.2020.105504
   Wu Y., 2019, AAAI, V0, P0
   Zhang H, 2020, J THERM ANAL CALORIM, V0, P0
   Zhang S., 2019, ARXIV191202424, V0, P0
   Zhu CC, 2019, PROC CVPR IEEE, V0, PP840, DOI 10.1109/CVPR.2019.00093
   Zhu X., 2018, ARXIV181111168, V0, P0
   Zhu XZ, 2019, IEEE I CONF COMP VIS, V0, PP6687, DOI 10.1109/ICCV.2019.00679
NR 65
TC 11
Z9 11
U1 0
U2 7
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD JUL 15
PY 2021
VL 13
IS 13
BP 
EP 
DI 10.3390/rs13132482
PG 25
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA TG0UB
UT WOS:000671127300001
DA 2023-04-26
ER
