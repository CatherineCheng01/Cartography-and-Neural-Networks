
PT J
AU Hu, TL
   Liao, Q
AF Hu, Tianlun
   Liao, Qi
TI Real-Time Camera Localization with Deep Learning and Sensor Fusion
SO IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2021)
LA English
DT Proceedings Paper
AB Real-time camera localization is a key enabler for interactive network service, e.g. visualizing network performance with augmented reality (AR) in user devices. We propose a deep learning and sensor fusion approach for real-time camera localization. A multi-input deep neural network is designed to regress the camera pose from a single image and motion sensor measurements. We perform a comprehensive analysis to find the best choices of input features, loss function, convolutional neural network model, and hyperparameters. We show that by adding features extracted from motion sensor data, our approach significantly outperforms the state-of-the-art visual-based camera localization approaches. In an indoor environment where we conduct a proof-of-concept of the proposed end-to-end AR-supported radio map visualization solution, our camera localization approach achieves an orientation error of 2.5179 degrees and a position error of 0.0222 meters with an inference time lower than 4 ms per frame.
C1 [Hu, Tianlun; Liao, Qi] Nokia Bell Labs, Stuttgart, Germany.
C3 Nokia Corporation
RP Hu, TL (corresponding author), Nokia Bell Labs, Stuttgart, Germany.
EM tianlun.hu@nokia.com; qi.liao@nokia-bell-labs.com
FU German Federal Ministry of Education and Research (BMBF) project KICK [16KIS1102K]
CR Altmann S.L., 2005, ROTATIONS QUATERNION, V0, P0
   Bai X., 2019, INT SYMP WIREL, V0, P1
   Brahmbhatt S, 2018, PROC CVPR IEEE, V0, PP2616, DOI 10.1109/CVPR.2018.00277
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Campos C., 2020, ORB SLAM3 ACCURATE O, V0, P0
   Chen CH, 2020, IEEE INTERNET THINGS, V7, P4431, DOI 10.1109/JIOT.2020.2966773
   Diebel J., 2006, MATRIX, V58, P15
   He K., 2015, PROC CVPR IEEE, V5, P6
   Hussain G, 2019, ELECTRONICS-SWITZ, V8, P0, DOI 10.3390/electronics8040375
   Kendall A, 2017, PROC CVPR IEEE, V0, PP6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2015, IEEE I CONF COMP VIS, V0, PP2938, DOI 10.1109/ICCV.2015.336
   Li W, 2018, BMVC, V0, P0
   Liao Q., 2019, US PATENT APP, V0, Patent No. 16347029
   Liao QY, 2021, IEEE T GEOSCI REMOTE, V59, P8992, DOI 10.1109/TGRS.2020.3036248
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Radwan N, 2018, IEEE ROBOT AUTOM LET, V3, P4407, DOI 10.1109/LRA.2018.2869640
   Sandler M, 2018, PROC CVPR IEEE, V0, PP4510, DOI 10.1109/CVPR.2018.00474
   Schubert D, 2018, IEEE INT C INT ROBOT, V0, PP1680, DOI 10.1109/IROS.2018.8593419
   Simonyan K, 2015, ARXIV, V0, P0
   Su H, 2015, IEEE I CONF COMP VIS, V0, PP2686, DOI 10.1109/ICCV.2015.308
   Szegedy C, 2017, AAAI CONF ARTIF INTE, V0, P4278
   TUM Computer Vision Group, 2018, VIS IN DAT, V0, P0
   Walch F, 2017, IEEE I CONF COMP VIS, V0, PP627, DOI 10.1109/ICCV.2017.75
NR 23
TC 0
Z9 0
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1550-3607
EI 
J9 IEEE ICC
PD JUN 15
PY 2021
VL 0
IS 
BP 
EP 
DI 10.1109/ICC42927.2021.9500770
PG 7
WC Telecommunications
SC Telecommunications
GA BS4HX
UT WOS:000719386003030
DA 2023-04-26
ER

PT J
AU Lee, H
   Lee, K
   Kim, JH
   Na, Y
   Park, J
   Choi, JP
   Hwang, JY
AF Lee, Haeyun
   Lee, Kyungsu
   Kim, Jun Hee
   Na, Younghwan
   Park, Juhum
   Choi, Jihwan P.
   Hwang, Jae Youn
TI Local Similarity Siamese Network for Urban Land Change Detection on Remote Sensing Images
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Remote sensing; Feature extraction; Decoding; Training; Network architecture; Task analysis; Deep learning; Change detection; remote sensing; Siamese network; similarity attention
ID attention
AB Change detection is an important task in the field of remote sensing. Various change detection methods based on convolutional neural networks (CNNs) have recently been proposed for remote sensing using satellite or aerial images. However, existing methods allow only the partial use of content information in images during change detection because they adopt simple feature similarity measurements or pixel-level loss functions to construct their network architectures. Therefore, when these methods are applied to complex urban areas, their performance in terms of change detection tends to be limited. In this article, a novel CNN-based change detection approach, referred to as a local similarity Siamese network (LSS-Net), with a cosine similarity measurement, was proposed for better urban land change detection in remote sensing images. To use content information on two sequential images, a new change attention map-based content loss function was developed in this study. In addition, to enhance the performance of the LSS-Net in terms of change detection, a suitable feature similarity measurement method, incorporated into a local similarity attention module, was determined through systemic experiments. To verify the change detection performance of the LSS-Net, it was compared with other state-of-the-art methods. The experimental results show that the proposed method outperforms the state-of-the-art methods in terms of the F1 score (0.9630, 0.9377, and 0.7751) and kappa (0.9581, 0.9351, and 0.7646) on the three test datasets, thus suggesting its potential for various remote sensing applications.
C1 [Lee, Haeyun; Lee, Kyungsu; Na, Younghwan; Hwang, Jae Youn] Daegu Gyeongbuk Inst Sci & Technol, Informat & Commun Engn, Daegu 42988, South Korea.
   [Kim, Jun Hee] Agcy Def Dev, Daejoen 34186, South Korea.
   [Park, Juhum] Dabeeo Inc, Seoul 04107, South Korea.
   [Choi, Jihwan P.] Korea Adv Inst Sci & Technol, Dept Aerosp Engn, Daejoen 34141, South Korea.
C3 Daegu Gyeongbuk Institute of Science & Technology (DGIST); Agency of Defense Development (ADD), Republic of Korea; Korea Advanced Institute of Science & Technology (KAIST)
RP Hwang, JY (corresponding author), Daegu Gyeongbuk Inst Sci & Technol, Informat & Commun Engn, Daegu 42988, South Korea.
EM haeyun@dgist.ac.kr; ks_lee@dgist.ac.kr; kjh1127@add.re.kr; nyh0426@dgist.ac.kr; juhum.park@dabeeo.com; jhch@kaist.ac.kr; jyhwang@dgist.ac.kr
FU National Research Fundation of Korea (NRF) [NRF-2020R1A2B5B01002786]; Bio & Medical Technology Development Program of the National Research Foundation (NRF) through the Korean Government (MSIT) [NRF-2017M3A9G8084463]
CR Atzberger C, 2013, REMOTE SENS-BASEL, V5, P949, DOI 10.3390/rs5020949
   Benedek C, 2009, IEEE T GEOSCI REMOTE, V47, P3416, DOI 10.1109/TGRS.2009.2022633
   Bourdis N, 2011, INT GEOSCI REMOTE SE, V0, PP4176, DOI 10.1109/IGARSS.2011.6050150
   Cai WW, 2022, IEEE GEOSCI REMOTE S, V19, P0, DOI 10.1109/LGRS.2020.3026587
   Chen H, 2019, I-PERCEPTION, V10, P0, DOI 10.1177/2041669519864971
   Chen LC, 2010, J APPL REMOTE SENS, V4, P0, DOI 10.1117/1.3525560
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Coppin P, 2004, INT J REMOTE SENS, V25, P1565, DOI 10.1080/0143116031000101675
   Daudt RC, 2018, IEEE IMAGE PROC, V0, PP4063, DOI 10.1109/ICIP.2018.8451652
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, V0, PP2758, DOI 10.1109/ICCV.2015.316
   He KM, 2015, IEEE I CONF COMP VIS, V0, PP1026, DOI 10.1109/ICCV.2015.123
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Hua YS, 2019, ISPRS J PHOTOGRAMM, V149, P188, DOI 10.1016/j.isprsjprs.2019.01.015
   Ji SP, 2019, IEEE T GEOSCI REMOTE, V57, P574, DOI 10.1109/TGRS.2018.2858817
   Jiang HW, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12030484
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim JH, 2019, IEEE GEOSCI REMOTE S, V16, P115, DOI 10.1109/LGRS.2018.2868880
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Lebedev M., 2018, INT ARCH PHOTOGRAM R, V42, P565, DOI 10.5194/isprs-archives-XLII-2-565-2018
   Lei T, 2019, IEEE GEOSCI REMOTE S, V16, P982, DOI 10.1109/LGRS.2018.2889307
   Liu J, 2018, IEEE T NEUR NET LEAR, V29, P545, DOI 10.1109/TNNLS.2016.2636227
   Liu SJ, 2021, IEEE T GEOSCI REMOTE, V59, P5085, DOI 10.1109/TGRS.2020.3018879
   Munyati C, 2000, INT J REMOTE SENS, V21, P1787, DOI 10.1080/014311600209742
   Na Y, 2021, IEEE T GEOSCI REMOTE, V59, P5171, DOI 10.1109/TGRS.2020.3010055
   Nghiem SV, 2001, J GLACIOL, V47, P539, DOI 10.3189/172756501781831738
   Paszke A., 2017, NIPS AUT WORKSH, V0, P0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi Q, 2020, IEEE GEOSCI REMOTE S, V17, P1430, DOI 10.1109/LGRS.2019.2947473
   Shi WZ, 2016, PROC CVPR IEEE, V0, PP1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, ARXIV, V0, P0
   SINGH A, 1989, INT J REMOTE SENS, V10, P989, DOI 10.1080/01431168908903939
   Singh PP, 2013, J INDIAN SOC REMOTE, V41, P631, DOI 10.1007/s12524-012-0241-4
   Varghese A., 2018, P EUR C COMP VIS ECC, V0, P0
   Zhan Y, 2017, IEEE GEOSCI REMOTE S, V14, P1845, DOI 10.1109/LGRS.2017.2738149
   Zhang JP, 2016, IEEE J-STARS, V9, P2343, DOI 10.1109/JSTARS.2016.2536943
   Zhao T, 2019, PROC CVPR IEEE, V0, PP3080, DOI 10.1109/CVPR.2019.00320
NR 36
TC 11
Z9 11
U1 9
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2021
VL 14
IS 
BP 4139
EP 4149
DI 10.1109/JSTARS.2021.3069242
PG 11
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA RU3XE
UT WOS:000645081200011
DA 2023-04-26
ER

PT J
AU Wu, FM
   Wu, BF
   Zhang, M
   Zeng, HW
   Tian, FY
AF Wu, Fangming
   Wu, Bingfang
   Zhang, Miao
   Zeng, Hongwei
   Tian, Fuyou
TI Identification of Crop Type in Crowdsourced Road View Photos with Deep Convolutional Neural Network
SO SENSORS
LA English
DT Article
DE crop type; crowdsourced road view photo; deep convolutional neural network; automatic photo identification; ensemble classification
ID google street view; classification; cultivation
AB In situ ground truth data are an important requirement for producing accurate cropland type map, and this is precisely what is lacking at vast scales. Although volunteered geographic information (VGI) has been proven as a possible solution for in situ data acquisition, processing and extracting valuable information from millions of pictures remains challenging. This paper targets the detection of specific crop types from crowdsourced road view photos. A first large, public, multiclass road view crop photo dataset named iCrop was established for the development of crop type detection with deep learning. Five state-of-the-art deep convolutional neural networks including InceptionV4, DenseNet121, ResNet50, MobileNetV2, and ShuffleNetV2 were employed to compare the baseline performance. ResNet50 outperformed the others according to the overall accuracy (87.9%), and ShuffleNetV2 outperformed the others according to the efficiency (13 FPS). The decision fusion schemes major voting was used to further improve crop identification accuracy. The results clearly demonstrate the superior accuracy of the proposed decision fusion over the other non-fusion-based methods in crop type detection of imbalanced road view photos dataset. The voting method achieved higher mean accuracy (90.6-91.1%) and can be leveraged to classify crop type in crowdsourced road view photos.
C1 [Wu, Fangming; Wu, Bingfang; Zhang, Miao; Zeng, Hongwei; Tian, Fuyou] Chinese Acad Sci, Aerosp Informat Res Inst, State Key Lab Remote Sensing Sci, Beijing 100101, Peoples R China.
   [Wu, Bingfang; Zeng, Hongwei; Tian, Fuyou] Univ Chinese Acad Sci, Coll Resources & Environm, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Wu, BF (corresponding author), Chinese Acad Sci, Aerosp Informat Res Inst, State Key Lab Remote Sensing Sci, Beijing 100101, Peoples R China.; Wu, BF (corresponding author), Univ Chinese Acad Sci, Coll Resources & Environm, Beijing 100049, Peoples R China.
EM wufm@radi.ac.cn; wubf@radi.ac.cn; zhangmiao@radi.ac.cn; zenghw@radi.ac.cn; tianfy@radi.ac.cn
FU National Key Research & Development Program of China [2016YFA0600301, 2019YFE0126900]; National Natural Science Foundation of China [41561144013, 41861144019]
CR [Anonymous], 2017, AAAI CONF ARTIF INTE, V0, P0, DOI DOI 10.1609/AAAI.V31I1.11231
   [Anonymous], 2016, OVERVIEW GRADIENT DE, V0, P0
   Antoniou V, 2016, ISPRS INT J GEO-INF, V5, P0, DOI 10.3390/ijgi5050064
   Chebrolu N, 2017, INT J ROBOT RES, V36, P1045, DOI 10.1177/0278364917720510
   Chen SW, 2017, IEEE ROBOT AUTOM LET, V2, P781, DOI 10.1109/LRA.2017.2651944
   Deng J, 2009, PROC CVPR IEEE, V0, PP248, DOI 10.1109/CVPRW.2009.5206848
   Deus E, 2016, ENVIRON MONIT ASSESS, V188, P0, DOI 10.1007/s10661-016-5555-1
   FAO, 2019, STAT FOOD AGR 2019 M, V0, P0
   Ferentinos KP, 2018, COMPUT ELECTRON AGR, V145, P311, DOI 10.1016/j.compag.2018.01.009
   Fine T.L., 2006, FEEDFORWARD NEURAL N, V0, P0
   Fritz S, 2017, SCI DATA, V4, P0, DOI 10.1038/sdata.2017.75
   Hajdu A, 2013, IEEE T IMAGE PROCESS, V22, P4182, DOI 10.1109/TIP.2013.2271116
   Hall D, 2015, IEEE WINT CONF APPL, V0, PP797, DOI 10.1109/WACV.2015.111
   He K., 2015, PROC CVPR IEEE, V5, P6
   Hestness J., 2017, ARXIV171200409, V0, P0
   Hiroshi O., 2018, ARXIV180109454, V0, P0
   Huang G, 2017, PROC CVPR IEEE, V0, PP2261, DOI 10.1109/CVPR.2017.243
   Hughes D.P., 2015, ARXIV, V0, P0
   Jorgensen R., 2017, P EFITA C MONTP FRAN, V0, P0
   Joulin A., 2015, ARXIV151102251, V0, P0
   Kamilaris A, 2018, COMPUT ELECTRON AGR, V147, P70, DOI 10.1016/j.compag.2018.02.016
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leung D., 2012, PROC 2012 ACM MULTIM, V0, PP3, DOI 10.1145/2390790.2390794
   Liu SP, 2019, NEUROCOMPUTING, V338, P191, DOI 10.1016/j.neucom.2019.01.090
   Ma N., 2018, LECT NOTES COMPUT SC, V0, P0, DOI DOI 10.1007/978-3-030-01264-9_8
   Mohanty SP, 2016, FRONT PLANT SCI, V7, P0, DOI 10.3389/fpls.2016.01419
   Montserrat D. M., 2017, ELECT IMAGING, V2017, P27, DOI 10.2352/ISSN.2470-1173.2017.10.IMAWM-163
   Mwebaze E., 2019, ARXIV190802900, V0, P0
   Nabil M, 2020, INT J APPL EARTH OBS, V85, P0, DOI 10.1016/j.jag.2019.102010
   Namin ST, 2018, PLANT METHODS, V14, P0, DOI 10.1186/s13007-018-0333-4
   Oliphant AJ, 2019, INT J APPL EARTH OBS, V81, P110, DOI 10.1016/j.jag.2018.11.014
   Olsen A, 2019, SCI REP-UK, V9, P0, DOI 10.1038/s41598-018-38343-3
   Rahman MS, 2019, AGRICULTURE-BASEL, V9, P0, DOI 10.3390/agriculture9010017
   Raja R, 2020, BIOSYST ENG, V192, P257, DOI 10.1016/j.biosystemseng.2020.02.002
   Ringland J, 2019, COMPUT ELECTRON AGR, V158, P36, DOI 10.1016/j.compag.2019.01.014
   Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, P0, DOI 10.1155/2016/3289801
   Sler M., 2019, PROC CVPR IEEE, V0, P0, DOI DOI 10.1109/CVPR.2018.00474
   Sridar P, 2019, ULTRASOUND MED BIOL, V45, P1259, DOI 10.1016/j.ultrasmedbio.2018.11.016
   Su Z., 2019, J NW A F U, V0, P0
   Sun C., 2017, IEEE I CONF COMP VIS, V0, P0, DOI DOI 10.1109/ICCV.2017.97
   Tian FY, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11060629
   Ueda K, 2020, INATURALIST RES GRAD, V0, P0, DOI DOI 10.15468/AB3S5X
   United Nations Development Programme, 2019, HUMAN DEV REPORT 201, V0, P0
   Waldner F, 2019, INT J APPL EARTH OBS, V80, P82, DOI 10.1016/j.jag.2019.01.002
   Wu BF, 2020, GEOGR SUSTAIN, V1, P25, DOI 10.1016/j.geosus.2020.03.006
   Wu BF, 2015, REMOTE SENS-BASEL, V7, P3907, DOI 10.3390/rs70403907
   Wu BF, 2014, INT J DIGIT EARTH, V7, P113, DOI 10.1080/17538947.2013.821185
   Wu BF, 2012, INT J APPL EARTH OBS, V16, P101, DOI 10.1016/j.jag.2011.12.006
   Xiong J, 2017, ISPRS J PHOTOGRAMM, V126, P225, DOI 10.1016/j.isprsjprs.2017.01.019
   Xue DX, 2016, J MED BIOL ENG, V36, P755, DOI 10.1007/s40846-016-0182-4
   Yan YL, 2021, ISPRS J PHOTOGRAMM, V171, P278, DOI 10.1016/j.isprsjprs.2020.11.022
   Zhang X, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10081200
   Zheng YY, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19051058
NR 55
TC 12
Z9 13
U1 6
U2 14
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD FEB 15
PY 2021
VL 21
IS 4
BP 
EP 
DI 10.3390/s21041165
PG 15
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA QQ6XC
UT WOS:000624665100001
PM 33562266
DA 2023-04-26
ER

PT J
AU Habibi, V
   Ahmadi, H
   Jafari, M
   Moeini, A
AF Habibi, Vahid
   Ahmadi, Hasan
   Jafari, Mohammad
   Moeini, Abolfazl
TI Mapping soil salinity using a combined spectral and topographical indices with artificial neural network
SO PLOS ONE
LA English
DT Article
ID prediction; regression; models; salt
AB Monitoring the status of natural and ecological resources is necessary for conservation and protection. Soil is one of the most important environmental resources in agricultural lands and natural resources. In this research study, we used Landsat 8 and Artificial Neural Network (ANN) to monitor soil salinity in Qom plain. The geographical location of 72 surface soil samples from 7 land types was determined by the Latin hypercube method, and the samples were taken to determine the electrical conductivity (EC). Thirty percent of the data was considered as a validation set and 70% as a test set. In addition to the Landsat 8 bands, we used spectral indices of salinity, vegetation, topography, and drainage (DEM, TWI, and TCI) because of their impacts on soil formation and development. We used ANN with different algorithms to model soil salinity. We found that the GFF algorithm is the best for soil salinity modeling. Also, the TWI topography index and SI5 salinity index and NDVI vegetation index had the most effect on the outputs of the selected model. It was also found that flood plains and lowlands had the highest levels of salinity accumulation.
C1 [Habibi, Vahid; Moeini, Abolfazl] Islamic Azad Univ, Sci & Res Branch, Dept Nat Resources & Environm, Tehran, Iran.
   [Ahmadi, Hasan; Jafari, Mohammad] Univ Tehran, Fac Nat Resource, Karaj, Iran.
C3 Islamic Azad University; University of Tehran
RP Jafari, M (corresponding author), Univ Tehran, Fac Nat Resource, Karaj, Iran.
EM jafary@ut.ac.ir
CR Abbas A, 2013, PHYS CHEM EARTH, V55-57, P43, DOI 10.1016/j.pce.2010.12.004
   Akramkhanov A, 2012, ENVIRON MONIT ASSESS, V184, P2475, DOI 10.1007/s10661-011-2132-5
   Alhammadi MS, 2008, INT J REMOTE SENS, V29, P1745, DOI 10.1080/01431160701395195
   Bagheri Bodaghabadi M, 2015, PEDOSPHERE, V25, P580, DOI 10.1016/S1002-0160(15)30038-2
   Baig MHA, 2014, REMOTE SENS LETT, V5, P423, DOI 10.1080/2150704X.2014.915434
   Bannari A, 2008, COMMUN SOIL SCI PLAN, V39, P2795, DOI 10.1080/00103620802432717
   Bennett SJ, 2013, CROP PASTURE SCI, V64, P285, DOI 10.1071/CP12417
   Conrad O, 2015, GEOSCI MODEL DEV, V8, P1991, DOI 10.5194/gmd-8-1991-2015
   Ebrahimi M, 2017, COMPUT ELECTRON AGR, V140, P409, DOI 10.1016/j.compag.2017.06.019
   Ennaji W., 2018, GEOL ECOL LANDSC, V2, P22, DOI 10.1080/24749508.2018.1438744
   Ennouri K, 2017, 3 BIOTECH, V7, P0, DOI 10.1007/s13205-017-0799-1
   Ghorbani MA, 2019, SOIL TILL RES, V186, P152, DOI 10.1016/j.still.2018.09.012
   Hoseini Y., 2017, IRAN AGRICULTURAL RESEARCH, V36, P91
   Jiang H, 2019, INT J REMOTE SENS, V40, P284, DOI 10.1080/01431161.2018.1513180
   Jin PB, 2015, ECOL INDIC, V54, P116, DOI 10.1016/j.ecolind.2015.02.028
   Jones HG, 2007, J EXP BOT, V58, P119, DOI 10.1093/jxb/erl118
   Khan NM, 2005, AGR WATER MANAGE, V77, P96, DOI 10.1016/j.agwat.2004.09.038
   Khan NM, 2001, MAPPING SALT AFFECTE, V0, P5
   McCulloch W., 1943, B MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Minasny B, 2010, GEODERMA, V155, P132, DOI 10.1016/j.geoderma.2009.04.024
   Mohamed ES, 2018, EGYPT J REMOTE SENS, V21, P1, DOI 10.1016/j.ejrs.2017.02.001
   MOORE ID, 1991, HYDROL PROCESS, V5, P3, DOI 10.1002/hyp.3360050103
   Nguyen AK, 2018, ESTIMATION SALINITY, V0, P0
   Noureddine K., 1900, V4531, V0, P401
   Patel RM, 2002, J AM WATER RESOUR AS, V38, P91, DOI 10.1111/j.1752-1688.2002.tb01537.x
   Plain A., 2017, DIGITAL MAPPING TOPS, V5, P1771
   QI J, 1994, REMOTE SENS ENVIRON, V48, P119, DOI 10.1016/0034-4257(94)90134-1
   Roustaei F., 2018, COMP ARTIFICIAL NEUR, V7, P11
   Saei M., 2014, FOREST CANOPY DENSIT, V0, P0
   Scudiero E, 2016, FRONT ENV SCI-SWITZ, V4, P0, DOI 10.3389/fenvs.2016.00065
   Shahabi M, 2017, ARCH AGRON SOIL SCI, V63, P151, DOI 10.1080/03650340.2016.1193162
   Silva S H G., 2016, THESIS U FEDERAL LAV, V0, P0
   Taghizadeh-Mehrjardi R, 2015, GEODERMA, V253, P67, DOI 10.1016/j.geoderma.2015.04.008
   Taghizadeh-Mehrjardi R, 2014, GEODERMA, V213, P15, DOI 10.1016/j.geoderma.2013.07.020
   Webster R, 2007, GEOSTATISTICS ENV SC, V0, P271
   Yang F, 2011, J ENVIRON PROT ECOL, V12, P1160
   Zou P, 2010, AGR WATER MANAGE, V97, P2009, DOI 10.1016/j.agwat.2010.02.011
NR 37
TC 10
Z9 10
U1 1
U2 5
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
EI 
J9 PLOS ONE
JI PLoS One
PD MAY 13
PY 2021
VL 16
IS 5
BP 
EP 
DI 10.1371/journal.pone.0228494
PG 13
WC Multidisciplinary Sciences
SC Science & Technology - Other Topics
GA SW6MO
UT WOS:000664628200001
PM 33983942
DA 2023-04-26
ER

PT J
AU Wang, LQ
   Lin, Y
   Liu, JY
   Li, ZW
   Wu, CL
AF Wang, Leiquan
   Lin, Yao
   Liu, Jinyun
   Li, Zhongwei
   Wu, Chunlei
TI Siamese Spectral Attention With Channel Consistency for Hyperspectral Image Classification
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Hyperspectral imaging; Feature extraction; Task analysis; Convolutional neural networks; Training; Convolution; Training data; Channel consistency; double-branch; hyperspectral image (HSI) classification; spectral siamese
ID cnn
AB Abundant spectral features are the precious wealth of hyperspectral images (HSI). Nevertheless, well-designed spectral feature is still a challenge that affects the performance of the classifier, especially with insufficient number of training samples. To make up the poor discriminability of spectral feature, double-branch methods are proposed by fusing parallel spectral and spatial branches. However, this structure does nothing to improve the quality of spectral feature, which is regarded as the most valuable information for HSI information. In this article, we propose a siamese spectral attention network with channel consistency (SSACC) to focus on obtaining discriminative spectral features, thus improving the generalization ability of the classifier. Two kinds of HSI cubes with different patch sizes are generated as the input of SSACC. The two cubes are divided into top and bottom branches and then be fed into the siamese network to obtain the refined spectral features. Then, self-attention is conducted to interacting with each channel for the spectral features enhancement. Meanwhile, two attention maps are obtained to display the spectral structures of each branch. A channel consistency regularization is performed on the two attention maps by enforcing the two branches to possess similar spectral patterns when identifying the same centric pixel. Extensive experiments conducted on the three HSI datasets verify the superiority of the obtained spectral feature. Furthermore, the proposed method applying convolution only on the spectral domain outperforms the state-of-the-art double-branch methods which integrate the spectral and spatial features simultaneously.
C1 [Wang, Leiquan; Wu, Chunlei] China Univ Petr, Coll Comp Sci & Technol, Qingdao 266555, Peoples R China.
   [Lin, Yao; Li, Zhongwei] China Univ Petr, Coll Oceanog & Space Informat, Qingdao 266555, Peoples R China.
   [Liu, Jinyun] SINO Pipeline Int Co Ltd, Beijing 100028, Peoples R China.
C3 China University of Petroleum; China University of Petroleum
RP Li, ZW (corresponding author), China Univ Petr, Coll Oceanog & Space Informat, Qingdao 266555, Peoples R China.
EM richiewlq@gmail.com; 2795561928@qq.com; jinyun.liu@cnpc.com.cn; lizhongwei@upc.edu.cn; wuchunlei@upc.edu.cn
FU National Natural Science Foundation of China [62071491, U1906217]; Fundamental Research Funds for the Central Universities [19CX05003A-11]
CR Ahmad, 2021, ARXIV210106116, V0, P0
   Anderson P, 2018, PROC CVPR IEEE, V0, PP3674, DOI 10.1109/CVPR.2018.00387
   Arabi B, 2020, REMOTE SENS ENVIRON, V239, P0, DOI 10.1016/j.rse.2020.111632
   Arun PV, 2019, IEEE J-STARS, V12, P1849, DOI 10.1109/JSTARS.2019.2913097
   Bandos TV, 2009, IEEE T GEOSCI REMOTE, V47, P862, DOI 10.1109/TGRS.2008.2005729
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Cao Y, 2019, IEEE INT CONF COMP V, V0, PP1971, DOI 10.1109/ICCVW.2019.00246
   Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107
   CHOE J, 1900, DOI 10.1109/TPAMI.2020.2999099, V0, P0
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Fang B, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11020159
   Gao HM, 2019, IEEE ACCESS, V7, P176587, DOI 10.1109/ACCESS.2019.2957163
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2335, DOI 10.1109/TGRS.2014.2358934
   Girshick R, 2014, PROC CVPR IEEE, V0, PP580, DOI 10.1109/CVPR.2014.81
   Hang RL, 2021, IEEE T GEOSCI REMOTE, V59, P2281, DOI 10.1109/TGRS.2020.3007921
   Haut JM, 2019, IEEE T GEOSCI REMOTE, V57, P8065, DOI 10.1109/TGRS.2019.2918080
   He L, 2018, IEEE T GEOSCI REMOTE, V56, P1579, DOI 10.1109/TGRS.2017.2765364
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P5966, DOI 10.1109/TGRS.2020.3015157
   Hu W, 2015, J SENSORS, V2015, P0, DOI 10.1155/2015/258619
   Hu WS, 2020, IEEE T GEOSCI REMOTE, V58, P4237, DOI 10.1109/TGRS.2019.2961947
   Huang G, 2017, PROC CVPR IEEE, V0, PP2261, DOI 10.1109/CVPR.2017.243
   Jia S, 2020, IEEE T GEOSCI REMOTE, V58, P5077, DOI 10.1109/TGRS.2020.2972294
   Kaushik P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, V0, P350
   Khodadadzadeh M, 2014, IEEE GEOSCI REMOTE S, V11, P2105, DOI 10.1109/LGRS.2014.2320258
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Lai QX, 2021, IEEE T MULTIMEDIA, V23, P2086, DOI 10.1109/TMM.2020.3007321
   Lee H, 2017, IEEE T IMAGE PROCESS, V26, P4843, DOI 10.1109/TIP.2017.2725580
   Li R, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12030582
   Li ST, 2019, IEEE T GEOSCI REMOTE, V57, P6690, DOI 10.1109/TGRS.2019.2907932
   Li W, 2017, IEEE T GEOSCI REMOTE, V55, P844, DOI 10.1109/TGRS.2016.2616355
   Li ZK, 2022, IEEE GEOSCI REMOTE S, V19, P0, DOI 10.1109/LGRS.2021.3052346
   Liang L, 2015, REMOTE SENS ENVIRON, V165, P123, DOI 10.1016/j.rse.2015.04.032
   Liu QS, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9121330
   Lu ZY, 2020, IEEE J-STARS, V13, P4311, DOI 10.1109/JSTARS.2020.3011992
   Ma WP, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11111307
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Mohanty PC, 2016, PROC SPIE, V9880, P0, DOI 10.1117/12.2227991
   Mou LC, 2020, IEEE T GEOSCI REMOTE, V58, P110, DOI 10.1109/TGRS.2019.2933609
   Mou LC, 2017, IEEE T GEOSCI REMOTE, V55, P3639, DOI 10.1109/TGRS.2016.2636241
   Pan ET, 2019, INT GEOSCI REMOTE SE, V0, PP413, DOI 10.1109/IGARSS.2019.8898758
   Pande S, 2021, PATTERN RECOGN LETT, V144, P6, DOI 10.1016/j.patrec.2021.01.015
   Roy SK, 2021, IEEE T GEOSCI REMOTE, V59, P7831, DOI 10.1109/TGRS.2020.3043267
   Sellars P, 2020, IEEE T GEOSCI REMOTE, V58, P4180, DOI 10.1109/TGRS.2019.2961599
   Shi C, 2018, PATTERN RECOGN, V74, P600, DOI 10.1016/j.patcog.2017.09.007
   Sun H, 2020, IEEE T GEOSCI REMOTE, V58, P3232, DOI 10.1109/TGRS.2019.2951160
   Szegedy C, 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, ADV NEUR IN, V30, P0
   Wang Q., 2020, CVF C COMP VIS PATT, V0, P0, DOI DOI 10.1109/CVPR42600.2020.01155
   Wang SL, 2018, PROC CVPR IEEE, V0, PP2589, DOI 10.1109/CVPR.2018.00274
   Wang WJ, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10071068
   Wang X, 2019, IEEE T GEOSCI REMOTE, V57, P7232, DOI 10.1109/TGRS.2019.2912468
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu YH, 2018, IEEE T GEOSCI REMOTE, V56, P5893, DOI 10.1109/TGRS.2018.2827407
   Yuxiang Zhang, 2021, IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, V59, P9646, DOI 10.1109/TGRS.2020.3046756
   Zhan Y, 2018, IEEE GEOSCI REMOTE S, V15, P212, DOI 10.1109/LGRS.2017.2780890
   Zhang CJ, 2019, IEEE T GEOSCI REMOTE, V57, P9201, DOI 10.1109/TGRS.2019.2925615
   Zhang MM, 2018, IEEE T IMAGE PROCESS, V27, P2623, DOI 10.1109/TIP.2018.2809606
   Zhong ZL, 2018, IEEE T GEOSCI REMOTE, V56, P847, DOI 10.1109/TGRS.2017.2755542
   Zhou F, 2019, NEUROCOMPUTING, V328, P39, DOI 10.1016/j.neucom.2018.02.105
   Zhu MH, 2021, IEEE T GEOSCI REMOTE, V59, P449, DOI 10.1109/TGRS.2020.2994057
   Zoran Daniel, 2020, 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR). PROCEEDINGS, V0, PP9480, DOI 10.1109/CVPR42600.2020.00950
NR 61
TC 2
Z9 2
U1 7
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2021
VL 14
IS 
BP 10226
EP 10241
DI 10.1109/JSTARS.2021.3115129
PG 16
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA WJ5GX
UT WOS:000709074200006
DA 2023-04-26
ER
