
PT J
AU Wang, Y
   Ye, HL
   Cao, FL
AF Wang, Yi
   Ye, Hailiang
   Cao, Feilong
TI A novel multi-discriminator deep network for image segmentation
SO APPLIED INTELLIGENCE
LA English
DT Article
DE Deep learning; Generative adversarial networks (GAN); Convolution neural network (CNN); Image segmentation
ID vessel segmentation
AB Several studies have shown the excellent performance of deep learning in image segmentation. Usually, this benefits from a large amount of annotated data. Medical image segmentation is challenging, however, since there is always a scarcity of annotated data. This study constructs a novel deep network for medical image segmentation, referred to as asymmetric U-Net generative adversarial networks with multi-discriminators (AU-MultiGAN). Specifically, the asymmetric U-Net is designed to produce multiple segmentation maps simultaneously and use the dual-dilated blocks in the feature extraction stage only. Further, the multi-discriminator module is embedded into the asymmetric U-Net structure, which can capture the available information of samples sufficiently and thereby promote the information transmission of features. A hybrid loss by the combination of segmentation and discriminator losses is developed, and an adaptive method of selecting the scale factors is devised for this new loss. More importantly, the convergence of the proposed model is proved mathematically. The proposed AU-MultiGAN approach is implemented on some standard medical image benchmarks. Experimental results show that the proposed architecture can be successfully applied to medical image segmentation, and obtain superior performance in comparison with the state-of-the-art baselines.
C1 [Wang, Yi; Ye, Hailiang; Cao, Feilong] China Jiliang Univ, Coll Sci, Hangzhou 310018, Peoples R China.
C3 China Jiliang University
RP Cao, FL (corresponding author), China Jiliang Univ, Coll Sci, Hangzhou 310018, Peoples R China.
EM kelly_sylvia@163.com; yh1575@163.com; icteam@163.com
FU National Natural Science Foundation of China [61672477]; Zhejiang Provincial Natural Science Foundation of China [LZ20F030001]
CR Alom MZ, 2019, J MED IMAGING, V6, P0, DOI 10.1117/1.JMI.6.1.014006
   Arganda-Carreras I, 2015, FRONT NEUROANAT, V9, P0, DOI 10.3389/fnana.2015.00142
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barkana BD, 2017, KNOWL-BASED SYST, V118, P165, DOI 10.1016/j.knosys.2016.11.022
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Boyd S., 2004, CONVEX OPTIMIZATION, V0, P0, DOI DOI 10.1017/CBO9780511804441
   Cao FL, 2020, KNOWL-BASED SYST, V189, P0, DOI 10.1016/j.knosys.2019.105122
   Cao FL, 2019, NEUROCOMPUTING, V358, P424, DOI 10.1016/j.neucom.2019.05.066
   Cardona A, 2010, PLOS BIOL, V8, P0, DOI 10.1371/journal.pbio.1000502
   Codella NCF, 2018, I S BIOMED IMAGING, V0, P168
   Coelho Luis Pedro, 2009, PROC IEEE INT SYMP BIOMED IMAGING, V5193098, P518
   CUI H, 2020, IEEE T MED IMAGING, V0, P0
   de Haan L., 2010, EXTREME VALUE THEORY, V1st, P0
   Dong X, 2019, MED PHYS, V46, P2157, DOI 10.1002/mp.13458
   Drozdzal M, 2016, LECT NOTES COMPUT SC, V10008, P179, DOI 10.1007/978-3-319-46976-8_19
   Fang LL, 2020, INFORM SCIENCES, V513, P504, DOI 10.1016/j.ins.2019.10.051
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   Li ZL, 2019, NEUROCOMPUTING, V350, P53, DOI 10.1016/j.neucom.2019.04.028
   Lin T, 2017, 2017 IEEE INT C COMP, V0, P0, DOI DOI 10.1109/ICCV.2017.324
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lu Z, 2017, IEEE J BIOMED HEALTH, V21, P441, DOI 10.1109/JBHI.2016.2519686
   Lu Z, 2015, IEEE T IMAGE PROCESS, V24, P1261, DOI 10.1109/TIP.2015.2389619
   Luc P., 2016, ARXIV161108408, V0, P0
   Milletari F., 2016, INT CONF 3D VISION, V0, PP565, DOI 10.1109/3DV.2016.79
   Mitra J, 2014, NEUROIMAGE, V98, P324, DOI 10.1016/j.neuroimage.2014.04.056
   Nazir, 2020, IEEE J BIOMED HEALTH, V24, P1
   Negi A, 2020, ARAB J SCI ENG, V45, P6399, DOI 10.1007/s13369-020-04480-z
   Oktay O, 2018, ARXIV180403999, V0, P0
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy AG, 2020, MED IMAGE ANAL, V59, P0, DOI 10.1016/j.media.2019.101587
   Shi WZ, 2016, PROC CVPR IEEE, V0, PP1874, DOI 10.1109/CVPR.2016.207
   Shu J., 2019, ADV NEURAL INFORM PR, V0, P0
   Simonyan K, 2015, ARXIV, V0, P0
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Tschandl P, 2018, SCI DATA, V5, P0, DOI 10.1038/sdata.2018.161
   Wang N, 2019, ANN IEEE INT CONF SE, V0, P0
   Xie H, 2020, NEURAL NETWORKS, V132, P477, DOI 10.1016/j.neunet.2020.09.005
   Yu HS, 2018, NEUROCOMPUTING, V304, P82, DOI 10.1016/j.neucom.2018.03.037
   Yu LQ, 2017, AAAI CONF ARTIF INTE, V0, P66
   Zhang C, 2020, IEEE T MED IMAGING, V39, P3309, DOI 10.1109/TMI.2020.2991266
   Zhang JL, 2020, NEUROCOMPUTING, V384, P346, DOI 10.1016/j.neucom.2019.12.011
   Zhou Zongwei, 2018, DEEP LEARN MED IMAGE ANAL MULTIMODAL LEARN CLIN DECIS SUPPORT (2018), V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhuang Juntang, 2018, ARXIV, V0, P0
NR 45
TC 3
Z9 3
U1 4
U2 18
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0924-669X
EI 1573-7497
J9 APPL INTELL
JI Appl. Intell.
PD JAN 15
PY 2022
VL 52
IS 1
BP 1092
EP 1109
DI 10.1007/s10489-021-02427-x
EA MAY 2021
PG 18
WC Computer Science, Artificial Intelligence
SC Computer Science
GA YG8PL
UT WOS:000651027400004
DA 2023-04-26
ER

PT J
AU McNamee, DC
   Stachenfeld, KL
   Botvinick, MM
   Gershman, SJ
AF McNamee, Daniel C.
   Stachenfeld, Kimberly L.
   Botvinick, Matthew M.
   Gershman, Samuel J.
TI Compositional Sequence Generation in the Entorhinal-Hippocampal System
SO ENTROPY
LA English
DT Article
DE compositionality; generative models; entorhinal cortex; hippocampus
ID representation; cortex; knowledge; memory; replay; space
AB Neurons in the medial entorhinal cortex exhibit multiple, periodically organized, firing fields which collectively appear to form an internal representation of space. Neuroimaging data suggest that this grid coding is also present in other cortical areas such as the prefrontal cortex, indicating that it may be a general principle of neural functionality in the brain. In a recent analysis through the lens of dynamical systems theory, we showed how grid coding can lead to the generation of a diversity of empirically observed sequential reactivations of hippocampal place cells corresponding to traversals of cognitive maps. Here, we extend this sequence generation model by describing how the synthesis of multiple dynamical systems can support compositional cognitive computations. To empirically validate the model, we simulate two experiments demonstrating compositionality in space or in time during sequence generation. Finally, we describe several neural network architectures supporting various types of compositionality based on grid coding and highlight connections to recent work in machine learning leveraging analogous techniques.
C1 [McNamee, Daniel C.] Champalimaud Res, Neurosci Programme, P-1400038 Lisbon, Portugal.
   [Stachenfeld, Kimberly L.; Botvinick, Matthew M.] Google DeepMind, London N1C 4DN, England.
   [Botvinick, Matthew M.] UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.
   [Gershman, Samuel J.] Harvard Univ, Dept Psychol, Cambridge, MA 02138 USA.
   [Gershman, Samuel J.] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
   [Gershman, Samuel J.] MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA.
C3 Google Incorporated; University of London; University College London; Harvard University; Harvard University; Massachusetts Institute of Technology (MIT)
RP McNamee, DC (corresponding author), Champalimaud Res, Neurosci Programme, P-1400038 Lisbon, Portugal.
EM daniel.mcnamee@research.fchampalimaud.org
FU Center for Brains, Minds and Machines - NSF STC award [CCF-1231216]
CR Achille A, 2018, J MACH LEARN RES, V19, P0
   Alvernhe A, 2011, EUR J NEUROSCI, V33, P1696, DOI 10.1111/j.1460-9568.2011.07653.x
   Baroni M, 2020, PHILOS T R SOC B, V375, P0, DOI 10.1098/rstb.2019.0307
   Barron HC, 2013, NAT NEUROSCI, V16, P1492, DOI 10.1038/nn.3515
   Behrens TEJ, 2018, NEURON, V100, P490, DOI 10.1016/j.neuron.2018.10.002
   Buzsaki G, 2018, TRENDS COGN SCI, V22, P853, DOI 10.1016/j.tics.2018.07.006
   Campbell MG, 2018, NAT NEUROSCI, V21, P1096, DOI 10.1038/s41593-018-0189-y
   Casas F, 2012, COMPUT PHYS COMMUN, V183, P2386, DOI 10.1016/j.cpc.2012.06.006
   Caselles-Dupre H., 2019, ARXIV, V0, P0
   Cohen T, 2014, PR MACH LEARN RES, V32, P1755
   Constantinescu AO, 2016, SCIENCE, V352, P1464, DOI 10.1126/science.aaf0941
   Craik K.J.W., 1943, NATURE EXPLANATION, V445, P0
   DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613
   Dragt A.J., 2021, LIE METHODS NONLINEA, V0, P0
   Dunn B., 2017, BIORXIV, V0, P0, DOI DOI 10.1101/101899
   Dusek JA, 1997, P NATL ACAD SCI USA, V94, P7109, DOI 10.1073/pnas.94.13.7109
   Eliasmith C., 2013, BUILD BRAIN NEURAL A, V0, P0
   FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5
   Frankland SM, 2020, ANNU REV PSYCHOL, V71, P273, DOI 10.1146/annurev-psych-122216-011829
   Gershman SJ, 2018, COGNITION, V173, P34, DOI 10.1016/j.cognition.2017.12.014
   Gershman SJ, 2012, NEURAL COMPUT, V24, P1553, DOI 10.1162/NECO_a_00282
   Goldberg A.E., 2015, ROUTLEDGE HDB SEMANT, V0, P435
   Gupta AS, 2012, NAT NEUROSCI, V15, P1032, DOI 10.1038/nn.3138
   Hassabis D, 2009, PHILOS T R SOC B, V364, P1263, DOI 10.1098/rstb.2008.0296
   Higgins I., 2017, ARXIV, V0, P0
   Higgins I, 2022, FRONT COMPUT NEUROSC, V16, P0, DOI 10.3389/fncom.2022.836498
   Hills TT, 2015, TRENDS COGN SCI, V19, P46, DOI 10.1016/j.tics.2014.10.004
   Ho MK, 2022, NATURE, V606, P129, DOI 10.1038/s41586-022-04743-9
   Jeffress, 1951, CEREBRAL MECH BEHAV, V0, PP112, DOI 10.1016/J.HUMOV.2007.04.001
   Kaplan R, 2020, COGN NEUROSCI-UK, V11, P122, DOI 10.1080/17588928.2019.1676711
   Koster R, 2018, NEURON, V99, P1342, DOI 10.1016/j.neuron.2018.08.009
   Kusmierz L, 2017, PHYS REV LETT, V119, P0, DOI 10.1103/PhysRevLett.119.250601
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Liu YZ, 2019, CELL, V178, P640, DOI 10.1016/j.cell.2019.06.012
   McNamee DC, 2021, NAT NEUROSCI, V24, P851, DOI 10.1038/s41593-021-00831-7
   Miller GA, 1960, PLANS STRUCTURE BEHA, V0, P0
   Moler C, 2003, SIAM REV, V45, P3, DOI 10.1137/S00361445024180
   Nieh EH, 2021, NATURE, V595, P80, DOI 10.1038/s41586-021-03652-7
   Norris J. R, 1997, MARKOV CHAINS, V0, P0
   ONeill J, 2017, SCIENCE, V355, P184, DOI 10.1126/science.aag2787
   Olafsdottir HF, 2017, NEURON, V96, P925, DOI 10.1016/j.neuron.2017.09.035
   Olafsdottir HF, 2016, NAT NEUROSCI, V19, P792, DOI 10.1038/nn.4291
   Olafsdottir HF, 2015, ELIFE, V4, P0, DOI 10.7554/eLife.06063
   Piray P, 2021, NAT COMMUN, V12, P0, DOI 10.1038/s41467-021-25123-3
   Saxe AM, 2017, PR MACH LEARN RES, V70, P0
   Schapiro AC, 2013, NAT NEUROSCI, V16, P486, DOI 10.1038/nn.3331
   Schlesiger MI, 2015, NAT NEUROSCI, V18, P1123, DOI 10.1038/nn.4056
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   SMOLENSKY P, 1990, ARTIF INTELL, V46, P159, DOI 10.1016/0004-3702(90)90007-M
   Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650
   Sternberg S., 2009, LIE ALGEBRAS, V0, P0
   Sun C, 2020, NAT NEUROSCI, V23, P651, DOI 10.1038/s41593-020-0614-x
   Sutton RS, 2018, ADAPT COMPUT MACH LE, V0, P1
   Todorov E., 2009, ADV NEURAL INFORM PR, V0, P1856
   Tweed DB, 1999, NATURE, V399, P261, DOI 10.1038/20441
   van der Velde F, 2006, BEHAV BRAIN SCI, V29, P37, DOI 10.1017/S0140525X06009022
   Waaga T, 2022, NEURON, V110, P1843, DOI 10.1016/j.neuron.2022.03.011
   Weber MF, 2017, REP PROG PHYS, V80, P0, DOI 10.1088/1361-6633/aa5ae2
   Witter MP, 2014, PHILOS T R SOC B, V369, P0, DOI 10.1098/rstb.2012.0515
   Wood ER, 1999, NATURE, V397, P613, DOI 10.1038/17605
   Yamamoto J, 2017, NEURON, V96, P217, DOI 10.1016/j.neuron.2017.09.017
NR 62
TC 0
Z9 0
U1 0
U2 0
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 1099-4300
J9 ENTROPY-SWITZ
JI Entropy
PD DEC 15
PY 2022
VL 24
IS 12
BP 
EP 
DI 10.3390/e24121791
PG 18
WC Physics, Multidisciplinary
SC Physics
GA 7G7WH
UT WOS:000902728900001
PM 36554196
DA 2023-04-26
ER

PT J
AU Schmitt, M
   Recla, M
AF Schmitt, Michael
   Recla, Michael
TI COMPARISON OF SINGLE-IMAGE URBAN HEIGHT RECONSTRUCTION FROM OPTICAL AND SAR DATA
SO XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II
LA English
DT Proceedings Paper
DE Remote Sensing; Photogrammetry; Radargrammetry; Single-Image Depth; 3D Reconstruction; Urban Areas
ID shape
AB Deep learning-based depth estimation has become an important topic in recent years, not only in the field of computer vision. Also in the context of remote sensing, scientists started a few years ago to adapt or develop suitable approaches to realize a reconstruction of the Earth's surface without requiring several images. There are many reasons for this: First, of course, the aspect of general economization, since especially high-resolution satellite images are often accompanied by high data acquisition costs. In addition, there is also the desire to be able to acquire high-quality geoinformation as quickly as possible in time-critical cases - for example, the provision of up-to-date maps for emergency forces in disaster scenarios. Finally, a reconstruction of topography based only on single images can also provide important approximate values for the classic multi-image methods. For example, various processing steps in a classical InSAR process chain require a rough knowledge of the Earth's surface in order to achieve the most accurate and reliable results. In this paper, we review the developments documented in the remote sensing literature so far. Using an established neural network architecture, we produce example results for both very-high-resolution SAR and optical imagery. The comparison shows that SAR-based single-image-height reconstruction seems to bear an even greater potential than single-image height reconstruction from optical data.
C1 [Schmitt, Michael; Recla, Michael] Univ Bundeswehr Munich, Dept Aerosp Engn, Neubiberg, Germany.
C3 Bundeswehr University Munich
RP Schmitt, M (corresponding author), Univ Bundeswehr Munich, Dept Aerosp Engn, Neubiberg, Germany.
EM michael.schmitt@unibw.de; michael.recla@unibw.de
FU German Research Foundation (DFG) [SCHM 3322/3-1]; German Aerospace Center (DLR) [MTH3753]
CR Amhar F., 1998, INT ARCH PHOTOGRAM 4, V32, P16
   Amirkolaee HA, 2019, ISPRS J PHOTOGRAMM, V149, P50, DOI 10.1016/j.isprsjprs.2019.01.013
   Christie G.A., 2020, P IEEECVF C COMPUTER, V0, P14500
   COOPER APR, 1994, IEEE T GEOSCI REMOTE, V32, P1196, DOI 10.1109/36.338370
   Di Martino G, 2014, INT GEOSCI REMOTE SE, V0, PP1333, DOI 10.1109/IGARSS.2014.6946680
   Eigen D., 2014, ADV NEURAL INFORM PR, V0, PP2366, DOI 10.5555/2969033.2969091
   Ghamisi P, 2018, IEEE GEOSCI REMOTE S, V15, P794, DOI 10.1109/LGRS.2018.2806945
   Heipke C., 1992, MUSTERERKENNUNG, V0, P186
   Li X., 2020, IEEE GEOSCI REMOTE S, V0, P0
   Mahmud J, 2020, PROC CVPR IEEE, V0, PP438, DOI 10.1109/CVPR42600.2020.00052
   Mertan A, 2021, ARXIV, V0, P0
   Mou LC, 2018, ARXIV, V0, P0
   Natsuaki R, 2013, IEEE J-STARS, V6, P953, DOI 10.1109/JSTARS.2012.2219506
   OHara R, 2012, ISPRS J PHOTOGRAMM, V67, P27, DOI 10.1016/j.isprsjprs.2011.07.004
   Pellegrin L, 2020, INT J REMOTE SENS, V41, P1970, DOI 10.1080/01431161.2019.1681601
   Recla M, 2022, ISPRS J PHOTOGRAMM, V183, P496, DOI 10.1016/j.isprsjprs.2021.11.012
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wegner J. D., 2008, INT ARCH PHOTOGRAMME, V37, P1071
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 19
TC 0
Z9 0
U1 0
U2 0
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2022
VL 43-B2
IS 
BP 1139
EP 1144
DI 10.5194/isprs-archives-XLIII-B2-2022-1139-2022
PG 6
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BT8VT
UT WOS:000855635300153
DA 2023-04-26
ER

PT J
AU Liu, W
   Quijano, K
   Crawford, MM
AF Liu, Wei
   Quijano, Karoll
   Crawford, Melba M.
TI YOLOv5-Tassel: Detecting Tassels in RGB UAV Imagery With Improved YOLOv5 Based on Transfer Learning
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Feature extraction; Object detection; Head; Agriculture; Neck; Deep learning; Transfer learning; CenterNet; SimAM attention module; small tassel detection; transfer learning; YOLOv5
ID object detection; neural-network
AB Unmanned aerial vehicles (UAVs) equipped with lightweight sensors, such as RGB cameras and LiDAR, have significant potential in precision agriculture, including object detection. Tassel detection in maize is an essential trait given its relevance as the beginning of the reproductive stage of growth and development of the plants. However, compared with general object detection, tassel detection based on RGB imagery acquired by UAVs is more challenging due to the small size, time-dependent variable shape, and complexity of the objects of interest. A novel algorithm referred to as YOLOv5-tassel is proposed to detect tassels in UAV-based RGB imagery. A bidirectional feature pyramid network is adopted for the path-aggregation neck to effectively fuse cross-scale features. The robust attention module of SimAM is introduced to extract the features of interest before each detection head. An additional detection head is also introduced to improve small-size tassel detection based on the original YOLOv5. Annotation is performed with guidance from center points derived from CenterNet to improve the selection of the bounding boxes for tassels. Finally, to address the issue of limited reference data, transfer learning based on the VisDrone dataset is adopted. Testing results for our proposed YOLOv5-tassel method achieved the mAP value of 44.7%, which is better than well-known object detection approaches, such as FCOS, RetinaNet, and YOLOv5.
C1 [Liu, Wei; Crawford, Melba M.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Quijano, Karoll] Purdue Univ, Dept Environm & Ecol Engn, W Lafayette, IN 47907 USA.
   [Crawford, Melba M.] Purdue Univ, Lyles Sch Civil Engn, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus
RP Crawford, MM (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.; Crawford, MM (corresponding author), Purdue Univ, Lyles Sch Civil Engn, W Lafayette, IN 47907 USA.
EM liu3044@purdue.edu; kquijano@purdue.edu; mcrawford@purdue.edu
FU Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy [DE-AR0000593]; National Science Foundation (NSF) under NSF [EEC-1941529]
CR Beyer L., 2021, ICLR, V0, P0
   Bhandari M, 2020, COMPUT ELECTRON AGR, V176, P0, DOI 10.1016/j.compag.2020.105665
   Bochkovskiy A, 2020, ARXIV, V0, P0
   Cai ZW, 2018, PROC CVPR IEEE, V0, PP6154, DOI 10.1109/CVPR.2018.00644
   Cao H, 2021, IEEE SENS J, V21, P24540, DOI 10.1109/JSEN.2021.3115016
   Chen G, 2022, IEEE T SYST MAN CY-S, V52, P936, DOI 10.1109/TSMC.2020.3005231
   Chen G, 2020, IEEE SIGNAL PROC MAG, V37, P34, DOI 10.1109/MSP.2020.2985815
   Chen K, 2019, ARXIV, V0, P0
   Chen Q, 2021, PROC CVPR IEEE, V0, PP13034, DOI 10.1109/CVPR46437.2021.01284
   Chen XY, 2021, NEUROCOMPUTING, V450, P14, DOI 10.1016/j.neucom.2021.03.128
   Dai JF, 2017, IEEE I CONF COMP VIS, V0, PP764, DOI 10.1109/ICCV.2017.89
   Delp E. J., 2021, 2021 IEEE INT GEOSCI, V0, PP6280, DOI 10.1109/IGARSS47720.2021.9554291
   Duan KW, 2019, IEEE I CONF COMP VIS, V0, PP6568, DOI 10.1109/ICCV.2019.00667
   Durand E, 2012, GENETICS, V190, P1547, DOI 10.1534/genetics.111.136903
   Ge Z, 2021, ARXIV, V0, P0
   Girshick R, 2015, IEEE I CONF COMP VIS, V0, PP1440, DOI 10.1109/ICCV.2015.169
   Gong B, 2021, SENSORS-BASEL, V21, P0, DOI 10.3390/s21010191
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Han K, 2017, IEEE I CONF COMP VIS, V0, PP1849, DOI 10.1109/ICCV.2017.203
   He KM, 2017, IEEE I CONF COMP VIS, V0, PP2980, DOI 10.1109/TPAMI.2018.2844175
   Hong DF, 2019, IEEE T IMAGE PROCESS, V28, P1923, DOI 10.1109/TIP.2018.2878958
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Jocher G., 2020, YOLOV5, V0, P0, DOI DOI 10.5281
   Karami A, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13152881
   Karami A, 2020, IEEE J-STARS, V13, P5872, DOI 10.1109/JSTARS.2020.3025790
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lin T, 2017, 2017 IEEE INT C COMP, V0, P0, DOI DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, V0, PP936, DOI 10.1109/CVPR.2017.106
   Lin YC, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13050860
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu S, 2018, PROC CVPR IEEE, V0, PP8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2021, IEEE SENS J, V21, P21675, DOI 10.1109/JSEN.2021.3059050
   Liu W, 2020, IET INTELL TRANSP SY, V14, P1183, DOI 10.1049/iet-its.2019.0826
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), V0, PP9992, DOI 10.1109/ICCV48922.2021.00986
   Lu F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), V0, PP15994, DOI 10.1109/ICCV48922.2021.01571
   Buchaillot ML, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19081815
   Mingxing Tan, 2020, 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR). PROCEEDINGS, V0, PP10778, DOI 10.1109/CVPR42600.2020.01079
   Oh S, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12182981
   Papaioannidis Christos, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA), V0, PP11074, DOI 10.1109/ICRA48506.2021.9560830
   Redmon J, 2018, ARXIV, V0, P0, DOI DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Ruan L, 2022, IEEE INTERNET THINGS, V9, P11560, DOI 10.1109/JIOT.2021.3130000
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Tetila EC, 2020, IEEE GEOSCI REMOTE S, V17, P903, DOI 10.1109/LGRS.2019.2932385
   Tian GY, 2021, NEUROCOMPUTING, V443, P292, DOI 10.1016/j.neucom.2021.03.016
   Tian Z, 2019, IEEE I CONF COMP VIS, V0, PP9626, DOI 10.1109/ICCV.2019.00972
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu X, 2019, IEEE T GEOSCI REMOTE, V57, P5146, DOI 10.1109/TGRS.2019.2897139
   Xin W, 2022, IEEE GEOSC REM SEN M, V10, P91, DOI 10.1109/MGRS.2021.3115137
   Xiong L, 2020, IEEE T VEH TECHNOL, V69, P10668, DOI 10.1109/TVT.2020.2983738
   Xu R., 2022, PROC EUR C COMPUT VI, V0, P0
   Xu RS, 2022, ARXIV, V0, P0
   Xue H, 2022, NEUROCOMPUTING, V468, P233, DOI 10.1016/j.neucom.2021.10.024
   Yang CY, 2021, IEEE INT CONF COMP V, V0, PP1390, DOI 10.1109/ICCVW54120.2021.00160
   Yang LX, 2021, PR MACH LEARN RES, V139, P0
   Yifan Wu, 2020, COMPUTER VISION - ECCV 2020 WORKSHOPS. PROCEEDINGS. LECTURE NOTES IN COMPUTER SCIENCE (LNCS 12540), V0, PP450, DOI 10.1007/978-3-030-65414-6_31
   Yinpeng Chen, 2020, 2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR). PROCEEDINGS, V0, PP11027, DOI 10.1109/CVPR42600.2020.01104
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, V0, P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhang S., 2020, CVPR, V0, PP9759, DOI 10.1109/CVPR42600.2020.00978
   Zhang ZX, 2021, IEEE INT CONF COMP V, V0, PP2799, DOI 10.1109/ICCVW54120.2021.00314
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou Q, 2021, IEEE T NEUR NET LEAR, V32, P5298, DOI 10.1109/TNNLS.2021.3093429
   Zhou XY, 2019, PROC CVPR IEEE, V0, PP850, DOI 10.1109/CVPR.2019.00094
   Zhu P., 2020, ARXIV, V0, P0
   Zhu XK, 2021, IEEE INT CONF COMP V, V0, PP2778, DOI 10.1109/ICCVW54120.2021.00312
NR 66
TC 8
Z9 8
U1 81
U2 84
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 8085
EP 8094
DI 10.1109/JSTARS.2022.3206399
PG 10
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 4Y3QO
UT WOS:000861443200003
DA 2023-04-26
ER

PT J
AU Nabi, MM
   Senyurek, V
   Gurbuz, AC
   Kurum, M
AF Nabi, M. M.
   Senyurek, Volkan
   Gurbuz, Ali C.
   Kurum, Mehmet
TI Deep Learning-Based Soil Moisture Retrieval in CONUS Using CYGNSS Delay-Doppler Maps
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Estimation; Scattering; Sea surface; Sea measurements; Global Positioning System; Earth; Surface topography; Convolutional neural network (CNN); Cyclone Global Navigation Satellite System (CYGNSS); deep learning (DL); Global Navigation Satellite System (GNSS)-reflectometry; Soil Moisture Active Passive (SMAP); soil moisture (SM) retrieval
ID gnss-r; ocean; reflectivity; signals
AB National Aeronautics and Space Administration's Cyclone Global Navigation Satellite System (CYGNSS) mission has gained significant attention within the land remote sensing community for estimating soil moisture (SM) by using the Global Navigation System Reflectometry (GNSS-R) technique. CYGNSS constellation generates Delay-Doppler Maps (DDMs), containing important Earth surface information from GNSS reflection measurements. Previous studies considered only designed features from CYGNSS DDM, whereas the whole DDM image is affected by SM, inundation, and vegetation. This paper presents a deep learning (DL) based framework for estimating SM in the Continental United States by leveraging spaceborne GNSS-R DDM observations provided by the CYGNSS constellation along with remotely sensed geophysical data. A data-driven approach utilizing convolutional neural networks (CNNs) is developed to determine complex relationships between the reflected measurements and surface parameters which can provide improved SM estimation. The model is trained jointly with three types of processed DDM images of analog power, effective scattering area, and bistatic radar cross-section with other auxiliary geophysical information such as elevation, soil properties, and vegetation water content (VWC). The model is trained and evaluated using the Soil Moisture Active Passive (SMAP) mission's enhanced SM products at a 9 km resolution with VWC less than 5 kg/m(2). The mean unbiased root-mean-square difference between concurrent CYGNSS and SMAP SM retrievals from 2017 to 2020 is 0.0366 m(3)/m(3) with a correlation coefficient of 0.93 over fivefold cross-validation and 0.0333 m(3)/m(3) with a correlation coefficient of 0.94 over year-based cross-validation at spatial resolution of 9 km and temporal resolution similar to CYGNSS mission.
C1 [Nabi, M. M.; Gurbuz, Ali C.; Kurum, Mehmet] Mississippi State Univ, Dept Elect & Comp Engn, Informat Proc & Sensing Lab, Mississippi State, MS 39672 USA.
   [Senyurek, Volkan] Mississippi State Univ, Geosyst Res Inst, Mississippi State, MS 39579 USA.
C3 Mississippi State University; Mississippi State University
RP Gurbuz, AC (corresponding author), Mississippi State Univ, Dept Elect & Comp Engn, Informat Proc & Sensing Lab, Mississippi State, MS 39672 USA.
EM mn918@msstate.edu; volkan@gri.msstate.edu; gurbuz@ece.msstate.edu; kurum@ece.msstate.edu
FU National Science Foundation [2047771]; USDA-ARS Award [NACA 58-6064-9-007]; Div Of Electrical, Commun & Cyber Sys; Directorate For Engineering [2047771] Funding Source: National Science Foundation
CR Al-Khaldi MM, 2021, IEEE T GEOSCI REMOTE, V59, P4454, DOI 10.1109/TGRS.2020.3009784
   Balakhder AM, 2019, IEEE T GEOSCI REMOTE, V57, P10426, DOI 10.1109/TGRS.2019.2935257
   Camps A, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10111856
   Chan S., 2013, D53061 JPL, V0, P0
   Chan SK, 2018, REMOTE SENS ENVIRON, V204, P931, DOI 10.1016/j.rse.2017.08.025
   Chang C P. Ruf, 2016, CYGNSS HDB CYCLONE G, V0, P154
   Chew CC, 2018, GEOPHYS RES LETT, V45, P4049, DOI 10.1029/2018GL077905
   Chew C, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12101558
   Clarizia M. P., 1900, V12, V0, P2227
   Clarizia MP, 2019, IEEE J-STARS, V12, P2227, DOI 10.1109/JSTARS.2019.2895510
   Dorigo WA, 2011, HYDROL EARTH SYST SC, V15, P1675, DOI 10.5194/hess-15-1675-2011
   Entekhabi D, 1996, J HYDROL, V184, P3, DOI 10.1016/0022-1694(95)02965-6
   Entekhabi D, 2010, P IEEE, V98, P704, DOI 10.1109/JPROC.2010.2043918
   Eroglu O, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11192272
   Fangni Lei, 2021, 2021 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM IGARSS, V0, PP6303, DOI 10.1109/IGARSS47720.2021.9554005
   Fernandez-Moran R, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9050457
   Gleason Scott, 2015, 2015 IEEE MTT-S INTERNATIONAL MICROWAVE SYMPOSIUM (IMS2015), V0, PP1, DOI 10.1109/MWSYM.2015.7166775
   Gleason S, 2014, 1480137 CYGNSS, V0, P0
   Gleason S, 2019, IEEE J-STARS, V12, P37, DOI 10.1109/JSTARS.2018.2832981
   Gruber A, 2013, VADOSE ZONE J, V12, P0, DOI 10.2136/vzj2012.0170
   Guan DL, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10020198
   Hengl T, 2017, PLOS ONE, V12, P0, DOI 10.1371/journal.pone.0169748
   Holzman ME, 2014, INT J APPL EARTH OBS, V28, P181, DOI 10.1016/j.jag.2013.12.006
   Jung M, 2010, NATURE, V467, P951, DOI 10.1038/nature09396
   Kerr YH, 2016, REMOTE SENS ENVIRON, V180, P40, DOI 10.1016/j.rse.2016.02.042
   Kim H, 2018, GEOPHYS RES LETT, V45, P8272, DOI 10.1029/2018GL078923
   Komjathy A, 2004, J ATMOS OCEAN TECH, V21, P515, DOI 10.1175/1520-0426(2004)021<0515:ROOSWS>2.0.CO;2
   Lei FN, 2022, REMOTE SENS ENVIRON, V276, P0, DOI 10.1016/j.rse.2022.113041
   Li WQ, 2020, IEEE T GEOSCI REMOTE, V58, P238, DOI 10.1109/TGRS.2019.2936108
   M-Khaldi MM, 2019, IEEE T GEOSCI REMOTE, V57, P4322, DOI 10.1109/TGRS.2018.2890646
   ONeill P., 2021, JET PROPULSION LAB, V0, P75
   Pekel JF, 2016, NATURE, V540, P418, DOI 10.1038/nature20584
   Peng J, 2017, WATER-SUI, V9, P0, DOI 10.3390/w9070530
   Roberts T. Maximillian, 2021, 2021 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM IGARSS, V0, PP147, DOI 10.1109/IGARSS47720.2021.9555022
   Robinson DA, 2008, VADOSE ZONE J, V7, P358, DOI 10.2136/vzj2007.0143
   Rodriguez-Alvarez N, 2019, REMOTE SENS ENVIRON, V230, P0, DOI 10.1016/j.rse.2019.05.021
   Rodriguez-Alvarez N, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11091053
   Santi E, 2019, INT GEOSCI REMOTE SE, V0, PP8680, DOI 10.1109/IGARSS.2019.8899140
   Senyurek V, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12213503
   Senyurek V, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12071168
   Torres R, 2012, REMOTE SENS ENVIRON, V120, P9, DOI 10.1016/j.rse.2011.05.028
   Valencia E, 2014, IEEE T GEOSCI REMOTE, V52, P3924, DOI 10.1109/TGRS.2013.2278151
   Vereecken H, 2008, WATER RESOUR RES, V44, P0, DOI 10.1029/2008WR006829
   Yan QY, 2020, REMOTE SENS ENVIRON, V247, P0, DOI 10.1016/j.rse.2020.111944
   Yueh SH, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2020.3035989
   Zavorotny VU, 2014, IEEE GEOSC REM SEN M, V2, P8, DOI 10.1109/MGRS.2014.2374220
NR 46
TC 5
Z9 5
U1 4
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 6867
EP 6881
DI 10.1109/JSTARS.2022.3196658
PG 15
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 4F0ZX
UT WOS:000848245900004
DA 2023-04-26
ER
