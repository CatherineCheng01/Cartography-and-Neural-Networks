<?xml version="1.0" encoding="UTF-8"?>
<searchresult>
<query>Global</query>
<document id="1">
<title>Improving Land Cover Segmentation Across Satellites Using Domain Adaptation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3042887</url>
<snippet>Land use and land cover mapping is essential to various fields of study, such as forestry, agriculture, and urban management. Generally, earth observation satellites facilitate and accelerate the mapping process. Subsequently, deep learning methods have been proven to be excellent in automating the mapping via semantic image segmentation. However, because deep neural networks require large amounts of labeled data, it is not easy to exploit the full potential of satellite imagery. Additionally, land cover tends to differ in appearance from one region to another; therefore, having labeled data from one location does not necessarily help map others. Furthermore, satellite images come in various multispectral bands, which range from RGB to over 12 bands. In this study, our aim is to use domain adaptation (DA) to solve the aforementioned problems. We applied a well-performing DA approach on the DeepGlobe land cover dataset as well as datasets that we built using RGB images from Sentinel-2, WorldView-2, and Pleiades-1B satellites with CORINE Land Cover as ground truth (GT) labels. The experiments revealed significant improvements over the results obtained without using DA. In some cases, an improvement of over 20&#37; mean intersection over union was obtained. Sometimes, our model manages to correct errors in the GT labels.
WOS:000607413900045
</snippet>
</document>

<document id="2">
<title>Visualizing Transform Relations of Multilayers in Deep Neural Networks for ISAR Target Recognition</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3200343</url>
<snippet>Deep neural networks (DNNs) achieve state-of-the-art performance in many of the tasks such as image classification, speech recognition, and so on, but the principle of them is like a black box. In this article, we propose a method to combine several connected layers into one layer to visualize the transform relations represented by the connected layers. In theory, this method can visualize the transformation between any two layers in DNNs and is more efficient to analyze the changes of the transformation across different layers compared with other visualization algorithms like deconvolution or saliency maps. Furthermore, we visualize the transform relations not only for a specific input image but the class which all the input images belong to.
WOS:000849263100008
</snippet>
</document>

<document id="3">
<title>Deep Fusion of DOM and DSM Features for Benggang Discovery</title>
<url>http://dx.doi.org/10.3390/ijgi10080556</url>
<snippet>Benggang is a typical erosional landform in southern and southeastern China. Since benggang poses significant risks to local ecological environments and economic infrastructure, it is vital to accurately detect benggang-eroded areas. Relying only on remote sensing imagery for benggang detection cannot produce satisfactory results. In this study, we propose integrating high-resolution Digital Orthophoto Map (DOM) and Digital Surface Model (DSM) data for efficient and automatic benggang discovery. The fusion of complementary rich information hidden in both DOM and DSM data is realized by a two-stream convolutional neural network (CNN), which integrates aggregated terrain and activation image features that are both extracted by supervised deep learning. We aggregate local low-level geomorphic features via a supervised diffusion-convolutional embedding branch for expressive representations of benggang terrain variations. Activation image features are obtained from an image-oriented convolutional neural network branch. The two sources of information (DOM and DSM) are fused via a gated neural network, which learns the most discriminative features for the detection of benggang. The evaluation of a challenging benggang dataset demonstrates that our method exceeds several baselines, even with limited training examples. The results show that the fusion of DOM and DSM data is beneficial for benggang detection via supervised convolutional and deep fusion networks.
WOS:000689272900001
</snippet>
</document>

<document id="4">
<title>Symbol detection in online handwritten graphics using Faster R-CNN</title>
<url>http://dx.doi.org/10.1109/DAS.2018.79</url>
<snippet>Symbol detection techniques in online handwritten graphics (e.g. diagrams and mathematical expressions) consist of methods specifically designed for a single graphic type. In this work, we evaluate the Faster R-CNN object detection algorithm as a general method for detection of symbols in handwritten graphics. We evaluate different configurations of the Faster R-CNN method, and point out issues relative to the handwritten nature of the data. Considering the online recognition context, we evaluate efficiency and accuracy trade-offs of using Deep Neural Networks of different complexities as feature extractors. We evaluate the method on publicly available flowchart and mathematical expression (CROHME-2016) datasets. Results show that Faster R-CNN can be effectively used on both datasets, enabling the possibility of developing general methods for symbol detection, and furthermore, general graphic understanding methods that could be built on top of the algorithm.
WOS:000467070300026
</snippet>
</document>

<document id="5">
<title>Class-Specific Anchor Based and Context-Guided Multi-Class Object Detection in High Resolution Remote Sensing Imagery with a Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/rs11030272</url>
<snippet>In this paper, the problem of multi-scale geospatial object detection in High Resolution Remote Sensing Images (HRRSI) is tackled. The different flight heights, shooting angles and sizes of geographic objects in the HRRSI lead to large scale variance in geographic objects. The inappropriate anchor size to propose the objects and the indiscriminative ability of features for describing the objects are the main causes of missing detection and false detection in multi-scale geographic object detection. To address these challenges, we propose a class-specific anchor based and context-guided multi-class object detection method with a convolutional neural network (CNN), which can be divided into two parts: a class-specific anchor based region proposal network (RPN) and a discriminative feature with a context information classification network. A class-specific anchor block providing better initial values for RPN is proposed to generate the anchor of the most suitable scale for each category in order to increase the recall ratio. Meanwhile, we proposed to incorporate the context information into the original convolutional feature to improve the discriminative ability of the features and increase classification accuracy. Considering the quality of samples for classification, the soft filter is proposed to select effective boxes to improve the diversity of the samples for the classifier and avoid missing or false detection to some extent. We also introduced the focal loss in order to improve the classifier in classifying the hard samples. The proposed method is tested on a benchmark dataset of ten classes to prove the superiority. The proposed method outperforms some state-of-the-art methods with a mean average precision (mAP) of 90.4&#37; and better detects the multi-scale objects, especially when objects show a minor shape change.
WOS:000459944400060
</snippet>
</document>

<document id="6">
<title>Landing Site Detection for UAVs Based on CNNs Classification and Optical Flow from Monocular Camera Images</title>
<url>http://dx.doi.org/10.20965/jrm.2021.p0292</url>
<snippet>The increased use of UAVs (Unmanned Aerial Vehicles) has heightened demands for an automated landing system intended for a variety of tasks and emergency landings. A key challenge of this system is finding a safe landing site in an unknown environment using on-board sensors. This paper proposes a method to generate a heat map for safety evaluation using images from a single on-board camera. The proposed method consists of the classification of ground surface by CNNs (Convolutional Neural Networks) and the estimation of surface flatness from optical flow. We present the results of applying this method to a video obtained from an on-board camera and discuss ways of improving the method.
WOS:000642051000012
</snippet>
</document>

<document id="7">
<title>Magnetic anomalies characterization: Deep learning and explainability</title>
<url>http://dx.doi.org/10.1016/j.cageo.2022.105227</url>
<snippet>In recent years, deep learning methods have shown great promise in the field of geophysics, especially for seismic interpretation. However, there is very little information with regard to its application in the field of magnetic methods. Our research introduces the use of convolutional neural networks for the characterization of magnetic anomalies. The models developed allow the localization of magnetic dipoles, including counting the number of dipoles, their geographical position, and the prediction of their parameters (magnetic moment, depth, and declination). To go even further, we applied visualization tools to understand our model???s predictions and its working principle. The Grad-CAM tool improved prediction performance by identifying several layers that had no influence on the prediction and the t-SNE tool confirmed the strong capacity of our model to differentiate between different parameter combinations. Then, we tested our model with real data to establish its limitations and application domain. Results demonstrate that our model detects dipolar anomalies in a real magnetic map even after learning from a synthetic database with a lower complexity, which indicates a significant general-ization capability. We also noticed that it is unable to identify dipole anomalies of shapes and sizes different from those considered for the creation of the synthetic database. Finally, the perspectives for this work consist of creating a more complex database to approach the complexity traditionally observed in magnetic maps, using real data from multiple acquisition campaigns, and other applications with alternative geophysical methods.
WOS:000867449100004
</snippet>
</document>

<document id="8">
<title>Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data</title>
<url>http://dx.doi.org/10.3390/rs12030547</url>
<snippet>Modern elevation-determining remote sensing technologies such as light-detection and ranging (LiDAR) produce a wealth of topographic information that is increasingly being used in a wide range of disciplines, including archaeology and geomorphology. However, automated methods for mapping topographic features have remained a significant challenge. Deep learning (DL) mask regional-convolutional neural networks (Mask R-CNN), which provides context-based instance mapping, offers the potential to overcome many of the difficulties of previous approaches to topographic mapping. We therefore explore the application of Mask R-CNN to extract valley fill faces (VFFs), which are a product of mountaintop removal (MTR) coal mining in the Appalachian region of the eastern United States. LiDAR-derived slopeshades are provided as the only predictor variable in the model. Model generalization is evaluated by mapping multiple study sites outside the training data region. A range of assessment methods, including precision, recall, and F1 score, all based on VFF counts, as well as area- and a fuzzy area-based users and producers accuracy, indicate that the model was successful in mapping VFFs in new geographic regions, using elevation data derived from different LiDAR sensors. Precision, recall, and F1-score values were above 0.85 using VFF counts while users and producers accuracy were above 0.75 and 0.85 when using the area- and fuzzy area-based methods, respectively, when averaged across all study areas characterized with LiDAR data. Due to the limited availability of LiDAR data until relatively recently, we also assessed how well the model generalizes to terrain data created using photogrammetric methods that characterize past terrain conditions. Unfortunately, the model was not sufficiently general to allow successful mapping of VFFs using photogrammetrically-derived slopeshades, as all assessment metrics were lower than 0.60; however, this may partially be attributed to the quality of the photogrammetric data. The overall results suggest that the combination of Mask R-CNN and LiDAR has great potential for mapping anthropogenic and natural landscape features. To realize this vision, however, research on the mapping of other topographic features is needed, as well as the development of large topographic training datasets including a variety of features for calibrating and testing new methods.
WOS:000515393800206
</snippet>
</document>

<document id="9">
<title>A monocular visual SLAM system augmented by lightweight deep local feature extractor using in-house and low-cost LIDAR-camera integrated device</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2138591</url>
<snippet>Simultaneous Localization and Mapping (SLAM) has been widely used in emergency response, self-driving and city-scale 3D mapping and navigation. Recent deep-learning based feature point extractors have demonstrated superior performance in dealing with the complex environmental challenges (e.g. extreme lighting) while the traditional extractors are struggling. In this paper, we have successfully improved the robustness and accuracy of a monocular visual SLAM system under various complex scenes by adding a deep learning based visual localization thread as an augmentation to the visual SLAM framework. In this thread, our feature extractor with an efficient lightweight deep neural network is used for absolute pose and scale estimation in real time using the highly accurate georeferenced prior map database at 20cm geometric accuracy created by our in-house and low-cost LiDAR and camera integrated device. The closed-loop error provided by our SLAM system with and without this enhancement is 1.03m and 18.28m respectively. The scale estimation of the monocular visual SLAM is also significantly improved (0.01 versus 0.98). In addition, a novel camera-LiDAR calibration workflow is also provided for large-scale 3D mapping. This paper demonstrates the application and research potential of deep-learning based vision SLAM with image and LiDAR sensors.
WOS:000879347300001
</snippet>
</document>

<document id="10">
<title>Detecting Pipeline Pathways in Landsat 5 Satellite Images with Deep Learning</title>
<url>http://dx.doi.org/10.3390/en14185642</url>
<snippet>Energy system modeling is essential in analyzing present and future system configurations motivated by the energy transition. Energy models need various input data sets at different scales, including detailed information about energy generation and transport infrastructure. However, accessing such data sets is not straightforward and often restricted, especially for energy infrastructure data. We present a detection model for the automatic recognition of pipeline pathways using a Convolutional Neural Network (CNN) to address this lack of energy infrastructure data sets. The model was trained with historical low-resolution satellite images of the construction phase of British gas transport pipelines, made with the Landsat 5 Thematic Mapper instrument. The satellite images have been automatically labeled with the help of high-resolution pipeline route data provided by the respective Transmission System Operator (TSO). We have used data augmentation on the training data and trained our model with four different initial learning rates. The models trained with the different learning rates have been validated with 5-fold cross-validation using the Intersection over Union (IoU) metric. We show that our model can reliably identify pipeline pathways despite the comparably low resolution of the used satellite images. Further, we have successfully tested the models capability in other geographic regions by deploying satellite images of the NEL pipeline in Northern Germany.
WOS:000699301700001
</snippet>
</document>

<document id="11">
<title>Simultaneous Extraction of Road and Centerline from Aerial Images Using a Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/ijgi10030147</url>
<snippet>The extraction of roads and centerlines from aerial imagery is considered an important topic because it contributes to different fields, such as urban planning, transportation engineering, and disaster mitigation. Many researchers have studied this topic as a two-separated task that affects the quality of extracted roads and centerlines because of the correlation between these two tasks. Accurate road extraction enhances accurate centerline extraction if these two tasks are processed simultaneously. This study proposes a multitask learning scheme using a gated deep convolutional neural network (DCNN) to extract roads and centerlines simultaneously. The DCNN is composed of one encoder and two decoders implemented on the U-Net backbone. The decoders are assigned to extract roads and centerlines from low-resolution feature maps. Before extraction, the images are processed within an encoder to extract the spatial information from a complex, high-resolution image. The encoder consists of the residual blocks (Res-Block) connected to a bridge represented by a Res-Block, and the bridge connects the two identical decoders, which consists of stacking convolutional layers (Conv.layer). Attention gates (AGs) are added to our model to enhance the selection process for the true pixels that represent road or centerline classes. Our model is trained on a dataset of high-resolution aerial images, which is open to the public. The model succeeds in efficiently extracting roads and centerlines compared with other multitask learning models.
WOS:000633715800001
</snippet>
</document>

<document id="12">
<title>WEAKLY SUPERVISED LEARNING FOR TREELINE ECOTONE CLASSIFICATION BASED ON AERIAL ORTHOIMAGES AND AN ANCILLARY DSM</title>
<url>http://dx.doi.org/10.5194/isprs-annals-V-3-2022-33-2022</url>
<snippet>Convolutional neural networks (CNNs) effectively classify standard datasets in remote sensing (RS). Yet, real-world data are more difficult to classify using CNNs because these networks require relatively large amounts of training data. To reduce training data requirements, two approaches can be followed - either pretraining models on larger datasets or augmenting the available training data. However, these commonly used strategies do not fully resolve the lack of training data for land cover classification in RS. Our goal is to classify trees and shrubs from aerial orthoimages in the treeline ecotone of the Krkonose Mountains, Czechia. Instead of training a model on a smaller, human-labelled dataset, we semiautomatically created training data using an ancillary normalised Digital Surface Model (nDSM) and image spectral information. This approach can complement existing techniques, trading accuracy for a larger labelled dataset while assuming that the classifier can handle the training data noise. Weakly supervised learning on a CNN led to 68.99&#37; mean Intersection over Union ( IoU) and 81.65&#37; mean F1-score for U-Net and 72.94&#37; IoU and 84.35&#37; mean F1-score for our modified U-Net on a test set comprising over 1000 manually labelled points. Notwithstanding the bias resulting from the noise in training data (especially in the least occurring tree class), our data show that standard semantic segmentation networks can be used for weakly supervised learning for local-scale land cover mapping.
WOS:000855203200006
</snippet>
</document>

<document id="13">
<title>A Deep Learning Assisted Ground Penetrating Radar Localization Method</title>
<url>http://dx.doi.org/10.11999/JEIT211072</url>
<snippet>Under harsh conditions, such as rain, snow, dust, strong light, and dark night, the vision and laser sensors commonly used in autonomous driving solutions may fail because they can not accurately sense the external environment. Therefore, a method for vehicle localization using underground target features sensed by deep learning assisted ground penetrating radar is proposed in this paper. The proposed method is divided into two phases: offline mapping phase and online localization phase. In the offline mapping phase, the ground penetrating radar is used to collect the echo data from the underground targets first, then the Deep Convolutional Neural Network (DCNN) is utilized to extract the target features from the collected echo data, and the extracted target features are saved with the current geographic location information to form a fingerprint map of underground target features. In the localization phase, the DCNN is used to extract the target features from the current echo data collected by the ground penetrating radar first, and then the target feature most similar to the current extracted target feature in the fingerprint map of underground target features is retrieved based on the particle swarm optimization method, and the geographic location information of the retrieved feature is marked as the vehicle localization result by the ground penetrating radar. Finally, the Kalman filter is used to fuse the ground penetrating radar localization result and the mileage information measured by the ranging wheel to obtain a high-precision localization result. The localization performance of the proposed localization method is tested on the experimental scenario with rich underground targets and the actual urban road scenario. The experimental results show that, compared with the single raw data-based ground penetrating radar localization method, the deep learning assisted ground penetrating radar localization method can avoid directly calculating the similarity between the raw radar data, reduce the amount of data computation and data transmission, and has the real-time localization capability. At the same time, the fingerprint map of underground target features is robust to the changes of the raw radar data, so the average localization error of the proposed method is reduced by about 70&#37;. The deep learning assisted ground penetrating radar localization method can be used as a supplement to the detection and localization method of autonomous vehicles in harsh environments in the future.
WOS:000810212800003
</snippet>
</document>

<document id="14">
<title>Vision based crown loss estimation for individual trees with remote aerial robots</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.04.002</url>
<snippet>With the capability of capturing high-resolution imagery data and the ease of accessing remote areas, aerial robots are becoming increasingly popular for forest health monitoring applications. For example, forestry tasks such as field surveys and foliar sampling which are generally manual and labour intensive can be automated with remotely controlled aerial robots. In this study, we propose two new online frameworks to quantify and rank the severity of individual tree crown loss. The real-time crown loss estimation (RTCLE) model localises and classifies individual trees into their respective crown loss percentage bins. Experiments are conducted to investigate if synthetically generated tree images can be used to train the RTCLE model as real images with diverse viewpoints are generally expensive to collect. Results have shown that synthetic data training helps to achieve a satisfactory baseline mean average precision (mAP) which can be further improved with just some additional real imagery data. We showed that the mAP can be increased approximately from 60&#37; to 78&#37; by mixing the real dataset with the generated synthetic data. For individual tree crown loss ranking, a two-step crown loss ranking (TSCLR) framework is developed to handle the inconsistently labelled crown loss data. The TSCLR framework detects individual trees before ranking them based on some relative crown loss severity measures. The tree detection model is trained with the combined dataset used in the RTCLE model training where we achieved an mAP of approximately 95&#37; suggesting that the model generalises well to unseen datasets. The relative crown loss severity of each tree is estimated, with deep representation learning, by a probabilistic encoder from a fully trained variational autoencoder (VAE) model. The VAE is trained end-to-end to reconstruct tree images in a background agnostic way. Based on a conservative evaluation, the estimated crown loss severity from the probabilistic encoder generally showed moderate agreement with the experts estimation across all species of trees present in the dataset. All the software pipelines, the dataset, and the synthetic dataset generation can be found in the GitHub link.
WOS:000806358700003
</snippet>
</document>

<document id="15">
<title>USE OF MACHINE LEARNING TECHNIQUES FOR RAPID DETECTION, ASSESSMENT AND MAPPING THE IMPACT OF DISASTERS ON TRANSPORT INFRASTRUCTURE</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLII-3-W8-195-2019</url>
<snippet>Road traffic infrastructure plays a key role in emergency management. It allows to evacuate people from the affected area in the shortest possible time, as well as to organize rapid emergency response. However, disasters often cause the destruction of roads, railways and pedestrian routes, which can significantly affect the evacuation plan and availability of facilities for emergency services, which increases the response time and thereby increases the losses. Therefore, it is very important to quickly provide emergency services with necessary post-disaster maps, created on the principles of rapid mapping. Change detection based on geospatial data before and after damage can make rapid and automatic assessment possible with reasonable accuracy and speed. This research proposes a new approach for detecting damage and detecting the state and availability of the road network based on the satellite imagery data, unmanned aerial vehicles (UAVs) and SAR using various methods of image analysis. We also provided an assessment of the resulting combined mathematical model based on neural networks and spatial analysis approaches.
WOS:000684596600032
</snippet>
</document>

<document id="16">
<title>KNOWLEDGE BASED 3D BUILDING MODEL RECOGNITION USING CONVOLUTIONAL NEURAL NETWORKS FROM LIDAR AND AERIAL IMAGERIES</title>
<url>http://dx.doi.org/10.5194/isprsarchives-XLI-B3-833-2016</url>
<snippet>In recent years, with the development of the high resolution data acquisition technologies, many different approaches and algorithms have been presented to extract the accurate and timely updated 3D models of buildings as a key element of city structures for numerous applications in urban mapping. In this paper, a novel and model-based approach is proposed for automatic recognition of buildings roof models such as flat, gable, hip, and pyramid hip roof models based on deep structures for hierarchical learning of features that are extracted from both LiDAR and aerial ortho-photos. The main steps of this approach include building segmentation, feature extraction and learning, and finally building roof labeling in a supervised pre-trained Convolutional Neural Network (CNN) framework to have an automatic recognition system for various types of buildings over an urban area. In this framework, the height information provides invariant geometric features for convolutional neural network to localize the boundary of each individual roofs. CNN is a kind of feed-forward neural network with the multilayer perceptron concept which consists of a number of convolutional and subsampling layers in an adaptable structure and it is widely used in pattern recognition and object detection application. Since the training dataset is a small library of labeled models for different shapes of roofs, the computation time of learning can be decreased significantly using the pre-trained models. The experimental results highlight the effectiveness of the deep learning approach to detect and extract the pattern of buildings roofs automatically considering the complementary nature of height and RGB information.
WOS:000392743800121
</snippet>
</document>

<document id="17">
<title>Assessment of Landslide Susceptibility Combining Deep Learning with Semi-Supervised Learning in Jiaohe County, Jilin Province, China</title>
<url>http://dx.doi.org/10.3390/app10165640</url>
<snippet>Accurate and timely landslide susceptibility mapping (LSM) is essential to effectively reduce the risk of landslide. In recent years, deep learning has been successfully applied to landslide susceptibility assessment due to the strong ability of fitting. However, in actual applications, the number of labeled samples is usually not sufficient for the training component. In this paper, a deep neural network model based on semi-supervised learning (SSL-DNN) for landslide susceptibility is proposed, which makes full use of a large number of spatial information (unlabeled data) with limited labeled data in the region to train the mode. Taking Jiaohe County in Jilin Province, China as an example, the landslide inventory from 2000 to 2017 was collected and 12 metrological, geographical, and human explanatory factors were compiled. Meanwhile, supervised models such as deep neural network (DNN), support vector machine (SVM), and logistic regression (LR) were implemented for comparison. Then, the landslide susceptibility was plotted and a series of evaluation tools such as class accuracy, predictive rate curves (AUC), and information gain ratio (IGR) were calculated to compare the prediction of models and factors. Experimental results indicate that the proposed SSL-DNN model (AUC = 0.898) outperformed all the comparison models. Therefore, semi-supervised deep learning could be considered as a potential approach for LSM.
WOS:000564644700001
</snippet>
</document>

<document id="18">
<title>Fractional Vegetation Cover Estimation Algorithm Based on Recurrent Neural Network for MODIS 250 m Reflectance Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3075624</url>
<snippet>Fractional vegetation cover (FVC) is a critical land surface parameter, and several large-scale FVC products have been generated based on remote sensing data. Among these existing products, the global land surface satellite (GLASS) FVC product, derived from moderate resolution imaging spectroradiometer (MODIS) 500 m reflectance data (MOD09A1), has achieved complete spatial-temporal continuity and satisfying accuracy. To further improve the spatial resolution of GLASS FVC product, this study developed a novel FVC estimation algorithm for MODIS 250 m reflectance data based on a recurrent neural network with the long short-term memory unit (RNN-LSTM). The RNN-LSTM was established using sequence training samples derived from the MODIS 250 m reflectance and GLASS FVC products, which were conducted over three vegetation types in mid-West China. Additionally, two machine learning methods, including the back propagation neural network (BPNN) and multivariate adaptive regression splines (MARS), were used to compare with the proposed method. The evaluation results showed that RNN-LSTM derived FVC had reliable spatial-temporal continuity and good consistency with the GLASS FVC product. Furthermore, the smooth temporal profiles of the RNN-LSTM FVC estimation indicated that the proposed method was capable of capturing the temporal characteristics of vegetation growth and reducing the uncertainties from the atmosphere and radiation. Finally, an independent validation case in the Heihe area indicated that the RNN-LSTM algorithm achieved the best accuracy (R-2 = 0.8081, rmse = 0.0951) compared with the BPNN (R-2 = 0.7320, rmse = 0.1127) and MARS (R-2 = 0.7361, rmse = 0.1117). This study provides a new approach by showing the potential of the RNN-LSTM method for land surface parameter estimation and related research.
WOS:000673442300013
</snippet>
</document>

<document id="19">
<title>Inconsistency among Landsat Sensors in Land Surface Mapping: A Comprehensive Investigation Based on Simulation</title>
<url>http://dx.doi.org/10.3390/rs13071383</url>
<snippet>Comprehensive investigations on the between-sensor comparability among Landsat sensors have been relatively limited compared with the increasing use of multi-temporal Landsat records in time series analyses. More seriously, the sensor-related difference has not always been considered in applications. Accordingly, comparisons were conducted among all Landsat sensors available currently, including Multispectral Scanner (MSS), Thematic Mappers (TM), Enhanced Thematic Mappers (ETM+), and Operational Land Imager (OLI)) in land cover mapping, based on a collection of synthesized, multispectral data. Compared to TM, OLI showed obvious between-sensor differences in channel reflectance, especially over the near infrared (NIR) and shortwave infrared (SWIR) channels, and presented positive bias in vegetation spectral indices. OLI did not always outperform TM and ETM+ in classification, which related to the methods used. Furthermore, the channels over SWIR of TM and its successors contributed largely to enhancement of inter-class separability and to improvement of classification. Currently, the inclusion of MSS data is confronted with significant challenges regarding the consistency of surface mapping. Considering the inconsistency among the Landsat sensors, it is applicable to generate a consistent time series of spectral indices through proper transformation models. Meanwhile, it suggests the generation of specific class(es) based on interest instead of including all classes simultaneously.
WOS:000638788500001
</snippet>
</document>

<document id="20">
<title>Chimney Detection Based on Faster R-CNN and Spatial Analysis Methods in High Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/s20164353</url>
<snippet>Spatially location and working status of pollution sources are very important pieces of information for environment protection. Waste gas produced by fossil fuel consumption in the industry is mainly discharged to the atmosphere through a chimney. Therefore, detecting the distribution of chimneys and their working status is of great significance to urban environment monitoring and environmental governance. In this paper, we use an open access dataset BUAA-FFPP60 and the faster regions with convolutional neural network (Faster R-CNN) algorithm to train the preliminarily detection model. Then, the trained model is used to detect the chimneys in three high-resolution remote sensing images of Google Maps, which is located in Tangshan city. The results show that a large number of false positive targets are detected. For working chimney detection, the recall rate is 77.27&#37;, but the precision is only 40.47&#37;. Therefore, two spatial analysis methods, the digital terrain model (DTM) filtering, and main direction test are introduced to remove the false chimneys. The DTM is generated by ZiYuan-3 satellite images and then registered to the high-resolution image. We set an elevation threshold to filter the false positive targets. After DTM filtering, we use principle component analysis (PCA) to calculate the main direction of each target image slice, and then use the main direction to remove false positive targets further. The results show that by using the combination of DTM filtering and main direction test, more than 95&#37; false chimneys can be removed and, therefore, the detection precision is significantly increased.
WOS:000568036000001
</snippet>
</document>

<document id="21">
<title>An ANNs-Based Method for Automated Labelling of Schematic Metro Maps</title>
<url>http://dx.doi.org/10.3390/ijgi11010036</url>
<snippet>Schematic maps are popular for representing transport networks. In the last two decades, some researchers have been working toward automated generation of network layouts (i.e., the network geometry of schematic maps), while automated labelling of schematic maps is not well considered. The descriptive-statistics-based labelling method, which models the labelling space by defining various station-based line relations in advance, has been specially developed for schematic maps. However, if a certain station-based line relation is not predefined in the database, this method may not be able to infer suitable labelling positions under this relation. It is noted that artificial neural networks (ANNs) have the ability to infer unseen relations. In this study, we aim to develop an ANNs-based method for the labelling of schematic metro maps. Samples are first extracted from representative schematic metro maps, and then they are employed to train and test ANNs models. Five types of attributes (e.g., station-based line relations) are used as inputs, and two types of attributes (i.e., directions and positions of labels) are used as outputs. Experiments show that this ANNs-based method can generate effective and satisfactory labelling results in the testing cases. Such a method has potential to be extended for the labelling of other transport networks.
WOS:000748791400001
</snippet>
</document>

<document id="22">
<title>Drone-borne sensing of major and accessory pigments in algae using deep learning modeling</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2027120</url>
<snippet>Intensive algal blooms increasingly degrade the inland water quality. Hence, this study aimed to analyze the algal phenomena quantitatively and qualitatively using synoptic monitoring, algal pigment analysis, and a deep learning model. Water surface reflectance was measured using field monitoring and drone hyperspectral image sensing. The algal experiment conducted on the water samples provided data on major pigments including chlorophyll-a and phycocyanin, accessory pigments including lutein, fucoxanthin, and zeaxanthin, and absorption coefficients. Based on the reflectance and absorption coefficient spectral inputs, a one-dimensional convolutional neural network (1D-CNN) was developed to estimate the concentrations of the major and minor pigments. The 1D-CNN could model periodic trends of chlorophyll-a, phycocyanin, lutein, fucoxanthin, and zeaxanthin compared to the observed ones, with R-2 values of 0.87, 0.71, 0.76, 0.78, and 0.74, respectively. In addition, major and secondary pigment maps developed by applying the trained 1D-CNN model to the processed drone hyperspectral image inputs successfully provided spatial information regarding the spots of interest. The model provided explicit algal biomass information using the estimated major pigments and implicit taxonomical information using accessory pigments such as green algae, diatoms, and cyanobacteria. Therefore, we provide strong evidence of the extendibility of deep learning models for analyzing various algal pigments to gain a better understanding of algal blooms.
WOS:000750121600001
</snippet>
</document>

<document id="23">
<title>A New Parallel Dual-Channel Fully Convolutional Network Via Semi-Supervised FCM for PolSAR Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3014966</url>
<snippet>Convolutional neural network (CNN) has achieved remarkable success in polarimetric synthetic aperture radar (PolSAR) image classification. However, the PolSAR image classification is a pixelwise prediction assignment. The disadvantages of repeated calculation, memory occupation, and inadequate labeled pixels make that CNN is less efficient when classifying PolSAR images. Compared to CNN, fully convolutional network (FCN) has obvious merits for PolSAR image classification: take arbitrary-size images as input; output a 3-D dense class map to maintain the structure of the input; settle PolSAR image classification tasks with efficient dense learning, which avoids repeated calculation and memory occupation. In view of accuracy, efficiency, and insufficient labeled pixels, our method proposes a new parallel dual-channel dilated fully convolutional network (DCDFCN) for PolSAR image classification. First, limited by insufficient labeled samples, the semi-supervised fuzzy c-means clustering algorithm is exploited to enlarge the labeled samples set. Then, to improve the density of output feature maps, we design a new FCN variant named dilated FCN (DFCN) by introducing a dilated convolution block to FCN. Finally, we use two similar DFCN frameworks in parallel with different convolutional kernels to design a new DCDFCN model, which can extract more discriminate features of PolSAR images. Experimental results on three PolSAR images demonstrate that the performance of our approach is superior to several state-of-the-art methods for PolSAR image classification.
WOS:000562040800003
</snippet>
</document>

<document id="24">
<title>Using Deep Learning and Very-High-Resolution Imagery to Map Smallholder Field Boundaries</title>
<url>http://dx.doi.org/10.3390/rs14133046</url>
<snippet>The mapping of field boundaries can provide important information for increasing food production and security in agricultural systems across the globe. Remote sensing can provide a viable way to map field boundaries across large geographic extents, yet few studies have used satellite imagery to map boundaries in systems where field sizes are small, heterogeneous, and irregularly shaped. Here we used very-high-resolution WorldView-3 satellite imagery (0.5 m) and a mask region-based convolutional neural network (Mask R-CNN) to delineate smallholder field boundaries in Northeast India. We found that our models had overall moderate accuracy, with average precision values greater than 0.67 and F1 Scores greater than 0.72. We also found that our model performed equally well when applied to another site in India for which no data were used in the calibration step, suggesting that Mask R-CNN may be a generalizable way to map field boundaries at scale. Our results highlight the ability of Mask R-CNN and very-high-resolution imagery to accurately map field boundaries in smallholder systems.
WOS:000824389000001
</snippet>
</document>

<document id="25">
<title>Exploring convolutional neural networks and spatial video for on-the-ground mapping in informal settlements</title>
<url>http://dx.doi.org/10.1186/s12942-021-00259-z</url>
<snippet>Background The health burden in developing world informal settlements often coincides with a lack of spatial data that could be used to guide intervention strategies. Spatial video (SV) has proven to be a useful tool to collect environmental and social data at a granular scale, though the effort required to turn these spatially encoded video frames into maps limits sustainability and scalability. In this paper we explore the use of convolution neural networks (CNN) to solve this problem by automatically identifying disease related environmental risks in a series of SV collected from Haiti. Our objective is to determine the potential of machine learning in health risk mapping for these environments by assessing the challenges faced in adequately training the required classification models. Results We show that SV can be a suitable source for automatically identifying and extracting health risk features using machine learning. While well-defined objects such as drains, buckets, tires and animals can be efficiently classified, more amorphous masses such as trash or standing water are difficult to classify. Our results further show that variations in the number of image frames selected, the image resolution, and combinations of these can be used to improve the overall model performance. Conclusion Machine learning in combination with spatial video can be used to automatically identify environmental risks associated with common health problems in informal settlements, though there are likely to be variations in the type of data needed for training based on location. Success based on the risk type being identified are also likely to vary geographically. However, we are confident in identifying a series of best practices for data collection, model training and performance in these settings. We also discuss the next step of testing these findings in other environments, and how adding in the simultaneously collected geographic data could be used to create an automatic health risk mapping tool.
WOS:000611537200001
</snippet>
</document>

<document id="26">
<title>Deep Learning to Unveil Correlations between Urban Landscape and Population Health</title>
<url>http://dx.doi.org/10.3390/s20072105</url>
<snippet>The global healthcare landscape is continuously changing throughout the world as technology advances, leading to a gradual change in lifestyle. Several diseases such as asthma and cardiovascular conditions are becoming more diffuse, due to a rise in pollution exposure and a more sedentary lifestyle. Healthcare providers deal with increasing new challenges, and thanks to fast-developing big data technologies, they can be faced with systems that provide direct support to citizens. In this context, within the EU-funded Participatory Urban Living for Sustainable Environments (PULSE) project, we are implementing a data analytic platform designed to provide public health decision makers with advanced approaches, to jointly analyze maps and geospatial information with healthcare and air pollution data. In this paper we describe a component of such platforms, which couples deep learning analysis of urban geospatial images with healthcare indexes collected by the 500 Cities project. By applying a pre-learned deep Neural Network architecture, satellite images ofNewYork City are analyzed and latent feature variables are extracted. These features are used to derive clusters, which are correlated with healthcare indicators by means of a multivariate classification model. Thanks to this pipeline, it is possible to show that, in New York City, health care indexes are significantly correlated to the urban landscape. This pipeline can serve as a basis to ease urban planning, since the same interventions can be organized on similar areas, even if geographically distant.
WOS:000537110500301
</snippet>
</document>

<document id="27">
<title>CLNet: Cross-layer convolutional neural network for change detection in optical remote sensing imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.03.005</url>
<snippet>Change detection plays a crucial role in observing earth surface transition and has been widely investigated using deep learning methods. However, the current deep learning methods for pixel-wise change detection still suffer from limited accuracy, mainly due to their insufficient feature extraction and context aggregation. To address this limitation, we propose a novel Cross Layer convolutional neural Network (CLNet) in this paper, where the UNet structure is used as the backbone and newly designed Cross Layer Blocks (CLBs) are embedded to incorporate the multi-scale features and multi-level context information. The designed CLB starts with one input and then split into two parallel but asymmetric branches, which are leveraged to extract the multi-scale features by using different strides; and the feature maps, which come from the opposite branches but have the same size, are concatenated to incorporate multi-level context information. The designed CLBs aggregate the multi-scale features and multi-level context information so that the proposed CLNet can reuse extracted feature information and capture accurate pixel-wise change in complex scenes. Quantitative and qualitative experiments were conducted on a public very-high-resolution satellite image dataset (VHR-Dataset), a newly released building change detection dataset (LEVIR-CD Dataset) and an aerial building change detection dataset (WHU Building Dataset). The CLNet reached an F1-score of 0.921 and an overall accuracy of 98.1&#37; with the VHR-Dataset, an F1-score of 0.900 and an overall accuracy of 98.9&#37; with the LEVIR-CD Dataset, and an F1-score of 0.963 and an overall accuracy of 99.7&#37; with the WHU Building Dataset. The experimental results with all the selected datasets showed that the proposed CLNet outperformed several state-of-the-art (SOTA) methods and achieved competitive accuracy and efficiency trade-offs. The code of CLNet will be released soon at: https://skyearth.org/publication/project/CLNet.
WOS:000644695700018
</snippet>
</document>

<document id="28">
<title>A Review of Landcover Classification with Very-High Resolution Remotely Sensed Optical Images-Analysis Unit, Model Scalability and Transferability</title>
<url>http://dx.doi.org/10.3390/rs14030646</url>
<snippet>As an important application in remote sensing, landcover classification remains one of the most challenging tasks in very-high-resolution (VHR) image analysis. As the rapidly increasing number of Deep Learning (DL) based landcover methods and training strategies are claimed to be the state-of-the-art, the already fragmented technical landscape of landcover mapping methods has been further complicated. Although there exists a plethora of literature review work attempting to guide researchers in making an informed choice of landcover mapping methods, the articles either focus on the review of applications in a specific area or revolve around general deep learning models, which lack a systematic view of the ever advancing landcover mapping methods. In addition, issues related to training samples and model transferability have become more critical than ever in an era dominated by data-driven approaches, but these issues were addressed to a lesser extent in previous review articles regarding remote sensing classification. Therefore, in this paper, we present a systematic overview of existing methods by starting from learning methods and varying basic analysis units for landcover mapping tasks, to challenges and solutions on three aspects of scalability and transferability with a remote sensing classification focus including (1) sparsity and imbalance of data; (2) domain gaps across different geographical regions; and (3) multi-source and multi-view fusion. We discuss in detail each of these categorical methods and draw concluding remarks in these developments and recommend potential directions for the continued endeavor.
WOS:000756126900001
</snippet>
</document>

<document id="29">
<title>Modelling Spatial Drivers for LU/LC Change Prediction Using Hybrid Machine Learning Methods in Javadi Hills, Tamil Nadu, India</title>
<url>http://dx.doi.org/10.1007/s12524-020-01258-6</url>
<snippet>The land-use/land-cover (LU/LC) information can be extracted through continuous monitoring and observation of the global environment in the field of RS and GIS (remote sensing and geographic information system). With many inventions on satellite technologies, RS plays a crucial role throughout the world, and the researchers had shown their interest in finding the past, present, and future LU/LC information using the RS satellite data. In this research work, the non-forest- and forest-covered changes of Javadi Hills located in India were simulated and predicted using the hybrid machine learning models. The Markov chain-artificial neural network with cellular automata (MC-ANN-CA) and Markov chain-logistic regression with cellular automata (MC-LR-CA) were used and compared using the actual LU/LC maps of 2009, 2012, and 2015 along with the spatial variables (slope, aspect, hill shade, and distance road map). The results of the comparative analysis between the predicted and actual map of 2015 had shown a higher percentage of correctness in the MC-ANN-CA model for the spatial variables like slope, aspect, and distance road map. The LU/LC for 2021 and 2027 was predicted using the MC-ANN-CA model. By 2021, the forest-covered area will decrease by nearly - 0.38&#37;, and the non-forest-covered area will increase by 0.79&#37;. By 2027, forest-covered areas will decrease by - 0.52&#37;, and non-forest-covered areas will increase by 1.06&#37;, respectively, indicating the impacts of human and urbanization on LU/LC in Javadi Hills.
WOS:000591961400002
</snippet>
</document>

<document id="30">
<title>Super-Resolution Land Cover Mapping Based on the Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/rs11151815</url>
<snippet>Super-resolution mapping (SRM) is used to obtain fine-scale land cover maps from coarse remote sensing images. Spatial attraction, geostatistics, and using prior geographic information are conventional approaches used to derive fine-scale land cover maps. As the convolutional neural network (CNN) has been shown to be effective in capturing the spatial characteristics of geographic objects and extrapolating calibrated methods to other study areas, it may be a useful approach to overcome limitations of current SRM methods. In this paper, a new SRM method based on the CNN (SRMCNN) is proposed and tested. Specifically, an encoder-decoder CNN is used to model the nonlinear relationship between coarse remote sensing images and fine-scale land cover maps. Two real-image experiments were conducted to analyze the effectiveness of the proposed method. The results demonstrate that the overall accuracy of the proposed SRMCNN method was 3&#37; to 5&#37; higher than that of two existing SRM methods. Moreover, the proposed SRMCNN method was validated by visualizing output features and analyzing the performance of different geographic objects.
WOS:000482442800079
</snippet>
</document>

<document id="31">
<title>Super-Resolution for MIMO Array SAR 3-D Imaging Based on Compressive Sensing and Deep Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3000760</url>
<snippet>Multiple-input multiple-output (MIMO) array synthetic aperture radar (SAR) can straightly obtain the 3-D imagery of the illuminated scene with the single-pass flight. Generally, the Rayleigh resolution of the elevation direction is unacceptable due to the length limitation of linear array. The super-resolution imaging algorithms within the compressive sensing (CS) framework have been extensively studied because of the essential spatial sparsity in the elevation direction. However, the super-resolution performance of the existing sparse reconstruction algorithms will deteriorate dramatically in the case of lower signal-to-noise ratio (SNR) level or a few antenna elements. To overcome this problem, a new super-resolution imaging structure based on CS and deep neural network (DNN) for MIMO array SAR is proposed in this article. In this new algorithm, the spatial filtering based on CS is first proposed to reserve the signals only impinging from the prespecified space subregions. Thereafter, a group of parallel end-to-end DNN regression models are designed for mapping the potential sparse recovery mathematical model and further locating the true scatterers in the elevation direction. Finally, extensive simulations and airborne MIMO array SAR experiments are investigated to validate that the proposed method can realize the state-of-the-art super-resolution imaging against other existing related methods.
WOS:000545017200001
</snippet>
</document>

<document id="32">
<title>Deep-Learning and Depth-Map Based Approach for Detection and 3-D Localization of Small Traffic Signs</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2966543</url>
<snippet>The three-dimensional (3-D) geographic locations of street furniture, such as traffic signs, comprise the basic content of 3-D city construction, and such information is indispensable for periodic statistics for road management and maintenance. This article presents a novel solution for acquiring 3-D information on small traffic signs based on mobile mapping system (MMS) data. First, a lightweight backbone network called VGG-L under an optimized faster region-based convolutional neural network detection framework is proposed for the detection of small traffic signs. An urban traffic sign detection (UTSD) dataset is created based on panoramic images obtained from test areas. Detection results from the UTSD dataset show that VGG-L outperforms other popular networks and achieves a mean average precision of 75.4&#37;, which is 4.2&#37;-14.8&#37; higher than those of VGG16, MobileNet, ResNet, and YOLOv3. Second, a novel depth-map-based 3-D spatial geolocation method is proposed for obtaining the 3-D geographic locations of the objects. Then, a center-based method is proposed to automatically extract the final 3-D vector of the target. Experimental results illustrate that the proposed method performs 3-D positioning and vectorization of the milestones and circular and triangular traffic signs, accurately and effectively, achieving greater than 86&#37; recall and precision for the three types of targets in the test areas. The experiments demonstrate that the overall 3-D information acquisition scheme is feasible and has great application potential.
WOS:000538081900009
</snippet>
</document>

<document id="33">
<title>Semantic Segmentation Deep Learning for Extracting Surface Mine Extents from Historic Topographic Maps</title>
<url>http://dx.doi.org/10.3390/rs12244145</url>
<snippet>Historic topographic maps, which are georeferenced and made publicly available by the United States Geological Survey (USGS) and the National Maps Historical Topographic Map Collection (HTMC), are a valuable source of historic land cover and land use (LCLU) information that could be used to expand the historic record when combined with data from moderate spatial resolution Earth observation missions. This is especially true for landscape disturbances that have a long and complex historic record, such as surface coal mining in the Appalachian region of the eastern United States. In this study, we investigate this specific mapping problem using modified UNet semantic segmentation deep learning (DL), which is based on convolutional neural networks (CNNs), and a large example dataset of historic surface mine disturbance extents from the USGS Geology, Geophysics, and Geochemistry Science Center (GGGSC). The primary objectives of this study are to (1) evaluate model generalization to new geographic extents and topographic maps and (2) to assess the impact of training sample size, or the number of manually interpreted topographic maps, on model performance. Using data from the state of Kentucky, our findings suggest that DL semantic segmentation can detect surface mine disturbance features from topographic maps with a high level of accuracy (Dice coefficient = 0.902) and relatively balanced omission and commission error rates (Precision = 0.891, Recall = 0.917). When the model is applied to new topographic maps in Ohio and Virginia to assess generalization, model performance decreases; however, performance is still strong (Ohio Dice coefficient = 0.837 and Virginia Dice coefficient = 0.763). Further, when reducing the number of topographic maps used to derive training image chips from 84 to 15, model performance was only slightly reduced, suggesting that models that generalize well to new data and geographic extents may not require a large training set. We suggest the incorporation of DL semantic segmentation methods into applied workflows to decrease manual digitizing labor requirements and call for additional research associated with applying semantic segmentation methods to alternative cartographic representations to supplement research focused on multispectral image analysis and classification.
WOS:000603267300001
</snippet>
</document>

<document id="34">
<title>Assisting UAV Localization Via Deep Contextual Image Matching</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3054832</url>
<snippet>In this article, we aim to explore the potential of using onboard cameras and pre-stored geo-referenced imagery for Unmanned Aerial Vehicle (UAV) localization. Such a vision-based localization enhancing system is of vital importance, particularly in situations where the integrity of the global positioning system (GPS) is in question (i.e., in the occurrence of GPS outages, jamming, etc.). To this end, we propose a complete trainable pipeline to localize an aerial image in a pre-stored orthomosaic map in the context of UAV localization. The proposed deep architecture extracts the features from the aerial imagery and localizes it in a pre-ordained, larger, and geotagged image. The idea is to train a deep learning model to find neighborhood consensus patterns that encapsulate the local patterns in the neighborhood of the established dense feature correspondences by introducing semi-local constraints. We qualitatively and quantitatively evaluate the performance of our approach on real UAV imagery. The training and testing data is acquired via multiple flights over different regions. The source code along with the entire dataset, including the annotations of the collected images has been made public.(1) Up-to our knowledge, such a dataset is novel and first of its kind which consists of 2052 high-resolution aerial images acquired at different times over three different areas in Pakistan spanning a total area of around 2 km(2).
WOS:000621403900007
</snippet>
</document>

<document id="35">
<title>Attention-Guided Label Refinement Network for Semantic Segmentation of Very High Resolution Aerial Orthoimages</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3073935</url>
<snippet>The recent applications of fully convolutional networks (FCNs) have shown to improve the semantic segmentation of very high resolution (VHR) remote-sensing images because of the excellent feature representation and end-to-end pixel labeling capabilities. While many FCN-based methods concatenate features from multilevel encoding stages to refine the coarse labeling results, the semantic gap between features of different levels and the selection of representative features are often overlooked, leading to the generation of redundant information and unexpected classification results. In this article, we propose an attention-guided label refinement network (ALRNet) for improved semantic labeling of VHR images. ALRNet follows the paradigm of the encoder-decoder architecture, which progressively refines the coarse labeling maps of different scales by using the channelwise attention mechanism. A novel attention-guided feature fusion module based on the squeeze-and-excitation module is designed to fuse higher level and lower level features. In this way, the semantic gaps among features of different levels are declined, and the category discrimination of each pixel in the lower level features is strengthened, which is helpful for subsequent label refinement. ALRNet is tested on three public datasets, including two ISRPS 2-D labeling datasets and the Wuhan University aerial building dataset. Results demonstrated that ALRNet had shown promising segmentation performance in comparison with state-of-the-art deep learning networks. The source code of ALRNet is made publicly available for further studies.
WOS:000650468700005
</snippet>
</document>

<document id="36">
<title>Fast and accurate multi-class geospatial object detection with large-size remote sensing imagery using CNN and Truncated NMS</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.07.019</url>
<snippet>Multi-class geospatial object detection with remote sensing imagery has broad prospects in urban planning, natural disaster warning, industrial production, military surveillance and other applications. Accuracy and ef-ficiency are two common measures for evaluating object detection models, and it is often difficult to achieve both at the same time. Developing a practical remote sensing object detection algorithm that balances the accuracy and efficiency is thus a big challenge in the Earth observation community. Here, we propose a comprehensive high-speed multi-class remote sensing object detection method. Firstly, we obtain a multi-volume YOLO (You Only Look Once) v4 model for balancing speed and accuracy, based on a pruning strategy of the convolutional neural network (CNN) and the one-stage object detection network YOLO v4. Moreover, we apply the Manhattan -Distance Intersection of Union (MIOU) loss function to the multi-volume YOLO v4 to further improve the ac-curacy without additional computational burden.Secondly, mainly due to computing limitations, a remote sensing image that is large-size relative to a natural image must first be divided into multiple smaller tiles, which are then detected separately, and finally, the detection results are spliced back to match the original image. In the process of remote sensing image slicing, a large number of truncated objects appear at the edge of tiles, which will produce a large number of false results in the subsequent detection links. To solve this problem, we propose a Truncated Non-Maximum Suppression (NMS) algorithm to filter out repeated and false detection boxes from truncated targets in the spliced detection results. We compare the proposed algorithm with the state-of-the-art methods on the Dataset for Object deTection in Aerial images (DOTA) and DOTA v2. Quantitative evaluations show that mAP and FPS reach 77.3 and 35 on DOTA, and 61.0 and 74 on DOTA v2. Overall, our method reaches the optimal balance between ef-ficiency and accuracy, and realizes the high-speed remote sensing object detection.
WOS:000879531000002
</snippet>
</document>

<document id="37">
<title>A novel deep learning neural network approach for predicting flash flood susceptibility: A case study at a high frequency tropical storm area</title>
<url>http://dx.doi.org/10.1016/j.scitotenv.2019.134413</url>
<snippet>This research proposes and evaluates a new approach for flash flood susceptibility mapping based on Deep Learning Neural Network (DLNN)) algorithm, with a case study at a high-frequency tropical storm area in the northwest mountainous region of Vietnam. Accordingly, a DLNN structure with 192 neurons in 3 hidden layers was proposed to construct an inference model that predicts different levels of susceptibility to flash flood. The Rectified Linear Unit (ReLU) and the sigmoid were selected as the activate function and the transfer function, respectively, whereas the Adaptive moment estimation (Adam) was used to update and optimize the weights of the DLNN. A database for the study area, which includes factors of elevation, slope, curvature, aspect, stream density, NDVI, soil type, lithology, and rainfall, was established to train and validate the proposed model. Feature selection was carried out for these factors using the Information gain ratio. The results show that the DLNN attains a good prediction accuracy with Classification Accuracy Rate = 92.05&#37;, Positive Predictive Value = 94.55&#37; and Negative Predictive Value = 89.55&#37;. Compared to benchmarks, Multilayer Perceptron Neural Network and Support Vector Machine, the DLNN performs better; therefore, it could be concluded that the proposed hybridization of GIS and deep learning can be a promising tool to assist the government authorities and involving parties in flash flood mitigation and land-use planning. (C) 2019 Elsevier B.V. All rights reserved.
WOS:000498801400006
</snippet>
</document>

<document id="38">
<title>Residual dense network for intensity-guided depth map enhancement</title>
<url>http://dx.doi.org/10.1016/j.ins.2019.05.003</url>
<snippet>The depth maps captured by sensors always suffer from low resolution and random noise. Recently, by introducing the guidance from the color image, deep convolutional neural network (DCNN) shows significant improvements for depth map enhancement. However, most DCNN-based methods do not make full use of multi-scale guidance from the color image, thereby achieving sub-optimal performances. In this paper, we propose a novel DCNN to progressively reconstruct the high-resolution depth map guided by the intensity image. Specially, the multi-scale intensity features are extracted to provide guidance for the refinement of depth features as their resolutions are gradually enhanced. Furthermore, local residual learning and global residual learning are adopted in the output of each up sampling sub-network and the whole network respectively. Such design can recover the high-frequency details from coarse to fine. In addition, according to the contiguous memory mechanism, the dense connections are designed to take the low-level features and the high-level features into account which further exploits the guidance from the intensity image. To balance the resource expense and the performance, the dimension reduction units are used to efficiently represent the features. The proposed network is compared with 17 state-of-the-art methods which shows improved performances. (C) 2019 Elsevier Inc. All rights reserved.
WOS:000470954700004
</snippet>
</document>

<document id="39">
<title>Structural Building Damage Detection with Deep Learning: Assessment of a State-of-the-Art CNN in Operational Conditions</title>
<url>http://dx.doi.org/10.3390/rs11232765</url>
<snippet>Remotely sensed data can provide the basis for timely and efficient building damage maps that are of fundamental importance to support the response activities following disaster events. However, the generation of these maps continues to be mainly based on the manual extraction of relevant information in operational frameworks. Considering the identification of visible structural damages caused by earthquakes and explosions, several recent works have shown that Convolutional Neural Networks (CNN) outperform traditional methods. However, the limited availability of publicly available image datasets depicting structural disaster damages, and the wide variety of sensors and spatial resolution used for these acquisitions (from space, aerial and UAV platforms), have limited the clarity of how these networks can effectively serve First Responder needs and emergency mapping service requirements. In this paper, an advanced CNN for visible structural damage detection is tested to shed some light on what deep learning networks can currently deliver, and its adoption in realistic operational conditions after earthquakes and explosions is critically discussed. The heterogeneous and large datasets collected by the authors covering different locations, spatial resolutions and platforms were used to assess the network performances in terms of transfer learning with specific regard to geographical transferability of the trained network to imagery acquired in different locations. The computational time needed to deliver these maps is also assessed. Results show that quality metrics are influenced by the composition of training samples used in the network. To promote their wider use, three pre-trained networks-optimized for satellite, airborne and UAV image spatial resolutions and viewing angles-are made freely available to the scientific community.
WOS:000508382100046
</snippet>
</document>

<document id="40">
<title>Accurate Detection of Built-Up Areas from High-Resolution Remote Sensing Imagery Using a Fully Convolutional Network</title>
<url>http://dx.doi.org/10.14358/PERS.85.10.737</url>
<snippet>The analysis of built-up areas has always been a popular research topic for remote sensing applications. However, automatic extraction of built-up areas from a wide range of regions remains challenging. In this article, a fully convolutional network (FCN)-based strategy is proposed to address built-up area extraction. The proposed algorithm can be divided into two main steps. First, divide the remote sensing image into blocks and extract their deep features by a lightweight multi-branch convolutional neural network (LMB-CNN). Second, rearrange the deep features into feature maps that are fed into a well-designed FCN for image segmentation. Our FCN is integrated with multi-branch blocks and outputs multi-channel segmentation masks that are utilized to balance the false alarm and missing alarm. Experiments demonstrate that the overall classification accuracy of the proposed algorithm can achieve 98.75&#37; in the test data set and that it has a faster processing compared with the existing state-of-the-art algorithms.
WOS:000486660100006
</snippet>
</document>

<document id="41">
<title>SQUEEZEPOSENET: IMAGE BASED POSE REGRESSION WITH SMALL CONVOLUTIONAL NEURAL NETWORKS FOR REAL TIME UAS NAVIGATION</title>
<url>http://dx.doi.org/10.5194/isprs-annals-IV-2-W3-49-2017</url>
<snippet>The number of unmanned aerial vehicles (UAVs) is increasing since low-cost airborne systems are available for a wide range of users. The outdoor navigation of such vehicles is mostly based on global navigation satellite system (GNSS) methods to gain the vehicles trajectory. The drawback of satellite-based navigation are failures caused by occlusions and multi-path interferences. Beside this, local image-based solutions like Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) can e.g. be used to support the GNSS solution by closing trajectory gaps but are computationally expensive. However, if the trajectory estimation is interrupted or not available a re-localization is mandatory. In this paper we will provide a novel method for a GNSS-free and fast image-based pose regression in a known area by utilizing a small convolutional neural network (CNN). With on-board processing in mind, we employ a lightweight CNN called SqueezeNet and use transfer learning to adapt the network to pose regression. Our experiments show promising results for GNSS-free and fast localization.
WOS:000426824000007
</snippet>
</document>

<document id="42">
<title>Roofpedia: Automatic mapping of green and solar roofs for an open roofscape registry and evaluation of urban sustainability</title>
<url>http://dx.doi.org/10.1016/j.landurbplan.2021.104167</url>
<snippet>Sustainable roofs, such as those with greenery and photovoltaic panels, contribute to the roadmap for reducing the carbon footprint of cities. However, research on sustainable urban roofscapes is rather focused on their potential and it is hindered by the scarcity of data, limiting our understanding of their current content, spatial distribution, and temporal evolution. To tackle this issue, we introduce Roofpedia, a set of three contributions: (i) automatic mapping of relevant urban roof typology from satellite imagery; (ii) an open roof registry mapping the spatial distribution and area of solar and green roofs of more than one million buildings across 17 cities; and (iii) the Roofpedia Index, a derivative of the registry, to benchmark the cities by the extent of sustainable roofscape in term of solar and green roof penetration. This project, partly inspired by its street greenery counterpart Treepedia, is made possible by a multi-step pipeline that combines deep learning and geospatial techniques, demonstrating the feasibility of an automated methodology that generalises successfully across cities with an accuracy of detecting sustainable roofs of up to 100&#37; in some cities. We offer our results as an interactive map and open dataset so that our work could aid researchers, local governments, and the public to uncover the pattern of sustainable rooftops across cities, track and monitor the current use of rooftops, complement studies on their potential, evaluate the effectiveness of existing incentives, verify the use of subsidies and fulfilment of climate pledges, estimate carbon offset capacities of cities, and ultimately support better policies and strategies to increase the adoption of instruments contributing to the sustainable development of cities.
WOS:000681123700004
</snippet>
</document>

<document id="43">
<title>Satellite product to map drought and extreme precipitation trend in Andalusia, Spain: A novel method to assess heritage landscapes at risk</title>
<url>http://dx.doi.org/10.1016/j.jag.2022.102810</url>
<snippet>The objective of this work is to develop a methodology for the identification of extreme rain and drought events that have occurred in the last 30 years using products derived from satellite images. Proposed methodology uses statistical reducers such as percentile, drought indexes, and map algebra at a geo big data scale. The daily precipitation data from the Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks-Climate Data Record and Climate Hazards Group InfraRed Precipitation with Station Data were validated by comparison with ground station data. Extreme event maps were obtained from the use of high percentiles. Drought maps were obtained from the standardized precipitation index using low percentiles. The data were migrated to a geographic information system that allows interrelation with other geographic data. Its application to the study of the fortifications preserved in Andalusia classified all structures according to the level of exposure to these dangers and identified two areas of precipitation with different characteristics according to the influence of existing teleconnection patterns. Applied to the study of heritage landscapes, this methodological model minimizes the uncertainty associated with the use of satellite precipitation products, facilitates the planning of preventive conservation activities, and the management of existing resources in occurrence of extreme events.
WOS:000806512000001
</snippet>
</document>

<document id="44">
<title>Generating satisfactory terrain by terrain maker generative adversarial nets</title>
<url>http://dx.doi.org/10.1117/12.2536635</url>
<snippet>Generative Adversarial Networks (GANs) is one of the most promising generative model in recently years. In this paper, we proposed a model called terrain maker Generative Adversarial Networks (TMGAN). It differs from the original GANs in three points: first, based on given topographic map, TMGAN can generate corresponding satellite aerial map, and vice versa. Second, TMGAN can modeled the terrain adaptively. Third, TMGAN can predict the height map of surface environment. We collected two data sets of paired and unpaired topographic maps and satellite aerial maps to train our model and test the influence of hidden variables. In this paper, we demonstrate the three-dimensional modeling ability of TMGAN.
WOS:000542922000025
</snippet>
</document>

<document id="45">
<title>Deep spatio-temporal residual neural networks for road-network-based data modeling</title>
<url>http://dx.doi.org/10.1080/13658816.2019.1599895</url>
<snippet>Recently, researchers have introduced deep learning methods such as convolutional neural networks (CNN) to model spatio-temporal data and achieved better results than those with conventional methods. However, these CNN-based models employ a grid map to represent spatial data, which is unsuitable for road-network-based data. To address this problem, we propose a deep spatio-temporal residual neural network for road-network-based data modeling (DSTR-RNet). The proposed model constructs locally-connected neural network layers (LCNR) to model road network topology and integrates residual learning to model the spatio-temporal dependency. We test the DSTR-RNet by predicting the traffic flow of Didi cab service, in an 8-km(2) region with 2,616 road segments in Chengdu, China. The results demonstrate that the DSTR-RNet maintains the spatial precision and topology of the road network as well as improves the prediction accuracy. We discuss the prediction errors and compare the prediction results to those of grid-based CNN models. We also explore the sensitivity of the model to its parameters; this will aid the application of this model to network-based data modeling.
WOS:000467143800001
</snippet>
</document>

<document id="46">
<title>Depth Map Enhancement by Revisiting Multi-Scale Intensity Guidance Within Coarse-to-Fine Stages</title>
<url>http://dx.doi.org/10.1109/TCSVT.2019.2962867</url>
<snippet>Being different from the most methods of guided depth map enhancement based on deep convolutional neural network which focus on increasing the depth of networks, this paper is to improve the effectiveness of intensity guidance when the network goes deep. Overall, the proposed network upsamples the low-resolution depth maps from coarse to fine. Within each refinement stage of certain-scale depth features, the current-scale and all coarse-scales of the guidance features are revisited by dense connection. Therefore, the multi-scale guidance is efficiently maintained as the propagation of features. Furthermore, the proposed network maintains the intensity features in the high-resolution domain from which the multi-scale guidance is directly extracted. This design further improves the quality of intensity guidance. In addition, the shallow depth features upsampled via transposed convolution layer are directly transferred to the final depth features for reconstruction, which is called global residual learning in feature domain. Similarly, the global residual learning in pixel domain learns the difference between the depth ground truth and the coarsely upsampled depth map. Also, the local residual learning is to maintain the low frequency within each refinement stage and progressively recover the high frequency. The proposed method is tested for noise-free and noisy cases which compares against 16 state-of-the-art methods. Our experimental results show the improved performances based on the qualitative and quantitative evaluations.
WOS:000597751000023
</snippet>
</document>

<document id="47">
<title>Boundary-Aware Multitask Learning for Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3043442</url>
<snippet>Semantic segmentation and height estimation play fundamental roles in the scene understanding of remote sensing images with their wide variety of aerial applications. Recently, deep convolutional neural networks (DCNNs) have achieved state-of-the-art performance in both tasks. However, DCNN-based methods learn to accumulate contextual information over large receptive fields while lose the local detailed information, resulting in blurry object boundaries. The complicated ground object distribution and low interclass variance further aggravate the difficulty in generating accurate predictions. To address the above-mentioned issues, we propose a novel boundary-aware multitask learning (BAMTL) framework to perform three tasks, semantic segmentation, height estimation, and boundary detection, within a unified model. The boundary detection is employed as an auxiliary task to regularize the other two master tasks at both the feature space and output space. We present a boundary attentive module to build the cross-task interaction for master tasks, which enforce the networks to filter out the confident area and focus on learning the high-frequency details. We then introduce a boundary regularized loss term to further refine the prediction maps to be locally consistent while preserving boundary structures. With these formulations, our model improves the performance of both segmentation and height tasks, especially along the boundaries. Experimental results on two publicly available remote sensing datasets demonstrate that the proposed approach performs favorably against the state-of-the-art methods.
WOS:000696430600017
</snippet>
</document>

<document id="48">
<title>High-resolution remote sensing image semantic segmentation based on a deep feature aggregation network</title>
<url>http://dx.doi.org/10.1088/1361-6501/abfbfd</url>
<snippet>Semantic segmentation of high-resolution remote sensing images has a wide range of applications, such as territorial planning, geographic monitoring and smart cities. The proper operation of semantic segmentation for remote sensing images remains challenging due to the complex and diverse transitions between different ground areas. Although several convolution neural networks (CNNs) have been developed for remote sensing semantic segmentation, the performance of CNNs is far from the expected target. This study presents a deep feature aggregation network (DFANet) for remote sensing image semantic segmentation. It is composed of a basic feature representation layer, an intermediate feature aggregation layer, a deep feature aggregation layer and a feature aggregation module (FAM). Specially, the basic feature representation layer is used to obtain feature maps with different resolutions: the intermediate feature aggregation layer and deep feature aggregation layer can fuse various resolution features and multi-scale features; the FAM is used to splice the features and form more abundant spatial feature maps; and the conditional random field module is used to optimize semantic segmentation results. We have performed extensive experiments on the ISPRS two-dimensional Vaihingen and Potsdam remote sensing image datasets and compared the proposed method with several variations of semantic segmentation networks. The experimental results show that DFANet outperforms the other state-of-the-art approaches.
WOS:000657489100001
</snippet>
</document>

<document id="49">
<title>Analysis and positioning of geographic tourism resources based on image processing method with Ra-CGAN modeling</title>
<url>http://dx.doi.org/10.3934/geosci.2022036</url>
<snippet>Peoples diversified tourism needs provide a broad development space and atmosphere for various tourism forms. The geographic resource information of the tourism unit can vividly highlight the units geographic spatial location and reflect the individuals spatial and attribute characteristics. It is not only the main goal of researching the information base of tourism resources, but it is also the difficulty that needs to be solved at present. This paper describes the use of image processing technology to realize the analysis and positioning of geographic tourism resources. Specifically, we propose a conditional generative adversarial network (CGAN) model, Ra-CGAN, with a multi-level channel attention mechanism. First, we built a generative model G with a multi-level channel attention mechanism. By fusing deep semantic and shallow detail information containing the attention mechanism, the network can extract rich contextual information. Second, we constructed a discriminative network D. We improved the segmentation results by correcting the difference between the ground-truth label map and the segmentation map generated by the generative model. Finally, through adversarial training between G and D with conditional constraints, we enabled high-order data distribution features learning to improve the boundary accuracy and smoothness of the segmentation results. In this study, the proposed method was validated on the large-scale remote sensing image object detection datasets DIOR and DOTA. Compared with the existing work, the method proposed in this paper achieves very good performance.
WOS:000863120200001
</snippet>
</document>

<document id="50">
<title>HD2A-Net: A novel dual gated attention network using comprehensive hybrid dilated convolutions for medical image segmentation</title>
<url>http://dx.doi.org/10.1016/j.compbiomed.2022.106384</url>
<snippet>The convolutional neural networks (CNNs) have been widely proposed in the medical image analysis tasks, especially in the image segmentations. In recent years, the encoder-decoder structures, such as the U-Net, were rendered. However, the multi-scale information transmission and effective modeling for long-range feature dependencies in these structures were not sufficiently considered. To improve the performance of the existing methods, we propose a novel hybrid dual dilated attention network (HD2A-Net) to conduct the lesion region segmentations. In the proposed network, we innovatively present the comprehensive hybrid dilated convolution (CHDC) module, which facilitates the transmission of the multi-scale information. Based on the CHDC module and the attention mechanisms, we design a novel dual dilated gated attention (DDGA) block to enhance the saliency of related regions from the multi-scale aspect. Besides, a dilated dense (DD) block is designed to expand the receptive fields. The ablation studies were performed to verify our proposed blocks. Besides, the interpretability of the HD2A-Net was analyzed through the visualization of the attention weight maps from the key blocks. Compared to the state-of-the-art methods including CA-Net, DeepLabV3+, and Attention U-Net, the HD2A-Net outperforms significantly, with the metrics of Dice, Average Symmetric Surface Distance (ASSD), and mean Intersection-over-Union (mIoU) reaching 93.16&#37;, 93.63&#37;, and 94.72&#37;, 0.36 pix, 0.69 pix, and 0.52 pix, and 88.03&#37;, 88.67&#37;, and 90.33&#37; on three publicly available medical image datasets: MAEDE-MAFTOUNI (COVID-19 CT), ISIC-2018 (Melanoma Dermoscopy), and Kvasir-SEG (Gastrointestinal Disease Polyp), respectively.
WOS:000900280100004
</snippet>
</document>

<document id="51">
<title>Directionally constrained fully convolutional neural network for airborne LiDAR point cloud classification</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.02.004</url>
<snippet>Point cloud classification plays an important role in a wide range of airborne light detection and ranging (LiDAR) applications, such as topographic mapping, forest monitoring, power line detection, and road detection. However, due to the sensor noise, high redundancy, incompleteness, and complexity of airborne LiDAR systems, point cloud classification is challenging. Traditional point cloud classification methods mostly focus on the development of handcrafted point geometry features and employ machine learning-based classification models to conduct point classification. In recent years, the advances of deep learning models have caused researchers to shift their focus towards machine learning-based models, specifically deep neural networks, to classify airborne LiDAR point clouds. These learning-based methods start by transforming the unstructured 3D point sets to regular 2D representations, such as collections of feature images, and then employ a 2D CNN for point classification. Moreover, these methods usually need to calculate additional local geometry features, such as planarity, sphericity and roughness, to make use of the local structural information in the original 3D space. Nonetheless, the 3D to 2D conversion results in information loss. In this paper, we propose a directionally constrained fully convolutional neural network (D-FCN) that can take the original 3D coordinates and LiDAR intensity as input; thus, it can directly apply to unstructured 3D point clouds for semantic labeling. Specifically, we first introduce a novel directionally constrained point convolution (D-Conv) module to extract locally representative features of 3D point sets from the projected 2D receptive fields. To make full use of the orientation information of neighborhood points, the proposed D-Conv module performs convolution in an orientation-aware manner by using a directionally constrained nearest neighborhood search. Then, we design a multiscale fully convolutional neural network with downsampling and upsampling blocks to enable multiscale point feature learning. The proposed D-FCN model can therefore process input point cloud with arbitrary sizes and directly predict the semantic labels for all the input points in an end-to-end manner. Without involving additional geometry features as input, the proposed method demonstrates superior performance on the International Society for Photogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark dataset. The results show that our model achieves a new stateof-the-art performance on powerline, car, and facade categories. Moreover, to demonstrate the generalization abilities of the proposed method, we conduct further experiments on the 2019 Data Fusion Contest Dataset. Our proposed method achieves superior performance than the comparing methods and accomplishes an overall accuracy of 95.6&#37; and an average F1 score of 0.810.
WOS:000527709200005
</snippet>
</document>

<document id="52">
<title>Mapping of Rill Erosion of the Middle Volga (Russia) Region Using Deep Neural Network</title>
<url>http://dx.doi.org/10.3390/ijgi11030197</url>
<snippet>Soil erosion worldwide is an intense, poorly controlled process. In many respects, this is a consequence of the lack of up-to-date high-resolution erosion maps. All over the world, the problem of insufficient information is solved in different ways, mainly on a point-by-point basis, within local areas. Extrapolation of the results obtained locally to a more extensive territory produces inevitable uncertainties and errors. For the anthropogenic-developed part of Russia, this problem is especially urgent because the assessment of the intensity of erosion processes, even with the use of erosion models, does not reach the necessary scale due to the lack of all the required global large-scale remote sensing data and the complexity of considering regional features of erosion processes over such vast areas. This study aims to propose a new methodology for large-scale automated mapping of rill erosion networks based on Sentinel-2 data. A LinkNet deep neural network with a DenseNet encoder was used to solve the problem of automated rill erosion mapping. The recognition results for the study area of more than 345,000 sq. km were summarized to a grid of 3037 basins and analyzed to assess the relationship with the main natural-anthropogenic factors. Generalized additive models (GAM) were used to model the dependency of rill erosion density to explore complex relationships. A complex nonlinear relationship between erosion processes and topographic, meteorological, geomorphological, and anthropogenic factors was shown.
WOS:000775281200001
</snippet>
</document>

<document id="53">
<title>Vehicle Object Detection in Remote Sensing Imagery Based on Multi-Perspective Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/ijgi7070249</url>
<snippet>Most traditional object detection approaches have a deficiency of features, slow detection speed, and high false-alarm rate. To solve these problems, we propose a multi-perspective convolutional neural network (Multi-PerNet) to extract remote sensing imagery features. Regions with CNN features (R-CNN) is a milestone in applying CNN method to object detection. With the help of the great feature extraction and classification performance of CNN, the transformation of object detection problem is realized by the Region Proposal method. Multi-PerNet trains a vehicle object detection model in remote sensing imagery based on Faster R-CNN. During model training, sample images and the labels are inputs, and the output is a detection model. First, Multi-PerNet extracts the feature map. Meanwhile, the area distribution and object-area aspect ratio in the sample images are obtained by k-means clustering. Then, the Faster R-CNN region proposal network generates the candidate windows based on the k-means clustering results. Features of candidate windows can be obtained by mapping candidate windows to the feature map. Finally, the candidate window and its features are inputted to the classifier to be trained to obtain the detection model. Experiment results show that the Multi-PerNet model detection accuracy is improved by 10.1&#37; compared with the model obtained by ZF-net and 1.6&#37; compared with the model obtained by PVANet. Moreover, the model size is reduced by 21.3&#37;.
WOS:000445150900016
</snippet>
</document>

<document id="54">
<title>Monocular Vision Aided Depth Map from RGB Images to Estimate of Localization and Support to Navigation of Mobile Robots</title>
<url>http://dx.doi.org/10.1109/JSEN.2020.2964735</url>
<snippet>Localization is one of the most challenging requirements needed for a mobile robots. Successful localization represents success in meeting the other principal requirements, such as perception and navigation. This article proposes a new approach for the localization of autonomous mobile robots using a Kinect sensor and the concept of Transfer Learning linked to Convolutional Neural Networks (CNNs). The images acquired from the sensor are applied in a mosaic form, which consists of the three color channels provided along with the depth information of the environment. Topological maps were used for indoor environment localization. The proposed computer vision system employed the Bayesian Classifier, k-Nearest Neighbor, Random Forest, Multi-layer Perceptron, and Support Vector Machine as the classifiers. The main focus of this work is the use of a unique configuration of RGB-D images transformed into a mosaic image, combined with the descriptive power of CNNs, in order to estimate the location of a mobile robot in an indoor environments. The results show that the proposed approach proved to be a convincing method for the tasks of localization and supporting the navigation of mobile robots; the results achieved 100&#37; in Accuracy and in the F1-Score. The values obtained for the processing times were also suitable for a computer vision system, with 31.885ms, 0.040s, and 0.044s for the extraction, training, and classification stages, respectively. All these results were from the RGB-D mosaic images. The data demonstrated the relevance of the proposed work, providing a successful localization and, consequently, a successful navigation for mobile robots.
WOS:000571774500037
</snippet>
</document>

<document id="55">
<title>An application of metadata-based image retrieval system for facility management</title>
<url>http://dx.doi.org/10.1016/j.aei.2021.101417</url>
<snippet>For facility management, photography is an efficient and accurate method of recording the physical state of infrastructure. However, without an effective organizational scheme, the difficulty of retrieving relevant photos from historical databases can become overly burdensome for highly complex or long-lived assets. To make strategic decisions, it is crucial to retrieve the right information from a plurality of sources in a timely manner. The main objective of this paper is to present a method for organizing and retrieving photos from massive facility management photo databases using photo-metadata: photographed location, camera perspective, and image semantic content information. Indoor localization experiments were performed using Bluetooth technology to infer the location information. Perspective is inferred from the devices on-board inertial measurement unit (IMU). Image semantic content is inferred using a Convolutional Neural Network (CNN)-based deep learning algorithm. Fusing these three features, seven query options were provided for the user when retrieving images. Leveraging Building Information Modeling (BIM) as a process and Geographic Information Systems (GIS) as a framework, this paper also envisions a federated information management by connecting 2D and 3D facility assets with our real-world map which can be smoothly bridged with our image retrieval system. The realization of the integrated application with BIM and GIS is significantly beneficial for the facility management domain by advancing the understanding of projects in a broader view with a federated data platform. In this research, the framework is illustrated with 21 institutional buildings within the University of Texas at Austins main campus, and the authors conclude that the proposed metadata-based image retrieval system can ultimately enhance the better-informed decision-making process through rapid information retrieval.
WOS:000701954700002
</snippet>
</document>

<document id="56">
<title>UAV-BASED STRUCTURAL DAMAGE MAPPING - RESULTS FROM 6 YEARS OF RESEARCH IN TWO EUROPEAN PROJECTS</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLII-3-W8-187-2019</url>
<snippet>Structural disaster damage detection and characterisation is one of the oldest remote sensing challenges, and the utility of virtually every type of active and passive sensor deployed on various air- and spaceborne platforms has been assessed. The proliferation and growing sophistication of UAV in recent years has opened up many new opportunities for damage mapping, due to the high spatial resolution, the resulting stereo images and derivatives, and the flexibility of the platform. We have addressed the problem in the context of two European research projects, RECONASS and INACHUS. In this paper we synthesize and evaluate the progress of 6 years of research focused on advanced image analysis that was driven by progress in computer vision, photogrammetry and machine learning, but also by constraints imposed by the needs of first responder and other civil protection end users. The projects focused on damage to individual buildings caused by seismic activity but also explosions, and our work centred on the processing of 3D point cloud information acquired from stereo imagery. Initially focusing on the development of both supervised and unsupervised damage detection methods built on advanced texture features and basic classifiers such as Support Vector Machine and Random Forest, the work moved on to the use of deep learning. In particular the coupling of image-derived features and 3D point cloud information in a Convolutional Neural Network (CNN) proved successful in detecting also subtle damage features. In addition to the detection of standard rubble and debris, CNN-based methods were developed to detect typical facade damage indicators, such as cracks and spalling, including with a focus on multi-temporal and multi-scale feature fusion. We further developed a processing pipeline and mobile app to facilitate near-real time damage mapping. The solutions were tested in a number of pilot experiments and evaluated by a variety of stakeholders.
WOS:000684596600031
</snippet>
</document>

<document id="57">
<title>Machine Recognition of Map Point Symbols Based on YOLOv3 and Automatic Configuration Associated with POI</title>
<url>http://dx.doi.org/10.3390/ijgi11110540</url>
<snippet>This study is oriented towards machine autonomous mapping and the need to improve the efficiency of map point symbol recognition and configuration. Therefore, an intelligent recognition method for point symbols was developed using the You Only Look Once Version 3 (YOLOv3) algorithm along with the Convolutional Block Attention Module (CBAM). Then, the recognition results of point symbols were associated with the point of interest (POI) to achieve automatic configuration. To quantitatively analyze the recognition effectiveness of this study algorithm and the comparison algorithm for map point symbols, the recall, precision and mean average precision (mAP) were employed as evaluation metrics. The experimental results indicate that the recognition efficiency of point symbols is enhanced compared to the original YOLOv3 algorithm, and that the mAP is increased by 0.55&#37;. Compared to the Single Shot MultiBox Detector (SSD) algorithm and Faster Region-based Convolutional Neural Network (Faster RCNN) algorithm, the precision, recall rate, and mAP all performed well, achieving 97.06&#37;, 99.72&#37; and 99.50&#37;, respectively. On this basis, the recognized point symbols are associated with POI, and the coordinate of point symbols are assigned through keyword matching and enrich their attribute information. This enables automatic configuration of point symbols and achieves a relatively good effect of map configuration.
WOS:000884004800001
</snippet>
</document>

<document id="58">
<title>A new fully convolutional neural network for semantic segmentation of polarimetric SAR imagery in complex land cover ecosystem</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.03.015</url>
<snippet>Despite the application of state-of-the-art fully Convolutional Neural Networks (CNNs) for semantic segmentation of very high-resolution optical imagery, their capacity has not yet been thoroughly examined for the classification of Synthetic Aperture Radar (SAR) images. The presence of speckle noise, the absence of efficient feature expression, and the limited availability of labelled SAR samples have hindered the application of the state-of-the-art CNNs for the classification of SAR imagery. This is of great concern for mapping complex land cover ecosystems, such as wetlands, where backscattering/spectrally similar signatures of land cover units further complicate the matter. Accordingly, we propose a new Fully Convolutional Network (FCN) architecture that can be trained in an end-to-end scheme and is specifically designed for the classification of wetland complexes using polarimetric SAR (PoISAR) imagery. The proposed architecture follows an encoder-decoder paradigm, wherein the input data are fed into a stack of convolutional filters (encoder) to extract high-level abstract features and a stack of transposed convolutional filters (decoder) to gradually up-sample the low resolution output to the spatial resolution of the original input image. The proposed network also benefits from recent advances in CNN designs, namely the addition of inception modules and skip connections with residual units. The former component improves multi-scale inference and enriches contextual information, while the latter contributes to the recovery of more detailed information and simplifies optimization. Moreover, an in-depth investigation of the learned features via opening the black box demonstrates that convolutional filters extract discriminative polarimetric features, thus mitigating the limitation of the feature engineering design in PoISAR image processing. Experimental results from full polarimetric RADARSAT-2 imagery illustrate that the proposed network outperforms the conventional random forest classifier and the state-of-the-art FCNs, such as FCN-32s, FCN-16s, FCN-8s, and SegNet, both visually and numerically for wetland mapping.
WOS:000469306300016
</snippet>
</document>

<document id="59">
<title>Super-resolution reconstruction of a digital elevation model based on a deep residual network</title>
<url>http://dx.doi.org/10.1515/geo-2020-0207</url>
<snippet>The digital elevation model (DEM) is an important basic data tool applied in geoscience applications. Because of its high cost and long development cycle of enhancing hardware performance, designing the related models and algorithms to improve the resolution of DEM is of considerable significance. At present, there is little research on DEM super-resolution based on deep learning, and the results of the reconstructed DEMs obtained by existing methods are inaccurate. Therefore, deepening of the network layers is utilized to improve the accuracy of a reconstructed DEM. This paper designs a neutral network model with 30 convolutional layers to learn the feature mapping relationship between a low- and high-resolution DEM. To avoid the problem of network degradation caused by increasing the number of convolutional layers, residual learning is introduced to accelerate the convergence speed of the model, thereby preferably realizing the DEM super-resolution process. The results show that DEM super-resolution based on a deep residual network is better than that obtained using a neural network with fewer convolutional layers, and the reconstructed result of the DEM based on a deep residual network is remarkably improved in terms of the peak signal to noise ratio and visual effect.
WOS:000599645800001
</snippet>
</document>

<document id="60">
<title>EDPNet: An Encoding-Decoding Network with Pyramidal Representation for Semantic Image Segmentation</title>
<url>http://dx.doi.org/10.3390/s23063205</url>
<snippet>This paper proposes an encoding-decoding network with a pyramidal representation module, which will be referred to as EDPNet, and is designed for efficient semantic image segmentation. On the one hand, during the encoding process of the proposed EDPNet, the enhancement of the Xception network, i.e., Xception+ is employed as a backbone to learn the discriminative feature maps. The obtained discriminative features are then fed into the pyramidal representation module, from which the context-augmented features are learned and optimized by leveraging a multi-level feature representation and aggregation process. On the other hand, during the image restoration decoding process, the encoded semantic-rich features are progressively recovered with the assistance of a simplified skip connection mechanism, which performs channel concatenation between high-level encoded features with rich semantic information and low-level features with spatial detail information. The proposed hybrid representation employing the proposed encoding-decoding and pyramidal structures has a global-aware perception and captures fine-grained contours of various geographical objects very well with high computational efficiency. The performance of the proposed EDPNet has been compared against PSPNet, DeepLabv3, and U-Net, employing four benchmark datasets, namely eTRIMS, Cityscapes, PASCAL VOC2012, and CamVid. EDPNet acquired the highest accuracy of 83.6&#37; and 73.8&#37; mIoUs on eTRIMS and PASCAL VOC2012 datasets, while its accuracy on the other two datasets was comparable to that of PSPNet, DeepLabv3, and U-Net models. EDPNet achieved the highest efficiency among the compared models on all datasets.
WOS:000958160300001
</snippet>
</document>

<document id="61">
<title>Short-term cognitive networks, flexible reasoning and nonsynaptic learning</title>
<url>http://dx.doi.org/10.1016/j.neunet.2019.03.012</url>
<snippet>While the machine learning literature dedicated to fully automated reasoning algorithms is abundant, the number of methods enabling the inference process on the basis of previously defined knowledge structures is scanter. Fuzzy Cognitive Maps (FCMs) are recurrent neural networks that can be exploited towards this goal because of their flexibility to handle external knowledge. However, FCMs suffer from a number of issues that range from the limited prediction horizon to the absence of theoretically sound learning algorithms able to produce accurate predictions. In this paper we propose a neural system named Short-term Cognitive Networks that tackle some of these limitations. In our model, used for regression and pattern completion, weights are not constricted and may have a causal nature or not. As a second contribution, we present a nonsynaptic learning algorithm to improve the network performance without modifying the previously defined weight matrix. Besides, we derive a stop condition to prevent the algorithm from iterating without significantly decreasing the global simulation error. (C) 2019 Elsevier Ltd. All rights reserved.
WOS:000468877100007
</snippet>
</document>

<document id="62">
<title>Airborne Laser Scanning Point Cloud Classification Using the DGCNN Deep Learning Method</title>
<url>http://dx.doi.org/10.3390/rs13050859</url>
<snippet>Classification of aerial point clouds with high accuracy is significant for many geographical applications, but not trivial as the data are massive and unstructured. In recent years, deep learning for 3D point cloud classification has been actively developed and applied, but notably for indoor scenes. In this study, we implement the point-wise deep learning method Dynamic Graph Convolutional Neural Network (DGCNN) and extend its classification application from indoor scenes to airborne point clouds. This study proposes an approach to provide cheap training samples for point-wise deep learning using an existing 2D base map. Furthermore, essential features and spatial contexts to effectively classify airborne point clouds colored by an orthophoto are also investigated, in particularly to deal with class imbalance and relief displacement in urban areas. Two airborne point cloud datasets of different areas are used: Area-1 (city of Surabaya-Indonesia) and Area-2 (cities of Utrecht and Delft-the Netherlands). Area-1 is used to investigate different input feature combinations and loss functions. The point-wise classification for four classes achieves a remarkable result with 91.8&#37; overall accuracy when using the full combination of spectral color and LiDAR features. For Area-2, different block size settings (30, 50, and 70 m) are investigated. It is found that using an appropriate block size of, in this case, 50 m helps to improve the classification until 93&#37; overall accuracy but does not necessarily ensure better classification results for each class. Based on the experiments on both areas, we conclude that using DGCNN with proper settings is able to provide results close to production.
WOS:000628508300001
</snippet>
</document>

<document id="63">
<title>M(3)Fusion: A Deep Learning Architecture for Multiscale Multimodal Multitemporal Satellite Data Fusion</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2876357</url>
<snippet>Modern Earth Observation systems provide remote sensing data at different temporal and spatial resolutions. Among all the available spatial mission, today the Sentinel-2 program supplies high temporal (every five days) and high spatial resolution (HSR) (10 m) images that can be useful to monitor land cover dynamics. On the other hand, very HSR (VHSR) imagery is still essential to figure out land cover mapping characterized by fine spatial patterns. Understanding how to jointly leverage these complementary sources in an efficient way when dealing with land cover mapping is a current challenge in remote sensing. With the aim of providing land cover mapping through the fusion of multi-temporal HSR and VHSR satellite images, we propose a suitable end-to-end deep learning framework, namely M-3 Fusion, which is able to simultaneously leverage the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR images. Experiments carried out on the Reunion Island study area confirm the quality of our proposal considering both quantitative and qualitative aspects.
WOS:000455462100035
</snippet>
</document>

<document id="64">
<title>Orientation-First Strategy With Angle Attention Module for Rotated Object Detection in Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3209349</url>
<snippet>Recently, object detection in remote sensing images (RSIs) have received extensive attention and made significant progress. Nonetheless, the arbitrary orientations of objects in RSIs make their detection a challenging task. Most of the existing detection methods are difficult to extract the orientation features of objects due to the lack of directionality of conventional convolutions. In addition, the boundary discontinuity in angle regression affects the detection of object orientations. In response to these problems, this article proposes an orientation-first refinement detector (OFRDet), which is based on a strategy that enables the detector to detect the angle of an object ahead of others and presets oriented anchors. In OFRDet, we propose an angle encoding regression module (AERM) and an angle channel attention module (ACAM). AERM transforms angle detection into multiparameter regression, which eliminates boundary discontinuities. ACAM uses convolution kernels with different angles to extract directional features purposefully according to the preset oriented anchors. After these two modules, more accurate bounding boxes are generated and sent to the refined stage to obtain the final detection results. We evaluate our method and demonstrate the effectiveness of it by conducting experiments on two challenging and credible datasets, DOTA, HRSC2016. OFRDet achieves competitive results 79.56&#37;, 96.29&#37; mAP on the two datasets, respectively.
WOS:000866530700008
</snippet>
</document>

<document id="65">
<title>Performance Analysis in the Segmentation of urban paved roads in RGB satellite images using K-Means++ and SegNet: case study in Sao Luis-MA</title>
<url>http://dx.doi.org/10.4114/intartif.vol24iss68pp89-103</url>
<snippet>The design and manual insertion of new terrestrial roads into geographic databases is a frequent activity in geoprocessing and their demand usually occurs as the most up-to-date satellite imagery of the territory is acquired. Continually, new urban and rural occupations emerge, for which specific vector geometries need to be designed to characterize the cartographic inputs and accommodate the relevant associated data. Therefore, it is convenient to develop a computational tool that, with the help of artificial intelligence, automates what is possible in this respect, since manual editing depends on the limits of user agility, and does it in images that are usually easy and free to access. To test the feasibility of this proposal, a database of RGB images containing asphalted urban roads is presented to the K-Means++ algorithm and the SegNet Convolutional Neural Network, and the performance of each one was evaluated and compared for accuracy and IoU of road identification. Under the conditions of the experiment, K-Means++ achieved poor and unviable results for use in a real-life application involving asphalt road detection in RGB satellite images, with average accuracy ranging from 41.67&#37; to 64.19&#37; and average IoU of 12.30&#37; to 16.16&#37;, depending on the preprocessing strategy used. On the other hand, the SegNet Convolutional Neural Network proved to be appropriate for precision applications not sensitive to discontinuities, achieving an average accuracy of 87.12&#37; and an average IoU of 71.93&#37;.
WOS:000770358700002
</snippet>
</document>

<document id="66">
<title>Retrieval of daily sea ice thickness from AMSR2 passive microwave data using ensemble convolutional neural networks</title>
<url>http://dx.doi.org/10.1080/15481603.2021.1943213</url>
<snippet>Recently, measurement of sea ice thickness (SIT) has received increasing attention due to the importance of thinning ice in the context of global warming. Although altimeter sensors onboard satellite missions enable continuous SIT measurements over larger areas compared to in situ observations, these sensors are inadequate for mapping daily Arctic SIT because of their small footprints. We exploited passive microwave data from AMSR2 (Advanced Microwave Scanning Radiometer 2) by incorporating a state-of-the-art deep learning (DL) approach to address this limitation. Passive microwave data offer better temporal resolutions than those from a single altimeter sensors, but are rarely used for SIT estimations due to their limited physical relationship with SIT. In this study, we proposed an ensemble DL model with different modalities to produce daily pan-Arctic SIT retrievals. The proposed model determined the hidden and unknown relationships between the brightness temperatures of AMSR2 channels and SITs measured by CryoSat-2 (CS2) from the extended input features defined by our feature augmentation strategy. Although AMSR2-based SITs agreed well with CS2-derived gridded SIT values, they had similar uncertainties and errors in the CS2 SIT measurements, particularly for thin ice. However, based on quantitative validations using long-term unseen data and IceBridge data, the proposed retrieval model consistently generated SITs from AMSR2 at 25 km spatial resolution, regardless of time and space.
WOS:000673377900001
</snippet>
</document>

<document id="67">
<title>In-season and dynamic crop mapping using 3D convolution neural networks and sentinel-2 time series</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.12.005</url>
<snippet>An accurate, frequently updated, automatic and reproducible mapping procedure to identify seasonal cultivated crops is a prerequisite for many crop monitoring activities. Deep learning was demonstrated to be an effective mapping approach already successfully applied to decametric resolution satellite images (like Sentinel-2 data) to produce yearly crop maps. In this framework, algorithm training is performed with ground truth typically consisting of spatially explicit information available after the end of the season (e.g. yearly crop maps and/or farmer declaration for subsidies at parcel level); however, such data (i) does not allow performing in-season prediction, and (ii) does not provide temporal details fundamental to describe a dynamic crop succession and/or to understand crop management (i.e. planting and harvesting). In this paper we present a Deep Neural Network-based approach capable of generating (i) a crop map of the current season at a specific point in time ("In season mapping"conventionally at the end of the current year), along with (ii) all intermediate maps during the season able to describe in near real-time the evolution of crop presence ("Dynamic-mapping"at the temporal granularity of satellite imagery revisiting, e.g., 5 days for Sentinel-2 data). This approach adopts a smart training procedure of a Deep Neural model by exploiting historical satellite data and ground truth. We introduce a method to automatically generate "short-term"ground truth maps (i.e. 5 days reference) starting from the "long-term"ones (i.e. available yearly static reference) and characterizing temporally the different crop presence by performing a phenological analysis of historical time series. The model was trained and validated in Lombardy (North of Italy) exploiting multi-annual authoritative crop maps from 2016 to 2019. Validation was performed both in time (same areas used for training in a different year) and space (different location) for the year 2019. The quantitative error metrics calculation and Spatio-temporal analysis clearly demonstrate that the model can predict in-season crop presence with a generalization capacity over the long-term (yearly maps: OA &gt; 70&#37; and Kappa &gt; 0.64&#37;) and that the short-term predictions (5 days maps) are coherent with the reference information from expert knowledge (local crop calendars). The model can produce dynamically along the season short-term maps with a medium-high crop-specific User Accuracy at the maximum green-up phase (UA &gt; 53&#37; up to 95&#37;). These products are of extreme interest for final users providing information at the peak of plant development that dynamically changes according to the considered crop, the specific location and the investigated season. These results demonstrate that it is possible to produce a crop map early in the season and extract useful additional information such as crop intensity (e.g. double crops presence) and crop dynamics related to different sowing dates.
WOS:000913152800001
</snippet>
</document>

<document id="68">
<title>DIAL: Deep Interactive and Active Learning for Semantic Segmentation in Remote Sensing</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3166551</url>
<snippet>In this article, we propose to build up a collaboration between a deep neural network and a human in the loop to swiftly obtain accurate segmentation maps of remote sensing images. In a nutshell, the agent iteratively interacts with the network to correct its initially flawed predictions. Concretely, these interactions are annotations representing the semantic labels. Our methodological contribution is twofold. First, we propose two interactive learning schemes to integrate user inputs into deep neural networks. The first one concatenates the annotations with the other networks inputs. The second one uses the annotations as a sparse ground truth to retrain the network. Second, we propose an active learning (AL) strategy to guide the user toward the most relevant areas to annotate. To this purpose, we compare different state-of-the-art acquisition functions to evaluate the neural network uncertainty such as ConfidNet, entropy, or ODIN. Through experiments on three remote sensing datasets, we show the effectiveness of the proposed methods. Notably, we show that AL based on uncertainty estimation enables to quickly lead the user toward mistakes and that it is thus relevant to guide the user interventions. Code will be open-source and released in this repository.(1)
WOS:000793810100005
</snippet>
</document>

<document id="69">
<title>Explain Black-box Image Classifications Using Superpixel-based Interpretation</title>
<url>http://dx.doi.org/</url>
<snippet>How to best understand and interpret the decisions of deep neural networks is a crucial topic, as the impact of intelligent deep network systems is prevalent in many applications. We propose a superpixel based method to interpret and explain the results of black-box deep networks in the widely-applied image classification tasks. We perform probabilistic prediction difference analysis upon one or more superpixels clustered from image pixels. Our method generates a superpixel score map visualization that can provide rich interpretation regarding image components. Such interpretation provides supportive/ unsupportive likelihood of image regions upon the decisions performed by the black-box classifier. We compare our method against state-of-art pixelwise interpretation methods over the latest deep neural network classifiers on the ImageNet dataset. Results show that our method produces more consistent interpretations in less computation time. Our method also supports interactive interpretation, where users can acquire explanations on specified regions through a convenient interface for a prompt reaction.
WOS:000455146801108
</snippet>
</document>

<document id="70">
<title>Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition</title>
<url>http://dx.doi.org/10.1016/j.patcog.2017.06.017</url>
<snippet>Machine recognition of a handwritten mathematical expression (HME) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. Inspired by recent work in deep learning, we present Watch, Attend and Parse (WAP), a novel end-to-end approach based on neural network that learns to recognize HMEs in a two-dimensional layout and outputs them as one-dimensional character sequences in LaTeX format. Inherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. Meanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. We employ a convolutional neural network encoder that takes HME images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate LaTeX sequences. Moreover, the correspondence between the input expressions and the output LaTeX sequences is learned automatically by the attention mechanism. We validate the proposed approach on a benchmark published by the CROHME international competition. Using the official training dataset, WAP significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55&#37; on CROHME 2014 and 44.55&#37; on CROHME 2016. (C) 2017 Elsevier Ltd. All rights reserved.
WOS:000406987400016
</snippet>
</document>

<document id="71">
<title>NeoSLAM: Neural Object SLAM for Loop Closure and Navigation</title>
<url>http://dx.doi.org/10.1007/978-3-031-15934-3_37</url>
<snippet>Simultaneous Localization and Mapping (SLAM) with fixed landmark objects creates topological maps by extracting semantic information from the environment. In this paper, we propose a new method for mapping, Neural Object SLAM (NeoSLAM), which uses objects seen in stereo images to learn associations between the pose of the robot and the observed landmark objects. We perform mapping with a biologically inspired approach based on creating patterns memorizing places in a network of grid cells and head direction cells. Our model is inspired by the object vector cells discovered recently by neuroscientists exploring the navigation of mammals. We model the firing field of these cells with a feed-forward neural network and create keyframes of objects with their 3D pose in a world-centered frame of reference. We train a Hebbian network connecting keyframe templates to the grid cells to memorize familiar places. We use the NeuroSLAM algorithm to train the grid cells and the head direction cells with the 4 Degree of Freedom (DoF) poses of the robot. Then, we detect loops in the trajectory by matching objects in the keyframes. Finally, we create an object experience map and correct the cumulative error if we detect loop closure candidates. Thus, our system performs object-based place recognition with a brain-inspired approach and produces 2D/3D object topological maps.
WOS:000866212600036
</snippet>
</document>

<document id="72">
<title>Self-Filtered Learning for Semantic Segmentation of Buildings in Remote Sensing Imagery With Noisy Labels</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3230625</url>
<snippet>Not all building labels for training improve the performance of the deep learning model. Some labels can be falsely labeled or too ambiguous to represent their ground truths, resulting in poor performance of the model. For example, building labels in OpenStreetMap (OSM) and Microsoft Building Footprints (MBF) are publicly available training sources that have great potential to train deep models, but directly using those labels for training can limit the models performance as their labels are incomplete and inaccurate, called noisy labels. This article presents self-filtered learning (SFL) that helps a deep model learn well with noisy labels for building extraction in remote sensing images. SFL iteratively filters out noisy labels during the training process based on loss of samples. Through a multiround manner, SFL makes a deep model learn progressively more on refined samples from which the noisy labels have been removed. Extensive experiments with the simulated noisy map as well as real-world noisy maps, OSM and MBF, showed that SFL can improve the deep models performance in diverse error types and different noise levels.
WOS:000912413700012
</snippet>
</document>

<document id="73">
<title>Analysis of patch and sample size effects for 2D-3D CNN models using multiplatform dataset: hyperspectral image classification of ROSIS and Jilin-1 GP01 imagery</title>
<url>http://dx.doi.org/10.55730/1300-0632.3929</url>
<snippet>Modern hyperspectral sensors provide a huge volume of data at spectral and spatial domains with high redundancy, which requires robust methods for analysis. In this study, 2D and 3D CNN models were applied to hyperspectral image datasets (ROSIS and Jilin-1 GP01) using varying patch and sample sizes to determine their combined impacts on the performance of deep learning models. Differences in classification performances in relation to particle and sample sizes were statistically analysed using McNemars test. According to the findings, raising the patch and sample size enhances the performance of the 2D/3D CNN model and produces more accurate results in the classification of hyperspectral imagery. To be more specific, the thematic maps produced with 400 and 600 samples resulted in a notable increase in overall accuracies (approximately 10&#37;) compared to 50 and 100 samples per class. Statistical test results confirmed the importance of patch size selection for the case of limited training data and the maximum number of samples required for deep learning. It could be also mentioned that an appropriate patch and sample size should be selected considering the characteristics of the dataset and the type of deep learning algorithms.
WOS:000884407400009
</snippet>
</document>

<document id="74">
<title>Multistrategy ensemble regression for mapping of built-up density and height with Sentinel-2 data</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.10.004</url>
<snippet>In this paper, we establish a workflow for estimation of built-up density and height based on multispectral Sentinel-2 data. To do so, we render the estimation of built-up density and height as a supervised learning problem. Given the rational level of measurement of those two target variables, the regression estimation problem is regarded as finding the mapping between an incoming vector, i.e., ubiquitously available features computed from Sentinel-2 data, and an observable output (i.e., training set), which is derived over spatially limited areas in an automated manner. As such, training sets are automatically generated from a joint exploitation of TanDEM-X mission elevation data and Sentinel-2 imagery, and, as an alternative, from cadastral sources. The training sets are used to regress the target variables for spatial processing units which correspond to urban neighborhood scales. From a methodological point of view, we introduce a novel ensemble regression approach, i.e., multistrategy ensemble regression (MSER), based on advanced machine learning-based regression algorithms including Random Forest Regression, Support Vector Regression, Gaussian Process Regression, and Neural Network Regression. To establish a robust ensemble, those algorithms are learned with a modified version of the AdaBoost.RT algorithm. However, to reliably ensure diversity between single boosted regressors, we include a random feature subspace method in the procedure. In contrast to existing approaches, we selectively prune non-favorable regressors trained during the boosting procedure and calculate the final prediction by a weighted mean function on the residual models to ensure enhanced accuracy properties of predictions. Finally, outputs are concatenated into a single prediction with a decision fusion strategy. Experimental results are obtained from four test areas which cover the settlement areas of the four largest German cites, i.e., Berlin, Hamburg, Munich, and Cologne. The results unambiguously underline the beneficial properties of the MSER approach, since all best predictions were obtained with a boosted regressor in conjunction with a decision fusion strategy in a comparative setup. The mean absolute errors of corresponding models vary between 3 and 16&#37; and 1-5.4 m with respect to built-up density and height, respectively, depending on the validation strategy, size of the spatial processing units, and test area. Also in a domain adaptation setup (i.e., when learning a model over a source domain and applying it over a geographically different target domain) numerous predictions show comparable accuracy levels as predictions obtained within a source domain. This further underlines the viability to transfer a model and, thus, enable a substitution of the training data in the target domains.
WOS:000592242600005
</snippet>
</document>

<document id="75">
<title>Benchmarking Anchor-Based and Anchor-Free State-of-the-Art Deep Learning Methods for Individual Tree Detection in RGB High-Resolution Images</title>
<url>http://dx.doi.org/10.3390/rs13132482</url>
<snippet>Urban forests contribute to maintaining livability and increase the resilience of cities in the face of population growth and climate change. Information about the geographical distribution of individual trees is essential for the proper management of these systems. RGB high-resolution aerial images have emerged as a cheap and efficient source of data, although detecting and mapping single trees in an urban environment is a challenging task. Thus, we propose the evaluation of novel methods for single tree crown detection, as most of these methods have not been investigated in remote sensing applications. A total of 21 methods were investigated, including anchor-based (one and two-stage) and anchor-free state-of-the-art deep-learning methods. We used two orthoimages divided into 220 non-overlapping patches of 512 x 512 pixels with a ground sample distance (GSD) of 10 cm. The orthoimages were manually annotated, and 3382 single tree crowns were identified as the ground-truth. Our findings show that the anchor-free detectors achieved the best average performance with an AP50 of 0.686. We observed that the two-stage anchor-based and anchor-free methods showed better performance for this task, emphasizing the FSAF, Double Heads, CARAFE, ATSS, and FoveaBox models. RetinaNet, which is currently commonly applied in remote sensing, did not show satisfactory performance, and Faster R-CNN had lower results than the best methods but with no statistically significant difference. Our findings contribute to a better understanding of the performance of novel deep-learning methods in remote sensing applications and could be used as an indicator of the most suitable methods in such applications.
WOS:000671127300001
</snippet>
</document>

<document id="76">
<title>Adaptive Residual Convolutional Neural Network for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2995445</url>
<snippet>In this article, we designed an adaptive residual convolutional neural network (ARCNN) that takes raw hyperspectral image (HSI) cubes as input data for land-cover classification. In this network, spectral and spatial feature extraction blocks are explored to learn discriminative features from abundant spectral information and spatial contexts in HSIs. The proposed ARCNN is an end-to-end deep learning framework that alleviates the declining-accuracy phenomenon of deep learning models, and it also ranks the correlation and importance of each band in HSIs. Furthermore, the residual blocks connect every other 3-D convolutional layer by using an identity mapping, which facilitates backpropagation of gradients. In order to address the common issue of imbalance between high dimensionality and limited availability of training samples for HSI classification, an attention mechanism and a feature fusion block are investigated to improve the performance of the ARCNN. Finally, some strategies, batch normalization and dropout, are imposed on every convolutional layer to regularize the learning process. Therefore, the ARCNN method brings benefits to extract discriminative features, and it is easier to avoid overfitting. Experimental results on three public HSI datasets demonstrate the effectiveness of the ARCNN over some state-of-the-art methods.
WOS:000542982300003
</snippet>
</document>

<document id="77">
<title>DAT-CNN: Dual Attention Temporal CNN for Time-Resolving Sentinel-3 Vegetation Indices</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3161190</url>
<snippet>The synergies between Sentinel-3 (S3) and the forthcoming fluorescence explorer (FLEX) mission bring us the opportunity of using S3 vegetation indices (VI) as proxies of the solar-induced chlorophyll fluorescence (SIF) that will be captured by FLEX. However, the highly dynamic nature of SIF demands a very temporally accurate monitoring of S3 VIs to become reliable proxies. In this scenario, this article proposes a novel temporal reconstruction convolutional neural network (CNN), named dual attention temporal CNN (DAT-CNN), which has been specially designed for time-resolving S3 VIs using S2 and S3 multitemporal observations. In contrast to other existing techniques, DAT-CNN implements two different branches for processing and fusing S2 and S3 multimodal data, while further exploiting intersensor synergies. Besides, DAT-CNN also incorporates a new spatial-spectral and temporal attention module to suppress uninformative spatial-spectral features, while focusing on the most relevant temporal stamps for each particular prediction. The experimental comparison, including several temporal reconstruction methods and multiple operational Sentinel data products, demonstrates the competitive advantages of the proposed model with respect to the state of the art. The codes of this article will be available at https://github.com/ibanezfd/DATCNN.
WOS:000779605200002
</snippet>
</document>

<document id="78">
<title>Local climate zone-based urban land cover classification from multi-seasonal Sentinel-2 images with a recurrent residual network</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.05.004</url>
<snippet>The local climate zone (LCZ) scheme was originally proposed to provide an interdisciplinary taxonomy for urban heat island (UHI) studies. In recent years, the scheme has also become a starting point for the development of higher-level products, as the LCZ classes can help provide a generalized understanding of urban structures and land uses. LCZ mapping can therefore theoretically aid in fostering a better understanding of spatio-temporal dynamics of cities on a global scale. However, reliable LCZ maps are not yet available globally. As a first step toward automatic LCZ mapping, this work focuses on LCZ-derived land cover classification, using multi-seasonal Sentinel-2 images. We propose a recurrent residual network (Re-ResNet) architecture that is capable of learning a joint spectral-spatial-temporal feature representation within a unitized framework. To this end, a residual convolutional neural network (ResNet) and a recurrent neural network (RNN) are combined into one end-to-end architecture. The ResNet is able to learn rich spectral-spatial feature representations from single-seasonal imagery, while the RNN can effectively analyze temporal dependencies of multi-seasonal imagery. Cross validations were carried out on a diverse dataset covering seven distinct European cities, and a quantitative analysis of the experimental results revealed that the combined use of the multi-temporal information and Re-ResNet results in an improvement of approximately 7 percent points in overall accuracy. The proposed framework has the potential to produce consistent-quality urban land cover and LCZ maps on a large scale, to support scientific progress in fields such as urban geography and urban climatology.
WOS:000480671500012
</snippet>
</document>

<document id="79">
<title>Identifying a Slums' Degree of Deprivation from VHR Images Using Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/rs11111282</url>
<snippet>In the cities of the Global South, slum settlements are growing in size and number, but their locations and characteristics are often missing in official statistics and maps. Although several studies have focused on detecting slums from satellite images, only a few captured their variations. This study addresses this gap using an integrated approach that can identify a slums degree of deprivation in terms of socio-economic variability in Bangalore, India using image features derived from very high resolution (VHR) satellite images. To characterize deprivation, we use multiple correspondence analysis (MCA) and quantify deprivation with a data-driven index of multiple deprivation (DIMD). We take advantage of spatial features learned by a convolutional neural network (CNN) from VHR satellite images to predict the DIMD. To deal with a small training dataset of only 121 samples with known DIMD values, insufficient to train a deep CNN, we conduct a two-step transfer learning approach using 1461 delineated slum boundaries as follows. First, a CNN is trained using these samples to classify slums and formal areas. The trained network is then fine-tuned using the 121 samples to directly predict the DIMD. The best prediction is obtained by using an ensemble non-linear regression model, combining the results of the CNN and models based on hand-crafted and geographic information system (GIS) features, with R-2 of 0.75. Our findings show that using the proposed two-step transfer learning approach, a deep CNN can be trained with a limited number of samples to predict the slums degree of deprivation. This demonstrates that the CNN-based approach can capture variations of deprivation in VHR images, providing a comprehensive understanding of the socio-economic situation of slums in Bangalore.
WOS:000472648000025
</snippet>
</document>

<document id="80">
<title>Comparison of Support Vector Machines and Random Forests for Corine Land Cover Mapping</title>
<url>http://dx.doi.org/10.3390/rs13040777</url>
<snippet>Land cover information is essential in European Union spatial management, particularly that of invasive species, natural habitats, urbanization, and deforestation; therefore, the need for accurate and objective data and tools is critical. For this purpose, the European Unions flagship program, the Corine Land Cover (CLC), was created. Intensive works are currently being carried out to prepare a new version of CLC+ by 2024. The geographical, climatic, and economic diversity of the European Union raises the challenge to verify various test areas methods and algorithms. Based on the Corine programs precise guidelines, Sentinel-2 and Landsat 8 satellite images were tested to assess classification accuracy and regional and spatial development in three varied areas of Catalonia, Poland, and Romania. The method is dependent on two machine learning algorithms, Random Forest (RF) and Support Vector Machine (SVM). The bias of classifications was reduced using an iterative of randomized training, test, and verification pixels. The ease of the implementation of the used algorithms makes reproducing the results possible and comparable. The results show that an SVM with a radial kernel is the best classifier, followed by RF. The high accuracy classes that can be updated and classes that should be redefined are specified. The methodologys potential can be used by developers of CLC+ products as a guideline for algorithms, sensors, and the possibilities and difficulties of classifying different CLC classes.
WOS:000624450400001
</snippet>
</document>

<document id="81">
<title>A 3D convolutional neural network method for land cover classification using LiDAR and multi-temporal Landsat imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.08.005</url>
<snippet>Terrestrial landscape has complex three-dimensional (3D) features that are difficult to extract using traditional methods based on 2D representations. These methods often relegate such features to raster or metric-based (two-dimensional) representations based on Digital Surface Models (DSM) or Digital Elevation Models (DEM), and thus are not suitable for resolving morphological and intensity features for fine-scale land cover mapping. Small footprint LiDAR provides an ideal way for capturing these 3D features. This research develops a novel method of integrating airborne LiDAR derived features and multi-temporal Landsat images to classify land cover types. We tested our approach in Williamson County, Illinois, which has diverse and mixed landscape features. Specifically, our method applied a 3D convolutional neural network (CNN) approach to extract features from LiDAR point clouds by (1) creating an occupancy grid, an intensity grid at 1-meter resolution, and then (2) normalizing and incorporating data into the 3D CNN. The extracted features (e.g., morphological and intensity features) from the 3D CNN were finally combined with multi-temporal spectral data to enhance the performance of land cover classification based on a Support Vector Machine classifier. Visual interpretation from both hyper-resolution photos and point clouds was used for training and preparation of testing data. The classification results show that our method outperforms a traditional method by 2.65&#37; (from 81.52&#37; to 84.17&#37;) when solely using LiDAR and 2.19&#37; (from 90.20&#37; to 92.57&#37;) when combining all available imageries. We demonstrate that our method can effectively extract LiDAR features and improve fine-scale land cover mapping through fusion of complementary types of remote sensing data.
WOS:000447109900031
</snippet>
</document>

<document id="82">
<title>Exploring the potential of multi-source unsupervised domain adaptation in crop mapping using Sentinel-2 images</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2156123</url>
<snippet>Accurate crop mapping is critical for agricultural applications. Although studies have combined deep learning methods and time-series satellite images to crop classification with satisfactory results, most of them focused on supervised methods, which are usually applicable to a specific domain and lose their validity in new domains. Unsupervised domain adaptation (UDA) was proposed to solve this limitation by transferring knowledge from source domains with labeled samples to target domains with unlabeled samples. Particularly, multi-source UDA (MUDA) is a powerful extension that leverages knowledge from multiple source domains and can achieve better results in the target domain than single-source UDA (SUDA). However, few studies have explored the potential of MUDA for crop mapping. This study proposed a MUDA crop classification model (MUCCM) for unsupervised crop mapping. Specifically, 11 states in the U.S. were selected as the multi-source domains, and three provinces in Northeast China were selected as individual target domains. Ten spectral bands and five vegetation indexes were collected at a 10-day interval from time-series Sentinel-2 images to build the MUCCM. Subsequently, a SUDA model Domain Adversarial Neural Network (DANN) and two direct transfer methods, namely, the deep neural network and random forest, were constructed and compared with the MUCCM. The results indicated that the UDA models outperformed the direct transfer models significantly, and the MUCCM was superior to the DANN, achieving the highest classification accuracy (OA&gt;85&#37;) in each target domain. In addition, the MUCCM also performed best in in-season forecasting and crop mapping. This study is the first to apply a MUDA to crop classification and demonstrate a novel, effective solution for high-performance crop mapping in regions without labeled samples.
WOS:000897090700001
</snippet>
</document>

<document id="83">
<title>Context-aware attention network for image recognition</title>
<url>http://dx.doi.org/10.1007/s00521-019-04281-y</url>
<snippet>Existing recognition methods based on deep learning have achieved impressive performance. However, most of these algorithms do not fully utilize the contexts and discriminative parts, which limit the recognition performance. In this paper, we propose a context-aware attention network that imitates the human visual attention mechanism. The proposed network mainly consists of a context learning module and an attention transfer module. Firstly, we design the context learning module that carries on contextual information transmission along four directions: left, right, top and down to capture valuable contexts. Second, the attention transfer module is proposed to generate attention maps that contain different attention regions, benefiting for extracting discriminative features. Specially, the attention maps are generated through multiple glimpses. In each glimpse, we generate the corresponding attention map and apply it to the next glimpse. This means that our attention is shifting constantly, and the shift is not random but is closely related to the last attention. Finally, we consider all located attention regions to achieve accurate image recognition. Experimental results show that our method achieves state-of-the-art performance with 97.68&#37; accuracy, 82.42&#37; accuracy, 80.32&#37; accuracy and 86.12&#37; accuracy on CIFAR-10, CIFAR-100, Caltech-256 and CUB-200, respectively.
WOS:000494051000082
</snippet>
</document>

<document id="84">
<title>Hyperspectral Image Classification With Mixed Link Networks</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3053567</url>
<snippet>Convolutional neural networks (CNNs) have improved the accuracy of hyperspectral image (HSI) classification significantly. However, CNN models usually generate a large number of feature maps, which lead to high redundancy and cannot guarantee to effectively extract discriminative features for well characterizing the complex structures of HSIs. In this article, two novel mixed link networks (MLNets) are proposed to enhance the representational ability of CNNs for HSI classification. Specifically, the proposed mixed link architectures integrate the feature reusage property of the residual network and the capability of effective new feature exploration of the densely convolutional network, extracting more discriminative features from HSIs. Compared with the dual path architecture, the proposed mixed link architectures can further improve the information flow throughout the network. Experimental results on three hyperspectral benchmark datasets demonstrate that our MLNets achieve competitive results compared with other state-of-the-art HSI classification approaches.
WOS:000622097100003
</snippet>
</document>

<document id="85">
<title>Exploitation of Time Series Sentinel-2 Data and Different Machine Learning Algorithms for Detailed Tree Species Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3098817</url>
<snippet>The classification of tree species through remote sensing data is of great significance to monitoring forest disturbances, biodiversity assessment, and carbon estimation. The dense time series and a wide swath of Sentinel-2 data provided the opportunity to map tree species accurately and in a timely manner over a large area. Many current studies have applied machine learning (ML) algorithms combined with Sentinel-2 images to classify tree species, but it is still unclear, which algorithm is more effective in the automotive extraction of tree species. In this study, five ML algorithms were compared to identify the composition of tree species with multitemporal Sentinel-2 images in the JianShe forest farm, Northeast China. Three major types of deep neural networks [Conv1D, AlexNet, and long short-term memory (LSTM)] were tested to classify Sentinel-2 time series, which represent three disparate but effective strategies to apply sequential data. The other two models are support vector machine (SVM) and random forest (RF), which are renowned for extensive adoption and high performance for various remote sensing applications. The results show that the overall accuracy of neural network models is better than that of SVM and RF. The Conv1D model had the highest classification accuracy (84.19&#37;), followed by the LSTM model (81.52&#37;), and the AlexNet model (76.02&#37;). For non-neural network models, RFs classification accuracy (79.04&#37;) is higher than that of SVM (72.79&#37;), but lower than that of Conv1D and LSTM. Therefore, the deep neural networks combined with multitemporal Sentinel-2 images can efficiently improve the accuracy of tree species classification.
WOS:000684698600004
</snippet>
</document>

<document id="86">
<title>Next Generation Mapping: Combining Deep Learning, Cloud Computing, and Big Remote Sensing Data</title>
<url>http://dx.doi.org/10.3390/rs11232881</url>
<snippet>The rapid growth of satellites orbiting the planet is generating massive amounts of data for Earth science applications. Concurrently, state-of-the-art deep-learning-based algorithms and cloud computing infrastructure have become available with a great potential to revolutionize the image processing of satellite remote sensing. Within this context, this study evaluated, based on thousands of PlanetScope images obtained over a 12-month period, the performance of three machine learning approaches (random forest, long short-term memory-LSTM, and U-Net). We applied these approaches to mapped pasturelands in a Central Brazil region. The deep learning algorithms were implemented using TensorFlow, while the random forest utilized the Google Earth Engine platform. The accuracy assessment presented F1 scores for U-Net, LSTM, and random forest of, respectively, 96.94&#37;, 98.83&#37;, and 95.53&#37; in the validation data, and 94.06&#37;, 87.97&#37;, and 82.57&#37; in the test data, indicating a better classification efficiency using the deep learning approaches. Although the use of deep learning algorithms depends on a high investment in calibration samples and the generalization of these methods requires further investigations, our results suggest that the neural network architectures developed in this study can be used to map large geographic regions that consider a wide variety of satellite data (e.g., PlanetScope, Sentinel-2, Landsat-8).
WOS:000508382100162
</snippet>
</document>

<document id="87">
<title>Debris flow susceptibility mapping in mountainous area based on multi-source data fusion and CNN model - taking Nujiang Prefecture, China as an example</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2142304</url>
<snippet>Efforts to evaluate the susceptibility of debris flows in large areas, especially in mountainous regions, are often hampered by the alpine and canyon terrain. This paper proposes a convolution neural network (CNN) model named dense residual shuffle net (DRSNet). It is successfully applied to Nujiang Prefecture in Yunnan Province of China, a typical alpine area with frequent debris flows. DRSNet uses digital elevation model, remote sensing, lithology, soil type and precipitation data as input. First, dense connection and residual structure were used to extract the shallow features of various data. Next, channel shuffle, fuse block and fully connection were applied to strengthen the correlation between different shallow features and give inner danger scores. Finally, precipitation as the activation factor was introduced giving the valleys susceptibility. To verify the feasibility of DRSNet, comparative tests were conducted on 7 CNN models and 3 other machine learning (ML) methods. Experimental results show that DRSNet can achieve 78.6&#37; accuracy in debris flow valley classification, which is at least 7.4&#37; higher than common CNN models and 15.2&#37; higher than other ML methods. This article provides new ideas for debris flow susceptibility evaluation.
WOS:000883193800001
</snippet>
</document>

<document id="88">
<title>One-Shot Summary Prototypical Network Toward Accurate Unpaved Road Semantic Segmentation</title>
<url>http://dx.doi.org/10.1109/LSP.2021.3087457</url>
<snippet>Recent studies of driving scene understanding based on image semantic segmentation have achieved dramatic advances in speed and accuracy. Large-scale public datasets for semantic segmentation of paved road driving scenes have led the advances, but there is no large-scale public dataset for unpaved road environments. Building a large-scale image semantic segmentation dataset for unpaved roads is very expensive, and domain gaps between geographically distributed locations and those of seasonal changes hinder building a training dataset that is adequate to train a convolutional neural network model. In this paper, to resolve the data insufficiency problem, we use an one-shot learning setting in unpaved road driving scene understanding. Our One-shot Summary Prototypical Network (OSPNet) is trained with paved road driving scenes, and it identifies drivable regions in unpaved roads given only a single support image and unpaved road mask data. The OSPNet improves previous two branch few-shot segmentation approaches by introducing the summary branch which enables channel-wise weighting for important features in the feature map of support and query branches. Our experiments show that our model quantitatively and qualitatively outperforms recent supervised and few-shot segmentation models.
WOS:000664979700010
</snippet>
</document>

<document id="89">
<title>Large-Scale Classification of Urban Structural Units From Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3052961</url>
<snippet>Remote sensing in combination with deep learning has become instrumental for efficiently and accurately classifying land-use and land-cover across large geographic areas. These technologies have also been successful in characterizing urban environments in terms of their structural units, structure types, or morphological regions. In these approaches, an urban area is partitioned into regions that exhibit homogeneous physical characteristics. However, existing approaches are typically limited to a single city, use inconsistent typologies, and lack scalability and generalization capacity. In this article, we propose an urban structural units categorization scheme and demonstrate its utility by applying it to 13 cities. Inspired by the lack of scalability and generalization capacity in urban structural units mapping, we extend the reach of deep learning and conduct a set of classification experiments in all 13 cities. These experiments offer insights into the strengths and limitations of deep neural networks for classifying urban structural units over diverse geographic regions and on heterogeneous collections of satellite imagery. The efficacy of the proposed deep learning approach is compared to a baseline method of multiscale image features and support vector machines. Our validation on five cities shows that better performance is achieved with deep neural networks. Additionally, we evaluate the impact of input size, model depth, and spatial pyramid pooling to assess the generalization capacity of deep neural networks.
WOS:000626519900003
</snippet>
</document>

<document id="90">
<title>An Automatic Approach for Generating Rich, Linked Geo-Metadata from Historical Map Images</title>
<url>http://dx.doi.org/10.1145/3394486.3403381</url>
<snippet>Historical maps contain detailed geographic information difficult to find elsewhere covering long-periods of time (e.g., 125 years for the historical topographic maps in the US). However, these maps typically exist as scanned images without searchable metadata. Existing approaches making historical maps searchable rely on tedious manual work (including crowd-sourcing) to generate the metadata (e.g., geolocations and keywords). Optical character recognition (OCR) software could alleviate the required manual work, but the recognition results are individual words instead of location phrases (e.g., "Black" and "Mountain" vs. "Black Mountain"). This paper presents an end-to-end approach to address the real-world problem of finding and indexing historical map images. This approach automatically processes historical map images to extract their text content and generates a set of metadata that is linked to large external geospatial knowledge bases. The linked metadata in the RDF (Resource Description Framework) format support complex queries for finding and indexing historical maps, such as retrieving all historical maps covering mountain peaks higher than 1,000 meters in California. We have implemented the approach in a system called mapKurator. We have evaluated mapKurator using historical maps from several sources with various map styles, scales, and coverage. Our results show significant improvement over the state-of-the-art methods. The code has been made publicly available as modules of the Kartta Labs project at https://github.com/kartta-labs/Project.
WOS:000749552303029
</snippet>
</document>

<document id="91">
<title>CLASSICATION OF LAND-COVER THROUGH MACHINE LEARNING ALGORITHMS FOR FUSION OF SENTINEL-2A AND PLANETSCOPE IMAGERY</title>
<url>http://dx.doi.org/</url>
<snippet>To monitor and manage the changes in the land use and land cover, is vital the process of classification; machine learning offers the potential for effective and efficient classification of remotely sensed imagery. However, not many articles have explicitly dealt with the effects of image fusion on land-cover classification accuracy. Although some studies have compared thematic mapping accuracy produced using different classification algorithms, there are no currently many studies that utilize image fusion for assessing different machine learning algorithms for classification purposes. The main aim of this study is to compare different machine learning algorithm for pixel classification of imagery fused with sensors Sentinel-2A and PlanetScope. The method used for image fusion is a variational model, the high spectral resolution of Sentinel-2A imagery and the high spatial resolution of PlanetScope imagery was fused; the machine learning algorithms evaluated are six that have been widely used in the remote sensing community: DT (Decision Tree), Boosted DT, RF (Random Forest), SVM radial base (Support Vector Machine), ANN (Artificial Neural Networks), KNN (k-Nearest Neighbors), for the classification four spectral indices (NDVI, NDM[, NDBI, MSAVI) were included, derived of the image fusion. The results show that the highest accuracy was produced by SVM radial base (OA: 87.8&#37;, Kappa: 87&#37;) respect to the other methods, nevertheless the methods RF, Boosted DT and KNN shown to be very powerful methods for classification of the study area.
WOS:000626733300048
</snippet>
</document>

<document id="92">
<title>A Deep Learning Approach to the Detection of Gossans in the Canadian Arctic</title>
<url>http://dx.doi.org/10.3390/rs12193123</url>
<snippet>Gossans are surficial deposits that form in host bedrock by the alteration of sulphides by acidic and oxidizing fluids. These deposits are typically a few meters to kilometers in size and they constitute important vectors to buried ore deposits. Hundreds of gossans have been mapped by field geologists in sparsely vegetated areas of the Canadian Arctic. However, due to Canadas vast northern landmass, it is highly probable that many existing occurrences have been missed. In contrast, a variety of remote sensing data has been acquired in recent years, allowing for a broader survey of gossans from orbit. These include band ratioing or methods based on principal component analysis. Spectrally, the 809 gossans used in this study show no significant difference from randomly placed points on the Landsat 8 imageries. To overcome this major issue, we propose a deep learning method based on convolutional neural networks and relying on geo big data (Landsat-8, Arctic digital elevation model lithological maps) that can be used for the detection of gossans. Its application in different regions in the Canadian Arctic shows great promise, with precisions reaching 77&#37;. This first order approach could provide a useful precursor tool to identify gossans prior to more detailed surveys using hyperspectral imaging.
WOS:000586666800001
</snippet>
</document>

<document id="93">
<title>Marine aquaculture mapping using GF-1 WFV satellite images and full resolution cascade convolutional neural network</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2133184</url>
<snippet>Growing demand for seafood and reduced fishery harvests have raised intensive farming of marine aquaculture in coastal regions, which may cause severe coastal water problems without adequate environmental management. Effective mapping of mariculture areas is essential for the protection of coastal environments. However, due to the limited spatial coverage and complex structures, it is still challenging for traditional methods to accurately extract mariculture areas from medium spatial resolution (MSR) images. To solve this problem, we propose to use the full resolution cascade convolutional neural network (FRCNet), which maintains effective features over the whole training process, to identify mariculture areas from MSR images. Specifically, the FRCNet uses a sequential full resolution neural network as the first-level subnetwork, and gradually aggregates higher-level subnetworks in a cascade way. Meanwhile, we perform a repeated fusion strategy so that features can receive information from different subnetworks simultaneously, leading to rich and representative features. As a result, FRCNet can effectively recognize different kinds of mariculture areas from MSR images. Results show that FRCNet obtained better performance than other classical and recently proposed methods. Our developed methods can provide valuable datasets for large-scale and intelligent modeling of the marine aquaculture management and coastal zone planning.
WOS:000891824900001
</snippet>
</document>

<document id="94">
<title>Thin cloud removal with residual symmetrical concatenation network</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.05.003</url>
<snippet>Thin cloud removal is important for enhancing the utilization of optical remote sensing imagery. Different from thick cloud removal, the pixels contaminated by thin clouds still preserve some surface information. Therefore, thin cloud removal methods usually focus on suppressing the cloud influence instead of replacing the cloudy pixels. In this paper, we proposed a deep residual symmetrical concatenation network (RSC-Net) to make end-to-end thin cloud removal. The RSC-Net is based on the encoding-decoding framework consisting of multiple residual convolutional layers and residual deconvolutional layers. The feature maps of each convolutional layer are copied and concatenated to the symmetrical deconvolutional layer. We used real cloud-contaminated and cloud-free Landsat-8 data very close in time for both training and testing. The RSC-Net is trained to take cloudy images as input and directly produce corresponding cloud-free images as output with all the bands together except the cirrus band and the panchromatic band. Compared with other traditional and state-of-the-art deep learning based methods, the experimental results show that our method has significant advantages in removing thin cloud contaminations in different bands.
WOS:000472590400010
</snippet>
</document>

<document id="95">
<title>Towards better flood risk management: Assessing flood risk and investigating the potential mechanism based on machine learning models</title>
<url>http://dx.doi.org/10.1016/j.jenvman.2021.112810</url>
<snippet>Integrating powerful machine learning models with flood risk assessment and determining the potential mechanism between risk and the driving factors are crucial for improving flood management. In this study, six machine learning models were utilized for flood risk assessment of the Pearl River Delta, in which the Gradient Boosting Decision Tree (GBDT), eXtreme Gradient Boosting (XGBoost), and Convolutional Neural Network (CNN) models were firstly applied in this field. Twelve indices were chosen and 2000 sample points were created for model training and testing. Hyperparameter optimization of the models was conducted to ensure fair comparisons. Due to uncertainty in the sample dataset, recorded inundation hot-spots were utilized to validate the rationality of the flood risk zoning maps. After determining the optimal model, the driving factors of different flood risk levels were investigated. Urban and rural areas and coastal and inland areas were also compared to determine the flood risk mechanism in different highest-risk areas. The results showed that the GBDT performed best and provided the most reasonable flood risk result among the six models. A comparison of the driving factors at different risk levels indicated that the disaster-inducing factor, disaster-breeding environment, and disasterbearing body were not definitely becoming more serious as the flood risk increased. In the highest-risk areas, rural areas were featured by worse disaster-breeding environment than urban areas, and the disaster-inducing factors of coastal areas were more serious than those of inland areas. Moreover, the Digital Elevation Model (DEM), maximum 1-day precipitation (M1DP), and road density (RD) were the top three significant driving factors and contributed 52&#37; to flood risk. This study not only expands the application of machine learning and deep learning methods for flood risk assessment, but also deepens our understanding of the potential mechanism of flood risk and provides insights into better flood risk management.
WOS:000677857000001
</snippet>
</document>

<document id="96">
<title>A framework for large-scale mapping of human settlement extent from Sentinel-2 images via fully convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.01.028</url>
<snippet>Human settlement extent (HSE) information is a valuable indicator of world-wide urbanization as well as the resulting human pressure on the natural environment. Therefore, mapping HSE is critical for various environmental issues at local, regional, and even global scales. This paper presents a deep-learning-based framework to automatically map HSE from multi-spectral Sentinel-2 data using regionally available geo-products as training labels. A straightforward, simple, yet effective fully convolutional network-based architecture, Sen2HSE, is implemented as an example for semantic segmentation within the framework. The framework is validated against both manually labelled checking points distributed evenly over the test areas, and the OpenStreetMap building layer. The HSE mapping results were extensively compared to several baseline products in order to thoroughly evaluate the effectiveness of the proposed HSE mapping framework. The HSE mapping power is consistently demonstrated over 10 representative areas across the world. We also present one regional-scale and one country-wide HSE mapping example from our framework to show the potential for upscaling. The results of this study contribute to the generalization of the applicability of CNN-based approaches for large-scale urban mapping to cases where no up-to-date and accurate ground truth is available, as well as the subsequent monitor of global urbanization.
WOS:000527712500010
</snippet>
</document>

<document id="97">
<title>Local climate zone mapping as remote sensing scene classification using deep learning: A case study of metropolitan China</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.04.008</url>
<snippet>China, with the worlds largest population, has gone through rapid development in the last forty years and now has over 800 million urban citizens. Although urbanization leads to great social and economic progress, they may be confronted with other issues, including extra heat and air pollution. Local climate zone (LCZ), a new concept developed for urban heat island research, provides a standard classification system for the urban environment. LCZs are defined by the context of the urban environment; the minimum diameter of an LCZ is expected to be 400-1,000 m so that it can have a valid effect on the urban climate. However, most existing methods (e.g., the WUDAPT method) regard this task as pixel-based classification, neglecting the spatial information. In this study, we argue that LCZ mapping should be considered as a scene classification task to fully exploit the environmental context. Fifteen cities covering 138 million population in three economic regions of China are selected as the study area. Sentinel-2 multispectral data with a 10 m spatial resolution are used to classify LCZs. A deep convolutional neural network composed of residual learning and the Squeeze-andExcitation block, namely the LCZNet, is proposed. We obtained an overall accuracy of 88.61&#37; by using a large image (48x48 corresponding to 480x480 m(2)) as the representation of an LCZ, 7.5&#37; higher than that using a small image representation (10x10) and nearly 20&#37; higher than that obtained by the standard WUDAPT method. Image sizes from 32x32 to 64x64 were found suitable for LCZ mapping, while a deeper network achieved better classification with larger inputs. Compared with natural classes, urban classes benefited more from a large input size, as it can exploit the environment context of urban areas. The combined use of the training data from all three regions led to the best classification, but the transfer of LCZ models cannot achieve satisfactory results due to the domain shift. More advanced domain adaptation methods should be applied in this application.
WOS:000535696600017
</snippet>
</document>

<document id="98">
<title>Hyperspectral Image Classification Based on Multilevel Joint Feature Extraction Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3123371</url>
<snippet>Over the past few years, convolutional neural network (CNN) has been broadly adopted in remote sensing (RS) imagery processing areas due to its impressive capabilities in feature extraction. Nevertheless, it is still a challenge for CNN-based hyperspectral image (HSI) classification methods to extract more effective spectral-spatial features considering all spectral bands. Driven by this issue, we propose a novel approach to cope with the HSI classification task, referring to the multilevel joint feature extraction network. The proposed network makes full use of the information on each channel of HSI and transforms it into valid channel-wised spatial features through a designed convolution process. Moreover, these feature maps form global attention details to guide the extraction of spectral-spatial features, which are taken to the next level for further feature mining. Then, the features obtained at different levels are integrated for ground object classification. In contrast with several state-of-the-art HSI classification methods on four public datasets, experimental results demonstrate the effectiveness and remarkable feature extraction capability of our proposed approach.
WOS:000716698800008
</snippet>
</document>

<document id="99">
<title>A MULTI-SCALE DEEP CONVOLUTIONAL NEURAL NETWORK FOR JOINT SEGMENTATION AND PREDICTION OF GEOGRAPHIC ATROPHY IN SD-OCT IMAGES</title>
<url>http://dx.doi.org/</url>
<snippet>Geographic atrophy (GA) generally appears in the advanced stage of age-related macular degeneration (AMD). It is a principle cause of the severe central visual loss for elder adults with non-exudative AMD in developed countries. In this paper, a multi-scale deep convolutional neural network is proposed for the joint segmentation and prediction of GA. First, restricted summed-area projection (RSAP) technique was used to generate GA projection images from the SD-OCT volumetric data. Then, GA projection images were sent to the multi-scale branches to acquire multi-scale feature maps. The final GA segmentation results were obtained by refining the multi-scale feature maps with a voting decision strategy. In the end, those multi-scale feature maps were cascaded with low-level features computed from the original images to predict the growth of the GA lesion. The segmented and predicted GA lesion in the tested scenarios resulted in a satisfying accuracy, comparing with the observed ground truth.
WOS:000485040000125
</snippet>
</document>

<document id="100">
<title>Detection of Undocumented Building Constructions from Official Geodata Using a Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/rs12213537</url>
<snippet>Undocumented building constructions are buildings or stories that were built years ago, but are missing in the official digital cadastral maps (DFK). The detection of undocumented building constructions is essential to urban planning and monitoring. The state of Bavaria, Germany, uses two semi-automatic detection methods for this task that suffer from a high false alarm rate. To solve this problem, we propose a novel framework to detect undocumented building constructions using a Convolutional Neural Network (CNN) and official geodata, including high resolution optical data and the Normalized Digital Surface Model (nDSM). More specifically, an undocumented building pixel is labeled as "building" by the CNN but does not overlap with a building polygon of the DFK. The class of old or new undocumented building can be further separated when a Temporal Digital Surface Model (tDSM) is introduced in the stage of decision fusion. In a further step, undocumented story construction is detected as the pixels that are "building" in both DFK and predicted results from CNN, but shows a height deviation from the tDSM. By doing so, we have produced a seamless map of undocumented building constructions for one-quarter of the state of Bavaria, Germany at a spatial resolution of 0.4 m, which has proved that our framework is robust to detect undocumented building constructions at large-scale. Considering that the official geodata exploited in this research is advantageous because of its high quality and large coverage, a transferability analysis experiment is also designed in our research to investigate the sampling strategies for building detection at large-scale. Our results indicate that building detection results in unseen areas at large-scale can be improved when training samples are collected from different districts. In an area where training samples are available, local training sampless collection and training can save much time and effort.
WOS:000589296900001
</snippet>
</document>

<document id="101">
<title>Understanding urban landuse from the above and ground perspectives: A deep learning, multimodal solution</title>
<url>http://dx.doi.org/10.1016/j.rse.2019.04.014</url>
<snippet>Landuse characterization is important for urban planning. It is traditionally performed with field surveys or manual photo interpretation, two practices that are time-consuming and labor-intensive. Therefore, we aim to automate landuse mapping at the urban-object level with a deep learning approach based on data from multiple sources (or modalities). We consider two image modalities: overhead imagery from Google Maps and ensembles of ground-based pictures (side-views) per urban-object from Google Street View (GSV). These modalities bring complementary visual information pertaining to the urban-objects. We propose an end-to-end trainable model, which uses OpenStreetMap annotations as labels. The model can accommodate a variable number of GSV pictures for the ground-based branch and can also function in the absence of ground pictures at prediction time. We test the effectiveness of our model over the area of Ile-de-France, France, and test its generalization abilities on a set of urban-objects from the city of Nantes, France. Our proposed multimodal Convolutional Neural Network achieves considerably higher-accuracies than methods that use a single image modality, making it suitable for automatic landuse map updates. Additionally, our approach could be easily scaled to multiple cities, because it is based on data sources available for many cities worldwide.
WOS:000470050500010
</snippet>
</document>

<document id="102">
<title>NASA NeMO-Net - A Neural Multimodal Observation &amp; Training Network for Marine Ecosystem Mapping at Diverse Spatiotemporal Scales</title>
<url>http://dx.doi.org/10.1109/IGARSS39084.2020.9323188</url>
<snippet>We present NeMO-Net, the first open-source fully convolutional neural network (FCNN) and interactive learning and training software aimed at assessing the present and past dynamics of shallow marine systems through habitat mapping into geomorphological (9 classes) and biological classes (22 classes). Shallow marine systems, particularly coral reefs, are under significant pressures due to climate change, ocean acidification, and other anthropogenic pressures, leading to rapid, often devastating changes, in these fragile and diverse ecosystems. Historically, remote sensing of shallow marine habitats has been limited to meter-scale imagery due to the optical effects of ocean wave distortion, refraction, and optical attenuation. NeMO-Net combines 3D cm-scale distortion-free imagery captured using NASA's airborne FluidCam and fluid lensing remote sensing technology with low resolution airborne and spaceborne datasets of varying spatial resolutions, spectral spaces, calibrations, and temporal cadence in a supercomputer-based deep learning framework. NeMO-Net augments and improves the benthic habitat classification accuracy of low-resolution datasets across large geographic and temporal scales using high-resolution training data from FluidCam. NeMO-Net's FCNN uses ResNet and RefineNet to perform semantic segmentation and cloud masking of remote sensing imagery of shallow marine systems from drones, manned aircraft, and satellites, including FluidCam, WorldView, Planet, Sentinel, and Landsat. Deep Laplacian Pyramid Super-Resolution Networks (LapSRN) alongside Domain Adversarial Neural Networks (DANNs) are used to augment low resolution imagery with high resolution drone-based datasets as well as recognize domain-invariant features across multiple instruments to achieve high classification accuracies, ameliorating inter-sensor spatial, spectral and temporal heterogeneities. An online active learning and citizen science application is used to allows users to provide interactive training data for NeMO-Net in 2D and 3D, fully integrated within an active learning framework. Preliminary results from a test case in Fiji demonstrate 9-class classification accuracy exceeding 84&#37;.
WOS:000664335303155
</snippet>
</document>

<document id="103">
<title>An Augmentation Attention Mechanism for High-Spatial-Resolution Remote Sensing Image Scene Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3006241</url>
<snippet>High-spatial-resolution remote sensing (HRRS) image scene classification, which categorizes HRRS images into an independent set of semantic-level land use and land cover classes based on image contents, has attracted much attention, and many methods have been proposed due to its wide application in earth observation tasks. In fact, categories of HRRS images depend on regions containing class-specific ground objects, while most of the existing methods for HRRS image scene classification only focus on global information, which introduces redundant information and results in the poor performance of HRRS image scene classification. To overcome the shortcomings of the existing methods, an attention mechanism-based convolutional neural network with multiaugmented schemes is proposed in this article. In the proposed method, augmentation operations over attention mechanism feature maps are used to force the model to capture class-specific features and eliminate redundant information and push the model to capture discriminative regions as much as possible, instead of using all global information without favor. Moreover, a bilinear pooling is utilized to expand the interclass discrimination. Still, feature center loss motivated by center loss is applied to narrow the intraclass gap. To verify the effectiveness of the proposed end-to-end model, three benchmarks are used for testing, and the experimental results have proven the superiority of the proposed method, compared with current state-of-the-art end-to-end methods for HRRS image scene classification.
WOS:000550641600001
</snippet>
</document>

<document id="104">
<title>A Novel Joint Change Detection Approach Based on Weight-Clustering Sparse Autoencoders</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2892951</url>
<snippet>With the rapid development of earth observation technology, the number of available remote sensing data has soared dramatically. It becomes a significant problem that how to use remote sensing images and how to improve the accuracy of change detection effectively. In this paper, a novel approach for change detection using weight-clustering sparse autoencoders (WCSAE) combined object-oriented classification with difference images (DIs) is proposed. First, bi-phase images are segmented as patches through the density-based spatial clustering of applications with noise algorithm. Afterward, the average and variance of superpixels are stacked as the input of WCSAE. To reduce the redundant information of extracted features, similar weights in the hidden layer of WCSAE are clustered layer-wise under termination conditions by using the hierarchical agglomerative clustering algorithm. For the improvement of the classification accuracy, L1/2 regularization is introduced in the objective function to extract more sparse features and avoid over-fitting. Next, the post-classification change detection map is obtained by means of comparing with classification results of two phase images. Then, by using the change vector analysis technology, the difference map is yielded and also classified under WCSAE to acquire the DI classification map. Finally, the joint probability judgment is implemented on the joint scopes to determine changed and unchanged areas. The effectiveness and superiority of the proposed method are verified in accordance with the experimental results on standard datasets and actual remote sensing images.
WOS:000460663600026
</snippet>
</document>

<document id="105">
<title>Structure learning and the posterior parietal cortex</title>
<url>http://dx.doi.org/10.1016/j.pneurobio.2019.101717</url>
<snippet>We propose a theory of structure learning in the primate brain. We argue that the parietal cortex is critical for learning about relations among the objects and categories that populate a visual scene. We suggest that current deep learning models exhibit poor global scene understanding because they fail to perform the relational inferences that occur in the primate dorsal stream. We review studies of neural coding in primate posterior parietal cortex (PPC), drawing the conclusion that neurons in this brain area represent potentially high-dimensional inputs on a low-dimensional manifold that encodes the relative position of objects or features in physical space, and relations among entities in abstract conceptual space. We argue that this low-dimensional code supports generalisation of relational information, even in nonspatial domains. Finally, we propose that structure learning is grounded in the actions that primates take when they reach for objects or fixate them with their eyes. We sketch a model of how this might occur in neural circuits.
WOS:000513296200004
</snippet>
</document>

<document id="106">
<title>Forest Fire Susceptibility Modeling Using a Convolutional Neural Network for Yunnan Province of China</title>
<url>http://dx.doi.org/10.1007/s13753-019-00233-1</url>
<snippet>Forest fires have caused considerable losses to ecologies, societies, and economies worldwide. To minimize these losses and reduce forest fires, modeling and predicting the occurrence of forest fires are meaningful because they can support forest fire prevention and management. In recent years, the convolutional neural network (CNN) has become an important state-of-the-art deep learning algorithm, and its implementation has enriched many fields. Therefore, we proposed a spatial prediction model for forest fire susceptibility using a CNN. Past forest fire locations in Yunnan Province, China, from 2002 to 2010, and a set of 14 forest fire influencing factors were mapped using a geographic information system. Oversampling was applied to eliminate the class imbalance, and proportional stratified sampling was used to construct the training/validation sample libraries. A CNN architecture that is suitable for the prediction of forest fire susceptibility was designed and hyperparameters were optimized to improve the prediction accuracy. Then, the test dataset was fed into the trained model to construct the spatial prediction map of forest fire susceptibility in Yunnan Province. Finally, the prediction performance of the proposed model was assessed using several statistical measures-Wilcoxon signed-rank test, receiver operating characteristic curve, and area under the curve (AUC). The results confirmed the higher accuracy of the proposed CNN model (AUC 0.86) than those of the random forests, support vector machine, multilayer perceptron neural network, and kernel logistic regression benchmark classifiers. The CNN has stronger fitting and classification abilities and can make full use of neighborhood information, which is a promising alternative for the spatial prediction of forest fire susceptibility. This research extends the application of CNN to the prediction of forest fire susceptibility.
WOS:000488611200009
</snippet>
</document>

<document id="107">
<title>Convolutional neural networks for disaggregated population mapping using open data</title>
<url>http://dx.doi.org/10.1109/DSAA.2018.00076</url>
<snippet>High resolution population count data are vital for numerous applications such as urban planning, transportation model calibration, and population growth impact measurements, among others. In this work, we present and evaluate an end-to-end framework for computing disaggregated population mapping employing convolutional neural networks (CNNs). Using urban data extracted from the OpenStreetMap database, a set of urban features are generated which are used to guide population density estimates at a higher resolution. A population density grid at a 200 by 200 meter spatial resolution is estimated, using as input gridded population data of 1 by 1 kilometer. Our approach relies solely on open data with a wide geographical coverage, ensuring replicability and potential applicability to a great number of cities in the world. Fine-grained gridded population data is used for 15 French cities in order to train and validate our model. A stand-alone city is kept out for the validation procedure. The results demonstrate that the neural network approach using massive OpenStreetMap data outperforms other approaches proposed in related works.
WOS:000459238600067
</snippet>
</document>

<document id="108">
<title>Joint Correlation Alignment-Based Graph Neural Network for Domain Adaptation of Multitemporal Hyperspectral Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3063460</url>
<snippet>In this article, we propose a novel deep domain adaptation method based on graph neural network (GNN) for multitemporal hyperspectral remote sensing images. In GNN, graphs are constructed for source and target data, respectively. Then the graphs are utilized in each hidden layer to obtain features. GNN operates on graph structure and the relations between data samples can be exploited. It aggregates features and propagate information through graph nodes. Thus, the extracted features have an improved smoothness in each spectral neighborhood which is beneficial to classification. Furthermore, the domain-wise correlation alignment (CORAL) and class-wise CORAL are jointly embedded in GNN network to achieve a joint distribution adaptation performance. By introducing the joint CORAL strategy in GNN, the extracted features can not only be aligned between domains but also have a superior discriminability in each domain. This domain adaptation network is named as joint CORAL-based graph neural network. Experiments using multitemporal Hyperion and NSF-funded center for airborne laser mapping datasets demonstrate the effectiveness of the proposed method.
WOS:000634496000006
</snippet>
</document>

<document id="109">
<title>Convolutional Neural Network-Based Land Cover Classification Using 2-D Spectral Reflectance Curve Graphs With Multitemporal Satellite Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2880783</url>
<snippet>Researchers constantly seek more efficient detection techniques to better utilize enhanced image resolution in accurately detecting and monitoring land cover. Recently, convolutional neural networks (CNNs) have shown high performances comparable to or even better than widely used and adopted machine learning techniques. The aim of this study is to investigate the application of CNNs for land cover classification by using two-dimensional (2-D) spectral curve graphs from multispectral satellite images. The land cover classification was conducted in Concord, New Hampshire, USA, and South Korea by using multispectral images acquired from 30-m Landsat-8 and 500-m Geostationary Ocean Color Imager images. For the construction of input data specific to CNNs, two seasons (winter and summer) of multispectral bands were transformed into 2-D spectral curve graphs for each class. Land cover classification results of CNNs were compared with the results of support vector machines (SVMs) and random forest (RFs). The CNNs model showed higher performance than RFs and SVMs in both study sites. The examination of land cover classification maps demonstrates a good agreement with reference maps, Google Earth images, and existing global scale land cover map, especially for croplands. Using the spectral curve graph could incorporate the phenological cycles on classifying the land cover types. This study shows that the use of a new transformation of spectral bands into a 2-D form for application in CNNs can improve land cover classification performance.
WOS:000455462100006
</snippet>
</document>

<document id="110">
<title>Multiple intra-urban land use simulations and driving factors analysis: a case study in Huicheng, China</title>
<url>http://dx.doi.org/10.1080/15481603.2018.1507074</url>
<snippet>Simulations of intra-urban land use changes have gradually attracted more attention as these approaches are extremely helpful in regard to decision making and policy formulation. While prior studies mostly focused on methods of developing intra-urban level simulations, very little research has been conducted explain the factors driving intra-urban land use change. Urban planners are highly concerned with how inner-city structures are formed and how they function. Here, to simulate multiple intra-urban land use changes and to identify the contribution of different driving factors, we developed a random forests (RF) algorithm-based cellular automata (CA) simulation model. In this study, the model applied diverse categories of spatial variables, including traffic location factors, environmental factors, public services, and population density, as the driving factors to enhance our understanding of the dynamics of internal urban land use. The CA model was tested using data from the Huicheng district of Huizhou city in the Guangdong province of China. The Model was validated using actual historical land use data from 2000 to 2010. By applying the validated model, multiple intra-urban land use maps were simulated for 2015. Simultaneously, spatial variable importance measures (VIMs) were calculated by using the out-of-bag (OOB) error estimation approach of the RF algorithm. Based on the calculation results, we assessed and analysed the significance of each intra-urban land use driver for this region. This study provides urban planners and relevant scholars with detailed and targeted information that can aid in the formulation of specific planning strategies for different intra-urban land uses and support the future evolution of this area.
WOS:000454923100006
</snippet>
</document>

<document id="111">
<title>Change Captioning: A New Paradigm for Multitemporal Remote Sensing Image Analysis</title>
<url>http://dx.doi.org/10.1109/TGRS.2022.3195692</url>
<snippet>Change detection (CD) is among the most important applications in remote sensing (RS) that allows identifying the changes that occurred in a given geographical area across different times. Even though CD systems have seen a lot of progress in RS, their output is either a binary map highlighting the changing area or a semantic change map that indicates the type of change for each pixel. The change maps are often difficult to interpret by end users, and they omit important information such as relationships and attributes of the changed areas. Motivated by the recent advancement of image captioning in the RS community, in this article, we propose to describe the changes over bitemporal images through change sentence descriptions. The aim of this article is to provide a user-friendly interpretation of the occurred changes. To this end, we propose two change captioning (CC) systems that take bitemporal images as input and generate coherent sentence descriptions of the occurred changes. Convolutional neural networks (CNNs) are used to extract discriminative features from the bitemporal images and recurrent neural networks (RNNs) or support vector machines (SVMs) are exploited to generate coherent change descriptions. Furthermore, in the absence of a CC dataset to test our systems, we propose two new datasets. One is based on very high-resolution RGB images, and the other one is based on multispectral RS images. The obtained experimental results show promising capabilities of the proposed systems to generate coherent change descriptions from the bitemporal images. The datasets are available at the following link: https://disi.unitn.it/similar to melgani/datasets.html.
WOS:000843314100015
</snippet>
</document>

<document id="112">
<title>Change detection using deep learning approach with object-based image analysis</title>
<url>http://dx.doi.org/10.1016/j.rse.2021.112308</url>
<snippet>In their applications, both deep learning techniques and object-based image analysis (OBIA) have shown better performance separately than conventional methods on change detection tasks. However, efforts to investigate the effect of combining these two techniques for advancing change detection techniques are unexplored in current literature. This study proposes a novel change detection method implementing change feature extraction using convolutional neural networks under an OBIA framework. To demonstrate the effectiveness of our proposed method, we compare the proposed method against benchmark pixel-based counterparts on aerial images for the task of multi-class change detection. To thoroughly assess the performance of our proposed method, this study also for the first time compared three common feature fusion schemes for change detection architecture: concatenation, differencing, and Long Short-Term Memory (LSTM). The proposed method was also tested on simulated misregistered images to evaluate its robustness, a factor that plays an important role in compromising change detection accuracy but has not been investigated for supervised change detection methods in the literature. Finally, the proposed change detection method was also tested using very high resolution (VHR) satellite images for binary class change detection to map an impacted area caused by natural disaster and the result was evaluated using reference data from the Federal Emergency Management Agency (FEMA). With the experimental results from these two sets of experiments, we showed that (1) our proposed method achieved substantially higher accuracy and computational efficiency when compared to pixel-based methods, (2) three feature fusion schemes did not show a significant difference for overall accuracy, (3) our proposed method was robust in image misregistration in both testing and training data, (4) we demonstrate the potential impact of automation to decision making by deploying our method to map a large geographic area affected by a recent natural disaster.
WOS:000626633100001
</snippet>
</document>

<document id="113">
<title>Segmentation-based multi-pixel cloud optical thickness retrieval using a convolutional neural network</title>
<url>http://dx.doi.org/10.5194/amt-15-5181-2022</url>
<snippet>We introduce a new machine learning approach to retrieve cloud optical thickness (COT) fields from visible passive imagery. In contrast to the heritage independent pixel approximation (IPA), our convolutional neural network (CNN) retrieval takes the spatial context of a pixel into account and thereby reduces artifacts arising from net horizontal photon transfer, which is commonly known as independent pixel (IP) bias. The CNN maps radiance fields acquired by imaging radiometers at a single wavelength channel to COT fields. It is trained with a low-complexity and therefore fast U-Net architecture with which the mapping is implemented as a segmentation problem with 36 COT classes. As a training data set, we use a single radiance channel (600 nm) generated from a 3D radiative transfer model using large eddy simulations (LESs) from the Sulu Sea. We study the CNN model under various conditions based on different permutations of cloud aspect ratio and morphology, and we use appropriate cloud morphology metrics to measure the performance of the retrievals. Additionally, we test the general applicability of the CNN on a new geographic location with LES data from the equatorial Atlantic. Results indicate that the CNN is broadly successful in overcoming the IP bias and outperforms IPA retrievals across all morphologies. Over the Atlantic, the CNN tends to overestimate the COT but shows promise in regions with high cloud fractions and high optical thicknesses, despite being outside the general training envelope. This work is intended to be used as a baseline for future implementations of the CNN that can enable generalization to different regions, scales, wavelengths, and sun-sensor geometries with limited training.
WOS:000853516500001
</snippet>
</document>

<document id="114">
<title>Learning Cartographic Building Generalization with Deep Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/ijgi8060258</url>
<snippet>Cartographic generalization is a problem, which poses interesting challenges to automation. Whereas plenty of algorithms have been developed for the different sub-problems of generalization (e.g., simplification, displacement, aggregation), there are still cases, which are not generalized adequately or in a satisfactory way. The main problem is the interplay between different operators. In those cases the human operator is the benchmark, who is able to design an aesthetic and correct representation of the physical reality. Deep learning methods have shown tremendous success for interpretation problems for which algorithmic methods have deficits. A prominent example is the classification and interpretation of images, where deep learning approaches outperform traditional computer vision methods. In both domains-computer vision and cartography-humans are able to produce good solutions. A prerequisite for the application of deep learning is the availability of many representative training examples for the situation to be learned. As this is given in cartography (there are many existing map series), the idea in this paper is to employ deep convolutional neural networks (DCNNs) for cartographic generalizations tasks, especially for the task of building generalization. Three network architectures, namely U-net, residual U-net and generative adversarial network (GAN), are evaluated both quantitatively and qualitatively in this paper. They are compared based on their performance on this task at target map scales 1:10,000, 1:15,000 and 1:25,000, respectively. The results indicate that deep learning models can successfully learn cartographic generalization operations in one single model in an implicit way. The residual U-net outperforms the others and achieved the best generalization performance.
WOS:000475307000016
</snippet>
</document>

<document id="115">
<title>Deep Convolutional Neural Network for Complex Wetland Classification Using Optical Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2846178</url>
<snippet>The synergistic use of spatial features with spectral properties of satellite images enhances thematic land cover information, which is of great significance for complex land cover mapping. Incorporating spatial features within the classification scheme have been mainly carried out by applying just low-level features, which have shown improvement in the classification result. By contrast, the application of high-level spatial features for classification of satellite imagery has been underrepresented. This study aims to address the lack of high-level features by proposing a classification framework based on convolutional neural network (CNN) to learn deep spatial features for wetland mapping using optical remote sensing data. Designing a fully trained new convolutional network is infeasible due to the limited amount of training data in most remote sensing studies. Thus, we applied fine tuning of a pre-existing CNN. Specifically, AlexNet was used for this purpose. The classification results obtained by the deep CNN were compared with those based on well-known ensemble classifiers, namely random forest (RF), to evaluate the efficiency of CNN. Experimental results demonstrated that CNN was superior to RF for complex wetland mapping even by incorporating the small number of input features (i.e., three features) for CNN compared to RF (i.e., eight features). The proposed classification scheme is the first attempt, investigating the potential of fine-tuning pre-existing CNN, for land cover mapping. It also serves as a baseline framework to facilitate further scientific research using the latest state-of-art machine learning tools for processing remote sensing data.
WOS:000444485100005
</snippet>
</document>

<document id="116">
<title>Shallow Neural Network Model for Hand-drawn Symbol Recognition in Multi-Writer Scenario</title>
<url>http://dx.doi.org/10.1109/ICDAR.2017.263</url>
<snippet>One of the main challenges in hand drawn symbol recognition is the variability among symbols because of the different writer styles. In this paper, we present and discuss some results recognizing hand-drawn symbols with a shallow neural network. A neural network model inspired from the LeNet architecture has been used to achieve state-of-the-art results with very less training data, which is very unlikely to the data hungry deep neural network. From the results, it has become evident that the neural network architectures can efficiently describe and recognize hand drawn symbols from different writers and can model the inter author aberration.
WOS:000428140900015
</snippet>
</document>

<document id="117">
<title>Deep Learning From Multiple Crowds: A Case Study of Humanitarian Mapping</title>
<url>http://dx.doi.org/10.1109/TGRS.2018.2868748</url>
<snippet>Satellite images are widely applied in humanitarian mapping that labels buildings, roads, and so on for humanitarian aid and economic development. However, the labeling now is mostly done by volunteers. In this paper, we utilize deep learning to solve humanitarian mapping tasks of a mobile software named MapSwipe. The current deep learning techniques, e.g., convolutional neural network (CNN), can recognize ground objects from satellite images but rely on numerous labels for training for each specific task. We solve this problem by fusing multiple freely accessible crowdsourced geographic data and propose an active learning-based CNN training framework named MC-CNN to deal with the quality issues of the labels extracted from these data, including incompleteness (e.g., some kinds of object are not labeled) and heterogeneity (e.g., different spatial granularities). The method is evaluated with building mapping in South Malawi and road mapping in Guinea with level-18 satellite images provided by Bing Map and volunteered geographic information from OpenStreetMap, MapSwipe, and OsmAnd. The results based on multiple metrics, including Precision, Recall, F1 Score, and area under the receiver operating characteristic curve, show that MC-CNN can fuse the crowdsourced labels for higher prediction performance and be successfully applied in MapSwipe for humanitarian mapping with 85&#37; labor saved and an overall accuracy of 0.86 achieved.
WOS:000460321300038
</snippet>
</document>

<document id="118">
<title>GlacierNet: A Deep-Learning Approach for Debris-Covered Glacier Mapping</title>
<url>http://dx.doi.org/10.1109/ACCESS.2020.2991187</url>
<snippet>Rising global temperatures over the past decades is directly affecting glacier dynamics. To understand glacier fluctuations and document regional glacier-state trends, glacier-boundary detection is necessary. Debris-covered glacier (DCG) mapping, however, is notoriously difficult using conventional geospatial technology methods. Therefore, in this research for automated DCG mapping, we evaluate the utility of a convolutional neural network (CNN), which is a deep learning feed-forward neural network. The CNN inputs include Landsat satellite images, an Advanced Land Observation Satellite (ALOS) digital elevation model (DEM) and DEM-derived land-surface parameters. Our CNN based deep-learning approach named GlacierNet was designed by appropriately choosing the type, number and size of layers and filters, and encoder depth based on the properties of the input data, CNN segmentation process and empirical results. The GlacierNet was then trained using input data and corresponding glacier boundaries from the Global Land Ice Measurements from Space (GLIMS) database, and tested on glaciers in the Karakoram and Nepal Himalaya. Our results show proof-of-concept that GlacierNet reasonably identifies the boundaries of DCGs with a relatively high degree of accuracy, and that morphometric parameters improves boundary detection.
WOS:000549502200147
</snippet>
</document>

<document id="119">
<title>YOLOv5-Tassel: Detecting Tassels in RGB UAV Imagery With Improved YOLOv5 Based on Transfer Learning</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3206399</url>
<snippet>Unmanned aerial vehicles (UAVs) equipped with lightweight sensors, such as RGB cameras and LiDAR, have significant potential in precision agriculture, including object detection. Tassel detection in maize is an essential trait given its relevance as the beginning of the reproductive stage of growth and development of the plants. However, compared with general object detection, tassel detection based on RGB imagery acquired by UAVs is more challenging due to the small size, time-dependent variable shape, and complexity of the objects of interest. A novel algorithm referred to as YOLOv5-tassel is proposed to detect tassels in UAV-based RGB imagery. A bidirectional feature pyramid network is adopted for the path-aggregation neck to effectively fuse cross-scale features. The robust attention module of SimAM is introduced to extract the features of interest before each detection head. An additional detection head is also introduced to improve small-size tassel detection based on the original YOLOv5. Annotation is performed with guidance from center points derived from CenterNet to improve the selection of the bounding boxes for tassels. Finally, to address the issue of limited reference data, transfer learning based on the VisDrone dataset is adopted. Testing results for our proposed YOLOv5-tassel method achieved the mAP value of 44.7&#37;, which is better than well-known object detection approaches, such as FCOS, RetinaNet, and YOLOv5.
WOS:000861443200003
</snippet>
</document>

<document id="120">
<title>Instance Segmentation, Body Part Parsing, and Pose Estimation of Human Figures in Pictorial Maps</title>
<url>http://dx.doi.org/10.1080/23729333.2021.1949087</url>
<snippet>In recent years, convolutional neural networks (CNNs) have been applied successfully to recognise persons, their body parts and pose keypoints in photos and videos. The transfer of these techniques to artificially created images is rather unexplored, though challenging since these images are drawn in different styles, body proportions, and levels of abstraction. In this work, we study these problems on the basis of pictorial maps where we identify included human figures with two consecutive CNNs: We first segment individual figures with Mask R-CNN, and then parse their body parts and estimate their poses simultaneously with four different UNet++ versions. We train the CNNs with a mixture of real persons and synthetic figures and compare the results with manually annotated test datasets consisting of pictorial figures. By varying the training datasets and the CNN configurations, we were able to improve the original Mask R-CNN model and we achieved moderately satisfying results with the UNet++ versions. The extracted figures may be used for animation and storytelling and may be relevant for the analysis of historic and contemporary maps.
WOS:000684795500001
</snippet>
</document>

<document id="121">
<title>3D Virtual Urban Scene Reconstruction From a Signal Optical Remote Sensing Image</title>
<url>http://dx.doi.org/10.1109/ACCESS.2019.2915932</url>
<snippet>This paper presents a low-cost and efficient method for 3D virtual urban scene reconstruction based on multi-source remote sensing big data and deep learning. By integrating maps, satellite optical images, and digital terrain model (DTM), the proposed method achieves a reasonable reconstructed 3D model for complex urban. The method consists of two independent convolutional neural networks (CNN) to process the land cover and the building height extraction. The proposed method is then tested on a 100 km(2) scene in San Diego, USA, including about 30 000 buildings. The land cover classification achieves an overall accuracy (OA) of 80.4&#37; for eight types of land as defined in NLCD 2011 datasets. Building height estimation achieves an average error at 1.9 meters on NYC open data, the building footprint. Furthermore, the scene reconstruction including the estimation of both land cover and building height can be finished in 10 min on a single NVidia Titan X GPU.
WOS:000471221600001
</snippet>
</document>

<document id="122">
<title>Multi-input deep learning architecture for predicting breast tumor response to chemotherapy using quantitative MR images</title>
<url>http://dx.doi.org/10.1007/s11548-020-02209-9</url>
<snippet>Purpose Neoadjuvant chemotherapy (NAC) aims to minimize the tumor size before surgery. Predicting response to NAC could reduce toxicity and delays to effective intervention. Computational analysis of dynamic contrast-enhanced magnetic resonance images (DCE-MRI) through deep convolution neural network (CNN) has shown a significant performance to distinguish responders and no responders patients. This study intends to present a new deep learning (DL) model predicting the breast cancer response to NAC based on multiple MRI inputs. Methods A cohort of 723 axial slices extracted from 42 breast cancer patients who underwent NAC therapy was used to train and validate the developed DL model. This dataset was provided by our collaborator institute of radiology in Brussels. Fourteen external cases were used to validate the best obtained model to predict pCR based on pre- and post-chemotherapy DCE-MRI. The model performance was assessed by area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, specificity, and feature map visualization. Results The developed multi-inputs deep learning architecture was able to predict the pCR to NAC treatment in the validation dataset with an AUC of 0.91 using combined pre- and post-NAC images. The visual results showed that the most important extracted features from non-pCR tumors are in the peripheral region. The proposed method was more productive than the previous ones. Conclusion Even with a limited training dataset size, the proposed and developed CNN model using DCE-MR images acquired before and after the first chemotherapy was able to classify pCR and non-pCR patients with substantial accuracy. This model could be used hereafter in clinical analysis after its evaluation based on more extra data.
WOS:000540990700002
</snippet>
</document>

<document id="123">
<title>Remote Sensing Data Augmentation Through Adversarial Training</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3110842</url>
<snippet>The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9&#37;, meeting actual generation requirements.
WOS:000698859700009
</snippet>
</document>

<document id="124">
<title>TanDEM-X Forest Mapping Using Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/rs11242980</url>
<snippet>In this work, we face the problem of forest mapping from TanDEM-X data by means of Convolutional Neural Networks (CNNs). Our study aims to highlight the relevance of domain-related features for the extraction of the information of interest thanks to their joint nonlinear processing through CNN. In particular, we focus on the main InSAR features as the backscatter, coherence, and volume decorrelation, as well as the acquisition geometry through the local incidence angle. By using different state-of-the-art CNN architectures, our experiments consistently demonstrate the great potential of deep learning in data fusion for information extraction in the context of synthetic aperture radar signal processing and specifically for the task of forest mapping from TanDEM-X images. We compare three state-of-the-art CNN architectures, such as ResNet, DenseNet, and U-Net, obtaining a large performance gain over the baseline approach for all of them, with the U-Net solution being the most effective one.
WOS:000507333400094
</snippet>
</document>

<document id="125">
<title>Counting From Sky: A Large-Scale Data Set for Remote Sensing Object Counting and a Benchmark Method</title>
<url>http://dx.doi.org/10.1109/TGRS.2020.3020555</url>
<snippet>Object counting, whose aim is to estimate the number of objects from a given image, is an important and challenging computation task. Significant efforts have been devoted to addressing this problem and achieved great progress, yet counting the number of ground objects from remote sensing images is barely studied. In this article, we are interested in counting dense objects from remote sensing images. Compared with object counting in a natural scene, this task is challenging in the following factors: large-scale variation, complex cluttered background, and orientation arbitrariness. More importantly, the scarcity of data severely limits the development of research in this field. To address these issues, we first construct a large-scale object counting data set with remote sensing images, which contains four important geographic objects: buildings, crowded ships in harbors, and large vehicles and small vehicles in parking lots. We then benchmark the data set by designing a novel neural network that can generate a density map of an input image. The proposed network consists of three parts, namely attention module, scale pyramid module, and deformable convolution module (DCM) to attack the aforementioned challenging factors. Extensive experiments are performed on the proposed data set and one crowd counting data set, which demonstrates the challenges of the proposed data set and the superiority and effectiveness of our method compared with state-of-the-art methods.
WOS:000642096400003
</snippet>
</document>

<document id="126">
<title>Efficient Net-XGBoost: An Implementation for Facial Emotion Recognition Using Transfer Learning</title>
<url>http://dx.doi.org/10.3390/math11030776</url>
<snippet>Researchers are interested in Facial Emotion Recognition (FER) because it could be useful in many ways and has promising applications. The main task of FER is to identify and recognize the original facial expressions of users from digital inputs. Feature extraction and emotion recognition make up the majority of the traditional FER. Deep Neural Networks, specifically Convolutional Neural Network (CNN), are popular and highly used in FER due to their inherent image feature extraction process. This work presents a novel method dubbed as EfficientNet-XGBoost that is based on Transfer Learning (TL) technique. EfficientNet-XGBoost is basically a cascading of the EfficientNet and the XGBoost techniques along with certain enhancements by experimentation that reflects the novelty of the work. To ensure faster learning of the network and to overcome the vanishing gradient problem, our model incorporates fully connected layers of global average pooling, dropout and dense. EfficientNet is fine-tuned by replacing the upper dense layer(s) and cascading the XGBoost classifier making it suitable for FER. Feature map visualization is carried out that reveals the reduction in the size of feature vectors. The proposed method is well-validated on benchmark datasets such as CK+, KDEF, JAFFE, and FER2013. To overcome the issue of data imbalance, in some of the datasets namely CK+ and FER2013, we augmented data artificially through geometric transformation techniques. The proposed method is implemented individually on these datasets and corresponding results are recorded for performance analysis. The performance is computed with the help of several metrics like precision, recall and F1 measure. Comparative analysis with competent schemes are carried out on the same sample data sets separately. Irrespective of the nature of the datasets, the proposed scheme outperforms the rest with overall rates of accuracy being 100&#37;, 98&#37; and 98&#37; for the first three datasets respectively. However, for the FER2013 datasets, efficiency is less promisingly observed in support of the proposed work.
WOS:000931077000001
</snippet>
</document>

<document id="127">
<title>Mapping Mangrove Forests Based on Multi-Tidal High-Resolution Satellite Imagery</title>
<url>http://dx.doi.org/10.3390/rs10091343</url>
<snippet>Mangrove forests, which are essential for stabilizing coastal ecosystems, have been suffering from a dramatic decline over the past several decades. Mapping mangrove forests using satellite imagery is an efficient way to provide key data for mangrove forest conservation. Since mangrove forests are periodically submerged by tides, current methods of mapping mangrove forests, which are normally based on single-date, remote-sensing imagery, often underestimate the spatial distribution of mangrove forests, especially when the images used were recorded during high-tide periods. In this paper, we propose a new method of mapping mangrove forests based on multi-tide, high-resolution satellite imagery. In the proposed method, a submerged mangrove recognition index (SMRI), which is based on the differential spectral signature of mangroves under high and low tides from multi-tide, high-resolution satellite imagery, is designed to identify submerged mangrove forests. The proposed method applies the SMRI values, together with textural features extracted from high-resolution imagery and geographical features of mangrove forests, to an object-based support vector machine (SVM) to map mangrove forests. The proposed method was evaluated via a case study with GF-1 images (high-resolution satellites launched by China) in Yulin City, Guangxi Zhuang Autonomous Region of China. The results show that our proposed method achieves satisfactory performance, with a kappa coefficient of 0.86 and an overall accuracy of 94&#37;, which is better than results obtained from object-based SVMs that use only single-date, remote sensing imagery.
WOS:000449993800023
</snippet>
</document>

<document id="128">
<title>A hybrid approach to model the dykes in Sungun porphyry copper deposit using Dempster-Shafer theory</title>
<url>http://dx.doi.org/10.1007/s12517-020-06241-6</url>
<snippet>Modeling geological units such as dykes is a powerful tool in mineral exploration studies. The most significant issue in modeling of uncertainty in the mineral exploration studies is the identification of the geological domains that consist of the zones with significant exploratory factor located in the area under study. The sequential indicator simulation (SIS) algorithm has the ability to model lithological facies. Although the two-point geostatistical simulation approaches are simple, they have some substantial drawbacks. For instance, they do not take into account the complex and nonlinear continuities in geological units like a swarm of the dyke. Therefore, if the area under study has complex geological units, it is necessary to improve the SIS algorithm considering the spatial patterns of complex and nonlinear structures. The pattern recognition method is an effective approach that can improve the two-point geostatistical simulation algorithm. It is based on the informative complex geological spatial patterns that contain the connectivity and nonlinear structures. This paper proposes a hybrid approach (PR-SIS) for simulation of geological units in the presence of several nonlinear structures. To perform hybrid approach, the SIS and pattern recognition methods were used in a combined form. To this end, the results of two methods were combined by the Dempster-Shafer theory (DST) approach. The main purpose of this paper is to investigate the impact of the proposed hybrid approach for modeling the dykes of the Sungun porphyry copper deposit (Iran). To this end, two evidences have been used including the sequential indicator simulation (SIS) method and the pattern recognition approach. These evidences were then applied for using the DST to obtain the final accuracy of the model. Final modeling of this approach shows the continuity of dykes in each location and optimal check with respect to the DST approach based on the geological map. The overall accuracy (OA) and the kappa index (KI) in different dimensions of the search windows showed that the dykes modeling using the hybrid approach by the DST provide a better accuracy. In this regard, the values of OA and KI were 0.889 and 0.792 and 0.901 and 0.805, as well as 0.982 and 0.891 for the SIS method, the pattern recognition method using 2.5x2.5 search window, and the proposed hybrid approach by the DST, respectively.
WOS:000606496500001
</snippet>
</document>

<document id="129">
<title>DOMAIN ADAPTATION FOR LARGE SCALE CLASSIFICATION OF VERY HIGH RESOLUTION SATELLITE IMAGES WITH DEEP</title>
<url>http://dx.doi.org/</url>
<snippet>Semantic segmentation of remote sensing images enables in particular land-cover map generation for a given set of classes. Very recent literature has shown the superior performance of deep convolutional neural networks (DCNN) for many tasks, from object recognition to semantic labelling, including the classification of Very High Resolution (VHR) satellite images. However, while plethora of works aim at improving object delineation on geographically restricted areas, few tend to solve this classification task at very large scales. New issues occur such as intra-class class variability, diachrony between surveys, and the appearance of new classes in a specific area, that do not exist in the predefined set of labels. Therefore, this work intends to (i) perform large scale classification and to (ii) expand a set of land-cover classes, using the off-the-shelf model learnt in a specific area of interest and adapting it to unseen areas.
WOS:000451039803151
</snippet>
</document>

<document id="130">
<title>Ventricular Arrhythmia Classification and Interpretation Using Residual Neural Network with Guided Backpropagation</title>
<url>http://dx.doi.org/10.1109/TENCON54134.2021.9707469</url>
<snippet>Sudden cardiac death is the leading cause of natural death ocurring due to life-threatening lethal ventricular arrhythmias such as Ventricular Tachycardia (VT) and Ventricular Fibrillation (VF). This paper employs a supervised deep-learning interpretable framework for detecting VT and VF from Electrocardiogram (ECG) signals. The framework consists of two stages: (i) single-lead ECG classification using convolution-based Residual Neural Network (ResNet); and (ii) interpretation of classified segments using gradient-based Guided Backpropagation. The single-lead ECG is decluttered from noise, augmented, and classified using the ResNet classifier. The convolution layers in ResNet encode temporal variations present in ECG to provide better feature abstraction. The fully connected layer aggregates the encoding based on clinical relevance and performs classification. Lastly, the saliency maps of the penultimate convolution layer are visualized using guided backpropagation to highlight important signal timestamps responsible for classification. The proposed method is robust and outperforms state-of-the-art methods when verified on datasets acquired from five different geographic locations.
WOS:000799485900101
</snippet>
</document>

<document id="131">
<title>Local and global encoder network for semantic segmentation of Airborne laser scanning point clouds</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.04.016</url>
<snippet>Interpretation of Airborne Laser Scanning (ALS) point clouds is a critical procedure for producing various geo-information products like 3D city models, digital terrain models and land use maps. In this paper, we present a local and global encoder network (LGENet) for semantic segmentation of ALS point clouds. Adapting the KPConv network, we first extract features by both 2D and 3D point convolutions to allow the network to learn more representative local geometry. Then global encoders are used in the network to exploit contextual information at the object and point level. We design a segment-based Edge Conditioned Convolution to encode the global context between segments. We apply a spatial-channel attention module at the end of the network, which not only captures the global interdependencies between points but also models interactions between channels. We evaluate our method on two ALS datasets namely, the ISPRS benchmark dataset and DCF2019 dataset. For the ISPRS benchmark dataset, our model achieves state-of-the-art results with an overall accuracy of 0.845 and an average F1 score of 0.737. With regards to the DFC2019 dataset, our proposed network achieves an overall accuracy of 0.984 and an average F1 score of 0.834.
WOS:000655474600012
</snippet>
</document>

<document id="132">
<title>Deep Learning of High-Resolution Aerial Imagery for Coastal Marsh Change Detection: A Comparative Study</title>
<url>http://dx.doi.org/10.3390/ijgi11020100</url>
<snippet>Deep learning techniques are increasingly being recognized as effective image classifiers. Aside from their successful performance in past studies, the accuracies have varied in complex environments, in comparison with the popularly of applied machine learning classifiers. This study seeks to explore the feasibility of using a U-Net deep learning architecture to classify bi-temporal, high-resolution, county-scale aerial images to determine the spatial extent and changes of land cover classes that directly or indirectly impact tidal marsh. The image set used in the analysis is a collection of a 1-m resolution collection of National Agriculture Imagery Program (NAIP) tiles from 2009 and 2019, covering Beaufort County, South Carolina. The U-Net CNN classification results were compared with two machine learning classifiers, the random trees (RT) and support vector machine (SVM). The results revealed a significant accuracy advantage in using the U-Net classifier (92.4&#37;), as opposed to the SVM (81.6&#37;) and RT (75.7&#37;) classifiers, for overall accuracy. From the perspective of a GIS analyst or coastal manager, the U-Net classifier is now an easily accessible and powerful tool for mapping large areas. Change detection analysis indicated little areal change on marsh extent, though increased land development throughout the county has the potential to negatively impact the health of the marshes. Future work should explore applying the constructed U-Net classifier to coastal environments in large geographic areas, while also implementing other data sources (e.g., LIDAR and multispectral data) to enhance classification accuracy.
WOS:000767289900001
</snippet>
</document>

<document id="133">
<title>SARDINE: A Self-Adaptive Recurrent Deep Incremental Network Model for Spatio-Temporal Prediction of Remote Sensing Data</title>
<url>http://dx.doi.org/10.1145/3380972</url>
<snippet>The timely and accurate prediction of remote sensing data is of utmost importance especially in a situation where the predicted data is utilized to provide insights into emerging issues, like environmental nowcasting. Significant research progress can be found to date in devising variants of neural network (NN) models to fulfil this requirement by improving feature extraction and dynamic process representation power. Nevertheless, all these existing NN models are built upon rigid structures that often fail to maintain tradeoff between bias and variance, and consequently, need to spend a lot of time to empirically determine the most appropriate network configuration. This article proposes a self-adaptive recurrent deep incremental network model (SARDINE) which is a novel variant of the deep recurrent neural network with intrinsic capability of self-constructing the network structure in a dynamic and incremental fashion while learning from observed data samples. Moreover, the proposed SARDINE is able to model the spatial feature evolution while scanning the data in a single pass manner, and this further saves significant time when dealing with remote sensing imagery containing millions of pixels. Subsequently, we employ SARDINE in combination with a spatial influence mapping unit to accomplish the prediction. The effectiveness of the proposed model is evaluated in terms of predicting a time series of normalized difference vegetation index (NDVI) data derived from Landsat Thematic Mapper (TM)-5 and Moderate Resolution Imaging Spectroradiometer (MODIS) Terra satellite imagery. The experimental result demonstrates that the SARDINE-based prediction is able to achieve state-of-the-art accuracy with significantly reduced computational cost.
WOS:000583755200003
</snippet>
</document>

<document id="134">
<title>Deep Collaborative Attention Network for Hyperspectral Image Classification by Combining 2-D CNN and 3-D CNN</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3016739</url>
<snippet>Deep learning-based methods based on convolutional neural networks (CNNs) have demonstrated remarkable performance in hyperspectral image (HSI) classification. Most of these approaches are only based on 2-D CNN or 3-D CNN. It is dramatic from the literature that using just 2-D CNN may result in missing channel relationship information, and using just 3-D CNN may make the model very complex. Moreover, the existing network models do not pay enough attention to extracting spectral-spatial correlation information. To address these issues, we propose a deep collaborative attention network for HSI classification by combining 2-D CNN, and 3-D CNN (CACNN). Specifically, we first extract spectral-spatial features by using 2-D CNN, and 3-D CNN, respectively, and then use a "NonLocalBlock" to combine these two kinds of features. This block serves as a typical spatial attention mechanism, and makes salient features be emphasized. Then, we propose a "Conv_Block" that is similar to the lightweight dense block to extract correlation information contained in the feature maps. Finally, we consider a deep multilayer feature fusion strategy, and thereby combine the features of different hierarchical layers to extract the strong correlated spectral-spatial information among them. To test the performance of CACNN approach, several experiments are performed on four well-known HSIs. The results are compared with the state-of-the-art approaches, and satisfactory performance is obtained by our proposed method. The code of CACNN method is available on Dr. J. Lius GitHub. (1) (1) Online. [Avaialble]: https://github.com/liuofficial.
WOS:000564195900012
</snippet>
</document>

<document id="135">
<title>Open-Pit Mine Area Mapping With Gaofen-2 Satellite Images Using U-Net</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3171290</url>
<snippet>Obtaining information on the surface coverage of open-pit mining areas (OPMAs) is of great significance to ecological governance and restoration. The current methods to map the OPMAs face problems such as low mapping accuracy due to complex landscapes. In this article, we propose a hybrid open-pit mining mapping (OPMM) framework with Gaofen-2 (GF-2) high-spatial resolution satellite images (HSRSIs), using an improved U-Net neural network (U-Net+). By concatenating the previous layers with each subsequent layer to ensure that there is a maximum of feature maps of each layer in the network, the U-Net+ can reduce the loss of feature information and make the extraction capability of the network more powerful. Two independent OPMAs were selected as the study area for the OPMM. By taking advantage of GF-2 HSRSIs, a total of 111 open-pit mine sites (OPMSs) were mapped and each OPMS boundary was validated by field surveys. Then, these OPMSs were used as input to assess the accuracy of the OPMM results obtained by the U-Net+. By comparing our results with those provided by five state-of-the-art deep learning algorithms: Fully Convolutional Network (FCN), SegNet, U-Net, Residual U-Net (ResU-Net), and U-Net++, we conclude that the proposed framework outperformed these methods by more than 0.02&#37; in Overall Accuracy, 0.06&#37; in Kappa Coefficient, 0.03&#37; in Mean Intersection over union, 8.36&#37; in producer accuracy and 4.44&#37; in user accuracy. Therefore, the proposed framework thus exhibits very promising applicability in the ecological restoration and governance of OPMAs.
WOS:000797454600007
</snippet>
</document>

<document id="136">
<title>Automatic detection of burial mounds (kurgans) in the Altai Mountains</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.05.010</url>
<snippet>The Altai Mountains are one of the most impressive and valuable archaeological areas in the world. Kurgans (burial mounds) of ancient civilizations, which are scattered across the vast Altai area, are an exceptionally valuable source of information for archaeology. These precious archaeological resources, which sometimes have been preserved intact in the permafrost underground for over two millennia, are now under various threats, such as natural disasters, farmland expansion, touristic development, and most notably global warming. A detailed map or inventory of the mounds is essential but is still not available. In this study, we test the deep convolutional neural network (CNN) technique for automatic detection of stone mounds from high-resolution satellite images in four regions in the Altai Mountains. We propose three improvement techniques to increase the performance of off-the-shelf object detection methods that are originally proposed for daily-life objects. Our results demonstrate that it is feasible to apply CNN to detect stone mounds, and the detection results are good enough to capture their spatial distribution. CNN-based object detection can largely narrow down the search area for archaeologists in yet un-surveyed regions, and is therefore useful for preparing field survey campaigns and directing archaeological fieldwork. We also applied the method to an un-surveyed Altai Mountain area and successfully discovered stone mounds that are yet undocumented. Our method can potentially be applied to construct an inventory for all stone mounds present in the whole Altai Mountain region.
WOS:000660980400015
</snippet>
</document>

<document id="137">
<title>Automatic Rural Road Centerline Detection and Extraction from Aerial Images for a Forest Fire Decision Support System</title>
<url>http://dx.doi.org/10.3390/rs15010271</url>
<snippet>To effectively manage the terrestrial firefighting fleet in a forest fire scenario, namely, to optimize its displacement in the field, it is crucial to have a well-structured and accurate mapping of rural roads. The landscapes complexity, mainly due to severe shadows cast by the wild vegetation and trees, makes it challenging to extract rural roads based on processing aerial or satellite images, leading to heterogeneous results. This article proposes a method to improve the automatic detection of rural roads and the extraction of their centerlines from aerial images. This method has two main stages: (i) the use of a deep learning model (DeepLabV3+) for predicting rural road segments; (ii) an optimization strategy to improve the connections between predicted rural road segments, followed by a morphological approach to extract the rural road centerlines using thinning algorithms, such as those proposed by Zhang-Suen and Guo-Hall. After completing these two stages, the proposed method automatically detected and extracted rural road centerlines from complex rural environments. This is useful for developing real-time mapping applications.
WOS:000909124400001
</snippet>
</document>

<document id="138">
<title>Hyperspectral Image Superresolution by Transfer Learning</title>
<url>http://dx.doi.org/10.1109/JSTARS.2017.2655112</url>
<snippet>Hyperspectral image superresolution is a highly attractive topic in computer vision and has attracted many researchers attention. However, nearly all the existing methods assume that multiple observations of the same scene are required with the observed low-resolution hyperspectral image. This limits the application of superresolution. In this paper, we propose a new framework to enhance the resolution of hyperspectral images by exploiting the knowledge from natural images: The relationship between low/high-resolution images is the same as that between low/high-resolution hyperspectral images. In the proposed framework, the mapping between low-and high-resolution images can be learned by deep convolutional neural network and be transferred to hyperspectral image by borrowing the idea of transfer learning. In addition, to study the spectral characteristic between low-and high-resolution hyperspectral image, collaborative nonnegative matrix factorization (CNMF) is proposed to enforce collaborations between the low-and high-resolution hyperspectral images, which encourages the estimated solution to extract the same endmembers with low-resolution hyperspectral image. The experimental results on ground based and remote sensing data suggest that the proposed method achieves comparable performance without requiring any auxiliary images of the same scene.
WOS:000399682500024
</snippet>
</document>

<document id="139">
<title>Semantic segmentation of citrus-orchard using deep neural networks and multispectral UAV-based imagery</title>
<url>http://dx.doi.org/10.1007/s11119-020-09777-5</url>
<snippet>Accurately mapping farmlands is important for precision agriculture practices. Unmanned aerial vehicles (UAV) embedded with multispectral cameras are commonly used to map plants in agricultural landscapes. However, separating plantation fields from the remaining objects in a multispectral scene is a difficult task for traditional algorithms. In this connection, deep learning methods that perform semantic segmentation could help improve the overall outcome. In this study, state-of-the-art deep learning methods to semantic segment citrus-trees in multispectral images were evaluated. For this purpose, a multispectral camera that operates at the green (530-570 nm), red (640-680 nm), red-edge (730-740 nm) and also near-infrared (770-810 nm) spectral regions was used. The performance of the following five state-of-the-art pixelwise methods were evaluated: fully convolutional network (FCN), U-Net, SegNet, dynamic dilated convolution network (DDCN) and DeepLabV3 + . The results indicated that the evaluated methods performed similarly in the proposed task, returning F1-Scores between 94.00&#37; (FCN and U-Net) and 94.42&#37; (DDCN). It was also determined the inference time needed per area and, although the DDCN method was slower, based on a qualitative analysis, it performed better in highly shadow-affected areas. This study demonstrated that the semantic segmentation of citrus orchards is highly achievable with deep neural networks. The state-of-the-art deep learning methods investigated here proved to be equally suitable to solve this task, providing fast solutions with inference time varying from 0.98 to 4.36 min per hectare. This approach could be incorporated into similar research, and contribute to decision-making and accurate mapping of plantation fields.
WOS:000604197700004
</snippet>
</document>

<document id="140">
<title>Snow Avalanche Segmentation in SAR Images With Fully Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3036914</url>
<snippet>Knowledge about frequency and location of snow avalanche activity is essential for forecasting and mapping of snow avalanche hazard. Traditional field monitoring of avalanche activity has limitations, especially when surveying large and remote areas. In recent years, avalanche detection in Sentinel-1 radar satellite imagery has been developed to improve monitoring. However, the current state-of-the-art detection algorithms, based on radar signal processing techniques, are still much less accurate than human experts. To reduce this gap, we propose a deep learning architecture for detecting avalanches in Sentinel-1 radar images. We trained a neural network on 6345 manually labeled avalanches from 117 Sentinel-1 images, each one consisting of six channels that include backscatter and topographical information. Then, we tested our trained model on a new synthetic aperture radar image. Comparing to the manual labeling (the gold standard), we achieved an F1 score above 66&#37;, whereas the state-of-the-art detection algorithm sits at an F1 score of only 38&#37;. A visual inspection of the results generated by our deep learning model shows that only small avalanches are undetected, whereas some avalanches that were originally not labeled by the human expert are discovered.
WOS:000607413900003
</snippet>
</document>

<document id="141">
<title>Using Convolutional Neural Networks for Detection and Morphometric Analysis of Carolina Bays from Publicly Available Digital Elevation Modelss</title>
<url>http://dx.doi.org/10.3390/rs13183770</url>
<snippet>Carolina Bays are oriented and sandy-rimmed depressions that are ubiquitous throughout the Atlantic Coastal Plain (ACP). Their origin has been a highly debated topic since the 1800s and remains unsolved. Past population estimates of Carolina Bays have varied vastly, ranging between as few as 10,000 to as many as 500,000. With such a large uncertainty around the actual population size, mapping these enigmatic features is a problem that requires an automated detection scheme. Using publicly available LiDAR-derived digital elevation models (DEMs) of the ACP as training images, various types of convolutional neural networks (CNNs) were trained to detect Carolina bays. The detection results were assessed for accuracy and scalability, as well as analyzed for various morphologic, land-use and land cover, and hydrologic characteristics. Overall, the detector found over 23,000 Carolina Bays from southern New Jersey to northern Florida, with highest densities along interfluves. Carolina Bays in Delmarva were found to be smaller and shallower than Bays in the southeastern ACP. At least a third of Carolina Bays have been converted to agricultural lands and almost half of all Carolina Bays are forested. Few Carolina Bays are classified as open water basins, yet almost all of the detected Bays were within 2 km of a water body. In addition, field investigations based upon detection results were performed to describe the sedimentology of Carolina Bays. Sedimentological investigations showed that Bays typically have 1.5 m to 2.5 m thick sand rims that show a gradient in texture, with coarser sand at the bottom and finer sand and silt towards the top. Their basins were found to be 0.5 m to 2 m thick and showed a mix of clayey, silty, and sandy deposits. Last, the results compiled during this study were compared to similar depressional features (i.e., playa-lunette systems) to pinpoint any similarities in origin processes. Altogether, this study shows that CNNs are valuable tools for automated geomorphic feature detection and can lead to new insights when coupled with various forms of remotely sensed and field-based datasets.
WOS:000702098400001
</snippet>
</document>

<document id="142">
<title>Change Detection from Remote Sensing to Guide OpenStreetMap Labeling</title>
<url>http://dx.doi.org/10.3390/ijgi9070427</url>
<snippet>The growing amount of openly available, meter-scale geospatial vertical aerial imagery and the need of the OpenStreetMap (OSM) project for continuous updates bring the opportunity to use the former to help with the latter, e.g., by leveraging the latest remote sensing data in combination with state-of-the-art computer vision methods to assist the OSM community in labeling work. This article reports our progress to utilize artificial neural networks (ANN) for change detection of OSM data to update the map. Furthermore, we aim at identifying geospatial regions where mappers need to focus on completing the global OSM dataset. Our approach is technically backed by the big geospatial data platform Physical Analytics Integrated Repository and Services (PAIRS). We employ supervised training of deep ANNs from vertical aerial imagery to segment scenes based on OSM map tiles to evaluate the technique quantitatively and qualitatively. Data Set License:ODbL
WOS:000557447100001
</snippet>
</document>

<document id="143">
<title>CrossGeoNet: A Framework for Building Footprint Generation of Label-Scarce Geographical Regions</title>
<url>http://dx.doi.org/10.1016/j.jag.2022.102824</url>
<snippet>Building footprints are essential for understanding urban dynamics. Planet satellite imagery with daily repetition frequency and high resolution has opened new opportunities for building mapping at large scales. However, suitable building mapping methods are scarce for less developed regions, as these regions lack massive annotated samples to provide strong supervisory information. To address this problem, we propose to learn crossgeolocation attention maps in a co-segmentation network, which is able to improve the discriminability of buildings within the target city and provide a more general building representation in different cities. In this way, the limited supervisory information resulting from insufficient training examples in target cities can be compensated. Our method is termed as CrossGeoNet, and consists of three elemental modules: a Siamese encoder, a cross-geolocation attention module, and a Siamese decoder. More specifically, the encoder learns feature maps from a pair of images from two different geo-locations. The cross-location attention module aims at learning similarity based on these two feature maps and can provide a global overview of common objects (e.g., buildings) in different cities. The decoder predicts segmentation masks of buildings using the learned crosslocation attention maps and the original convolved images. The proposed method is evaluated on two datasets with different spatial resolutions, i.e., Planet dataset (3 m/pixel) and Inria dataset (0.3 m/pixel), which are collected from various locations around the world. Experimental results show that CrossGeoNet can well extract buildings of different sizes and alleviate false detections, which significantly outperforms other competitors.
WOS:000811611600003
</snippet>
</document>

<document id="144">
<title>You are how you travel: A multi-task learning framework for Geodemographic inference using transit smart card data</title>
<url>http://dx.doi.org/10.1016/j.compenvurbsys.2020.101517</url>
<snippet>Geodemographics, providing the information of populations characteristics in the regions on a geographical basis, is of immense importance in urban studies, public policy-making, social research and business, among others. Such data, however, are difficult to collect from the public, which is usually done via census, with a low update frequency. In urban areas, with the increasing prevalence of public transit equipped with automated fare payment systems, researchers can collect massive transit smart card (SC) data from a large population. The SC data record human daily activities at an individual level with high spatial and temporal resolutions. It can reveal frequent activity areas (e.g., residential areas) and travel behaviours of passengers that are intimately intertwined with personal interests and characteristics. This provides new opportunities for geodemographic study. This paper seeks to develop a framework to infer travellers demographics (such as age, income level and car ownership, et al.) and their residential areas for geodemographic mapping using SC data with a household survey. We first use a decision tree diagram to detect passengers residential areas. We then represent each individuals spatio-temporal activity pattern derived from multi-week SC data as a 2D image. Leveraging this representation, a multi-task convolutional neural network (CNN) is employed to predict multiple demographics of individuals from the images. Combing the demographics and locations of their residence, geodemographic information is further obtained. The methodology is applied to a large-scale SC dataset provided by Transport for London. Results provide new insights in understanding the relationship between human activity patterns and demographics. To the best of our knowledge, this is the first attempt to infer geodemographics by using the SC data.
WOS:000558452100007
</snippet>
</document>

<document id="145">
<title>Traffic Intersection Re-Identification Using Monocular Camera Sensors</title>
<url>http://dx.doi.org/10.3390/s20226515</url>
<snippet>Perception of road structures especially the traffic intersections by visual sensors is an essential task for automated driving. However, compared with intersection detection or visual place recognition, intersection re-identification (intersection re-ID) strongly affects driving behavior decisions with given routes, yet has long been neglected by researchers. This paper strives to explore intersection re-ID by a monocular camera sensor. We propose a Hybrid Double-Level re-identification approach which exploits two branches of Deep Convolutional Neural Network to accomplish multi-task including classification of intersection and its fine attributes, and global localization in topological maps. Furthermore, we propose a mixed loss training for the network to learn the similarity of two intersection images. As no public datasets are available for the intersection re-ID task, based on the work of RobotCar, we propose a new dataset with carefully-labeled intersection attributes, which is called "RobotCar Intersection" and covers more than 30,000 images of eight intersections in different seasons and day time. Additionally, we provide another dataset, called "Campus Intersection" consisting of panoramic images of eight intersections in a university campus to verify our updating strategy of topology map. Experimental results demonstrate that our proposed approach can achieve promising results in re-ID of both coarse road intersections and its global pose, and is well suited for updating and completion of topological maps.
WOS:000595061300001
</snippet>
</document>

<document id="146">
<title>A Tomato Quality Identification Method Based on Raman Spectroscopy and Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1088/1742-6596/1438/1/012029</url>
<snippet>in recent years, more and more technologies have been applied in monitoring growth and production efficiency of plants, i.e. agricultural Internet of Things (IOT) and new information- aware technologies. The architecture of the IOT is divided into four layers, i.e., the sensing layer, network layer, processing layer and application layer[1-4]. Among them, the perception layer is the facial features and the skin of the IOT, which is the basis of the IOT [5]. Raman spectroscopy technology has the advantages of fast, simplicity, accuracy, non-destructive and automatic identification, which has become a powerful analytical verifying method. The method of tomato quality identification that based on the Raman spectroscopy combined with convolutional neural network (CNN)[6]was explored. The Raman spectrum of tomato was collected by Raman sensor to construct a neural network with deep network structure. Through repeatedly learning and training in Raman map, we can determine the map recognition model of high quality tomatoes and use matplotlib to realize the identification simulation.
WOS:000618445200029
</snippet>
</document>

<document id="147">
<title>Urban Flood Mapping With Bitemporal Multispectral Imagery Via a Self-Supervised Learning Framework</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3047677</url>
<snippet>Near realtime flood mapping in densely populated urban areas is critical for emergency response. The strong heterogeneity of urban areas poses a big challenge for accurate near realtime flood mapping. However, previous studies on automatic methods for urban flood mapping perform infeasible in near realtime or fail to generalize well to other floods, for several reasons. First, multitemporal pixel-wise flood mapping requires accurate image registration, hindering the efficiency of large-scale processing. Although automatic image registration has been investigated, precisely coregistered multitemporal image sequence requires time-consuming fine tuning. Additionally, the floods may lead to the loss of many corresponding image points across multitemporal images for accurate coregistration. Second, existing unsupervised methods generally rely on hand-crafted features for floodwater detection. Such features may not well represent the patterns of floodwaters in different areas due to inconsistent weather conditions, illumination, and floodwater spectra. This article proposes a self-supervised learning framework for patch-wise urban flood mapping using bitemporal multispectral satellite imagery. Patch-wise change vector analysis is used with patch features learned through a self-supervised autoencoder to produce patch-wise change maps showing potentially flood-affected areas. Postprocessing including spectral and spatial filtering is applied to these patch-wise change maps to remove nonflood related changes. Final flood maps and parameter sensitivities were evaluated using several performance metrics. Two flood events from areas with differing degrees of urbanization were considered: Hurricane Harvey flood (2017) in Houston, Texas, and Hurricane Florence flood (2018) in Lumberton, North Carolina. The proposed method shows strong performance for self-supervised urban flood mapping.
WOS:000615042800002
</snippet>
</document>

<document id="148">
<title>Local motion simulation using deep reinforcement learning</title>
<url>http://dx.doi.org/10.1111/tgis.12620</url>
<snippet>Traditional local motion simulation focuses largely on avoiding collisions in the next frame. However, due to its lack of forward looking, the global trajectory of agents usually seems unreasonable. As a method of optimizing the overall reward, deep reinforcement learning (DRL) can better correct the problems that exist in the traditional local motion simulation method. In this article, we propose a local motion simulation method integrating optimal reciprocal collision avoidance (ORCA) and DRL, referred to as ORCA-DRL. The main idea of ORCA-DRL is to perform local collision avoidance detection via ORCA and smooth the trajectory at the same time via DRL. We use a deep neural network (DNN) as the state-to-action mapping function, where the state information is detected by virtual visual sensors and the action space includes two continuous spaces: speed and direction. To improve data utilization and speed up the training process, we use the proximal policy optimization based on the actor-critic (AC) framework to update the DNN parameters. Three scenes (circle, hallway, and crossing) are designed to evaluate the performance of ORCA-DRL. The results reveal that, compared with the ORCA, our proposed ORCA-DRL method can: (a) reduce the total number of frames, leading to less time for agents to reach their destination; and (b) effectively avoid local optima, evidenced by smoothed global trajectories.
WOS:000522348900001
</snippet>
</document>

<document id="149">
<title>OmbriaNet-Supervised Flood Mapping via Convolutional Neural Networks Using Multitemporal Sentinel-1 and Sentinel-2 Data Fusion</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3155559</url>
<snippet>Regions around the world experience adverse climate-change-induced conditions that pose severe risks to the normal and sustainable operations of modern societies. Extreme weather events, such as floods, rising sea levels, and storms, stand as characteristic examples that impair the core services of the global ecosystem. Especially floods have a severe impact on human activities, hence, early and accurate delineation of the disaster is of top priority since it provides environmental, economic, and societal benefits and eases relief efforts. In this article, we introduce OmbriaNet, a deep neural network architecture, based on convolutional neural networks, that detects changes between permanent and flooded water areas by exploiting the temporal differences among flood events extracted by different sensors. To demonstrate the potential of the proposed approach, we generated OMBRIA, a bitemporal and multimodal satellite imagery dataset for image segmentation through supervised binary classification. It consists of a total number of 3.376 images, synthetic aperture radar imagery from Sentinel-1, and multispectral imagery from Sentinel-2, accompanied with ground-truth binary images produced from data derived by experts and provided from the Emergency Management Service of the European Space Agency Copernicus Program. The dataset covers 23 flood events around the globe, from 2017 to 2021. We collected, co-registrated and preprocessed the data in Google Earth Engine. To validate the performance of our method, we performed different benchmarking experiments on the OMBRIA dataset and we compared with several competitive state-of-the-art techniques. The experimental analysis demonstrated that the proposed formulation is able to produce high-quality flood maps, achieving a superior performance over the state-of-the-art. We provide OMBRIA dataset, as well as OmbriaNet code at: https://github.com/geodrak/OMBRIA.
WOS:000773262200005
</snippet>
</document>

<document id="150">
<title>Flood Extent Mapping: An Integrated Method Using Deep Learning and Region Growing Using UAV Optical Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3051873</url>
<snippet>Flooding occurs frequently and causes loss of lives, and extensive damages to infrastructure and the environment. Accurate and timely mapping of flood extent to ascertain damages is critical and essential for relief activities. Recently, deep-learning-based approaches, including convolutional neural network (CNN) has shown promising results for flood extent mapping. However, these methods cannot extract floods underneath vegetation canopy using the optical imagery. This article attempts to address this problem by introducing an integrated CNN and region growing (RG) method for the mapping of both visible and underneath vegetation flooded areas. The CNN-based classifier is used to extract flooded areas from the optical images, whereas, the RG method is applied to estimate the extent of floods underneath vegetation that are not visible from imagery using the digital elevation model. A data augmentation technique is applied for training the CNN-based classifier to improve the classification results. The results show that the data augmentation can enhance the accuracy of image classification and the proposed integrated method efficiently detects floods in both the visible and the areas covered by vegetation, which is essential to supporting effective flood emergency response and recovery activities.
WOS:000615042800012
</snippet>
</document>

<document id="151">
<title>A Parcel-Based Deep-Learning Classification to Map Local Climate Zones From Sentinel-2 Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3071577</url>
<snippet>Local climate zones (LCZ) describe urban surface structures, supporting studies of urban heat islands, sustainable urbanization, and energy balance. The existing studies mapped LCZs from satellite images using scene-based classification, which trained deep-learning classifiers by labeled image patches, segmented satellite images into patches by sliding windows to match the size of training data, and finally classified the segmented patches to obtain LCZ maps. However, sliding windows are different from the real footprints of LCZs, which leads to large errors in classification. To address this problem, this article proposes a parcel-based method for LCZ classification using Sentinel-2 images, road networks, and elevation data. First, the Sentinel-2 images are segmented by the road network to obtain the land parcels as classification units. Second, each image parcel is standardized to match the training dataset, So2Sat LCZ42. Third, the trained convolutional neural network (CNN) is used to classify the standardized parcels into LCZs. Finally, the building height information derived from elevation data is used to refine the LCZs by a rule-based classifier. The results of the four test sites show that the overall accuracy of our method is 0.75, higher than the sliding-window-based methods accuracy of 0.47. Additional simulation experiments demonstrated that parcels derived from road networks can reduce the mixture effect in image patches, and parcel standardization can ensure the transferability of the CNN model trained by regular image patches. Considering that the road network and elevation data are widely available, the proposed method has the potential of mapping LCZs in large areas.
WOS:000645081200014
</snippet>
</document>

<document id="152">
<title>Using deep convolutional neural networks to forecast spatial patterns of Amazonian deforestation</title>
<url>http://dx.doi.org/10.1111/2041-210X.13953</url>
<snippet>1.Tropical forests are subject to diverse deforestation pressures while their conservation is essential to achieve global climate goals. Predicting the location of deforestation is challenging due to the complexity of the natural and human systems involved but accurate and timely forecasts could enable effective planning and on-the-ground enforcement practices to curb deforestation rates. New computer vision technologies based on deep learning can be applied to the increasing volume of Earth observation data to generate novel insights and make predictions with unprecedented accuracy. 2. Here, we demonstrate the ability of deep convolutional neural networks (CNNs) to learn spatiotemporal patterns of deforestation from a limited set of freely available global data layers, including multispectral satellite imagery, the Hansen maps of annual forest change (2001-2020) and the ALOS PALSAR digital surface model, to forecast deforestation (2021). We designed four model architectures, based on 2D CNNs, 3D CNNs, and Convolutional Long Short-Term Memory (ConvLSTM) Recurrent Neural Networks (RNNs), to produce spatial maps that indicate the risk to each forested pixel (similar to 30 m) in the landscape of becoming deforested within the next year. They were trained and tested on data from two similar to 80,000 km(2) tropical forest regions in the Southern Peruvian Amazon. 3. The networks could predict the location of future forest loss to a high degree of accuracy (F-1 = 0.58-0.71). Our best performing model (3D CNN) had the highest pixel-wise accuracy (F-1 = 0.71) when validated on 2020 forest loss (2014-2019 training). Visual interpretation of the mapped forecasts indicated that the network could automatically discern the drivers of forest loss from the input data. For example, pixels around new access routes (e.g. roads) were assigned high risk, whereas this was not the case for recent, concentrated natural loss events (e.g. remote landslides). 4. Convolutional neural networks can harness limited time-series data to predict near-future deforestation patterns, an important step in harnessing the growing volume of satellite remote sensing data to curb global deforestation. The modelling framework can be readily applied to any tropical forest location and used by governments and conservation organisations to prevent deforestation and plan protected areas.
WOS:000835796500001
</snippet>
</document>

<document id="153">
<title>Navigation behavior based on self-organized spatial representation in hierarchical recurrent neural network</title>
<url>http://dx.doi.org/10.1080/01691864.2019.1566088</url>
<snippet>A cognitive map is an internal model of the external world and contains the spatial representation of the surrounding environment. The existence of the cognitive map was first identified in rats; rats can navigate to their desired destination using cognitive maps while dealing with environmental uncertainty. We performed a mobile robot navigation experiment where obstacles were randomly placed using hierarchical recurrent neural network (HRNN) with multiple timescales. The HRNN was trained to navigate the mobile robot to the destination indicated by a snapshot image. After the training, the HRNN was able to successfully avoid the obstacles and navigate to the destination from any location in the environment. Analysis of the internal states of the HRNN showed that the module with fast timescale handles obstacle avoidance and the one with slow timescale has spatial representation corresponding to the spatial position of the destination. Moreover, in the experiment wherein the novel path appeared, the trained HRNN performed shortcut behavior. The shortcut behavior shows that the HRNN performed navigation using the self-organized spatial representation in the slow recurrent neural network. This indicates that training of goal-oriented navigation, i.e. the navigation motivated by a snapshot image of the destination results in the self-organization of cognitive map-like representation.
WOS:000473486200002
</snippet>
</document>

<document id="154">
<title>HA U-Net: Improved Model for Building Extraction From High Resolution Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/ACCESS.2021.3097630</url>
<snippet>Automatic extraction of buildings from high-resolution remote sensing images becomes an important research. Since the convolutional neural network can perform pixel-level segmentation, this technology has been applied in this field. But the increase in resolution prone to blurry segmentation because the model needs more edge detail and multi-scale detail learning. To solve this problem, a method is proposed in this paper, which consists of three parts: (1) an improved model named Holistically-Nested Attention U-Net (HA U-Net) is designed, which integrates the attention mechanism and multi-scale nested modules to supervise prediction; (2) During model training, an improved weighted loss function is proposed to make the designed model more focused on learning boundary features; (3) watershed algorithm is exploited for image post-processing to optimize segmentation results. The designed HA U-Net performs well on WHU Building Dataset and Urban3d Challenge dataset, and achieves 9.31&#37;, 2.17&#37; better F1-score and 10.78&#37;, 1.77&#37; better IOU than the standard U-Net respectively. The experimental results indicate that the proposed method can well solve the building adhesion problem. The research can serve as updating geographic databases.
WOS:000678303900001
</snippet>
</document>

<document id="155">
<title>Multilabel Remote Sensing Image Retrieval Based on Fully Convolutional Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2961634</url>
<snippet>Conventional remote sensing image retrieval (RSIR) system usually performs single-label retrieval where each image is annotated by a single label representing the most significant semantic content of the image. In this scenario, however, the scene complexity of remote sensing images is ignored, where an image might have multiple classes (i.e., multiple labels), resulting in poor retrieval performance. We therefore propose a novel multilabel RSIR approach based on fully convolutional network (FCN). Specifically, FCN is first trained to predict segmentation map of each image in the considered image archive. We then obtain multilabel vector and extract region convolutional features of each image based on its segmentation map. The extracted region features are finally used to perform region-based multilabel retrieval. The experimental results show that our approach achieves state-of-the-art performance in contrast to handcrafted and convolutional neural network features.
WOS:000526639900026
</snippet>
</document>

<document id="156">
<title>Deep gradient prior network for DEM super-resolution: Transfer learning from image to DEM</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.02.008</url>
<snippet>Digital elevation model (DEM) super-resolution (SR) aims to increase the spatial resolution of a DEM through data processing, rather than using sensors with higher accuracy. Inspired by the success of convolutional neural networks (CNNs) in image SR, this study introduces the CNN into DEM SR. However, directly training a robust network for DEM SR has remained a challenge as it has been difficult to obtain sufficient high-resolution DEM samples. Therefore, we proposed a novel method that includes two measures to address this issue. The first is to design a deep CNN for acquiring gradient prior knowledge. Based on this prior, high-resolution gradient maps of the studied DEM can be estimated. The second measure is to introduce transfer learning, which applies the knowledge learned from the natural image to the DEM SR problem. Therefore, a CNN is pretrained by the gradient of numerous high-resolution natural images and then fine-tuned with the gradient maps of some DEM training data. Finally, the high-resolution DEM is reconstructed under the constraints of the estimated gradient maps and the original low-resolution DEM. Overall, the experiments indicate that the proposed framework is superior to current state-of-the-art methods.
WOS:000464088400007
</snippet>
</document>

<document id="157">
<title>Early-Season Crop Mapping on an Agricultural Area in Italy Using X-Band Dual-Polarization SAR Satellite Data and Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3198475</url>
<snippet>Early-season crop mapping provides decision-makers with timely information on crop types and conditions that are crucial for agricultural management. Current satellite-based mapping solutions mainly rely on optical imagery, albeit limited by weather conditions. Very few exploit long-time series of polarized synthetic aperture radar (SAR) imagery. To address this gap, we assessed the performance of COSMO-SkyMed X-band dual-polarized (HH, VV) data in a test area in Ponte a Elsa (central Italy) in January-September 2020 and 2021. A deep learning convolutional neural network (CNN) classifier arranged with two different architectures (1-D and 3-D) was trained and used to recognize ten classes. Validation was undertaken with in situ measurements from regular field campaigns carried out during satellite overpasses over more than 100 plots each year. The 3-D classifier structure and the combination of HH+VV backscatter provide the best classification accuracy, especially during the first months of each year, i.e., 80&#37; already in April 2020 and in May 2021. Overall accuracy above 90&#37; is always marked from June using the 3-D classifier with HH, VV, and HH+VV backscatter. These experiments showcase the value of the developed SAR-based early-season crop mapping approach. The influence of vegetation phenology, structure, density, biomass, and turgor on the CNN classifier using X-band data requires further investigations, along with the relatively low producer accuracy marked by vineyard and uncultivated fields.
WOS:000845070100009
</snippet>
</document>

<document id="158">
<title>Learning geometric soft constraints for multi-view instance matching across street-level panoramas</title>
<url>http://dx.doi.org/</url>
<snippet>We present a new approach for matching tree instances across multiple street-view panorama images for the ultimate goal of city-scale street-tree mapping with high positioning accuracy. What makes this task challenging is the strong change in view-point, different lighting conditions, high similarity of neighboring trees, and variability in scale. We propose to turn (tree) instance matching into a learning task, where image-appearance and geometric relationships between views fruitfully interact. Our approach constructs a Siamese convolutional neural network that learns to match two views of the same tree given many candidate tree image cut-outs and geographic information of the two panorama images. In addition to image features, we propose utilizing location information about the camera and the tree. Our method is compared to existing patch matching methods to prove its edge over state-of-the-art. This takes us one step closer to the ultimate goal of city-wide tree mapping based solely on panorama imagery to benefit city administration.
WOS:000560604400003
</snippet>
</document>

<document id="159">
<title>Multi-task generative topographic mapping in virtual screening</title>
<url>http://dx.doi.org/10.1007/s10822-019-00188-x</url>
<snippet>The previously reported procedure to generate universal Generative Topographic Maps (GTMs) of the drug-like chemical space is in practice a multi-task learning process, in which both operational GTM parameters (example: map grid size) and hyperparameters (key example: the molecular descriptor space to be used) are being chosen by an evolutionary process in order to fit/select universal GTM manifolds. After selection (a one-time task aimed at optimizing the compromise in terms of neighborhood behavior compliance, over a large pool of various biological targets), for any further use the manifolds are ready to provide fit-free predictive models. Using any structure-activity setirrespectively whether the associated target served at map fitting stage or notthe generation or coloring a property landscape enables predicting the property for any external molecule, with zero additional fitable parameters involved. While previous works have signaled the excellent behavior of such models in aggressive three-fold cross-validation assessments of their predictive power, the present work wished to explore their behavior in Virtual Screening (VS), here simulated on hand of external DUD ligand and decoy series that are fully disjoint from the ChEMBL-extracted landscape coloring sets. Beyond the rather robust results of the universal GTM manifolds in this challenge, it could be shown that the descriptor spaces selected by the evolutionary multi-task learner were intrinsically able to serve as an excellent support for many other VS procedures, starting from parameter-free similarity searching, to local (target-specific) GTM models, to parameter-rich, nonlinear Random Forest and Neural Network approaches.
WOS:000461038500002
</snippet>
</document>

<document id="160">
<title>Detection, classification, and mapping of coffee fruits during harvest with computer vision</title>
<url>http://dx.doi.org/10.1016/j.compag.2021.106066</url>
<snippet>In this study, an algorithm is implemented with a computer vision model to detect and classify coffee fruits and map the fruits maturation stage during harvest. The main contribution of this study is with respect to the assignment of geographic coordinates to each frame, which enables the mapping of detection summaries across coffee rows. The model used to detect and classify coffee fruits was implemented using the Darknet, an open source framework for neural networks written in C. The coffee fruits detection and classification were performed using the object detection system named YOLOv3-tiny. For this study, 90 videos were recorded at the end of the discharge conveyor of a coffee harvester during the 2020 harvest of arabica coffee (Catuai 144) at a commercial area in the region of Patos de Minas, in the state of Minas Gerais, Brazil. The model performance peaked around the similar to 3300th iteration when considering an image input resolution of 800 x 800 pixels. The model presented an mAP of 84&#37;, F1-Score of 82&#37;, precision of 83&#37;, and recall of 82&#37; for the validation set. The average precision for the classes of unripe, ripe, and overripe coffee fruits was 86&#37;, 85&#37;, and 80&#37;, respectively. As the algorithm enabled the detection and classification in videos collected during the harvest, it was possible to map the qualitative attributes regarding the coffee maturation stage along the crop lines. These attribute maps provide managers important spatial information for the application of precision agriculture techniques in crop management. Additionally, this study should incentive future research to customize the deep learning model for certain tasks in agriculture and precision agriculture.
WOS:000634877300005
</snippet>
</document>

<document id="161">
<title>Promising Artificial Intelligence-Machine Learning-Deep Learning Algorithms in Ophthalmology</title>
<url>http://dx.doi.org/10.22608/APO.2018479</url>
<snippet>The lifestyle of modern society has changed significantly with the emergence of artificial intelligence (AI), machine learning (ML), and deep learning (DL) technologies in recent years. Artificial intelligence is a multidimensional technology with various components such as advanced algorithms, ML and DL. Together, AI, ML, and DL are expected to provide automated devices to ophthalmologists for early diagnosis and timely treatment of ocular disorders in the near future. In fact, AI, ML, and DL have been used in ophthalmic setting to validate the diagnosis of diseases, read images, perform corneal topographic mapping and intraocular lens calculations. Diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are the 3 most common causes of irreversible blindness on a global scale. Ophthalmic imaging provides a way to diagnose and objectively detect the progression of a number of pathologies including DR, AMD, glaucoma, and other ophthalmic disorders. There are 2 methods of imaging used as diagnostic methods in ophthalmic practice: fundus digital photography and optical coherence tomography (OCT). Of note, OCT has become the most widely used imaging modality in ophthalmology settings in the developed world. Changes in population demographics and lifestyle, extension of average lifespan, and the changing pattern of chronic diseases such as obesity, diabetes, DR, AMD, and glaucoma create a rising demand for such images. Furthermore, the limitation of availability of retina specialists and trained human graders is a major problem in many countries. Consequently, given the current population growth trends, it is inevitable that analyzing such images is time-consuming, costly, and prone to human error. Therefore, the detection and treatment of DR, AMD, glaucoma, and other ophthalmic disorders through unmanned automated applications system in the near future will be inevitable. We provide an overview of the potential impact of the current AI, ML, and DL methods and their applications on the early detection and treatment of DR, AMD, glaucoma, and other ophthalmic diseases.
WOS:000500779400013
</snippet>
</document>

<document id="162">
<title>A Global Gravity Reconstruction Method for Mercury Employing Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/rs12142293</url>
<snippet>Mercury, the enigmatic innermost planet in the solar system, is one of the most important targets of space exploration. High-quality gravity field data are significant to refine the physical characterization of Mercury in planetary exploration missions. However, Mercurys gravity model is limited by relatively low spatial resolution and stripe noises, preventing fine-scale analysis and applications. By analyzing Mercurys gravity data and topography data in the 2D spatial field, we find they have fairly high spatial structure similarity. Based on this, in this paper, a novel convolution neural network (CNN) approach is proposed to improve the quality of Mercurys gravity field data. CNN can extract the spatial structure features of gravity data and construct a nonlinear mapping between low- and high-degree data directly. From a low-degree gravity input, the corresponding initial high-degree result can be obtained. Meanwhile, the structure characteristics of the high-resolution digital elevation model (DEM) are extracted and fused to the initial data, to get the final stripe-free result with improved resolution. Given the paucity of Mercurys data, high-quality lunar datasets are employed as pretraining data after verifying the spatial similarity between gravity and terrain data of the Moon. The HgM007 gravity field obtained by the MErcury Surface, Space ENvironment, GEochemistry and Ranging (MESSENGER) mission at Mercury is selected for experiments to test the ability of the proposed algorithm to remove the stripes caused by quality differences of the highly eccentric orbit data. Experimental results show that our network can directly obtain stripe-free and higher-degree data via inputting low-degree data and implicitly assuming a lunar-like relation between crustal density and porosity. Albeit the CNN-based method cannot be sensitive to subsurface features not present in the initial dataset, it still provides a new perspective for the gravity field refinement.
WOS:000554232100001
</snippet>
</document>

<document id="163">
<title>Detecting Buildings and Nonbuildings from Satellite Images Using U-Net</title>
<url>http://dx.doi.org/10.1155/2022/4831223</url>
<snippet>Automatic building detection from high-resolution satellite imaging images has many applications. Understanding socioeconomic development and keeping track of population migrations are essential for effective civic planning. These civil feature systems may also help update maps after natural disasters or in geographic regions undergoing dramatic population expansion. To accomplish the desired goal, a variety of image processing techniques were employed. They are often inaccurate or take a long time to process. Convolutional neural networks (CNNs) are being designed to extract buildings from satellite images, based on the U-Net, which was first developed to segment medical images. The minimal number of images from the open dataset, in RGB format with variable shapes, reveals one of the advantages of the U-Net; that is, it develops excellent accuracy from a limited amount of training material with minimal effort and training time. The encoder portion of U-Net was altered to test the feasibility of using a transfer learning facility. VGGNet and ResNet were both used for the same purpose. The findings of these models were also compared to our own bespoke U-Net, which was designed from the ground up. With an accuracy of 84.9&#37;, the VGGNet backbone was shown to be the best feature extractor. Compared to the current best models for tackling a similar problem with a larger dataset, the present results are considered superior.
WOS:000800221600017
</snippet>
</document>

<document id="164">
<title>Instance segmentation of fallen trees in aerial color infrared imagery using active multi-contour evolution with fully convolutional network-based intensity priors</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.06.016</url>
<snippet>Over the last several years, semantic image segmentation based on deep neural networks has been greatly advanced. On the other hand, single-instance segmentation still remains a challenging problem. In this paper, we introduce a framework for segmenting instances of a common object class by multiple active contour evolution over semantic segmentation maps of images obtained through fully convolutional networks. The contour evolution is cast as an energy minimization problem, where the aggregate energy functional incorporates a data fit term, an explicit shape model, and accounts for object overlap. Efficient solution neighborhood operators are proposed, enabling optimization through metaheuristics such as simulated annealing. We instantiate the proposed framework in the context of segmenting individual fallen stems from high-resolution aerial multispectral imagery, providing problem-specific energy potentials. We validated our approach on 3 real-world scenes of varying complexity, using 730 manually labeled polygon outlines as ground truth. The test plots were situated in regions of the Bavarian Forest National Park, Germany, which sustained a heavy bark beetle infestation. Evaluations were performed on both the polygon and line segment level, showing that the multi-contour segmentation can achieve up to 0.93 precision and 0.82 recall. An improvement of up to 7 percentage points (pp) in recall and 6 in precision compared to an iterative sample consensus line segment detection baseline was achieved. Despite the simplicity of the applied shape parametrization, an explicit shape model incorporated into the energy function improved the results by up to 4 pp of recall. Finally, we show the importance of using a high-quality semantic segmentation method (e.g. U-net) as the basis for individual stem detection, as the quality of the results degraded dramatically in our baseline experiment utilizing a simpler method. Our method is a step towards increased accessibility of automatic fallen tree mapping in forests, due to higher cost efficiency of aerial imagery acquisition compared to laser scanning. The precise fallen tree maps could be further used as a basis for plant and animal habitat modeling, studies on carbon sequestration as well as soil quality in forest ecosystems.
WOS:000669954900020
</snippet>
</document>

<document id="165">
<title>Automated Extraction of Human Settlement Patterns From Historical Topographic Map Series Using Weakly Supervised Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1109/ACCESS.2019.2963213</url>
<snippet>Information extraction from historical maps represents a persistent challenge due to inferior graphical quality and the large data volume of digital map archives, which can hold thousands of digitized map sheets. Traditional map processing techniques typically rely on manually collected templates of the symbol of interest, and thus are not suitable for large-scale information extraction. In order to digitally preserve such large amounts of valuable retrospective geographic information, high levels of automation are required. Herein, we propose an automated machine-learning based framework to extract human settlement symbols, such as buildings and urban areas from historical topographic maps in the absence of training data, employing contemporary geospatial data as ancillary data to guide the collection of training samples. These samples are then used to train a convolutional neural network for semantic image segmentation, allowing for the extraction of human settlement patterns in an analysis-ready geospatial vector data format. We test our method on United States Geological Survey historical topographic maps published between 1893 and 1954. The results are promising, indicating high degrees of completeness in the extracted settlement features (i.e., recall of up to 0.96, F-measure of up to 0.79) and will guide the next steps to provide a fully automated operational approach for large-scale geographic feature extraction from a variety of historical map series. Moreover, the proposed framework provides a robust approach for the recognition of objects which are small in size, generalizable to many kinds of visual documents.
WOS:000525422700019
</snippet>
</document>

<document id="166">
<title>Performance of deep learning in mapping water quality of Lake Simcoe with long-term Landsat archive</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.11.023</url>
<snippet>Remote sensing provides full-coverage and dynamic water quality monitoring with high efficiency and low consumption. Deep learning (DL) has been progressively used in water quality retrieval because it efficiently captures the potential relationship between target variables and imagery. In this study, the multimodal deep learning (MDL) models were developed and rigorously validated using atmospherically corrected Landsat remote sensing reflectance data and synchronous water quality measurements for estimating long-term Chlorophyll-a (Chl-a), total phosphorus (TP), and total nitrogen (TN) in Lake Simcoe, Canada. Since TP and TN are non optically active, their retrievals were based on the fact that they are closely related to the optically active constituents (OACs) such as Chl-a. We trained the MDL models with one in-situ measured data set (for Chl-a, N = 315, for TP and TN, N = 303), validated the models with two independent data sets (N = 147), and compared the model performances with several DL, machine learning, and empirical algorithms. The results indicated that the MDL models adequately estimated Chl-a (mean absolute error (MAE) = 32.57&#37;, Bias = 10.61&#37;), TP (MAE = 42.58&#37;, Bias =-2.82&#37;), and TN (MAE = 35.05&#37;, Bias = 13.66&#37;), and outperformed several other candidate algorithms, namely the progressively decreasing deep neural network (DNN), a DNN with trainable parameters similar to MDL but without splitting input features, the eXtreme Gradient Boosting, the support vector regression, the NASA Ocean Color two-band and three-band ratio algorithms, and another empirical algorithm of Landsat data in clear lakes. Using the MDL models, we reconstructed the historical spatiotemporal patterns of Chl-a, TP, and TN in Lake Simcoe since 1984, and investigated the effects of two water quality improvement programs. In addition, the physical mechanism and interpretability of the MDL models were explored by quantifying the contribution of each feature to the model outputs. The framework proposed in this study provides a practical method for long-term Chl-a, TP, and TN estimation at the regional scale.
WOS:000782441900001
</snippet>
</document>

<document id="167">
<title>Gabor Features Assist Semantic Feature Learning for Handwritten Formula Symbol Recognition</title>
<url>http://dx.doi.org/</url>
<snippet>At present, the recognition of formula symbols is very challenging. Since a large number of similarities and a variety of styles in the standard library, the overall recognition rate of formula symbols is difficult to further improve. In order to deal with these problems, this paper proposes an off-line multidirectional feature fusion decision discriminant algorithm, called MFFD. The novelty lies in the construction of the MMFD based on the convolutional neural network. In addition, we explore the directional gradient feature that facilitates the classification of handwritten formula symbols. The standard mathematical formula symbol library provided by the CROHME would verify the proposed algorithm. The error rates of CROHME2014 and CROHME2016 are 8.28&#37; and 6.88&#37;, respectively, which is higher than that of existing algorithms.
WOS:000501822200052
</snippet>
</document>

<document id="168">
<title>Deep learning of DEM image texture for landform classification in the Shandong area, China</title>
<url>http://dx.doi.org/10.1007/s11707-021-0884-y</url>
<snippet>Landforms are an important element of natural geographical environment, and textures are the research basis for the spatial differentiation, evolution features, and analysis rules of the landform. Using the regional difference of texture to describe the spatial distribution pattern of macro landform features is helpful to the landform classification and identification. Digital elevation model (DEM) image texture, which gives full expression to texture difference, is key data source to reflect the surface features and landform classification. Following the texture analysis, landform features analysis is assistant to different landforms classification, even in landform boundary. With the increasing accuracy requirement of landform information acquisition in geomorphic thematic mapping, hierarchical landform classification has become the focus and difficulty in research. Recently, the pattern recognition represented by Convolutional Neural Network has made great achievements in landform research, whose multichannel feature fusion structure satisfies the network structure of different landform classification. In this paper, DEM image texture was taken as the data source, and gray level co-occurrence matrix was applied to extract texture measures. Owing to the similarity of similar landform and the difference of different landform in a certain scale, a comprehensive texture factor reflecting landform features was proposed, and the spatial distribution pattern of landform features was systematically analyzed. On this basis, the coupling relationship between texture and landform type was explored. Thus, the deep learning method of Convolutional Neural Network is used to train the texture features, and the second-class landform classification is carried out through softmax. The classification results in small relief and mid-relief low mountains, overall accuracy are 84.35&#37; and 69.95&#37; respectively, while kappa coefficient are 0.72 and 0.40 respectively, were compared to that of traditional unsupervised landform classification results, and the superiority of Convolutional Neural Network classification was verified, it approximately improved 6&#37; in overall accuracy and 0.4 in kappa coefficient.
WOS:000672115500001
</snippet>
</document>

<document id="169">
<title>Fusion of images and point clouds for the semantic segmentation of large-scale 3D scenes based on deep learning</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.04.022</url>
<snippet>We address the issue of the semantic segmentation of large-scale 3D scenes by fusing 2D images and 3D point clouds. First, a Deeplab-Vgg16 based Large-Scale and High-Resolution model (DVLSHR) based on deep Visual Geometry Group (VGG16) is successfully created and fine-tuned by training seven deep convolutional neural networks with four benchmark datasets. On the val set in CityScapes, DVLSHR achieves a 74.98&#37; mean Pixel Accuracy (rnPA) and a 64.17&#37; mean Intersection over Union (mIoU), and can be adapted to segment the captured images (image resolution 2832 * 4256 pixels). Second, the preliminary segmentation results with 2D images are mapped to 3D point clouds according to the coordinate relationships between the images and the point clouds. Third, based on the mapping results, fine features of buildings are further extracted directly from the 3D point clouds. Our experiments show that the proposed fusion method can segment local and global features efficiently and effectively.
WOS:000442709900008
</snippet>
</document>

<document id="170">
<title>Object-based correction of LiDAR DEMs using RTK-GPS data and machine learning modeling in the coastal Everglades</title>
<url>http://dx.doi.org/10.1016/j.envsoft.2018.11.003</url>
<snippet>Light Detection and Ranging (LiDAR) Digital Elevation Models (DEMs) are frequently applied in modeling coastal environments. We present an object-based correction approach for accurate and precise DEMs by integrating LiDAR point data, aerial imagery, and Real Time Kinematic-Global Positioning Systems. Four machine learning techniques (Random Forest, Support Vector Machine, k-Nearest Neighbor, and Artificial Neural Network) were compared with the commonly used bias-correction method. The Random Forest object-based model produced best predictions for two study areas: Nine Mile (Mean Bias Error (MBE) reduced 0.18 to - 0.02 m, Root Mean Square Error (RMSE) reduced 0.22 to 0.08 m) and Flamingo (MBE reduced 0.17 to 0.02 m, RMSE reduced 0.24 to 0.10 m). A Monte Carlo model was developed to combine errors into the object-based machine learning corrected DEMs, and uncertainty maps spatially revealed the likelihood of error. The object-based correction approach provides an attractive alternative to the bias-correction method.
WOS:000453923900016
</snippet>
</document>

<document id="171">
<title>Accuracy Evaluation of Automated Object Recognition Using Multispectral Aerial Images and Neural Network</title>
<url>http://dx.doi.org/10.1117/12.2502905</url>
<snippet>A methodology of accuracy evaluation of automated object recognition using sub-meter spatial resolution multispectral aerial images and neural network is proposed. The methodology is applied to detection of 5 land cover classes from visible and infrared images using a multilevel convolutional neural network (CNN). In this work the well-known indicators of accuracy classification have been chosen: the confusion matrix and Kappa coefficient. Image processing results are analyzed. It is shown that the recognized object boundaries are delineated with sufficiently high accuracy and classes are well separated. The results of testing confirmed sufficiently high qualitative and quantitative indicators of the developed methodology (classification accuracy, sustainability, reproducibility).
WOS:000452819600016
</snippet>
</document>

<document id="172">
<title>Developing a Flood Risk Assessment Using Support Vector Machine and Convolutional Neural Network: A Conceptual Framework</title>
<url>http://dx.doi.org/</url>
<snippet>Flooding is one of the most devastating natural hazards that affect not only to infrastructures and agriculture but also to human lives. The prominent effect of global warming boasted its danger and impact in a wider range. In order to address and provide more effective measures to lessen the impact of flood hazards, it would be better to identify first the areas with such flood vulnerability. The proposed study aims to exploit the data available from the Geographical Information System (GIS) and the technology advancement in the modern world in producing a reliable flood susceptibility and probability map. Fusing ConvNet, a feedforward neural networks that specialize in image processing and prediction with SVM, a supervised machine learning for classification and regression analysis for a better image map results. Distinct image prediction output from dilated convolution and deconvolution network will be used as an input to SVM in producing its final output.
WOS:000494343900052
</snippet>
</document>

<document id="173">
<title>NASA NeMO-Net's Convolutional Neural Network: Mapping Marine Habitats with Spectrally Heterogeneous Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3018719</url>
<snippet>Recent advances in machine learning and computer vision have enabled increased automation in benthic habitat mapping through airborne and satellite remote sensing. Here, we applied deep learning and neural network architectures in NASA NeMO-Net, a novel neural multimodal observation and training network for global habitat mapping of shallow benthic tropical marine systems. These ecosystems, particularly coral reefs, are undergoing rapid changes as a result of increasing ocean temperatures, acidification, and pollution, among other stressors. Remote sensing from air and space has been the primary method in which changes are assessed within these important, often remote, ecosystems at a global scale. However, such global datasets often suffer from large spectral variances due to the time of observation, atmospheric effects, water column properties, and heterogeneous instruments and calibrations. To address these challenges, we developed an object-based fully convolutional network (FCN) to improve upon the spatial-spectral classification problem inherent in multimodal datasets. We showed that with training upon augmented data in conjunction with classical methods, such as K-nearest neighbors, we were able to achieve better overall classification and segmentation results. This suggests FCNs are able to effectively identify the relative applicable spectral and spatial spaces within an image, whereas pixel-based classical methods excel at classification within those identified spaces. Our spectrally invariant results, based on minimally preprocessed WorldView-2 and Planet satellite imagery, show a total accuracy of approximately 85&#37; and 80&#37;, respectively, over nine classes when trained and tested upon a chain of Fijian islands imaged under highly variable day-to-day spectral inputs.
WOS:000571765800006
</snippet>
</document>

<document id="174">
<title>Automatic threat recognition of prohibited items at aviation checkpoints with x-ray imaging: a deep learning approach</title>
<url>http://dx.doi.org/10.1117/12.2309484</url>
<snippet>The Transportation Security Administration safeguards all United States air travel. To do so, they employ human inspectors to screen x-ray images of carry-on baggage for threats and other prohibited items, which can be challenging. On the other hand, recent research applying deep learning techniques to computer-aided security screening to assist operators has yielded encouraging results. Deep learning is a subfield of machine learning based on learning abstractions from data, as opposed to engineering features by hand. These techniques have proven to be quite effective in many domains, including computer vision, natural language processing, speech recognition, self-driving cars, and geographical mapping technology. In this paper, we present initial results of a collaboration between Smiths Detection and Duke University funded by the Transportation Security Administration. Using convolutional object detection algorithms trained on annotated x-ray images, we show real-time detection of prohibited items in carry-on luggage. Results of the work so far indicate that this approach can detect selected prohibited items with high accuracy and minimal impact on operational false alarm rates.
WOS:000454439400001
</snippet>
</document>

<document id="175">
<title>Mapping land-use intensity of grasslands in Germany with machine learning and Sentinel-2 time series</title>
<url>http://dx.doi.org/10.1016/j.rse.2022.112888</url>
<snippet>Information on grassland land-use intensity (LUI) is crucial for understanding trends and dynamics in biodi-versity, ecosystem functioning, earth system science and environmental monitoring. LUI is a major driver for numerous environmental processes and indicators, such as primary production, nitrogen deposition and resil-ience to climate extremes. However, large extent, high resolution data on grassland LUI is rare. New satellite generations, such as Copernicus Sentinel-2, enable a spatially comprehensive detection of the mainly subtle changes induced by land-use intensification by their fine spatial and temporal resolution. We developed a methodology quantifying key parameters of grassland LUI such as grazing intensity, mowing frequency and fertiliser application across Germany using Convolutional Neural Networks (CNN) on Sentinel-2 satellite data with 20 m x 20 m spatial resolution. Subsequently, these land-use components were used to calculate a continuous LUI index. Predictions of LUI and its components were validated using comprehensive in situ grassland management data. A feature contribution analysis using Shapley values substantiates the applicability of the methodology by revealing a high relevance of springtime satellite observations and spectral bands related to vegetation health and structure. We achieved an overall classification accuracy of up to 66&#37; for grazing in-tensity, 68&#37; for mowing, 85&#37; for fertilisation and an r2 of 0.82 for subsequently depicting LUI. We evaluated the methodologys robustness with a spatial 3-fold cross-validation by training and predicting on geographically distinctly separated regions. Spatial transferability was assessed by delineating the models area of applicability. The presented methodology enables a high resolution, large extent mapping of land-use intensity of grasslands.
WOS:000804945700003
</snippet>
</document>

<document id="176">
<title>AUTOMATIC EXTRACTION OF WEAK LABELED SAMPLES FROM EXISTING THEMATIC PRODUCTS FOR TRAINING CONVOLUTIONAL NEURAL NETWORKS</title>
<url>http://dx.doi.org/</url>
<snippet>The accuracy in classification of remote sensing (RS) images using deep learning architectures is affected by the lack of large sets of training samples. Although a significant effort is currently devoted to generate databases of annotated satellite images, these datasets may not be large enough to accurately model at global level different types of land-cover surfaces. To solve such a problem, this paper presents an unsupervised approach which aims to exploit the RS image that has to be classified and publicly available thematic products to generate a training database of weak samples representative of the considered study area. First, we harmonize the thematic map and the RS image. Then, samples having the highest probability to be correctly associated to their labels are extracted from the map by exploiting the information provided by the RS image to be classified. Finally, the weak labeled samples are used to train a convolutional neural network (CNN). Experimental results obtained training a CNN on Sentinel 2 images with weak labels extracted from the 2018 corine land cover (CLC) map demonstrate the effectiveness of the proposed method.
WOS:000519270605126
</snippet>
</document>

<document id="177">
<title>An Efficient Residual Learning Neural Network for Hyperspectral Image Superresolution</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2901752</url>
<snippet>Deep learning, especially a discriminative model for image reconstruction, has shown great potential for single image superresolution (SR) of hyperspectral images (HSI). For HSI SR task, it is crucial to predicting each pixel according to the surrounding context, exploiting both spatial and spectral correlation information simultaneously. In this paper, an efficient three-dimensional (3-D) HSI SR convolution neural network (CNN) based on residual learning is proposed. The network builds convolutional layers in low-resolution (LR) space and extracts the features along both spatial and spectral dimensions using 3-D dilated kernel. Then, 3-D deconvolution is employed at the last layer, which enlarges the image to the desired size. By employing multibranch and multiscale fusion in the architecture, the network can learn a better and more complex LR to high-resolution mapping. The overall network combines the global with local residual learning to reduce training difficulty and improve the performance. The design philosophy of our model is to find the best tradeoff between performance and computational cost. We train the network in an end-to-end fashion, and the experimental results of the quantitative and qualitative evaluation show that our proposed method yields satisfactory SR performance.
WOS:000464756600016
</snippet>
</document>

<document id="178">
<title>A differential information residual convolutional neural network for pansharpening</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.03.006</url>
<snippet>Deep learning based methods are the state-of-the-art in panchromatic (PAN)/multispectral (MS) fusion (which is generally called "pansharpening"). In this paper, to solve the problem of the insufficient spatial enhancement in most of the existing deep learning based pansharpening methods, we propose a novel pansharpening method based on a residual convolutional neural network (RCNN). Differing from the existing deep learning based pansharpening methods that are mainly devoted to designing an effective network, we make novel changes to the input and the output of the network and propose a simple but effective mapping strategy. This strategy involves utilizing the network to map the differential information between the high spatial resolution panchromatic (HR-PAN) image and the low spatial resolution multispectral (LR-MS) image to the differential information between the HR-PAN image and the high spatial resolution multispectral (HR-MS) image, which is called the "differential information mapping strategy". Moreover, to further boost the spatial information in the fusion results, the proposed method makes full use of the LR-MS image and utilizes the gradient information of the up-sampled LR-MS image (Up-LR-MS) as auxiliary data to assist the network. Furthermore, an attention module and residual blocks are incorporated in the proposed network structure to maximize the ability of the network to extract features. Experiments on four data sets collected by different satellites confirm the superior performance of the proposed method compared to the state-of-the-art pansharpening methods.
WOS:000527712500016
</snippet>
</document>

<document id="179">
<title>Arbitrary-Shaped Building Boundary-Aware Detection With Pixel Aggregation Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3017934</url>
<snippet>Large-scale building extraction is an essential work in the field of a remote sensing image analysis. The high-resolution image extraction methods based on deep learning have achieved state-of-the-art performance. However, most of the previous work has focused on region accuracy rather than boundary quality. Aiming at the low-accuracy problems and incomplete boundary of the building extraction method, we propose a predictive optimization architecture, BAPANet. Notably, the architecture consists of an encoder-decoder network, and residual refinement modules responsible for prediction, and refinement. The objective function optimizes the network in the form of three levels (pixel, feature map, and patch) by fusing three loss functions: binary cross-entropy, intersection over-union, and structural similarity. The five public datasets experimental results show that the extraction method in this article has high region accuracy, and the boundary of buildings is clear and complete.
WOS:000735511500001
</snippet>
</document>

<document id="180">
<title>DCFF-Net: A Densely Connected Feature Fusion Network for Change Detection in High-Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3129318</url>
<snippet>Change detection is one of the main applications of remote sensing image analysis. Due to the strong capabilities of neural networks in other fields, a growing number of research works of automatic remote sensing change detection focus on deep learning algorithms. The network architectures of change detection are mostly based on the encoder-decoder architecture. Although the encoder-decoder architecture can acquire high-level semantic information for change detection, it still exists some problems in high-resolution remote sensing images, such as the loss of high-resolution location information during the down-sampling process, insufficient high-resolution information during the up-sampling reconstruction process, and small changes are challenging to detect. To address these issues, we propose a densely connected feature fusion network (DCFF-Net) for change detection. First, we extract the multiscale raw image features by two-stream network architecture with the same weights. At the same time, bitemporal images are concatenated as one input with six channels to generate the change map by a difference extraction network based on encoder-decoder architecture. In order to better reconstruct the edge details of the change map and the changes with the small region, an attention mechanism is employed in each up-sampling process to fuse the previously extracted raw image features with difference features. The deep supervision strategy is adopted to alleviate the problem of gradient vanishing. In addition, a novel weighted loss is proposed by combining self-adjusting dice loss and binary cross-entropy loss to alleviate the data imbalance issue. We perform extensive experiments on two public change detection datasets. The visual comparison and quantitative evaluation confirm that our proposed method outperformsother state-of-the-art methods.
WOS:000728159900002
</snippet>
</document>

<document id="181">
<title>Glacial Lakes Mapping Using Multi Satellite PlanetScope Imagery and Deep Learning</title>
<url>http://dx.doi.org/10.3390/ijgi9100560</url>
<snippet>Glacial lakes mapping using satellite remote sensing data are important for studying the effects of climate change as well as for the mitigation and risk assessment of a Glacial Lake Outburst Flood (GLOF). The 3U cubesat constellation of Planet Labs offers the capability of imaging the whole Earth landmass everyday at 3-4 m spatial resolution. The higher spatial, as well as temporal resolution of PlanetScope imagery in comparison with Landsat-8 and Sentinel-2, makes it a valuable data source for monitoring the glacial lakes. Therefore, this paper explores the potential of the PlanetScope imagery for glacial lakes mapping with a focus on the Hindu Kush, Karakoram and Himalaya (HKKH) region. Though the revisit time of the PlanetScope imagery is short, courtesy of 130+ small satellites, this imagery contains only four bands and the imaging sensors in these small satellites exhibit varying spectral responses as well as lower dynamic range. Furthermore, the presence of cast shadows in the mountainous regions and varying spectral signature of the water pixels due to differences in composition, turbidity and depth makes it challenging to automatically and reliably extract surface water in PlanetScope imagery. Keeping in view these challenges, this work uses state of the art deep learning models for pixel-wise classification of PlanetScope imagery into the water and background pixels and compares the results with Random Forest and Support Vector Machine classifiers. The deep learning model is based on the popular U-Net architecture. We evaluate U-Net architecture similar to the original U-Net as well as a U-Net with a pre-trained EfficientNet backbone. In order to train the deep neural network, ground truth data are generated by manual digitization of the surface water in PlanetScope imagery with the aid of Very High Resolution Satellite (VHRS) imagery. The created dataset consists of more than 5000 water bodies having an area of approx. 71km2 in eight different sites in the HKKH region. The evaluation of the test data show that the U-Net with EfficientNet backbone achieved the highest F1 Score of 0.936. A visual comparison with the existing glacial lake inventories is then performed over the Baltoro glacier in the Karakoram range. The results show that the deep learning model detected significantly more lakes than the existing inventories, which have been derived from Landsat OLI imagery. The trained model is further evaluated on the time series PlanetScope imagery of two glacial lakes, which have resulted in an outburst flood. The output of the U-Net is also compared with the GLakeMap data. The results show that the higher spatial and temporal resolution of PlanetScope imagery is a significant advantage in the context of glacial lakes mapping and monitoring.
WOS:000585322600001
</snippet>
</document>

<document id="182">
<title>Multisource Hyperspectral and LiDAR Data Fusion for Urban Land-Use Mapping based on a Modified Two-Branch Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/ijgi8010028</url>
<snippet>Accurate urban land-use mapping is a challenging task in the remote-sensing field. With the availability of diverse remote sensors, synthetic use and integration of multisource data provides an opportunity for improving urban land-use classification accuracy. Neural networks for Deep Learning have achieved very promising results in computer-vision tasks, such as image classification and object detection. However, the problem of designing an effective deep-learning model for the fusion of multisource remote-sensing data still remains. To tackle this issue, this paper proposes a modified two-branch convolutional neural network for the adaptive fusion of hyperspectral imagery (HSI) and Light Detection and Ranging (LiDAR) data. Specifically, the proposed model consists of a HSI branch and a LiDAR branch, sharing the same network structure to reduce the time cost of network design. A residual block is utilized in each branch to extract hierarchical, parallel, and multiscale features. An adaptive-feature fusion module is proposed to integrate HSI and LiDAR features in a more reasonable and natural way (based on "Squeeze-and-Excitation Networks"). Experiments indicate that the proposed two-branch network shows good performance, with an overall accuracy of almost 92&#37;. Compared with single-source data, the introduction of multisource data improves accuracy by at least 8&#37;. The adaptive fusion model can also increase classification accuracy by more than 3&#37; when compared with the feature-stacking method (simple concatenation). The results demonstrate that the proposed network can effectively extract and fuse features for a better urban land-use mapping accuracy.
WOS:000458582700027
</snippet>
</document>

<document id="183">
<title>A Framework for High-Resolution Mapping of Soil Organic Matter (SOM) by the Integration of Fourier Mid-Infrared Attenuation Total Reflectance Spectroscopy (FTIR-ATR), Sentinel-2 Images, and DEM Derivatives</title>
<url>http://dx.doi.org/10.3390/rs15041072</url>
<snippet>Soil organic matter (SOM), as the greatest carbon storage in the terrestrial environment, is inextricably related to the global carbon cycle and global climate change. Accurate estimation and mapping of SOM content are crucial for guiding agricultural output and management, as well as controlling the climate issue. Traditional chemical analysis is unable to satisfy the dynamic estimation of SOM due to its low timeliness. Remote and proximal sensing have significant advantages in terms of ease of use, estimation accuracy, and geographical resolution. In this study, we developed a framework based on machine learning to estimate SOM with high accuracy and resolution using Fourier mid-infrared attenuation total reflectance spectroscopy (FTIR-ATR), Sentinel-2 images, and DEM derivatives. This frameworks performance was evaluated on a regional scale using 245 soil samples from northeast China. Results indicated that the calibration size could be shrunk to 50&#37; while achieving a fair prediction performance for SOM content. The Lasso, partial least squares (PLS), support vector regression (SVR), and convolutional neural networks (CNN) performed well in predicting SOM from FTIR-ATR spectra, and the performance was enhanced further by using Sentinel-2 images and DEM derivates. The PLS, SVR, and CNN models created SOM maps with higher spatial resolution and variation than the Kriging approach. The PLS and SVR models provided enough variety and were more realistic in the local SOM map, making them usable at the field scale, and the suggested framework took a fresh look at high-resolution SOM mapping.
WOS:000940084500001
</snippet>
</document>

<document id="184">
<title>Wide-Area Land Cover Mapping With Sentinel-1 Imagery Using Deep Learning Semantic Segmentation Models</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3116094</url>
<snippet>Land cover (LC) mapping is essential for monitoring the environment and understanding the effects of human activities on it. Recent studies demonstrated successful applications of specific deep learning models to small-scale LC mapping tasks (e.g., wetland mapping). However, it is not readily clear which of the existing state-of-the-art models for natural images are the best candidates to be taken for the particular remote sensing task and data. In this article, we answer that question for mapping the fundamental LC classes using the satellite imaging radar data. We took ESA Sentinel-1 C-band SAR images acquired during the whole summer season of 2018 in Finland, which are representative of the land cover in the country. CORINE LC map was used as a reference, and the models were trained to distinguish between the five major CORINE-based classes. We selected seven among the state-of-the-art semantic segmentation models so that they cover a diverse set of approaches: U-Net, DeepLabV3+, PSPNet, BiSeNet, SegNet, FC-DenseNet, and FRRN-B, and further fine-tuned them. Upon evaluation and benchmarking, all the models demonstrated solid performance with overall accuracy between 87.9&#37; and 93.1&#37;, with good to a very good agreement (Kappa statistic between 0.75 and 0.86). The two best models were fully convolutional DenseNets (FC-DenseNet) and SegNet (encoder-decoder-skip), with the latter having a much shorter inference time. Overall, our results indicate that the semantic segmentation models are suitable for efficient wide-area mapping using satellite SAR imagery and provide baseline accuracy against which the newly proposed models should be evaluated.
WOS:000711641000010
</snippet>
</document>

<document id="185">
<title>Automatic Abstraction of Combinational Logic Circuit from Scanned Document Page Images</title>
<url>http://dx.doi.org/10.1134/S1054661819020068</url>
<snippet>Information extraction from scanned document page images is an important issue in image analysis. The main objectives of this work are: vectorization of image of the digital logic-gate circuits as graph, and automatic generation of Boolean expression. We have employed a novel method for circuit component separation using morphological operators. Connecting wires (in the form of poly lines in the image) lead to adjacency matrix describing directed interconnection between logic gates. Logic gate symbols are recognized by support vector machine (SVM) based on the features obtained by deep convolutional neural network (DCNN). Finally, we exploit this abstract representation of digital logic circuit as a graph to determine the Boolean expression. The approach is tested on a dataset developed by us and the results are encouraging.
WOS:000705649400002
</snippet>
</document>

<document id="186">
<title>Multitemporal Relearning With Convolutional LSTM Models for Land Use Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3055784</url>
<snippet>In this article, we present a novel hybrid framework, which integrates spatial-temporal semantic segmentation with postclassification relearning, for multitemporal land use and land cover (LULC) classification based on very high resolution (VHR) satellite imagery. To efficiently obtain optimal multitemporal LULC classification maps, the hybrid framework utilizes a spatial-temporal semantic segmentation model to harness temporal dependency for extracting high-level spatial-temporal features. In addition, the principle of postclassification relearning is adopted to efficiently optimize model output. Thereby, the initial outcome of a semantic segmentation model is provided to a subsequent model via an extended input space to guide the learning of discriminative feature representations in an end-to-end fashion. Last, object-based voting is coupled with postclassification relearning for coping with the high intraclass and low interclass variances. The framework was tested with two different postclassification relearning strategies (i.e., pixel-based relearning and object-based relearning) and three convolutional neural network models, i.e., UNet, a simple Convolutional LSTM, and a UNet Convolutional-LSTM. The experiments were conducted on two datasets with LULC labels that contain rich semantic information and variant building morphologic features (e.g., informal settlements). Each dataset contains four time steps from WorldView-2 and Quickbird imagery. The experimental results unambiguously underline that the proposed framework is efficient in terms of classifying complex LULC maps with multitemporal VHR images.
WOS:000637186500003
</snippet>
</document>

<document id="187">
<title>Synthetic Aperture Radar Image Change Detection via Siamese Adaptive Fusion Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3120381</url>
<snippet>Synthetic aperture radar (SAR) image change detection is a critical yet challenging task in the field of remote sensing image analysis. The task is nontrivial due to the following challenges: First, intrinsic speckle noise of SAR images inevitably degrades the neural network because of error gradient accumulation. Furthermore, the correlation among various levels or scales of feature maps is difficult to be achieved through summation or concatenation. Toward this end, we proposed a siamese adaptive fusion (AF) network for SAR image change detection. To be more specific, two-branch CNN is utilized to extract high-level semantic features of multitemporal SAR images. Besides, an AF module is designed to adaptively combine multiscale responses in convolutional layers. Therefore, the complementary information is exploited, and feature learning in change detection is further improved. Moreover, a correlation layer is designed to further explore the correlation between multitemporal images. Thereafter, robust feature representation is utilized for classification through a fully connected layer with softmax. Experimental results on four real SAR datasets demonstrate that the proposed method exhibits superior performance against several state-of-the-art methods. Our codes are available at https://github.com/summitgao/SAR_CD_SAFNet.
WOS:000714204000012
</snippet>
</document>

<document id="188">
<title>High-Resolution Remote Sensing Data Classification over Urban Areas Using Random Forest Ensemble and Fully Connected Conditional Random Field</title>
<url>http://dx.doi.org/10.3390/ijgi6080245</url>
<snippet>As an intermediate step between raw remote sensing data and digital maps, remote sensing data classification has been a challenging and long-standing problem in the remote sensing research community. In this work, an automated and effective supervised classification framework is presented for classifying high-resolution remote sensing data. Specifically, the presented method proceeds in three main stages: feature extraction, classification, and classified result refinement. In the feature extraction stage, both multispectral images and 3D geometry data are used, which utilizes the complementary information from multisource data. In the classification stage, to tackle the problems associated with too many training samples and take full advantage of the information in the large-scale dataset, a random forest (RF) ensemble learning strategy is proposed by combining several RF classifiers together. Finally, an improved fully connected conditional random field (FCCRF) graph model is employed to derive the contextual information to refine the classification results. Experiments on the ISPRS Semantic Labeling Contest dataset show that the presented 3-stage method achieves 86.9&#37; overall accuracy, which is a new state-of-the-art non-CNN (convolutional neural networks)-based classification method.
WOS:000408868400017
</snippet>
</document>

<document id="189">
<title>Exploring geo-tagged photos for land cover validation with deep learning</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.04.025</url>
<snippet>Land cover validation plays an important role in the process of generating and distributing land cover thematic maps, which is usually implemented by high cost of sample interpretation with remotely sensed images or field survey. With an increasing availability of geo-tagged landscape photos, the automatic photo recognition methodologies, e.g., deep learning, can be effectively utilised for land cover applications. However, they have hardly been utilised in validation processes, as challenges remain in sample selection and classification for highly heterogeneous photos. This study proposed an approach to employ geo-tagged photos for land cover validation by using the deep learning technology. The approach first identified photos automatically based on the VGG-16 network. Then, samples for validation were selected and further classified by considering photos distribution and classification probabilities. The implementations were conducted for the validation of the GlobeLand30 land cover product in a heterogeneous area, western California. Experimental results represented promises in land cover validation, given that GlobeLand30 showed an overall accuracy of 83.80&#37; with classified samples, which was close to the validation result of 80.45&#37; based on visual interpretation. Additionally, the performances of deep learning based on ResNet-50 and AlexNet were also quantified, revealing no substantial differences in final validation results. The proposed approach ensures geo-tagged photo quality, and supports the sample classification strategy by considering photo distribution, with accuracy improvement from 72.07&#37; to 79.33&#37; compared with solely considering the single nearest photo. Consequently, the presented approach proves the feasibility of deep learning technology on land cover information identification of geo-tagged photos, and has a great potential to support and improve the efficiency of land cover validation.
WOS:000436911700018
</snippet>
</document>

<document id="190">
<title>Cognitive map self-organization from subjective visuomotor experiences in a hierarchical recurrent neural network</title>
<url>http://dx.doi.org/10.1177/1059712317711487</url>
<snippet>Animals develop and use cognitive maps, which are internal models of the external environment, to understand the spatial characteristics of their natural environment. Previous studies have shown that a hierarchical structure of recurrent neural networks contributes to the extraction of high-level concepts in sequential sensorimotor experiences. However, the previous studies did not focus on the spatial aspects of these experiences and did not acquire cognitive maps. We modified previous models and trained the proposed model with the visuomotor experiences of an agent in a simulated two-dimensional environment. The proposed model was trained to predict future visual and motion inputs even when only one modality was provided (crossmodal prediction). The proposed model correctly predicted visual images, even when the agent experienced unknown paths. Comparisons of the crossmodal predictions of the models under different conditions revealed that the crossmodal predictions related to motion resulted in self-organization of the cognitive map. Further experiments of mental simulation abilities showed that two-way crossmodal predictions (from vision and motion only) were required for consistent generation of vision and motion. These results indicated that predictive learning involving integrated vision and motion was necessary for self-organization of spatial recognition with a cognitive map.
WOS:000405450600002
</snippet>
</document>

<document id="191">
<title>Enteromorpha Coverage Information Extraction by 1D-CNN and Bi-LSTM Networks Considering Sample Balance From GOCI Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3110854</url>
<snippet>Remote sensing technology is widely used for the dynamic monitoring of Enteromorpha prolifera (EP) blooms due to its high temporal resolution and large scale monitoring. Recently, deep learning(DL) methods have been applied to EP analysis due to their excellent feature representation. However, EP information extraction methods based on DL from low-spatial-resolution satellite images are still immature. The main problems with such methods include the insufficiency of spectral and spatial feature learning in low-resolution satellite images, as well as the sample imbalance that DL-based neural networks face in EP information extraction. To solve the above problems, a neural network-based EP extraction method considering sample balance is proposed in this article and named EP rough-then-accurate extraction network. The method consists of two components: EP rough extraction, a strategy that attends to sample balance, and EP accurate extraction, a deep neural network based on one-dimensional convolutional neural network and bidirectional long short-term memory (Bi-LSTM), which fully considers the learned spectral information of each pixel and interpixel contextual dependencies. Geostationary Ocean Color Imager images with 500-m resolution were applied as the LR images in the experiments. The experimental results show that the proposed method has the capability to enhance adaptability in areas with different EP densities (achieving stable and excellent performance) and exhibits at least a 10&#37; gain in F1-score and at least a 6&#37; gain in IoU in extracting EP coverage information over other representative and traditional EP extraction methods in the Yellow Sea region.
WOS:000698859700008
</snippet>
</document>

<document id="192">
<title>An Improved Segmentation Method for Automatic Mapping of Cone Karst from Remote Sensing Data Based on DeepLab V3+Model</title>
<url>http://dx.doi.org/10.3390/rs13030441</url>
<snippet>The South China Karst, a United Nations Educational, Scientific and Cultural Organization (UNESCO) natural heritage site, is one of the worlds most spectacular examples of humid tropical to subtropical karst landscapes. The Libo cone karst in the southern Guizhou Province is considered as the world reference site for these types of karst, forming a distinctive and beautiful landscape. Geomorphic information and spatial distribution of cone karst is essential for conservation and management for Libo heritage site. In this study, a deep learning (DL) method based on DeepLab V3+ network was proposed to document the cone karst landscape in Libo by multi-source data, including optical remote sensing images and digital elevation model (DEM) data. The training samples were generated by using Landsat remote sensing images and their combination with satellite derived DEM data. Each group of training dataset contains 898 samples. The input module of DeepLab V3+ network was improved to accept four-channel input data, i.e., combination of Landsat RGB images and DEM data. Our results suggest that the mean intersection over union (MIoU) using the four-channel data as training samples by a new DL-based pixel-level image segmentation approach is the highest, which can reach 95.5&#37;. The proposed method can accomplish automatic extraction of cone karst landscape by self-learning of deep neural network, and therefore it can also provide a powerful and automatic tool for documenting other type of geological landscapes worldwide.
WOS:000615468100001
</snippet>
</document>

<document id="193">
<title>Geo-Parcel-Based Change Detection Using Optical and SAR Images in Cloudy and Rainy Areas</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3038169</url>
<snippet>In this article, we deal with the problem of change detection in cloudy and rainy areas using multisource remote sensing images. While previous methods mostly focus on change detection on pixel or super-pixel levels, in this article, we introduce the concept of geo-parcel and use it as the basic processing unit for our change detection method. Concretely, we first extract geo-parcel from an optical high spatial resolution remote sensing image. Then, we divide each geo-parcel into fine-grained segments with refined boundaries using image segmentation methods. These fine-grained segments are used as the basic processing units for our change detection method. After that, an unsupervised learning-based method is adopted to obtain the difference map by comparing synthetic aperture radar images of two periods. Training samples with labels are automatically generated from the difference map. Finally, a deep neural network is trained using the generated samples and is further used to predict the refined change map. Experiments on the collected images from Guian, Guizhou Province, China demonstrate the effectiveness of the proposed method for change detection in a cloudy and rainy area with an overall accuracy surpasses 94&#37;.
WOS:000607413900043
</snippet>
</document>

<document id="194">
<title>Mapping the distribution of invasive tree species using deep one-class classification in the tropical montane landscape of Kenya</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.03.005</url>
<snippet>Some invasive tree species threaten biodiversity and cause irreversible damage to global ecosystems. The key to controlling and monitoring the propagation of invasive tree species is to detect their occurrence as early as possible. In this regard, one-class classification (OCC) shows potential in forest areas with abundant species richness since it only requires a few positive samples of the invasive tree species to be mapped, instead of all the species. However, the classical OCC method in remote sensing is heavily dependent on manually designed features, which have a limited ability in areas with complex species distributions. Deep learning based tree species classification methods mostly focus on multi-class classification, and there have been few studies of the deep OCC of tree species. In this paper, a deep positive and unlabeled learning based OCC framework-ITreeDet-is proposed for identifying the invasive tree species of Eucalyptus spp. (eucalyptus) and Acacia mearnsii (black wattle) in the Taita Hills of southern Kenya. In the ITreeDet framework, an absNegative risk estimator is designed to train a robust deep OCC model by fully using the massive unlabeled data. Compared with the state-of-the-art OCC methods, ITreeDet represents a great improvement in detection accuracy, and the F1 score was 0.86 and 0.70 for eucalyptus and black wattle, respectively. The study area covers 100 km(2) of the Taita Hills, where, according to our findings, the total area of eucalyptus and black wattle is 1.61 km(2) and 3.24 km(2), respectively, which represent 6.78&#37; and 13.65&#37; of the area covered by trees and forest. In addition, both invasive tree species are located in the higher elevations, and the extensive spread of black wattle around the study area confirms its invasive tendency. The maps generated by the use of the proposed algorithm will help local government to develop management strategies for these two invasive species.
WOS:000806368700001
</snippet>
</document>

<document id="195">
<title>Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</title>
<url>http://dx.doi.org/10.3390/rs12081233</url>
<snippet>One of the fundamental tasks in remote sensing is the semantic segmentation on the aerial and satellite images. It plays a vital role in applications, such as agriculture planning, map updates, route optimization, and navigation. The state-of-the-art model is the Enhanced Global Convolutional Network (GCN152-TL-A) from our previous work. It composes two main components: (i) the backbone network to extract features and (ii) the segmentation network to annotate labels. However, the accuracy can be further improved, since the deep learning network is not designed for recovering low-level features (e.g., river, low vegetation). In this paper, we aim to improve the semantic segmentation network in three aspects, designed explicitly for the remotely sensed domain. First, we propose to employ a modern backbone network called "High-Resolution Representation (HR)" to extract features with higher quality. It repeatedly fuses the representations generated by the high-to-low subnetworks with the restoration of the low-resolution representations to the same depth and level. Second, "Feature Fusion (FF)" is added to our network to capture low-level features (e.g., lines, dots, or gradient orientation). It fuses between the features from the backbone and the segmentation models, which helps to prevent the loss of these low-level features. Finally, "Depthwise Atrous Convolution (DA)" is introduced to refine the extracted features by using four multi-resolution layers in collaboration with a dilated convolution strategy. The experiment was conducted on three data sets: two private corpora from Landsat-8 satellite and one public benchmark from the "ISPRS Vaihingen" challenge. There are two baseline models: the Deep Encoder-Decoder Network (DCED) and our previous model. The results show that the proposed model significantly outperforms all baselines. It is the winner in all data sets and exceeds more than 90&#37; of F1: 0.9114, 0.9362, and 0.9111 in two Landsat-8 and ISPRS Vaihingen data sets, respectively. Furthermore, it achieves an accuracy beyond 90&#37; on almost all classes.
WOS:000534628800006
</snippet>
</document>

<document id="196">
<title>Fully Convolutional Networks and Geographic Object-Based Image Analysis for the Classification of VHR Imagery</title>
<url>http://dx.doi.org/10.3390/rs11050597</url>
<snippet>Land cover Classified maps obtained from deep learning methods such as Convolutional neural networks (CNNs) and fully convolutional networks (FCNs) usually have high classification accuracy but with the detailed structures of objects lost or smoothed. In this work, we develop a methodology based on fully convolutional networks (FCN) that is trained in an end-to-end fashion using aerial RGB images only as input. Skip connections are introduced into the FCN architecture to recover high spatial details from the lower convolutional layers. The experiments are conducted on the city of Goma in the Democratic Republic of Congo. We compare the results to a state-of-the art approach based on a semi-automatic Geographic object image-based analysis (GEOBIA) processing chain. State-of-the art classification accuracies are obtained by both methods whereby FCN and the best baseline method have an overall accuracy of 91.3&#37; and 89.5&#37; respectively. The maps have good visual quality and the use of an FCN skip architecture minimizes the rounded edges that is characteristic of FCN maps. Additional experiments are done to refine FCN classified maps using segments obtained from GEOBIA generated at different scale and minimum segment size. High OA of up to 91.5&#37; is achieved accompanied with an improved edge delineation in the FCN maps, and future work will involve explicitly incorporating boundary information from the GEOBIA segmentation into the FCN pipeline in an end-to-end fashion. Finally, we observe that FCN has a lower computational cost than the standard patch-based CNN approach especially at inference.
WOS:000462544500125
</snippet>
</document>

<document id="197">
<title>Evaluation of Goddard's LiDAR, hyperspectral, and thermal data products for mapping urban land-cover types</title>
<url>http://dx.doi.org/10.1080/15481603.2017.1364837</url>
<snippet>Goddards LiDAR (Light Detection And Ranging), hyperspectral and thermal (G-LiHT) airborne imager is a new system to advance concepts of data fusion for worldwide applications. A recent G-LiHT mission conducted in June 2016 over an urban area opens a new opportunity to assess the G-LiHT products for urban land-cover mapping. In this study, the G-LiHT hyperspectral and LiDAR-canopy height model (LiDAR-CHM) products were evaluated to map five broad land-cover types. A feature/decision-level fusion strategy was developed to integrate two products. Contemporary data processing techniques were applied, including object-based image analysis, machine-learning algorithms, and ensemble analysis. Evaluation focused on the capability of G-LiHT hyperspectral products compared with multispectral data with similar spatial resolution, the contribution of LiDAR-CHM, and the potential of ensemble analysis in land-cover mapping. The results showed that there was no significant difference between the application of the G-LiHT hyperspectral product and simulated Quickbird data in the classification. A synthesis of G-LiHT hyperspectral and LiDAR-CHM products achieved the best result with an overall accuracy of 96.3&#37; and a Kappa value of 0.95 when ensemble analysis was applied. Ensemble analysis of the three classifiers not only increased the classification accuracy but also generated an uncertainty map to show regions with a robust classification as well as areas where classification errors were most likely to occur. Ensemble analysis is a promising tool for land-cover classification.
WOS:000422682200005
</snippet>
</document>

<document id="198">
<title>Multi-scale object detection in remote sensing imagery with convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.04.003</url>
<snippet>Automatic detection of multi-class objects in remote sensing images is a fundamental but challenging problem faced for remote sensing image analysis. Traditional methods are based on hand-crafted or shallow-learning-based features with limited representation power. Recently, deep learning algorithms, especially Faster region based convolutional neural networks (FRCN), has shown their much stronger detection power in computer vision field. However, several challenges limit the applications of FRCN in multi-class objects detection from remote sensing images: (1) Objects often appear at very different scales in remote sensing images, and FRCN with a fixed receptive field cannot match the scale variability of different objects; (2) Objects in large-scale remote sensing images are relatively small in size and densely peaked, and FRCN has poor localization performance with small objects; (3) Manual annotation is generally expensive and the available manual annotation of objects for training FRCN are not sufficient in number. To address these problems, this paper proposes a unified and effective method for simultaneously detecting multi-class objects in remote sensing images with large scales variability. Firstly, we redesign the feature extractor by adopting Concatenated ReLU and Inception module, which can increases the variety of receptive field size. Then, the detection is preformed by two sub-networks: a multi-scale object proposal network (MS-OPN) for object-like region generation from several intermediate layers, whose receptive fields match different object scales, and an accurate object detection network (AODN) for object detection based on fused feature maps, which combines several feature maps that enables small and densely packed objects to produce stronger response. For large-scale remote sensing images with limited manual annotations, we use cropped image blocks for training and augment them with re-scalings and rotations. The quantitative comparison results on the challenging NWPU VHR-10 data set, aircraft data set, Aerial-Vehicle data set and SAR-Ship data set show that our method is more accurate than existing algorithms and is effective for multi-modal remote sensing images. (C) 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000449125400002
</snippet>
</document>

<document id="199">
<title>Deep Subpixel Mapping Based on Semantic Information Modulated Network for Urban Land Use Mapping</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3050824</url>
<snippet>Mixed pixel problem is omnipresent in remote sensing images for urban land use interpretation due to the hardware limitations. Subpixel mapping (SPM) is a usual way to solve this problem by improving the observation scale and realizing a finer spatial resolution land cover mapping. Recently, deep learning-based subpixel mapping network (DLSMNet) was proposed, benefited from its strong representation and learning ability, to restore a visually pleasing finer mapping. However, the spatial context features of artifacts are usually aggregated and progressively lost during the forward pass of the network without sufficient representation, which make it difficult to be learned and restored. In this article, a semantic information modulated (SIM) deep subpixel mapping network (SIMNet) is proposed, which uses low-resolution semantic images as prior, to reinforce the representation of spatial context features. In SIMNet, SIM module is proposed to parametrically incorporate the semantic prior into the state-of-the-art (SOTA) feed forward network architecture in an end-to-end training fashion. Furthermore, stacked SIM module with residual blocks (SIM_ResBlock) is adopted to pass the representation of spatial context feature to the deep layers, to get it fully learned during backpropagation. Experiments have been implemented on three public urban scenario data sets, and the SIMNet generates a clearer outline of artificial facilities with sufficient spatial context, and is distinctive for even individual building, which is challenging for other SOTA DLSMNet. The results demonstrate that the proposed SIMNet is a promising way for high-resolution urban land use mapping from easily available lower resolution remote sensing images.Mixed pixel problem is omnipresent in remote sensing images for urban land use interpretation due to the hardware limitations. Subpixel mapping (SPM) is a usual way to solve this problem by improving the observation scale and realizing a finer spatial resolution land cover mapping. Recently, deep learning-based subpixel mapping network (DLSMNet) was proposed, benefited from its strong representation and learning ability, to restore a visually pleasing finer mapping. However, the spatial context features of artifacts are usually aggregated and progressively lost during the forward pass of the network without sufficient representation, which make it difficult to be learned and restored. In this article, a semantic information modulated (SIM) deep subpixel mapping network (SIMNet) is proposed, which uses low-resolution semantic images as prior, to reinforce the representation of spatial context features. In SIMNet, SIM module is proposed to parametrically incorporate the semantic prior into the state-of-the-art (SOTA) feed forward network architecture in an end-to-end training fashion. Furthermore, stacked SIM module with residual blocks (SIM_ResBlock) is adopted to pass the representation of spatial context feature to the deep layers, to get it fully learned during backpropagation. Experiments have been implemented on three public urban scenario data sets, and the SIMNet generates a clearer outline of artificial facilities with sufficient spatial context, and is distinctive for even individual building, which is challenging for other SOTA DLSMNet. The results demonstrate that the proposed SIMNet is a promising way for high-resolution urban land use mapping from easily available lower resolution remote sensing images.
WOS:000722170500062
</snippet>
</document>

<document id="200">
<title>Artificial Intelligence in the Context of Psychological Security: Theoretical and Practical Implications</title>
<url>http://dx.doi.org/</url>
<snippet>The academic community cannot ignore the growing opportunities offered by artificial intelligence. Especially relevant are developments in this ares against the background of growing challenges and threats arising from the use of the digital environment by actors of psychological warfare - by strategists and tactics of "color revolutions as well as terrorist and criminal groups. The main objective of the paper is to develop efective instruments to counter the destructive psychological impact on the individual, society, and the state. As a tool in such psychological warfare, the authors see the use of hybrid intellectual systems for decision support bazed on fuzzy cognitive maps, the method of hierarchies, and artificial neural networks. The authors also state for the creation of the mathematical models of decision support in psychological warfare and discuss the need for training based on data mining, obtained from the Internet, using deep learning networks.
WOS:000558710000016
</snippet>
</document>

<document id="201">
<title>Urban Green Plastic Cover Mapping Based on VHR Remote Sensing Images and a Deep Semi-Supervised Learning Framework</title>
<url>http://dx.doi.org/10.3390/ijgi9090527</url>
<snippet>With the rapid process of both urban sprawl and urban renewal, large numbers of old buildings have been demolished in China, leading to wide spread construction sites, which could cause severe dust contamination. To alleviate the accompanied dust pollution, green plastic mulch has been widely used by local governments of China. Therefore, timely and accurate mapping of urban green plastic covered regions is of great significance to both urban environmental management and the understanding of urban growth status. However, the complex spatial patterns of the urban landscape make it challenging to accurately identify these areas of green plastic cover. To tackle this issue, we propose a deep semi-supervised learning framework for green plastic cover mapping using very high resolution (VHR) remote sensing imagery. Specifically, a multi-scale deformable convolution neural network (CNN) was exploited to learn representative and discriminative features under complex urban landscapes. Afterwards, a semi-supervised learning strategy was proposed to integrate the limited labeled data and massive unlabeled data for model co-training. Experimental results indicate that the proposed method could accurately identify green plastic-covered regions in Jinan with an overall accuracy (OA) of 91.63&#37;. An ablation study indicated that, compared with supervised learning, the semi-supervised learning strategy in this study could increase the OA by 6.38&#37;. Moreover, the multi-scale deformable CNN outperforms several classic CNN models in the computer vision field. The proposed method is the first attempt to map urban green plastic-covered regions based on deep learning, which could serve as a baseline and useful reference for future research.
WOS:000581995800001
</snippet>
</document>

<document id="202">
<title>Interpretation of Convolutional Neural Networks for Acid Sulfate Soil Classification</title>
<url>http://dx.doi.org/10.3389/fenvs.2021.809995</url>
<snippet>Convolutional neural networks (CNNs) have been originally used for computer vision tasks, such as image classification. While several digital soil mapping studies have been assessing these deep learning algorithms for the prediction of soil properties, their potential for soil classification has not been explored yet. Moreover, the use of deep learning and neural networks in general has often raised concerns because of their presumed low interpretability (i.e., the black box pitfall). However, a recent and fast-developing sub-field of Artificial Intelligence (AI) called explainable AI (XAI) aims to clarify complex models such as CNNs in a systematic and interpretable manner. For example, it is possible to apply model-agnostic interpretation methods to extract interpretations from any machine learning model. In particular, SHAP (SHapley Additive exPlanations) is a method to explain individual predictions: SHAP values represent the contribution of a covariate to the final model predictions. The present study aimed at, first, evaluating the use of CNNs for the classification of potential acid sulfate soils located in the wetland areas of Jutland, Denmark (c. 6,500 km(2)), and second and most importantly, applying a model-agnostic interpretation method on the resulting CNN model. About 5,900 soil observations and 14 environmental covariates, including a digital elevation model and derived terrain attributes, were utilized as input data. The selected CNN model yielded slightly higher prediction accuracy than the random forest models which were using original or scaled covariates. These results can be explained by the use of a common variable selection method, namely recursive feature elimination, which was based on random forest and thus optimized the selection for this method. Notably, the SHAP method results enabled to clarify the CNN model predictions, in particular through the spatial interpretation of the most important covariates, which constitutes a crucial development for digital soil mapping.
WOS:000749777000001
</snippet>
</document>

<document id="203">
<title>A shape-attention Pivot-Net for identifying central pivot irrigation systems from satellite images using a cloud computing platform: an application in the contiguous US</title>
<url>http://dx.doi.org/10.1080/15481603.2023.2165256</url>
<snippet>Forty percent of global food production relies upon irrigation, which accounts for 70&#37; of total global freshwater use. Thus, the mapping of cropland irrigation plays a significant role in agricultural water management and estimating food production. However, current spaceborne irrigated cropland mapping is highly reliant upon its spectral behavior, which often has high uncertainty and lacks information about the method of irrigation. Deep learning (DL) allows for the classification of irrigated cropland according to unique spatial patterns, such as the central pivot irrigation system (CPIS). But convolutional neural networks (CNNs) are usually biased toward color and texture features, a spatial transferable and accurate CPIS identification model is lacking owing to previous model seldom involves the round shapes of CPIS, which is usually key to distinguishing CPIS. To address this lack, we proposed a shape attention neural network by integrating spatial-attention gate, residual block, and multi-task learning, Pivot-Net, to incorporate shape information identify CPIS in satellite imagery. Specifically, we employed CPIS in Kansas to train our model using Sentinel-2 and Landsat-8 optical images. We found that Pivot-Net is superior to seven state-of-the-art semantic segmentation models on second-stage validation. We also evaluated the performance of Pivot-Net at three validation sites, which had an average F-1-score and mean IOU of 90.68&#37; and 90.45&#37;, respectively, which further demonstrated the high accuracy of the proposed model. Moreover, to show that the proposed Pivot-Net can map CPIS at the country scale, we generated the first CPIS map at 30 m for the contiguous US using a cloud computing platform and our Pivot-Net model. The total CPIS area for the contiguous US was 61,094 km(2) in 2018, which comprised 26.22&#37; of all irrigated lands. Our results can be accessed at . Therefore, the proposed shape-attention Pivot-Net demonstrates the ability to classify CPIS at large spatial scales and are feasible to map CPIS at national scales.
WOS:000915597900001
</snippet>
</document>

<document id="204">
<title>Learning Calibrated-Guidance for Object Detection in Aerial Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3158903</url>
<snippet>Object detection is one of the most fundamental yet challenging research topics in the domain of computer vision. Recently, the study on this topic in aerial images has made tremendous progress. However, complex background and worse imaging quality are obvious problems in aerial object detection. Most state-of-the-art approaches tend to develop elaborate attention mechanisms for the space-time feature calibrations with arduous computational complexity, while surprisingly ignoring the importance of feature calibrations in channel-wise. In this work, we propose a simple yet effective calibrated-guidance (CG) scheme to enhance channel communications in a feature transformer fashion, which can adaptively determine the calibration weights for each channel based on the global feature affinity correlations. Specifically, for a given set of feature maps, CG first computes the feature similarity between each channel and the remaining channels as the intermediary calibration guidance. Then, rerepresenting each channel by aggregating all the channels weighted together via the guidance operation. Our CG is a general module that can be plugged into any deep neural networks, which is named as CG-Net. To demonstrate its effectiveness and efficiency, extensive experiments are carried out on both oriented object detection task and horizontal object detection task in aerial images. Experimental results on two challenging benchmarks (i.e., DOTA and HRSC2016) demonstrate that our CG-Net can achieve the new state-of-the-art performance in accuracy with a fair computational overhead. The source code has been open sourced at https://github.com/WeiZongqi/CG-Net.
WOS:000782413900002
</snippet>
</document>

<document id="205">
<title>Automatic mapping of urban green spaces using a geospatial neural network</title>
<url>http://dx.doi.org/10.1080/15481603.2021.1933367</url>
<snippet>Detailed and precise urban green spaces (UGS) maps provide essential data for the sustainable urban development and related studies (e.g. heatwave events, heat related health risk, urban flooding, urban biodiversity and ecosystem services). However, remote sensing of mapping UGS is challenging due to the existence of mixed pixels and the cost and difficulty of collecting quality training data. This study presents a neural network-based automatic mapping method of UGS that integrates the use of Sentinel-2A satellite images and crowdsourced geospatial big data. The proposed neural network consists of three parts: (i) a multi-scale feature extraction module; (ii) a multi-modal information fuse module; and (iii) and a boundary enhancement module. The results showed that the proposed method achieved a high overall classification accuracy of 94.6&#37;, which presents a clear UGS structure of a large scale. This study provides a fresh insight into how remote-sensing and crowdsourced geospatial big data can be integrated to improve urban mapping of green spaces through neural network.
WOS:000657539000001
</snippet>
</document>

<document id="206">
<title>Automatic detection of potential mosquito breeding sites from aerial images acquired by unmanned aerial vehicles</title>
<url>http://dx.doi.org/10.1016/j.compenvurbsys.2021.101692</url>
<snippet>The World Health Organization (WHO) has stated that effective vector control measures are critical to achieving and sustaining reduction of vector-borne infectious disease incidence. Unmanned aerial vehicles (UAVs), popularly known as drones, can be an important technological tool for health surveillance teams to locate and eliminate mosquito breeding sites in areas where vector-borne diseases such as dengue, zika, chikungunya or malaria are endemic, since they allow the acquisition of aerial images with high spatial and temporal resolution. Currently, though, such images are often analyzed through manual processes that are excessively time-consuming when implementing vector control interventions. In this work we propose computational approaches for the automatic identification of objects and scenarios suspected of being potential mosquito breeding sites from aerial images acquired by drones. These approaches were developed using convolutional neural networks (CNN) and Bag of Visual Words combined with the Support Vector Machine classifier (BoVW + SVM), and their performances were evaluated in terms of mean Average Precision - mAP-50. In the detection of objects using a CNN YOLOv3 model the rate of 0.9651 was obtained for the mAP-50. In the detection of scenarios, in which the performances of BoVW+SVM and a CNN YOLOv3 were compared, the respective rates of 0.6453 and 0.9028 were obtained. These findings indicate that the proposed CNN-based approaches can be used to identify potential mosquito breeding sites from images acquired by UAVs, providing substantial improvements in vector control programs aiming the reduction of mosquito-breeding sources in the environment.
WOS:000702393800005
</snippet>
</document>

<document id="207">
<title>DEEP CONVOLUTION NEURAL NETWORKS WITH RESNET ARCHITECTURE FOR SPECTRAL-SPATIAL CLASSIFICATION OF DRONE BORNE AND GROUND BASED HIGH RESOLUTION HYPERSPECTRAL IMAGERY</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B2-2022-577-2022</url>
<snippet>Drones have been of vital importance in the fields of surveillance, mapping, and infrastructure inspection. Drones have played a vital role in acquiring high-resolution images and with the present need for precision farming, drones have helped in crop classification and monitoring various crop patterns. With the recent advancement in computational power and development of robust algorithms to carry out deep feature learning and neural network, based learning such techniques have regained prominence in contemporary research areas such as classification of common 2-D and 3-D images, object detection, etc. In our research, we propose a deep convolutional neural network architecture (CNN) for the classification of aerial images captured by drones and high-resolution Terrestrial Hyperspectral (THS or HSI) which includes 6-layers and with weights optimized along with the input layer, the convolutional layer, the max-pooling layer, the fully connected layer, softmax probability classifier, and the output layer. We have acquired THS (using Cubert-GmbH data) and drone agricultural data of seasonal crops sowed during the months of March-June for the year 2017. Crop patterns include Cabbage, Eggplant, and Tomato with varying nitrogen concentrations in the region of Bangalore, Southern India. To study the influence and impact of CNN, the ResNets model has been applied. ResNets model and architecture are combined with a deep learning network followed by a recurrent neural learning network model (RCNN). The HSI input layer with corresponding ground truth data for the region is fed into the ResNets model with a spectral and spatial residual network for the 7*7*139 input Hyperspectral Imagery (HSI) volume. The network includes two spectral and two spatial residual blocks. An average pooling layer and a fully connected layer transform into a 5*5*24 spectral-spatial feature volume further to a single output feature vector. At present we use an RMSProp optimizer for error loss minimization which when applied to the drone data was able to achieve an overall accuracy of 97.16&#37;. Similarly, for cabbage, eggplant and tomato acquired through the same method we achieved overall accuracy at 87.619&#37;, 89.25&#37;, and 80.566&#37; respectively in comparison to ground truth labels. Drones and ground-based datasets equipped with good computational techniques have become promising tools for improving the quality and efficiency of precision agriculture today.
WOS:000855635300080
</snippet>
</document>

<document id="208">
<title>Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics</title>
<url>http://dx.doi.org/10.1109/TVCG.2020.3030410</url>
<snippet>Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Morans I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.
WOS:000706330100069
</snippet>
</document>

<document id="209">
<title>Detecting and mapping traffic signs from Google Street View images using deep learning and GIS</title>
<url>http://dx.doi.org/10.1016/j.compenvurbsys.2019.101350</url>
<snippet>Street traffic sign infrastructure remains an extremely difficult asset for local government to manage due to its diverse physical structure and geographical distribution. A spatial registrar of traffic infrastructure is currently a required component of local government councils mandatory road management plans. Recent advancements of object detection technology in machine learning have presented an automated approach for the detection and classification of street signage captured by Googles Street View (GSV) imagery. This paper explores the possibility of using deep learning to produce an autonomous system for detecting traffic signs on GSV images to assist in traffic assets monitoring and maintenance. By leveraging Googles Street View API, this research offers an economic approach of building purposeful street sign computer vision datasets. A custom object detection model was trained to detect and classify Stop and Give Way signs from images captured at intersection approaches. Considering the output detected bounding box coordinates, photogrammetry approach was applied to calculate the approximate location of each detected sign in two-dimensional geographical space. The newly located and classified street signs can be combined with relevant spatial data for implementation into an asset management system. By combining GIS and the GSV API, the process is completely scalable to any level of street sign classification scope. The experiments conducted on the road network of study area recorded a detection accuracy of 95.63&#37; and classification accuracy of 97.82&#37;. Our proposed automated approach to the detection and localisation of street sign infrastructure has displayed a promising potential for its use by local government authorities. Our workflow can be used to detect other traffic signs and applied to other road sections and other cities. Of primary importance, this approach takes an entirely free and open-source approach throughout. The continuation of Googles Street View program will account for the spatiotemporal representation of street sign infrastructure for the ongoing maintenance and renewal programs of this valuable asset.
WOS:000488657500007
</snippet>
</document>

<document id="210">
<title>A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis</title>
<url>http://dx.doi.org/</url>
<snippet>Visual sentiment analysis aims to recognize emotions from visual contents. It is a very useful yet challenging task, especially when fine-grained emotions (such as love, joy, surprise, sadness, fear, anger, disgust, and anxiety) are analyzed. Existing methods based on convolutional neural networks learn sentiment representations based on global visual features, while ignoring the fact that both the local regions of the images and their relationships can have impact on sentiment representation learning. To address this limitation, in this paper, we propose a new Multi-Attentive Pyramidal model (MAP) for visual sentiment analysis. The model performs pyramidal segmentation and pooling upon the visual feature blocks obtained from a fully convolutional network, aiming to extract local visual features from multiple local regions at different scales of the global image. It then implants a self-attention mechanism to mine the associations between local visual features, and achieves the final sentiment representation. Extensive experiments on six benchmark datasets show the proposed MAP model outperforms the state-of-the-art methods in visual sentiment analysis.
WOS:000530893804121
</snippet>
</document>

<document id="211">
<title>Mapping salt marsh along coastal South Carolina using U-Net</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.07.011</url>
<snippet>Coastal wetland mapping is often difficult because of the heterogeneous vegetation compositions and associated tidal effects. In this study, we employed the U-Net and developed an adaptive deep learning approach to map statewide salt marshes in estuarine emergent wetlands of South Carolina (SC), USA, from 20 Sentinel-2A&amp;B images. Considering the spatial heterogeneity of the coastal environment, two NOAA National Estuarine Research Reserves (NERRs) in SC were examined, the North Inlet-Winyah Bay (NIWB) NERR for model training and the ACE Basin NERR for testing. A high-resolution land cover map in the NIWB was downloaded for the training process. Ground reference points recorded by the NERR, as well as Google Earth were utilized during the accuracy assessment. The highest overall accuracy (90&#37;) was achieved when all scenes with 10 Sentinel bands and the Normalized Difference Vegetation Index (NDVI) were included. The time used to train the model was 5 h, while then the statewide classification was performed in 20 min. Low marsh and high marsh distributions were successfully delineated. Compared to the national marsh maps from the NOAA Coastal Change Analysis Program (C-CAP), this study refined the land cover details concerning low marsh and high marsh distributions on the SC coast. Owing to the computational power of the U-Net, the seasonality and tide influence on marsh classification were mitigated by using multi-temporal images. With images available, the deep learning approach developed in this study could be easily adopted in other coastal areas.
WOS:000686386900009
</snippet>
</document>

<document id="212">
<title>Road Characteristics Detection Based on Joint Convolutional Neural Networks with Adaptive Squares</title>
<url>http://dx.doi.org/10.3390/ijgi10060377</url>
<snippet>The importance of road characteristics has been highlighted, as road characteristics are fundamental structures established to support many transportation-relevant services. However, there is still huge room for improvement in terms of types and performance of road characteristics detection. With the advantage of geographically tiled maps with high update rates, remarkable accessibility, and increasing availability, this paper proposes a novel simple deep-learning-based approach, namely joint convolutional neural networks (CNNs) adopting adaptive squares with combination rules to detect road characteristics from roadmap tiles. The proposed joint CNNs are responsible for the foreground and background image classification and various types of road characteristics classification from previous foreground images, raising detection accuracy. The adaptive squares with combination rules help efficiently focus road characteristics, augmenting the ability to detect them and provide optimal detection results. Five types of road characteristics-crossroads, T-junctions, Y-junctions, corners, and curves-are exploited, and experimental results demonstrate successful outcomes with outstanding performance in reality. The information of exploited road characteristics with location and type is, thus, converted from human-readable to machine-readable, the results will benefit many applications like feature point reminders, road condition reports, or alert detection for users, drivers, and even autonomous vehicles. We believe this approach will also enable a new path for object detection and geospatial information extraction from valuable map tiles.
WOS:000666537700001
</snippet>
</document>

<document id="213">
<title>Which CAM is Better for Extracting Geographic Objects? A Perspective From Principles and Experiments</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3188493</url>
<snippet>As a method of deep learning interpretability, class activation mapping (CAM) is efficient and convenient for extracting geographic objects supervised by image-level labels. However, in addition to the inherent problem of inaccuracy and incompleteness of CAM, we have to deal with the spectral and spatial variance of geographic objects when applying CAM methods to remote sensing images. To explore the capabilities of CAM methods on extracting various geographic objects, we make a comprehensive comparison of five commonly-used CAM methods, including original CAM, GradCAM, GradCAM++, SmoothGradCAM++, and ScoreCAM, in four aspects: efficiency; accuracy; effectiveness on dealing with the spectral and spatial variance; and performance of delineating different geographic object categories. The results demonstrate that the original CAM, GradCAM, and GradCAM++ achieves the highest efficiency, accuracy, and integrity for extracting geographic objects, respectively, which can help us choose the appropriate CAM methods according to the specific requirements of different extraction tasks. Benefiting from the capability in extracting various geographic objects and adaptability in complex scenes, GradCAM achieves the best performance in dealing with the spectral and spatial variance problem and shows the advantage of capturing object details and keeping object completeness at the same time. In addition to the comparison experiments and suggestions, we also provide the principle explanations of the performance differences. The findings of this article could contribute to a deep understanding of different CAM methods and benefit to selecting suitable CAM methods for extracting geographic objects from the perspectives of both principles and experiments.
WOS:000846865100010
</snippet>
</document>

<document id="214">
<title>Google Earth Engine for geo-big data applications: A meta-analysis and systematic review</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.04.001</url>
<snippet>Google Earth Engine (GEE) is a cloud-based geospatial processing platform for large-scale environmental monitoring and analysis. The free-to-use GEE platform provides access to (1) petabytes of publicly available remote sensing imagery and other ready-to-use products with an explorer web app; (2) high-speed parallel processing and machine learning algorithms using Googles computational infrastructure; and (3) a library of Application Programming Interfaces (APIs) with development environments that support popular coding languages, such as JavaScript and Python. Together these core features enable users to discover, analyze and visualize geospatial big data in powerful ways without needing access to supercomputers or specialized coding expertise. The development of GEE has created much enthusiasm and engagement in the remote sensing and geospatial data science fields. Yet after a decade since GEE was launched, its impact on remote sensing and geospatial science has not been carefully explored. Thus, a systematic review of GEE that can provide readers with the "big picture" of the current status and general trends in GEE is needed. To this end, the decision was taken to perform a meta-analysis investigation of recent peer-reviewed GEE articles focusing on several features, including data, sensor type, study area, spatial resolution, application, strategy, and analytical methods. A total of 349 peer-reviewed articles published in 146 different journals between 2010 and October 2019 were reviewed. Publications and geographical distribution trends showed a broad spectrum of applications in environmental analyses at both regional and global scales. Remote sensing datasets were used in 90&#37; of studies while 10&#37; of the articles utilized ready-to-use products for analyses. Optical satellite imagery with medium spatial resolution, particularly Landsat data with an archive exceeding 40 years, has been used extensively. Linear regression and random forest were the most frequently used algorithms for satellite imagery processing. Among ready-to-use products, the normalized difference vegetation index (NDVI) was used in 27&#37; of studies for vegetation, crop, land cover mapping and drought monitoring. The results of this study confirm that GEE has and continues to make substantive progress on global challenges involving process of geo-big data.
WOS:000535696600012
</snippet>
</document>

<document id="215">
<title>IDENTIFYING ROADSIDE OBJECTS IN MOBILE LASER SCANNING DATA USING IMAGE-BASED POINT CLOUD SEGMENTATION</title>
<url>http://dx.doi.org/10.36680/j.itcon.2020.031</url>
<snippet>Capturing geographic information from a mobile platform, a method known as mobile mapping, is today one of the best methods for rapid and safe data acquisition along roads and railroads. The digitalization of society and the use of information technology in the construction industry is increasing the need for structured geometric and semantic information about the built environment. This puts an emphasis on automatic object identification in data such as point clouds. Most point clouds are accompanied by RGB images, and a recent literature review showed that these are possibly underutilized for object identification. This article presents a method (image-based point cloud segmentations - IBPCS) where semantic segmentation of images is used to filter point clouds, which drastically reduces the number of points that have to be considered in object identification and allows simpler algorithms to be used. An example implementation where IBPCS is used to identify roadside game fences along a country road is provided, and the accuracy and efficiency of the method is compared to the performance of PointNet, which is a neural network designed for end-to-end point cloud classification and segmentation. The results show that our implementation of IBPCS outperforms PointNet for the given task. The strengths of IBPCS are the ability to filter point clouds based on visual appearance and that it efficiently can process large data sets. This makes the method a suitable candidate for object identification along rural roads and railroads, where the objects of interest are scattered over long distances.
WOS:000605615000001
</snippet>
</document>

<document id="216">
<title>Automated mapping of glacial lakes using multisource remote sensing data and deep convolutional neural network</title>
<url>http://dx.doi.org/10.1016/j.jag.2022.103085</url>
<snippet>The characteristics of glacial lakes are a precursor to glacier retreat, ice mass loss, velocity, and potential risk of Glacial Lake Outburst Floods (GLOF). The current state of the art for glacial lake mapping, especially in a high mountainous region, is limited to manual or semi-automated threshold-based methods. Here, we propose a fully automated novel approach for glacial lake mapping using a Deep Convolutional Neural Network (DCNN) and remote sensing data originating from various sources. A combination of these multisource remote sensing data (i. e., multispectral, thermal, microwave, and a Digital Elevation Model) is fed to the fully connected DCNN. The DCNN architecture, namely GLNet, is designed by choosing an optimum number and size of convolutional layers, filters, and other hyperparameters. Our proposed GLNet is trained on 660 images covering twelve sites spread across diverse climatic and topographic regions of the Himalaya. The robustness of the model is tested over three sites in the Eastern Himalaya and one site in the Western Himalaya. The classification results outperform the existing state-of-the-art datasets by achieving 0.98 accuracy, 0.95 precision, 0.95 recall, and 0.95 F- score over the test data. The results over test sites (F-score test site1: 0.91, test site 2: 0.80, test site3: 0.97, and test site4: 0.70) showed promising results and spatiotemporal transferability of the proposed method. The coefficient of determination (R-2) between GLNet predicted lake boundaries and reference lake boundaries exhibits excellent results (0.90). The study provides proof of concept for automated glacial mapping for large geographical regions via integrated capabilities of deep convolutional neural networks and multisource remote sensing data.
WOS:000891670800001
</snippet>
</document>

<document id="217">
<title>Self-supervised monocular depth estimation from oblique UAV videos</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.03.024</url>
<snippet>Unmanned Aerial Vehicles (UAVs) have become an essential photogrammetric measurement as they are affordable, easily accessible and versatile. Aerial images captured from UAVs have applications in small and large scale texture mapping, 3D modelling, object detection tasks, Digital Terrain Model (DTM) and Digital Surface Model (DSM) generation etc. Photogrammetric techniques are routinely used for 3D reconstruction from UAV images where multiple images of the same scene are acquired. Developments in computer vision and deep learning techniques have made Single Image Depth Estimation (SIDE) a field of intense research. Using SIDE techniques on UAV images can overcome the need for multiple images for 3D reconstruction. This paper aims to estimate depth from a single UAV aerial image using deep learning. We follow a self-supervised learning approach, Self-Supervised Monocular Depth Estimation (SMDE), which does not need ground truth depth or any extra information other than images for learning to estimate depth. Monocular video frames are used for training the deep learning model which learns depth and pose information jointly through two different networks, one each for depth and pose. The predicted depth and pose are used to reconstruct one image from the viewpoint of another image utilising the temporal information from videos. We propose a novel architecture with two 2D Convolutional Neural Network (CNN) encoders and a 3D CNN decoder for extracting information from consecutive temporal frames. A contrastive loss term is introduced for improving the quality of image generation. Our experiments are carried out on the public UAVid video dataset. The experimental results demonstrate that our model outperforms the state-of-the-art methods in estimating the depths.
WOS:000655474600001
</snippet>
</document>

<document id="218">
<title>Evaluation of Different Machine Learning Methods and Deep-Learning Convolutional Neural Networks for Landslide Detection</title>
<url>http://dx.doi.org/10.3390/rs11020196</url>
<snippet>There is a growing demand for detailed and accurate landslide maps and inventories around the globe, but particularly in hazard-prone regions such as the Himalayas. Most standard mapping methods require expert knowledge, supervision and fieldwork. In this study, we use optical data from the Rapid Eye satellite and topographic factors to analyze the potential of machine learning methods, i.e., artificial neural network (ANN), support vector machines (SVM) and random forest (RF), and different deep-learning convolution neural networks (CNNs) for landslide detection. We use two training zones and one test zone to independently evaluate the performance of different methods in the highly landslide-prone Rasuwa district in Nepal. Twenty different maps are created using ANN, SVM and RF and different CNN instantiations and are compared against the results of extensive fieldwork through a mean intersection-over-union (mIOU) and other common metrics. This accuracy assessment yields the best result of 78.26&#37; mIOU for a small window size CNN, which uses spectral information only. The additional information from a 5 m digital elevation model helps to discriminate between human settlements and landslides but does not improve the overall classification accuracy. CNNs do not automatically outperform ANN, SVM and RF, although this is sometimes claimed. Rather, the performance of CNNs strongly depends on their design, i.e., layer depth, input window sizes and training strategies. Here, we conclude that the CNN method is still in its infancy as most researchers will either use predefined parameters in solutions like Google TensorFlow or will apply different settings in a trial-and-error manner. Nevertheless, deep-learning can improve landslide mapping in the future if the effects of the different designs are better understood, enough training samples exist, and the effects of augmentation strategies to artificially increase the number of existing samples are better understood.
WOS:000457939400091
</snippet>
</document>

<document id="219">
<title>An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval</title>
<url>http://dx.doi.org/10.1109/ACCESS.2019.2940055</url>
<snippet>Due to the rapid development of mobile Internet techniques, such as online social networking and location-based services, massive amount of multimedia data with geographical information is generated and uploaded to the Internet. In this paper, we propose a novel type of cross-modal multimedia retrieval, called geo-multimedia cross-modal retrieval, which aims to find a set of geo-multimedia objects according to geographical distance proximity and semantic concept similarity. Previous studies for cross-modal retrieval and spatial keyword search cannot address this problem effectively because they do not consider multimedia data with geo-tags (geo-multimedia). Firstly, we present the definition of kNN geo-multimedia cross-modal query and introduce relevant concepts such as spatial distance and semantic similarity measurement. As the key notion of this work, cross-modal semantic representation space is formulated at the first time. A novel framework for geo-multimedia cross-modal retrieval is proposed, which includes multi-modal feature extraction, cross-modal semantic space mapping, geo-multimedia spatial index and cross-modal semantic similarity measurement. To bridge the semantic gap between different modalities, we also propose a method named cross-modal semantic matching (CoSMat for shot) which contains two important components, i.e., CorrProj and LogsTran, which aims to build a common semantic representation space for cross-modal semantic similarity measurement. In addition, to implement semantic similarity measurement, we employ deep learning based method to learn multi-modal features that contains more high level semantic information. Moreover, a novel hybrid index, GMR-Tree is carefully designed, which combines signatures of semantic representations and R-Tree. An efficient GMR-Tree based kNN search algorithm called kGMCMS is developed. Comprehensive experimental evaluations on real and synthetic datasets clearly demonstrate that our approach outperforms the-state-of-the-art methods.
WOS:000509483800259
</snippet>
</document>

<document id="220">
<title>Attention Receptive Pyramid Network for Ship Detection in SAR Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2997081</url>
<snippet>With the development of deep learning (DL) and synthetic aperture radar (SAR) imaging techniques, SAR automatic target recognition has come to a breakthrough. Numerous algorithms have been proposed and competitive results have been achieved in detecting different targets. However, due to the influence of various sizes and complex background of ships, detecting multiscale ships in SAR images is still challenging. To solve the problems, a novel network, called attention receptive pyramid network (ARPN), is proposed in this article. ARPN is a two-stage detector and designed to improve the performance of detecting multiscale ships in SAR images by enhancing the relationships among nonlocal features and refining information at different feature maps. Specifically, receptive fields block (RFB) and convolutional block attention module (CBAM) are employed and combined reasonably in attention receptive block to build a top-down fine-grained feature pyramid. RFB, composed of several branches of convolutional layers with specifically asymmetric kernel sizes and various dilation rates, is used for grabbing features of ships with large aspect ratios and enhancing local features with their global dependences. CBAM, which consists of channel and spatial attention mechanisms, is utilized to boost significant information and suppress interference caused by surroundings. To evaluate the effectiveness of ARPN, experiments are conducted on SAR Ship Detection Dataset and two large-scene SAR images. The detection results illustrate that competitive performance has been achieved by our method in comparison with several CNN-based algorithms, e.g., Faster-RCNN, RetinaNet, feature pyramid network, YOLOv3, Dense Attention Pyramid Network, Depth-wise Separable Convolutional Neural Network, High-Resolution Ship Detection Network, and Squeeze and Excitation Rank Faster-RCNN.
WOS:000544047400015
</snippet>
</document>

<document id="221">
<title>Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land cover mapping via a multi-source deep learning architecture</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.09.016</url>
<snippet>The huge amount of data currently produced by modern Earth Observation (EO) missions has allowed for the design of advanced machine learning techniques able to support complex Land Use/Land Cover (LULC) mapping tasks. The Copernicus programme developed by the European Space Agency provides, with missions such as Sentinel-1 (S1) and Sentinel-2 (S2), radar and optical (multi-spectral) imagery, respectively, at 10 m spatial resolution with revisit time around 5 days. Such high temporal resolution allows to collect Satellite Image Time Series (SITS) that support a plethora of Earth surface monitoring tasks. How to effectively combine the complementary information provided by such sensors remains an open problem in the remote sensing field. In this work, we propose a deep learning architecture to combine information coming from S1 and S2 time series, namely TWINNS (TWIn Neural Networks for Sentinel data), able to discover spatial and temporal dependencies in both types of SITS. The proposed architecture is devised to boost the land cover classification task by leveraging two levels of complementarity, i.e., the interplay between radar and optical SITS as well as the synergy between spatial and temporal dependencies. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the Koumbia site in Burkina Faso and Reunion Island, a overseas department of France in the Indian Ocean), demonstrate the significance of our proposal.
WOS:000501404100002
</snippet>
</document>

<document id="222">
<title>A Satellite Data Mining Approach Based on Self-Organized Maps for the Early Warning of Ground Settlements in Urban Areas</title>
<url>http://dx.doi.org/10.3390/app12052679</url>
<snippet>Featured Application The proposed method is a neural-network-based tool for the early warning of ground settlement hazard in urban areas. On the basis of the analysis of MT-InSAR data through an unsupervised learning, the method can find precursors of similar time-evolving phenomena. The method can be applied under different warning criteria and for different hyper-parameters of the monitoring system. Structural failure prevention is a crucial issue in civil engineering. The causes of structure or infrastructure collapse include phenomena that slowly deform the ground and could affect the stability of foundations such as differential settlements, subsidence, groundwater changes, slope failure, or landslides. When large urban areas need to be monitored, such phenomena are hard to be mapped by means of classical structural health monitoring methods due to the unaffordable quantity of in situ measurements these methods would entail. A very effective alternative is exploiting multitemporal interferometric synthetic aperture radar (MT-InSAR) displacement timeseries which would enable the monitoring of wide geographical areas over a weekly basis and extended spatial coverage. Analyzing the enormous amount of data produced by MT-InSAR may help to assess the time evolution of phenomena but can barely highlight "anomalous" ground deformations in time, to prevent likely structural failure. This paper proposes a method which analyzes the InSAR data through an unsupervised learning paradigm with the purpose of detecting critical events at their early stage. On the basis of similarities among time sequences, this method allows the finding of precursors of anomalous ground settlement behaviors, the correct framing of which should be directed to specialist evaluation and in situ inspections.
WOS:000771674000001
</snippet>
</document>

<document id="223">
<title>Aerial image semantic segmentation using DCNN predicted distance maps</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.01.023</url>
<snippet>This paper addresses the challenge of learning spatial context for the semantic segmentation of high-resolution aerial images using Deep Convolutional Neural Networks (DCNNs). The proposed solution involves deriving a signed distance map for each semantic class from a ground truth label map and training a DCNN to predict this distance map instead of a score map for each class. Since the distance between a target pixel and its nearest object boundary measures how far the pixel penetrates an object, the distance maps encode spatial context, particularly spatial smoothness. Positive pixel values in the distance maps correspond to the correct class and negative values correspond to the incorrect class. A final label map is derived from the predicted distance maps by selecting the class with the maximum distance. Since neighboring pixels in the distance maps have similar values, the segmentation results are smoother than current approaches. The results are shown to be even better than performing post-processing using fully connected Conditional Random Fields (CRFs), a common approach to smoothing the segmentations produced DCNNs. Experimental results on the semantic labeling challenge dataset show the proposed approach outperforms most state-of-the-art methods. Our main contribution, though, is the novel idea of replacing the pixel-wise class score maps of DCNNs with distance maps. This is therefore orthogonal and complementary to other techniques employed by the state-of-the-art methods and could therefore be used to improve upon them.
WOS:000517849600024
</snippet>
</document>

<document id="224">
<title>Height Estimation From Single Aerial Images Using a Deep Ordinal Regression Network</title>
<url>http://dx.doi.org/10.1109/LGRS.2020.3019252</url>
<snippet>Understanding the 3-D geometric structure of the Earths surface has been an active research topic in photogrammetry and remote sensing community for decades, serving as an essential building block for various applications such as 3-D digital city modeling, change detection, and city management. Previous research studies have extensively studied the problem of height estimation from aerial images based on stereo or multiview image matching. These methods require two or more images from different perspectives to reconstruct 3-D coordinates with camera information provided. In this letter, we deal with the ambiguous and unsolved problem of height estimation from a single aerial image. Driven by the great success of deep learning, especially deep convolutional neural networks (CNNs), some research studies have proposed to estimate height information from a single aerial image by training a deep CNN model with large-scale annotated data sets. These methods treat height estimation as a regression problem and directly use an encoder-decoder network to regress the height values. In this letter, we propose to divide height values into spacing-increasing intervals and transform the regression problem into an ordinal regression problem, using an ordinal loss for network training. To enable multiscale feature extraction, we further incorporate an Atrous Spatial Pyramid Pooling (ASPP) module to extract features from multiple dilated convolution layers. After that, a postprocessing technique is designed to transform the predicted height map of each patch into a seamless height map. Finally, we conduct extensive experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets. Experimental results demonstrate significantly better performance of our method compared to state-of-the-art methods.
WOS:000733952000049
</snippet>
</document>

<document id="225">
<title>FuelNet: An Artificial Neural Network for Learning and Updating Fuel Types for Fire Research</title>
<url>http://dx.doi.org/10.1109/TGRS.2020.3037160</url>
<snippet>Wildfire is a significant driver of forest and land cover change in the central interior of British Columbia, Canada. Fuel type maps are a primary input to fire behavior calculations and simulation studies that assess wildfire threat at the landscape level. However, these thematic maps are not easily produced at the scale and speed needed to assess and mitigate wildfire threat on an annual basis. The objective of this research was to explore how an artificial neural network could be used with remotely sensed satellite imagery to map and update fuel types on an annual basis. We applied the artificial neural network over a 40 000-km(2) landscape in central interior British Columbia that burned from a megafire in 2017. Fuel maps were generated for the years 2014-2018, assessed through an independent validation, and evaluated against an existing fuel type map. The highest cross-validation overall accuracy during training was 66.5&#37; and overall accuracy from the independent validation was 63.1&#37;. Generally, the maps had fair agreement with the existing fuel type map (circa 2016), with Cohens Kappa ranging from 0.28 in 2018 to 0.35 in 2015. Several recommendations are provided for future research using artificial neural networks for fuel typing such as assuring quality of training samples through rigorous standards, designing the network architecture, choosing appropriate cost functions and regularization, incorporating learning of temporal features, and identifying novel fuel types from the output activations.
WOS:000690968800020
</snippet>
</document>

<document id="226">
<title>Towards Automatic Extraction and Updating of VGI-Based Road Networks Using Deep Learning</title>
<url>http://dx.doi.org/10.3390/rs11091012</url>
<snippet>This work presents an approach to road network extraction in remote sensing images. In our earlier work, we worked on the extraction of the road network using a multi-agent approach guided by Volunteered Geographic Information (VGI). The limitation of this VGI-only approach is its inability to update the new road developments as it only follows the VGI. In this work, we employ a deep learning approach to update the road network to include new road developments not captured by the existing VGI. The output of the first stage is used to train a Convolutional Neural Network (CNN) in the second stage to generate a general model to classify road pixels. Post-processing is used to correct the undesired artifacts such as buildings, vegetation, occlusions, etc. to generate a final road map. Our proposed method is tested on the satellite images acquired over Abu Dhabi, United Arab Emirates and the aerial images acquired over Massachusetts, United States of America, and is observed to produce accurate results.
WOS:000469763600020
</snippet>
</document>

<document id="227">
<title>Land-Use Mapping for High-Spatial Resolution Remote Sensing Image Via Deep Learning: A Review</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3078631</url>
<snippet>Land-use mapping (LUM) using high-spatial resolution remote sensing images (HSR-RSIs) is a challenging and crucial technology. However, due to the characteristics of HSR-RSIs, such as different image acquisition conditions and massive, detailed information, and performing LUM faces unique scientific challenges. With the emergence of new deep learning (DL) algorithms in recent years, methods to LUM with DL have achieved huge breakthroughs, which offer novel opportunities for the development of LUM for HSR-RSIs. This article aims to provide a thorough review of recent achievements in this field. Existing high spatial resolution datasets in the research of semantic segmentation and single-object segmentation are presented first. Next, we introduce several basic DL approaches that are frequently adopted for LUM. After reviewing DL-based LUM methods comprehensively, which highlights the contributions of researchers in the field of LUM for HSR-RSIs, we summarize these DL-based approaches based on two LUM criteria. Individually, the first one has supervised learning, semisupervised learning, or unsupervised learning, while another one is pixel-based or object-based. We then briefly review the fundamentals and the developments of the development of semantic segmentation and single-object segmentation. At last, quantitative results that experiment on the dataset of ISPRS Vaihingen and ISPRS Potsdam are given for several representative models such as fully convolutional network (FCN) and U-Net, following up with a comparison and discussion of the results.
WOS:000660636600008
</snippet>
</document>

<document id="228">
<title>Automatic Detection and Segmentation of Barchan Dunes on Mars and Earth Using a Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3109900</url>
<snippet>The morphology of isolated barchan dunes on Mars and Earth may shed light on the dynamic conditions that form them, their migration direction and the physical properties of the sediments composing them. Prior to this study, dune fields have been largely analyzed manually from aerial and satellite imagery, as automatic detection techniques are often not sufficiently accurate in outlining dunes. Here, we employ an instance segmentation neural network to detect and outline isolated barchan dunes on Mars and Earth. We train and test the model on martian targets using Mars reconnaissance orbiter (MRO) context camera (CTX) images, and find it sufficiently accurate (mAP=77&#37; on the test dataset) to characterize dune field dynamics. Using our trained model, we detect and map the global distribution of barchan dunes relative to previously mapped dune fields, and find that barchan dunes are more abundant in the northern hemisphere than in the southern hemisphere. These contrasting abundances of barchans may reflect latitudinally dependent wind regimes, sediment supply, or sediment availability.
WOS:000701240700003
</snippet>
</document>

<document id="229">
<title>Vector data partition correction method supported by deep learning</title>
<url>http://dx.doi.org/10.1080/01431161.2022.2107411</url>
<snippet>Road vector data are an important part of geographic information databases and play a leading role in social and economic development. Among them, the accuracy and current situation of vector data are the key values of their application. In recent years, with the development of remote sensing technology, remote sensing images contain increasingly more road information, which provides a large number of available features for the establishment of vector correction models. Therefore, the vector-to-image vector correction method based on remote sensing images has become an important means to ensure the accuracy and current situation of vector data. However, in actual scenes, there are great differences in the design structure and design index between urban roads and rural roads, which is reflected in the serious spatial heterogeneity between them in the remote sensing images. Therefore, when the existing methods use remote sensing images for vector correction, the universality is limited by the change of regional scenes, so it is difficult to realize the simultaneous correction of urban road and rural road vectors, with poor applicability and low compatibility. To solve this problem, this paper proposes a vector data partition correction method supported by deep learning. First, the U-2-net model and line segment sequence method are used to generate the feature set. Second, according to the characteristics of the poor quality of urban road extraction results, the regular geometric form of vector data and the poor structural information of road images, the methods of vector line decomposition, subvector line correction, subvector line centralization and vector line synthesis are proposed to correct the vector lines of urban roads. Finally, according to the characteristics of high-quality rural road extraction, the irregular geometric form of vector data and the strong information of road image structures, this paper proposes road edge extraction, a road centreline reasoning model and a vector line connection method to complete the correction of rural vector road data. The quantitative analysis of the experimental results shows that compared with other methods, the urban road vector correction method in this paper is much better than the comparison methods and has achieved better correction results in rural areas. This method has the advantages of compatibility and better accuracy for roads in different areas.
WOS:000858465600001
</snippet>
</document>

<document id="230">
<title>Explainable Deep Learning for Biomarker Classification of OCT Images</title>
<url>http://dx.doi.org/10.1109/BIBE50027.2020.00041</url>
<snippet>Advanced form of age-related macular degeneration (AMD) is a major health burden that can lead to irreversible vision loss in the elderly population. The early signs of AMD are drusen, which appear as yellowish deposits under the retina. The end-stages of AMD include two forms: wet AMD (neovascular) and geographic atrophy (GA, non-neovascular). We propose a deep learning approach using a pre-trained VGG-19 neural network to classify the optical coherence tomography (OCT) images into wet AMD, GA, drusen, and healthy images. To explain the results, we quantify and present the prediction confidence and reliability as well as the regions of interest deemed to be important by the deep learning model when classifying OCT images. The sensitivity of classification for wet AMD, GA, drusen, and healthy images was 91.67&#37;, 88.00&#37;, 96.30&#37; and 100&#37; respectively, with the most confident predictions being for the drusen and healthy images. Visual inspection of the misclassified images using heatmaps using the Gradient-weighted Class Activation Mapping (Grad-CAM) algorithm revealed that, for most of the misclassified images that had high prediction confidence, the algorithm identified correctly more than one region of interest, each belonging to a different AMD category. We concluded that, rather than assigning just one label to an AMD image, algorithms for AMD classification should allow multi-labels as images generally show evidence of more than one stage of AMD simultaneously (e.g., drusen as the predominant region and GA as a small region of interest).
WOS:000659298300033
</snippet>
</document>

<document id="231">
<title>Survey of Road Extraction Methods in Remote Sensing Images Based on Deep Learning</title>
<url>http://dx.doi.org/10.1007/s41064-022-00194-z</url>
<snippet>Road information plays a fundamental role in application fields such as map updating, traffic management, and road monitoring. Extracting road features from remote sensing images is a hot and frontier issue in the remote sensing field, and it is also one of the most challenging research topics. In view of this, this research systematically reviews the deep learning technology applied to road extraction in remote sensing images and summarizes the existing theories and methods. According to the different annotation types and learning methods, they can be divided into three methods: fully supervised, weakly supervised and unsupervised learning. Then, the datasets and performance evaluation metrics related to road extraction from remote sensing images are summarized, and on this basis, the effects of common road extraction methods are analysed. Finally, suggestions and prospects for the development of road extraction are proposed.
WOS:000756440400001
</snippet>
</document>

<document id="232">
<title>Sequential Localizing and Mapping: A Navigation Strategy via Enhanced Subsumption Architecture</title>
<url>http://dx.doi.org/10.3390/s20174815</url>
<snippet>In this paper, we present a navigation strategy exclusively designed for social robots with limited sensors for applications in homes. The overall system integrates a reactive design based on subsumption architecture and a knowledge system with learning capabilities. The component of the system includes several modules, such as doorway detection and room localization via convolutional neural network (CNN), avoiding obstacles via reinforcement learning, passing the doorway via Canny edges detection, building an abstract map called a Directional Semantic Topological Map (DST-Map) within the knowledge system, and other predefined layers within the subsumption architecture. The individual modules and the overall system are evaluated in a virtual environment using Webots simulator.
WOS:000569673500001
</snippet>
</document>

<document id="233">
<title>From Google Maps to a fine-grained catalog of street trees</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2017.11.008</url>
<snippet>Up-to-date catalogs of the urban tree population are of importance for municipalities to monitor and improve quality of life in cities. Despite much research on automation of tree mapping, mainly relying on dedicated airborne LiDAR or hyperspectral campaigns, tree detection and species recognition is still mostly done manually in practice. We present a fully automated tree detection and species recognition pipeline that can process thousands of trees within a few hours using publicly available aerial and street view images of Google Maps (TM). These data provide rich information from different viewpoints and at different scales from global tree shapes to bark textures. Our work-flow is built around a supervised classification that automatically learns the most discriminative features from thousands of trees and corresponding, publicly available tree inventory data. In addition, we introduce a change tracker that recognizes changes of individual trees at city-scale, which is essential to keep an urban tree inventory up-to-date. The system takes street-level images of the same tree location at two different times and classifies the type of change (e.g., tree has been removed). Drawing on recent advances in computer vision and machine learning, we apply convolutional neural networks (CNN) for all classification tasks. We propose the following pipeline: download all available panoramas and overhead images of an area of interest, detect trees per image and combine multi-view detections in a probabilistic framework, adding prior knowledge; recognize fine-grained species of detected trees. In a later, separate module, track trees over time, detect significant changes and classify the type of change. We believe this is the first work to exploit publicly available image data for city-scale street tree detection, species recognition and change tracking, exhaustively over several square kilometers, respectively many thousands of trees. Experiments in the city of Pasadena, California, USA show that we can detect &gt;70&#37; of the street trees, assign correct species to &gt;80&#37; for 40 different species, and correctly detect and classify changes in &gt;90&#37; of the cases. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000423895100002
</snippet>
</document>

<document id="234">
<title>Roof Shape Classification from LiDAR and Satellite Image Data Fusion Using Supervised Learning</title>
<url>http://dx.doi.org/10.3390/s18113960</url>
<snippet>Geographic information systems (GIS) provide accurate maps of terrain, roads, waterways, and building footprints and heights. Aircraft, particularly small unmanned aircraft systems (UAS), can exploit this and additional information such as building roof structure to improve navigation accuracy and safely perform contingency landings particularly in urban regions. However, building roof structure is not fully provided in maps. This paper proposes a method to automatically label building roof shape from publicly available GIS data. Satellite imagery and airborne LiDAR data are processed and manually labeled to create a diverse annotated roof image dataset for small to large urban cities. Multiple convolutional neural network (CNN) architectures are trained and tested, with the best performing networks providing a condensed feature set for support vector machine and decision tree classifiers. Satellite image and LiDAR data fusion is shown to provide greater classification accuracy than using either data type alone. Model confidence thresholds are adjusted leading to significant increases in models precision. Networks trained from roof data in Witten, Germany and Manhattan (New York City) are evaluated on independent data from these cities and Ann Arbor, Michigan.
WOS:000451598900370
</snippet>
</document>

<document id="235">
<title>Using Satellite Images and Deep Learning to Identify Associations Between County-Level Mortality and Residential Neighborhood Features Proximal to Schools: A Cross-Sectional Study</title>
<url>http://dx.doi.org/10.3389/fpubh.2021.766707</url>
<snippet>What is the relationship between mortality and satellite images as elucidated through the use of Convolutional Neural Networks?Background: Following a century of increase, life expectancy in the United States has stagnated and begun to decline in recent decades. Using satellite images and street view images, prior work has demonstrated associations of the built environment with income, education, access to care, and health factors such as obesity. However, assessment of learned image feature relationships with variation in crude mortality rate across the United States has been lacking.Objective: We sought to investigate if county-level mortality rates in the U.S. could be predicted from satellite images.Methods: Satellite images of neighborhoods surrounding schools were extracted with the Google Static Maps application programming interface for 430 counties representing ~68.9&#37; of the US population. A convolutional neural network was trained using crude mortality rates for each county in 2015 to predict mortality. Learned image features were interpreted using Shapley Additive Feature Explanations, clustered, and compared to mortality and its associated covariate predictors.Results: Predicted mortality from satellite images in a held-out test set of counties was strongly correlated to the true crude mortality rate (Pearson r = 0.72). Direct prediction of mortality using a deep learning model across a cross-section of 430 U.S. counties identified key features in the environment (e.g., sidewalks, driveways, and hiking trails) associated with lower mortality. Learned image features were clustered, and we identified 10 clusters that were associated with education, income, geographical region, race, and age.Conclusions: The application of deep learning techniques to remotely-sensed features of the built environment can serve as a useful predictor of mortality in the United States. Although we identified features that were largely associated with demographic information, future modeling approaches that directly identify image features associated with health-related outcomes have the potential to inform targeted public health interventions.
WOS:000720173800001
</snippet>
</document>

<document id="236">
<title>A traceability chain algorithm for artificial neural networks using T-S fuzzy cognitive maps in blockchain</title>
<url>http://dx.doi.org/10.1016/j.future.2017.09.077</url>
<snippet>Blockchain acts on a big data analytics because transaction data belongs to streaming data and high-dimensional data from distributed computing network. Accordingly, such operation produces irrelevant data problem and further poorly optimized traceability in blockchain. So, we claim that the artificial intelligence of blockchain mining algorithm like traceability chain algorithm runs faster than consensus algorithm because of inference mechanism. Our main goal is to reach traceability decision not consensus decision as fast as possible. Thus, this article proposes a novelty approach called Takagi-Sugeno Fuzzy cognitive maps ANN as traceability chain algorithm. The numerical example of the proposed algorithm in blockchain mining is evaluated and optimized decisions experiment is analyzed. Objective functions for optimized decision computation is described as participant nodes constraint method. Thus contribution succeeds in meeting the reduction mining efforts for the traceability chain being processed. Our findings also provide a preliminary indication of deep learning applied big blockchain transactions data. (C) 2017 Elsevier B.V. All rights reserved.
WOS:000419409200015
</snippet>
</document>

<document id="237">
<title>A Hierarchical Airport Detection Method Using Spatial Analysis and Deep Learning</title>
<url>http://dx.doi.org/10.3390/rs11192204</url>
<snippet>Airports have a profound impact on our lives, and uncovering their distribution around the world has great significance for research and development. However, existing airport databases are incomplete and have a high cost of updating. Thus, a fast and automatic worldwide airport detection method can be of significance for global airport detection at regular intervals. However, previous airport detection studies are usually based on single remote sensing (RS) imagery, which seems an overwhelming burden for worldwide airport detection with traversal searching. Thus, we propose a hierarchical airport detection method consisting of broad-scale extraction of worldwide candidate airport regions based on spatial analysis of released RS products, including impervious surfaces from FROM-GLC10 (fine resolution observation and monitoring of global land cover 10) product, building distribution from OSMs (open street maps) and digital surface model from AW3D30 (ALOS World 3D-30 m). Moreover, narrow-scale aircraft detection was initially conducted by the Faster R-CNN (regional-convolutional neural networks) deep learning method. To avoid overestimation of background regions by Faster R-CNN, a second CNN classifier is used to refine the class labeling with negative samples. Specifically, our research focuses on target airports with at least 2 km length in three experimental regions. Results show that spatial analysis reduced the possible regions to 0.56&#37; of the total area of 75,691 km(2). The initial aircraft detection by Faster R-CNN had a mean users accuracy of 88.90&#37; and ensured that all the aircrafts could be detected. Then, by introducing the CNN reclassifier, the users accuracy of aircraft detection was significantly increased to 94.21&#37;. Finally, through an experienced threshold of aircraft number, 19 of the total 20 airports were detected correctly. Our results reveal the overall workflow is reliable for automatic and rapid airport detection around the world with the help of released RS products. This research promotes the application and progression of deep learning.
WOS:000496827100018
</snippet>
</document>

<document id="238">
<title>LARGE EARTHQUAKE MAGNITUDE PREDICTION IN TAIWAN BASED ON DEEP LEARNING NEURAL NETWORK</title>
<url>http://dx.doi.org/10.14311/NNW.2018.28.009</url>
<snippet>In this paper, a deep learning-based method for earthquake prediction is proposed. Large-magnitude earthquakes and tsunamis triggered by earthquakes can kill thousands of people and cause millions of dollars worth of economic losses. The accurate prediction of large-magnitude earthquakes is a worldwide problem. In recent years, deep learning technology that can automatically extract features from mass data has been applied in image recognition, natural language processing, object recognition, etc., with great success. We explore to apply deep learning technology to earthquake prediction. We propose a deep learning method for continuous earthquake prediction using historical seismic events. First, we project the historical seismic events onto a topographic map. Taking Taiwan as an example, we generate the images of the dataset for deep learning and mark a label "1" or "0", depending on whether in the upcoming 30 days a greater than M6 earthquake will occur. Second, we train our deep leaning network model, using the images of the dataset. Finally, we make earthquake predictions, using the trained network model. The result shows that we can get the best result, when we predict earthquakes in the upcoming 30 days using data from the past 120 days. Here, we use R score as the performance metrics. The best R score is 0.303. Although the R score is not high enough, using the past 120 days historic seismic event to predict the upcoming 30 days biggest earthquake magnitude can be seen as the pattern of Taiwan earthquake because the R score is rather good compared to other datasets. The proposed method performs well without manually designing feature vectors, as in the traditional neural network method. This method can be applied to earthquake prediction in other seismic zones.
WOS:000432888400004
</snippet>
</document>

<document id="239">
<title>Superpixel-Based Multiscale CNN Approach Toward Multiclass Object Segmentation From UAV-Captured Aerial Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2023.3239119</url>
<snippet>Unmanned aerial vehicles (UAVs) are promising remote sensors capable of reforming remote sensing applications. However, for artificial-intelligence-guided tasks, such as land cover mapping and ground-object mapping, most deep-learning-based architectures fail to extract scale-invariant features, resulting in poor performance accuracy. In this context, the article proposes a superpixel-aided multiscale convolutional neural network (CNN) architecture to avoid misclassification in complex urban aerial images. The proposed framework is a two-tier deep-learning-based segmentation architecture. In the first stage, a superpixel-based simple linear iterative cluster algorithm produces superpixel images with crucial contextual information. The second stage comprises a multiscale CNN architecture that uses these information-rich superpixel images to extract scale-invariant features for predicting the object class of each pixel. Two UAV-image-based aerial image datasets: 1) NITRDrone dataset and 2) urban drone dataset (UDD), are considered to perform the experiment. The proposed model outperforms the considered state-of-the-art methods with an intersection of union of 76.39&#37; and 86.85&#37; on UDD and NITRDrone datasets, respectively. Experimentally obtained results prove that the proposed architecture performs superior by achieving better performance accuracy in complex and challenging scenarios.
WOS:000934999600002
</snippet>
</document>

<document id="240">
<title>Monitoring offshore oil pollution using multi-class convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.envpol.2021.117884</url>
<snippet>Oil and gas production operations are a major source of environmental pollution that expose people and habitats in many coastal communities around the world to adverse health effects. Detecting oil spills in a timely and precise manner can help improve the oil spill response process and channel required resources more effectively to affected regions. In this research, convolutional neural networks, a branch of artificial intelligence (AI), are trained on a visual dataset of oil spills containing images from different altitudes and geographical locations. In particular, a VGG16 model is adopted through transfer learning for oil spill classification (i.e., detecting if there is oil spill in an image) with an accuracy of 92&#37;. Next, Mask R-CNN and PSPNet models are used for oil spill segmentation (i.e., pixel-level detection of oil spill boundaries) with a mean intersection over union (IoU) of 49&#37; and 68&#37;, respectively. Lastly, to determine if there is an oil rig or vessel in the vicinity of a detected oil spill and provide a holistic view of the oil spill surroundings, a YOLOv3 model is trained and used, yielding a maximum mean average precision (mAP) of similar to 71&#37;. Findings of this research can improve the current practices of oil pollution cleanup and predictive maintenance, ultimately leading to more resilient and healthy coastal communities.
WOS:000697063100003
</snippet>
</document>

<document id="241">
<title>Toward Fast and Accurate Vehicle Detection in Aerial Images Using Coupled Region-Based Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1109/JSTARS.2017.2694890</url>
<snippet>Vehicle detection in aerial images, being an interesting but challenging problem, plays an important role for a wide range of applications. Traditional methods are based on sliding-window search and handcrafted or shallow-learning-based features with heavy computational costs and limited representation power. Recently, deep learning algorithms, especially region-based convolutional neural networks (R-CNNs), have achieved state-of-the-art detection performance in computer vision. However, several challenges limit the applications of R-CNNs in vehicle detection from aerial images: 1) vehicles in large-scale aerial images are relatively small in size, and R-CNNs have poor localization performance with small objects; 2) R-CNNs are particularly designed for detecting the bounding box of the targets without extracting attributes; 3) manual annotation is generally expensive and the available manual annotation of vehicles for training R-CNNs are not sufficient in number. To address these problems, this paper proposes a fast and accurate vehicle detection framework. On one hand, to accurately extract vehicle-like targets, we developed an accurate-vehicle-proposal-network (AVPN) based on hyper feature map which combines hierarchical feature maps that are more accurate for small object detection. On the other hand, we propose a coupled R-CNN method, which combines an AVPN and a vehicle attribute learning network to extract the vehicles location and attributes simultaneously. For original large-scale aerial images with limited manual annotations, we use cropped image blocks for training with data augmentation to avoid overfitting. Comprehensive evaluations on the public Munich vehicle dataset and the collected vehicle dataset demonstrate the accuracy and effectiveness of the proposed method.
WOS:000407706200020
</snippet>
</document>

<document id="242">
<title>A new strategy to map landslides with a generalized convolutional neural network</title>
<url>http://dx.doi.org/10.1038/s41598-021-89015-8</url>
<snippet>Rapid mapping of event landslides is crucial to identify the areas affected by damages as well as for effective disaster response. Traditionally, such maps are generated with visual interpretation of remote sensing imagery (manned/unmanned airborne systems or spaceborne sensors) and/or using pixel-based and object-based methods exploiting data-intensive machine learning algorithms. Recent works have explored the use of convolutional neural networks (CNN), a deep learning algorithm, for mapping landslides from remote sensing data. These methods follow a standard supervised learning workflow that involves training a model using a landslide inventory covering a relatively small area. The trained model is then used to predict landslides in the surrounding regions. Here, we propose a new strategy, i.e., a progressive CNN training relying on combined inventories to build a generalized model that can be applied directly to a new, unexplored area. We first prove the effectiveness of CNNs by training and validating on event landslides inventories in four regions after earthquakes and/or extreme meteorological events. Next, we use the trained CNNs to map landslides triggered by new events spread across different geographic regions. We found that CNNs trained on a combination of inventories have a better generalization performance, with a bias towards high precision and low recall scores. In our tests, the combined training model achieved the highest (Matthews correlation coefficient) MCC score of 0.69 when mapping landslides in new unseen regions. The mapping was done on images from different optical sensors, resampled to a spatial resolution of 6 m, 10 m, and 30 m. Despite a slightly reduced performance, the main advantage of combined training is to overcome the requirement of a local inventory for training a new deep learning model. This implementation can facilitate automated pipelines providing fast response for the generation of landslide maps in the post-disaster phase. In this study, the study areas were selected from seismically active zones with a high hydrological hazard distribution and vegetation coverage. Hence, future works should also include regions from less vegetated geographic locations.
WOS:000656988100030
</snippet>
</document>

<document id="243">
<title>Artificial Intelligence: Issues, Challenges, Opportunities and Threats</title>
<url>http://dx.doi.org/10.1007/978-3-030-29743-5_2</url>
<snippet>The world is experiencing a period of instability in a range of pillar institutions in the international system. These instabilities and unsustainable systems may have serious implications for humanity. Catastrophic physical phenomena are on the rise, lately and many say that this is due to human disrespect to the environment. Urgently valuable and sustainable solutions are needed. One scientific approach to address these challenging questions is Artificial Intelligence (AI). Theories of AI are reviewed. Machine learning (ML), Neural Networks (NN) and Deep Learning (DL) are briefly presented. Certain criticisms of AI and DL are carefully analyzed. A number of challenges and opportunities of AI are identified. The future of AI and potential threats of it are discussed. Artificial Intelligence (AI) and Deep Learning (DL) are relying mainly on data analysis without taking into consideration the human nature. Theories of Fuzzy Cognitive Maps (FCM) seem to provide a useful tool in developing new AI theories answering this problem.
WOS:000560806800002
</snippet>
</document>

<document id="244">
<title>Data-Driven Spectrum Cartography via Deep Completion Autoencoders</title>
<url>http://dx.doi.org/</url>
<snippet>Spectrum maps, which provide RF spectrum metrics such as power spectral density for every location in a geographic area, find numerous applications in wireless communications such as interference control, spectrum management, resource allocation, and network planning to name a few. Spectrum cartography techniques construct these maps from a collection of measurements collected by spatially distributed sensors. Due to the nature of the propagation of electromagnetic waves, spectrum maps are complicated functions of the spatial coordinates. For this reason, model-free approaches have been preferred. However, all existing schemes rely on some interpolation algorithm unable to learn from data. This paper proposes a novel approach to spectrum cartography where propagation phenomena are learned from data. The resulting algorithms can therefore construct a spectrum map from a significantly smaller number of measurements than existing schemes since the spatial structure of shadowing and other phenomena is previously learned from maps in other environments. Besides the aforementioned new paradigm, this is also the first work to perform spectrum cartography with deep neural networks. To exploit the manifold structure of spectrum maps, a deep network architecture is proposed based on completion autoencoders.
WOS:000606970304155
</snippet>
</document>

<document id="245">
<title>A Phase Filtering Method with Scale Recurrent Networks for InSAR</title>
<url>http://dx.doi.org/10.3390/rs12203453</url>
<snippet>Phase filtering is a key issue in interferometric synthetic aperture radar (InSAR) applications, such as deformation monitoring and topographic mapping. The accuracy of the deformation and terrain height is highly dependent on the quality of phase filtering. Researchers are committed to continuously improving the accuracy and efficiency of phase filtering. Inspired by the successful application of neural networks in SAR image denoising, in this paper we propose a phase filtering method that is based on deep learning to efficiently filter out the noise in the interferometric phase. In this method, the real and imaginary parts of the interferometric phase are filtered while using a scale recurrent network, which includes three single scale subnetworks based on the encoder-decoder architecture. The network can utilize the global structural phase information contained in the different-scaled feature maps, because RNN units are used to connect the three different-scaled subnetworks and transmit current state information among different subnetworks. The encoder part is used for extracting the phase features, and the decoder part restores detailed information from the encoded feature maps and makes the size of the output image the same as that of the input image. Experiments on simulated and real InSAR data prove that the proposed method is superior to three widely-used phase filtering methods by qualitative and quantitative comparisons. In addition, on the same simulated data set, the overall performance of the proposed method is better than another deep learning-based method (DeepInSAR). The runtime of the proposed method is only about 0.043s for an image with a size of 1024x1024 pixels, which has the significant advantage of computational efficiency in practical applications that require real-time processing.
WOS:000585582800001
</snippet>
</document>

<document id="246">
<title>Correcting rural building annotations in OpenStreetMap using convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.11.010</url>
<snippet>Rural building mapping is paramount to support demographic studies and plan actions in response to crisis that affect those areas. Rural building annotations exist in OpenStreetMap (OSM), but their quality and quantity are not sufficient for training models that can create accurate rural building maps. The problems with these annotations essentially fall into three categories: (i) most commonly, many annotations are geometrically misaligned with the updated imagery; (ii) some annotations do not correspond to buildings in the images (they are misannotations or the buildings have been destroyed); and (iii) some annotations are missing for buildings in the images (the buildings were never annotated or were built between subsequent image acquisitions). First, we propose a method based on Markov Random Field (MRF) to align the buildings with their annotations. The method maximizes the correlation between annotations and a building probability map while enforcing that nearby buildings have similar alignment vectors. Second, the annotations with no evidence in the building probability map are removed. Third, we present a method to detect non-annotated buildings with predefined shapes and add their annotation. The proposed methodology shows considerable improvement in accuracy of the OSM annotations for two regions of Tanzania and Zimbabwe, being more accurate than state-of-the-art baselines.
WOS:000455690800021
</snippet>
</document>

<document id="247">
<title>A Wavelet-Driven Subspace Basis Learning Network for High-Resolution Synthetic Aperture Radar Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2023.3241944</url>
<snippet>The feature learning strategy of convolutional neural networks learns the deep spatial features from high-resolution (HR) synthetic aperture radar (SAR) images while ignoring the speckle noise based on the SAR imaging mechanism. In the feature learning module, the noise reduction by feature-adaptive projection guided by a powerful embedded wavelet feature reconstruction mechanism can effectively learn the deep feature statistics. In this article, we present a wavelet-driven subspace basis learning network (WDSBLN), following an encoder-decoder architecture, for the HR SAR image classification. The powerful wavelet module, including wavelet decomposition and reconstruction, is employed for keeping the structures of learned features well under speckle noise. Specifically, a compact second-order feature enhancement mechanism is designed for improving the contour and edge information of low-frequency components in the feature decomposition stage, and a local feature attention module based on the point-wise convolutional layer is adopted to aggregate the contextual information of the local channel and reserves detail information in the high-frequency components. Then, the reconstructed feature map is employed as a guided standard in the subspace basis learning (SBL) module. The SBL module, including basis generation (generating the subspace basis vectors) and subspace projection (transforming deep feature maps into a signal subspace), maintains the local structure of HR SAR image patches and acquires the robust feature statistics. We conduct evaluations on three real HR SAR image classification datasets, achieving superior performances as compared to other related networks.
WOS:000940195200001
</snippet>
</document>

<document id="248">
<title>A CNN-LSTM Model for Soil Organic Carbon Content Prediction with Long Time Series of MODIS-Based Phenological Variables</title>
<url>http://dx.doi.org/10.3390/rs14184441</url>
<snippet>The spatial distribution of soil organic carbon (SOC) serves as critical geographic information for assessing ecosystem services, climate change mitigation, and optimal agriculture management. Digital mapping of SOC is challenging due to the complex relationships between the soil and its environment. Except for the well-known terrain and climate environmental covariates, vegetation that interacts with soils influences SOC significantly over long periods. Although several remote-sensing-based vegetation indices have been widely adopted in digital soil mapping, variables indicating long term vegetation growth have been less used. Vegetation phenology, an indicator of vegetation growth characteristics, can be used as a potential time series environmental covariate for SOC prediction. A CNN-LSTM model was developed for SOC prediction with inputs of static and dynamic environmental variables in Xuancheng City, China. The spatially contextual features in static variables (e.g., topographic variables) were extracted by the convolutional neural network (CNN), while the temporal features in dynamic variables (e.g., vegetation phenology over a long period of time) were extracted by a long short-term memory (LSTM) network. The ten-year phenological variables derived from moderate-resolution imaging spectroradiometer (MODIS) observations were adopted as predictors with historical temporal changes in vegetation in addition to the commonly used static variables. The random forest (RF) model was used as a reference model for comparison. Our results indicate that adding phenological variables can produce a more accurate map, as tested by the five-fold cross-validation, and demonstrate that CNN-LSTM is a potentially effective model for predicting SOC at a regional spatial scale with long-term historical vegetation phenology information as an extra input. We highlight the great potential of hybrid deep learning models, which can simultaneously extract spatial and temporal features from different types of environmental variables, for future applications in digital soil mapping.
WOS:000856760700001
</snippet>
</document>

<document id="249">
<title>Rainfall-Induced Shallow Landslide Susceptibility Mapping at Two Adjacent Catchments Using Advanced Machine Learning Algorithms</title>
<url>http://dx.doi.org/10.3390/ijgi9100569</url>
<snippet>Landslides impact on human activities and socio-economic development, especially in mountainous areas. This study focuses on the comparison of the prediction capability of advanced machine learning techniques for the rainfall-induced shallow landslide susceptibility of Deokjeokri catchment and Karisanri catchment in South Korea. The influencing factors for landslides, i.e., topographic, hydrologic, soil, forest, and geologic factors, are prepared from various sources based on availability, and a multicollinearity test is also performed to select relevant causative factors. The landslide inventory maps of both catchments are obtained from historical information, aerial photographs and performed field surveys. In this study, Deokjeokri catchment is considered as a training area and Karisanri catchment as a testing area. The landslide inventories contain 748 landslide points in training and 219 points in testing areas. Three landslide susceptibility maps using machine learning models, i.e., Random Forest (RF), Extreme Gradient Boosting (XGBoost) and Deep Neural Network (DNN), are prepared and compared. The outcomes of the analyses are validated using the landslide inventory data. A receiver operating characteristic curve (ROC) method is used to verify the results of the models. The results of this study show that the training accuracy of RF is 0.756 and the testing accuracy is 0.703. Similarly, the training accuracy of XGBoost is 0.757 and testing accuracy is 0.74. The prediction of DNN revealed acceptable agreement between the susceptibility map and the existing landslides, with a training accuracy of 0.855 and testing accuracy of 0.802. The results showed that the DNN model achieved lower prediction error and higher accuracy results than other models for shallow landslide modeling in the study area.
WOS:000586922700001
</snippet>
</document>

<document id="250">
<title>Learning sequential slice representation with an attention-embedding network for 3D shape recognition and retrieval in MLS point clouds</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.01.003</url>
<snippet>The representation of 3D data is the key issue for shape analysis. However, most of the existing representations suffer from high computational cost and structure information loss. This paper presents a novel sequential slice representation with an attention-embedding network, named RSSNet, for 3D point cloud recognition and retrieval in road environments. RSSNet has two main branches. Firstly, a sequential slice module is designed to map disordered 3D point clouds to ordered sequence of shallow feature vectors. A gated recurrent unit (GRU) module is applied to encode the spatial and content information of these sequential vectors. The second branch consists of a key-point based graph convolution network (GCN) with an embedding attention strategy to fuse the sequential and global features to refine the structure discriminability. Three datasets were used to evaluate the proposed method, one acquired by our mobile laser scanning (MLS) system and two public datasets (KITTI and Sydney Urban Objects). Experimental results indicated that the proposed method achieved better performance than recognition and retrieval state-of-the-art methods. RSSNet provided recognition rates of 98.08&#37;, 95.77&#37; and 70.83&#37; for the above three datasets, respectively. For the retrieval task, RSSNet obtained excellent mAP values of 95.56&#37;, 87.16&#37; and 69.99&#37; on three datasets, respectively.
WOS:000517849600012
</snippet>
</document>

<document id="251">
<title>Automatic Detection of Road Types From the Third Military Mapping Survey of Austria-Hungary Historical Map Series With Deep Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1109/ACCESS.2021.3074897</url>
<snippet>With the increased amount of digitized historical documents, information extraction from them gains pace. Historical maps contain valuable information about historical, geographical and economic aspects of an era. Retrieving information from historical maps is more challenging than processing modern maps due to lower image quality, degradation of documents and the massive amount of non-annotated digital map archives. Convolutional Neural Networks (CNN) solved many image processing challenges with great success, but they require a vast amount of annotated data. For historical maps, this means an unprecedented scale of manual data entry and annotation. In this study, we first manually annotated the Third Military Mapping Survey of Austria-Hungary historical map series conducted between 1884 and 1918 and made them publicly accessible. We recognized different road types and their pixel-wise positions automatically by using a CNN architecture and achieved promising results.
WOS:000645853900001
</snippet>
</document>

<document id="252">
<title>Multi-Scale Feature Aggregation Network for Semantic Segmentation of Land Cover</title>
<url>http://dx.doi.org/10.3390/rs14236156</url>
<snippet>Land cover semantic segmentation is an important technique in land. It is very practical in land resource protection planning, geographical classification, surveying and mapping analysis. Deep learning shows excellent performance in picture segmentation in recent years, but there are few semantic segmentation algorithms for land cover. When dealing with land cover segmentation tasks, traditional semantic segmentation networks often have disadvantages such as low segmentation precision and weak generalization due to the loss of image detail information and the limitation of weight distribution. In order to achieve high-precision land cover segmentation, this article develops a multi-scale feature aggregation network. Traditional convolutional neural network downsampling procedure has problems of detail information loss and resolution degradation; to fix these problems, a multi-scale feature extraction spatial pyramid module is made to assemble regional context data from different areas. In order to address the issue of incomplete information of traditional convolutional neural networks at multiple sizes, a multi-scale feature fusion module is developed to fuse attributes from various layers and several sizes to boost segmentation accuracy. Finally, a multi-scale convolutional attention module is presented to enhance the segmentation's attention to the target in order to address the issue that the classic convolutional neural network has low attention capacity to the building waters in land cover segmentation. Through the contrast experiment and generalization experiment, it can be clearly demonstrated that the segmentation algorithm proposed in this paper realizes the high precision segmentation of land cover.
WOS:000896525000001
</snippet>
</document>

<document id="253">
<title>3D map-guided single indoor image localization refinement</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.01.008</url>
<snippet>Image localization is an important supplement to GPS-based methods, especially in indoor scenes. Traditional methods depending on image retrieval or structure from motion (SfM) techniques either suffer from low accuracy or even fail to work due to the texture-less or repetitive indoor surfaces. With the development of range sensors, 3D colourless maps are easily constructed in indoor scenes. How to utilize such a 3D colourless map to improve single image localization performance is a timely but unsolved research problem. In this paper, we present a new approach to addressing this problem by inferring the 3D geometry from a single image with an initial 6DOF pose estimated by a neural network based method. In contrast to previous methods that rely multiple overlapping images or videos to generate sparse point clouds, our new approach can produce dense point cloud from only a single image. We achieve this through estimating the depth map of the input image and performing geometry matching in the 3D space. We have developed a novel depth estimation method by utilizing both the 3D map and RGB images where we use the RGB image to estimate a dense depth map and use the 3D map to guide the depth estimation. We will show that our new method significantly outperforms current RGB image based depth estimation methods for both indoor and outdoor datasets. We also show that utilizing the depth map predicted by the new method for single indoor image localization can improve both position and orientation localization accuracy over state-of-the-art methods.
WOS:000517849600002
</snippet>
</document>

<document id="254">
<title>A hierarchical self-attention augmented Laplacian pyramid expanding network for change detection in high-resolution remote sensing images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.10.001</url>
<snippet>Change detection methods can achieve high learning ability and recognition accuracy with the introduction of deep convolutional neural networks, but due to the influence of the convolution kernel and deep feature sampling, problems such as the limited feature extraction range and loss of information are inevitable. In addition, there can be severe imbalance in the proportions of changed and unchanged pixels compromising the stability of model training. To address these interrelated problems, we propose a hierarchical self-attention augmented Laplacian pyramid expanding network (H-SALENet) for supervised change detection in high-resolution remote sensing images. H-SALENet is composed of an encoder and a decoder. In the encoder, H-SALENet combines a deep convolutional module with a hierarchical and long-range context augmentation module (HLAM) to extract the deep features of bi-temporal images; the representation capability of multi-level and long-range dependent change features is enhanced through deep convolution and 2D transformer-structured multihead self-attention learning. In the decoder, a Laplacian pyramid expansion module (LPEM) is proposed to catch change information at different scales and reduce high-frequency information loss simultaneously, thus weakening the influence of deep feature resampling on the change map. In addition, a data-balanced loss function concentrating on both the quantity and the complexity of the pixels was designed to reduce the influence of the imbalance between changed and unchanged pixels. H-SALENet was tested on two kinds of public datasets; the qualitative and quantitative experimental results show that the proposed network outperformed three benchmark change detection networks in terms of the integrity of change objects and the capability to obtain change contours.
WOS:000709811500004
</snippet>
</document>

<document id="255">
<title>Learning Deep Features for Classification of Typical Ecological Environmental Elements in High-Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/ISCID.2017.200</url>
<snippet>Ecological environmental elements are greatly related to both humans and nature. The rapid development of remote sensing technology has provided us more and more high-resolution remote sensing images for monitoring these elements timely and objectively. However, it has been a great challenge to recognize these elements from such images due to their diversity and complexity. In this paper, a classification approach of ecological environmental elements based on deep learning features of objects is proposed. At first, a deep convolutional neural network (DCNN) is trained for discriminating different ecological environmental elements. To extract deep features of irregular-shaped regions, sub images are clipped from each region and used to represent the corresponding region. Then, deep features of these sub images are extracted by the trained DCNN. After that, the softmax classifier is used to predict class probabilities of all sub images. The class of one region is determined by considering the class probabilities of its sub-images according to the "winner takes-all" strategy. Finally, the thematic maps of ecological environmental elements are achieved. The proposed approach is evaluated by the classification experiments on a test set of typical ecological environmental elements in high-resolution remote sensing images, and the classification accuracy reaches to 98.44&#37;. Moreover, the classification accuracy on irregular shaped regions also reaches to 96.77&#37;. These results have testified the effectiveness of the proposed approach.
WOS:000427991100050
</snippet>
</document>

<document id="256">
<title>Mapping Post-Earthquake Landslide Susceptibility: A U-Net Like Approach</title>
<url>http://dx.doi.org/10.3390/rs12172767</url>
<snippet>A serious earthquake could trigger thousands of landslides and produce some slopes more sensitive to slide in future. Landslides could threaten humans lives and properties, and thus mapping the post-earthquake landslide susceptibility is very valuable for a rapid response to landslide disasters in terms of relief resource allocation and posterior earthquake reconstruction. Previous researchers have proposed many methods to map landslide susceptibility but seldom considered the spatial structure information of the factors that influence a slide. In this study, we first developed a U-net like model suitable for mapping post-earthquake landslide susceptibility. The post-earthquake high spatial airborne images were used for producing a landslide inventory. Pre-earthquake Landsat TM (Thematic Mapper) images and the influencing factors such as digital elevation model (DEM), slope, aspect, multi-scale topographic position index (mTPI), lithology, fault, road network, streams network, and macroseismic intensity (MI) were prepared as the input layers of the model. Application of the model to the heavy-hit area of the destructive 2008 Wenchuan earthquake resulted in a high validation accuracy (precision 0.77, recall 0.90, F1 score 0.83, and AUC 0.90). The performance of this U-net like model was also compared with those of traditional logistic regression (LR) and support vector machine (SVM) models on both the model area and independent testing area with the former being stronger than the two traditional models. The U-net like model introduced in this paper provides us the inspiration that balancing the environmental influence of a pixel itself and its surrounding pixels to perform a better landslide susceptibility mapping (LSM) task is useful and feasible when using remote sensing and GIS technology.
WOS:000569642300001
</snippet>
</document>

<document id="257">
<title>A deep learning approach for automatic mapping of poplar plantations using Sentinel-2 imagery</title>
<url>http://dx.doi.org/10.1080/15481603.2021.1988427</url>
<snippet>Poplars are one of the most widespread fast-growing tree species used for forest plantations. Owing to their distinct features (fast growth and short rotation) and the dependency on the timber price market, poplar plantations are characterized by large inter-annual fluctuations in their extent and distribution. Therefore, monitoring poplar plantations requires a frequent update of information - not feasible by National Forest Inventories due to their periodicity - achievable by remote sensing systems applications. In particular, the new Sentinel-2 mission, with a revisiting period of 5 days, represents a potentially efficient tool for meeting this need. In this paper, we present a deep learning approach for mapping poplar plantations using Sentinel-2 time series. A reference dataset of poplar plantations was available for a large study area of more than 46,000 km(2) in Northern Italy and served as training and testing data. Two classification methods were compared: (1) a fully connected neural network (also called multilayer perceptron), and (2) a traditional logistic regression. The performance of the two approaches was estimated through bootstrapping procedure with a confidence interval of 99&#37;. Results indicated for deep learning an omission error rate of 2.77&#37;+/- 2.76&#37;, showing improvements compared to logistic regression, omission error rate = 8.91&#37;+/- 4.79&#37;.
WOS:000708841900001
</snippet>
</document>

<document id="258">
<title>EffCDNet: Transfer learning with deep attention network for change detection in high spatial resolution satellite images</title>
<url>http://dx.doi.org/10.1016/j.dsp.2021.103250</url>
<snippet>High spatial resolution satellite (HRS) images are being extensively utilized for the detection of changes like urban dynamics, infrastructure surveillance, disaster management, and topographic map-making applications. The enormous information and challenging data are important for the change detection (CD) in these images. However, lack of training data and an excessive amount of information, which discourage the researcher from developing a deep-learning-based efficient algorithm for CD. Therefore we want to develop an efficient algorithm in terms of accuracy and speed. Hence, we gain attention on designing an optimized Convolution Neural Network (CNN) while maintaining the speed and segmentation accuracy of the network. We develop an optimized architecture called as EffCDNet which adopts a siamese-based pre-trained encoder with an Attention-based UNet decoder for semantic segmentation. The network is built with pre-trained EfficientNet architecture with shared weights for strong feature extraction and to overcome the limitations caused by insufficient training data. The attention-based UNet decoder uses the attention-gate layer mechanism right before concatenation operation. This obtains more discriminative relevant features for improving the segmentation performance. Also, it is used for the reconstruction of fine-grained feature maps with significant use of context information. For improvement in the change map, we used the Undecimated Discrete Wavelet Transform (UDWT) fusion as a post-processing technique for spatial and temporal analysis of multi-resolution images to obtain a much more enhanced information difference map. The resulting image is less affected by noise, shift-invariable, and overcomes the mixed pixel problem to detect small possible changes. Experimental results on LEVIR-CD, SZATKI AirChange (AC), and Onera Satellite Change Detection (OSCD) benchmark datasets proved that the proposed approach outperforms its superiority in terms of Intersection over Union (IoU) and inference time over the existing methods. (C) 2021 Elsevier Inc. All rights reserved.
WOS:000704035900017
</snippet>
</document>

<document id="259">
<title>Pansharpening via Subpixel Convolutional Residual Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3117944</url>
<snippet>In this article, we propose a new pansharpening architecture called subpixel convolutional residual network to obtain high-resolution multispectral (MS) images. Different from previous works, we extract features from MS images in a low-resolution space and pay more attention to the balance of spectral and spatial information. Our architecture consists of two branches: the feature extraction branch and the residual branch. The former adopts a four-layer convolutional network to extract features, and then upsamples the feature maps using a subpixel convolution layer. For the latter, we combine the nearest neighbor interpolation and guided filter to yield a preliminary image with fundamental spectral and spatial information. With the outputs of the two branches, we can merge them and yield a pansharpened image. The proposed method was compared with several representative methods. The experimental results demonstrate that our method achieves high fusion accuracy while maintaining a good balance between the spectral and the spatial resolution.
WOS:000711641000006
</snippet>
</document>

<document id="260">
<title>DeepAutoMapping: low-cost and real-time geospatial map generation method using deep learning and video streams</title>
<url>http://dx.doi.org/10.1007/s12145-020-00529-7</url>
<snippet>Field data collection and geospatial map generation are critical aspects in different fields such as road asset management, urban planning, and geospatial applications. However, one of the primary impediments to data collection is the availability of spatial and attribute data. This issue is aggravated by the high cost of conventional data collection and data processing methods and by the lack of geospatial data collection policies. This study proposes an inexpensive approach that enables real-time field data observation and geospatial data generation from video streams connected to a laptop and positioning sensors using deep learning technology. This proposed method was evaluated via an application called "DeepAutoMapping", which was built on top of Python, then underwent through two different evaluation scenarios. The results demonstrated that the proposed approach is quick, easy to use and that it provides a high detection accuracy and an acceptable positioning accuracy in the outdoor environment. The proposed solution may also be considered as a pipeline for efficient and economical method of geospatial data collection and auto-map generation in the future.
WOS:000577987000001
</snippet>
</document>

<document id="261">
<title>A small-patched convolutional neural network for mangrove mapping at species level using high-resolution remote-sensing image</title>
<url>http://dx.doi.org/10.1080/19475683.2018.1564791</url>
<snippet>The understanding of mangrove forest structure and dynamics at species level is essential for mangrove conservation and management. To classify mangrove species, remote-sensing technologies provide a better way with high spatial resolution image. The spatial structure is usually viewed as effective complementary information for classification. However, it is still a challenge to design handcrafted features for mangrove species due to their non-structure texture. To leverage the advantage of convolutional neural networks (CNNs) in abstract feature exploration, a small patch-based CNN is proposed to overcome the requirement of fixed and large input which limits the applicability of CNNs to fringe mangrove forests. The function of down-sampling technology was substantially reduced to make deeper network for small input in our work. Meanwhile, the inception structure is used to exploit the multi-scale features of mangrove forests. Furthermore, the network is optimized with lesser convolution kernels and a single fully connected layer to reduce overfitting via reducing the training parameters. Compared to the features of grey level co-occurrence matrix with support vector machine, our proposed CNN shows better performance in classification accuracy. Moreover, the differences between mangrove species can be perceptive via CNN visualization, which offers better understanding of mangrove forests.
WOS:000495707200005
</snippet>
</document>

<document id="262">
<title>A Spectral Feature Based Convolutional Neural Network for Classification of Sea Surface Oil Spill</title>
<url>http://dx.doi.org/10.3390/ijgi8040160</url>
<snippet>Spectral characteristics play an important role in the classification of oil film, but the presence of too many bands can lead to information redundancy and reduced classification accuracy. In this study, a classification model that combines spectral indices-based band selection (SIs) and one-dimensional convolutional neural networks was proposed to realize automatic oil films classification using hyperspectral remote sensing images. Additionally, for comparison, the minimum Redundancy Maximum Relevance (mRMR) was tested for reducing the number of bands. The support vector machine (SVM), random forest (RF), and Hus convolutional neural networks (CNN) were trained and tested. The results show that the accuracy of classifications through the one dimensional convolutional neural network (1D CNN) models surpassed the accuracy of other machine learning algorithms such as SVM and RF. The model of SIs+1D CNN could produce a relatively higher accuracy oil film distribution map within less time than other models.
WOS:000467499300001
</snippet>
</document>

<document id="263">
<title>Machine-learning classification of debris-covered glaciers using a combination of Sentinel-1/-2 (SAR/optical), Landsat 8 (thermal) and digital elevation data</title>
<url>http://dx.doi.org/10.1016/j.geomorph.2020.107365</url>
<snippet>Debris cover on glacier surfaces hampers the accurate detection of debris-covered ice using traditional techniques based on image band ratios. Therefore, this study tests a new automatic classification scheme for hierarchical mapping of glacier surfaces based on machine learning classifiers including k-nearest neighbors (KNN), support vector machine (SVM), gradient boosting (GB), decision tree (DT), random forest (RF) and multi-layer perceptron (MLP). Several raster layer combinations (synthetic aperture radar (SAR) coherence image derived from Sentinel-1 data, visible near-infrared to short wave infrared bands from Sentinel-2, thermal information from Landsat 8 and geomorphometric parameters from the Advanced Land Observing Satellite (ALOS) World 3D 30 m mesh (AW3D30) digital elevation model) were tested to delineate the debris-covered glaciers in the Gilgit-Baltistan, Pakistan and Shaksgam valley, China. The highest over classification accuracy (97&#37;) was obtained using the RF classifier (followed by the GB and SVM with radial basis function kernel) and utilizing all of the multisensor Sentinel/Landsat/ALOS data. Notably, the RF classifier showed to be robust to parameter settings, fast and accurate for mapping debris-covered ice. GB classifier showed similar performance as RF despite it has a moderately lower accuracy compared to RF. Although SVM classifier has a slower in the speed of tuning hyper-parameter, it still performs the third-best classification accuracy. As the multisensory data we used is freely and (near-)globally available, our methodology potentially could be applied for precise delineation of debris-covered glaciers in other areas. (C) 2020 Elsevier B.V. All rights reserved.
WOS:000571831500001
</snippet>
</document>

<document id="264">
<title>U-Net Convolutional Neural Network Model for Deep Red Tide Learning Using GOCI</title>
<url>http://dx.doi.org/10.2112/SI90-038.1</url>
<snippet>GOCI launched in 2010 is a geostationary satellite image sensor that monitors ocean color. It captures 8-band spectral satellite images of northeast Asian regions hourly, eight times a day. The spatial resolution of GOCI is about 500 m. GOCI is capable of monitoring a large ocean area for sensing various events such as red tide occurrences, tidal movement changes and ocean disasters. In this study, we propose a deep convolutional neural network model, U-Net, for automatic pixel-based detection of red tide occurrence from the spectral images captured by GOCI. We construct two training datasets with GOCI images and the corresponding red-tide index maps (RI maps) accumulated through 2011 to 2018. The RI maps indicate where red tides occurred and what kind of red tide species were there. U-Net consists of five U-shaped encoder and decoder layers to extract spectral features relating to red-tide species from GOCI images. We compared the performances of U-Nets trained from two datasets (i) consisting of only four spectral bands and (ii) consisting of all six spectral bands. The RI maps predicted by the trained U-Nets showed considerably matching spatial occurrence tendencies of three red tide species to the ground truths for validation images. The mean target accuracy with the four-band dataset was 13 &#37; lower than that with the six-band dataset. The trained U-Net for pixel-wise red tide detection would be able to effectively inspect red tide occurrences in the huge area of water surrounding the Korean peninsula.
WOS:000485714500039
</snippet>
</document>

<document id="265">
<title>Iterative feature mapping network for detecting multiple changes in multi-source remote sensing images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.09.002</url>
<snippet>Owing to the rapid development of remote sensing technology, various types of data can be easily acquired at present. However, it has become an important but more challenging task for effectively highlighting changes occurring on the land surface from these available data. In this paper, we propose an iterative feature mapping network learning framework for identifying multiple changes with focus on multi-source images, which are often obtained from sensors with different imaging modalities. Firstly, high-level and robust feature representations are extracted from multi-source images via unsupervised feature learning. Then, on this basis, an iterative feature mapping network is established to transform these features into a common high-dimensional feature space. It aims to learn more discriminative features by shrinking the difference between the paired features of unchanged positions while enlarging that of changed ones. Note that the network parameters are learned by optimizing a well-designed objective function, and the whole learning process is fully unsupervised. Finally, based on a hierarchical tree for clustering analysis, all possible change classes can be detected accurately. In addition, the proposed framework is found to be also suitable for change detection in homogeneous images. The impressive experimental results obtained over different types of remote sensing images demonstrate the effectiveness and robustness of the proposed model.
WOS:000453499400004
</snippet>
</document>

<document id="266">
<title>LW-CMDANet: A Novel Attention Network for SAR Automatic Target Recognition</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3195074</url>
<snippet>Deep-learning-based synthetic aperture radar automatic target recognition (SAR-ATR) plays a significant role in the military and civilian fields. However, data limitation and large computational cost are still severe challenges in the actual application of SAR-ATR. To improve the performance of the convolutional neural network (CNN) model with limited data samples in SAR-ATR, this article proposes a novel multidomain feature subspace fusion representation learning method, i.e., a lightweight cascaded multidomain attention network, namely, LW-CMDANet. First, we design a four-layer CNN model to perform hierarchical feature representation learning via the hinge loss function, which can efficiently alleviate the overfitting problem of the CNN model by a nongreedy training style with a small dataset. Then, a cascaded multidomain attention module, based on discrete cosine transform and discrete wavelet transform, is embedded into the previous CNN to further complete the class-specific feature extraction from both the frequency and wavelet transform domains of the input feature maps. Thus, the multidomain attention can enhance the feature extraction ability of previous nongreedy learning manner, to effectively improve the recognition accuracy of the CNN model. Experimental results on small SAR datasets show that our proposed method can achieve better or competitive performance than that of many current existing state-of-the-art methods in terms of recognition accuracy and computational cost.
WOS:000844129000002
</snippet>
</document>

<document id="267">
<title>Pansharpening via Triplet Attention Network With Information Interaction</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3171423</url>
<snippet>Pansharpening aims to obtain high spatial resolution multispectral (MS) images by fusing the spatial and spectral information in low spatial resolution (LR) MS and panchromatic (PAN) images. Recently, deep neural network (DNN) based pansharpening methods have been advanced extensively. Although most DNN-based methods show good performance, it is difficult for them to preserve the spatial details in the fused image. In this article, we propose a new pansharpening method based on a triplet attention network with information interaction to efficiently enhance the spatial and spectral information in the fused image. First, different attention mechanisms are designed to model the spatial and spectral feature properties in LR MS and PAN images. Then, the complementarity among different feature maps is enhanced by information interaction, which promotes the compatibility of features from subnetworks. Finally, we utilize a graph attention module to capture the similarity within feature maps. According to the graph, the informative feature maps are selected to provide more details for the reconstruction of the fused image. Extensive experiments on QuickBird and GeoEye-1 satellite datasets show that the proposed method can produce competitive fused images when compared with some state-of-the-art methods.
WOS:000797454600006
</snippet>
</document>

<document id="268">
<title>Spatial context-aware method for urban land use classification using street view images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.07.020</url>
<snippet>Street view images (SVIs) have great potential for automatic land use classification. Previous studies have paid little attention to the spatial context of SVIs and land parcels, leaving room for improvement in classification accuracy and identification of parcels without SVIs. This study proposes a novel spatial context-aware method for land use classification that synthesizes SVI content and spatial context among SVIs and land parcels through a derived spatial context graph convolution network (SC-GCN). Specifically, the method characterizes the spatial context among SVIs and land parcels into a graph, which formalizes SVIs and land parcels as nodes. The spatial relationships among SVIs and land parcels are represented as graph edges. SC-GCN is designed to model the spatial context of relevant SVIs and land parcels by incorporating heterogeneous structural information into land use classification. Experimental results show that the proposed method outperforms the baseline methods of land use classification at the parcel level and can successfully identify land use types of land parcels without SVIs. Specifically, precision, recall and F1-score values of the proposed method are 72.22&#37;, 64.22&#37; and 68.13&#37;, respectively, which are 2.38&#37;, 12.40&#37; and 13.56&#37; higher than those of the Random Forest method. This work contributes to land use mapping with limited available data by exploring the modeling of complex geospatial relationships, and it serves as a methodological reference for the prediction and supplementation of missing geographic data.
WOS:000844610600001
</snippet>
</document>

<document id="269">
<title>Simulation of urban expansion via integrating artificial neural network with Markov chain - cellular automata</title>
<url>http://dx.doi.org/10.1080/13658816.2019.1600701</url>
<snippet>Accurate simulations and predictions of urban expansion are critical to manage urbanization and explicitly address the spatiotemporal trends and distributions of urban expansion. Cellular Automata integrated Markov Chain (CA-MC) is one of the most frequently used models for this purpose. However, the urban suitability index (USI) map produced from the conventional CA-MC is either affected by human bias or cannot accurately reflect the possible nonlinear relations between driving factors and urban expansion. To overcome these limitations, a machine learning model (Artificial Neural Network, ANN) was integrated with CA-MC instead of the commonly used Analytical Hierarchy Process (AHP) and Logistic Regression (LR) CA-MC models. The ANN was optimized to create the USI map and then integrated with CA-MC to spatially allocate urban expansion cells. The validated results of kappa and fuzzy kappa simulation indicate that ANN-CA-MC outperformed other variously coupled CA-MC modelling approaches. Based on the ANN-CA-MC model, the urban area in South Auckland is predicted to expand to 1340.55 ha in 2026 at the expense of non-urban areas, mostly grassland and open-bare land. Most of the future expansion will take place within the planned new urban growth zone.
WOS:000467186700001
</snippet>
</document>

<document id="270">
<title>CNN-FCM: System modeling promotes stability of deep learning in time series prediction</title>
<url>http://dx.doi.org/10.1016/j.knosys.2020.106081</url>
<snippet>Time series data are usually non-stationary and evolve over time. Even if deep learning has been found effective in dealing with sequential data, the stability of deep neural networks in coping with the situations unseen during the training stage is also important. This paper deals with this problem based on a fuzzy cognitive block (FCB) which embeds the learning of high-order fuzzy cognitive maps into the deep learning architecture. Thereafter, computers can automatically model the complex system that produces the observation rather than simply regress the available data. Respectively, we design a deep neural network termed CNN-FCM which has combined the available convolution network with FCB. To validate the advantages of our design and verify the effectiveness of FCB, twelve benchmark datasets are employed and classic deep learning architectures are introduced as the comparison. The experimental results show that the performance of many current popular deep learning architectures declines when handling data deviated from the training set. FCB plays an important role in promoting the performance of CNN-FCM in the corresponding experiments. Thereafter, we conclude that system modeling can promote the stability of deep learning in time series prediction. (C) 2020 Elsevier B.V. All rights reserved.
WOS:000552126200005
</snippet>
</document>

<document id="271">
<title>Exploring multiscale object-based convolutional neural network (multi-OCNN) for remote sensing image classification at high spatial resolution</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.08.004</url>
<snippet>Convolutional Neural Network (CNN) has been increasingly used for land cover mapping of remotely sensed imagery. However, large-area classification using traditional CNN is computationally expensive and produces coarse maps using a sliding window approach. To address this problem, object-based CNN (OCNN) becomes an alternative solution to improve classification performance. However, previous studies were mainly focused on urban areas or small scenes, and implementation of OCNN method is still needed for large-area classification over heterogeneous landscape. Additionally, the massive labeling of segmented objects requires a practical approach for less computation, including object analysis and multiple CNNs. This study presents a new multiscale OCNN (multi-OCNN) framework for large-scale land cover classification at 1-m resolution over 145,740 km(2). Our approach consists of three main steps: (i) image segmentation, (ii) object analysis with skeleton-based algorithm, and (iii) application of multiple CNNs for final classification. Also, we developed a large benchmark dataset, called IowaNet, with 1 million labeled images and 10 classes. In our approach, multiscale CNNs were trained to capture the best contextual information during the semantic labeling of objects. Meanwhile, skeletonization algorithm provided morphological representation ("medial axis") of objects to support the selection of convolutional locations for CNN predictions. In general, proposed multi-OCNN presented better classification accuracy (overall accuracy similar to 87.2&#37;) compared to traditional patch-based CNN (81.6&#37;) and fixed-input OCNN (82&#37;). In addition, the results showed that this framework is 8.1 and 111.5 times faster than traditional pixel-wise CNN16 or CNN256, respectively. Multiple CNNs and object analysis have proved to be essential for accurate and fast classification. While multi-OCNN produced a high-level of spatial details in the land cover product, misclassification was observed for some classes, such as road versus buildings or shadow versus lake. Despite these minor drawbacks, our results also demonstrated the benefits of IowaNet training dataset in the model performance; overfitting process reduces as the number of samples increases. The limitations of multi-OCNN are partially explained by segmentation quality and limited number of spectral bands in the aerial data. With the advance of deep learning methods, this study supports the claim of multi-OCNN benefits for operational large-scale land cover product at 1-m resolution.
WOS:000567932300005
</snippet>
</document>

<document id="272">
<title>Deep-Learning-Based Precipitation Observation Quality Control</title>
<url>http://dx.doi.org/10.1175/JTECH-D-20-0081.1</url>
<snippet>We present a novel approach for the automated quality control (QC) of precipitation for a sparse station observation network within the complex terrain of British Columbia, Canada. Our QC approach uses convolutional neural networks (CNNs) to classify bad observation values, incorporating a multiclassifier ensemble to achieve better QC performance. We train CNNs using human QCd labels from 2016 to 2017 with gridded precipitation and elevation analyses as inputs. Based on the classification evaluation metrics, our QC approach shows reliable and robust performance across different geographical environments (e.g., coastal and inland mountains), with 0.927 area under curve (AUC) and type I/type II error lower than 15&#37;. Based on the saliency-map-based interpretation studies, we explain the success of CNN-based QC by showing that it can capture the precipitation patterns around, and upstream of the station locations. This automated QC approach is an option for eliminating bad observations for various applications, including the preprocessing of training datasets for machine learning. It can be used in conjunction with human QC to improve upon what could be accomplished with either method alone.
WOS:000660815200011
</snippet>
</document>

<document id="273">
<title>Automatic Sea-Ice Classification of SAR Images Based on Spatial and Temporal Features Learning</title>
<url>http://dx.doi.org/10.1109/TGRS.2020.3049031</url>
<snippet>Sea ice has a significant effect on climate change and ship navigation. Hence, it is crucial to draw sea-ice maps that reflect the geographical distribution of different types of sea ice. Many automatic sea-ice classification methods using synthetic aperture radar (SAR) images are based on the polarimetric characteristics or image texture features of sea ice. They either require professional knowledge to design the parameters and features or are sensitive to noise and condition changes. Moreover, ice changes over time are often ignored. In this article, we propose a new SAR sea-ice image classification method based on a combined learning of spatial and temporal features, derived from residual convolutional neural networks (ResNet) and long short-term memory (LSTM) networks. In this way, we achieve automatic and refined classification of sea-ice types. First, we construct a seven-type ice data set according to the Canadian Ice Service ice charts. We extract spatial feature vectors of a time series of sea-ice samples using a trained ResNet network. Then, using the feature vectors as inputs, the LSTM network further learns the variation of the set of sea-ice samples with time. Finally, the extracted high-level features are fed into a softmax classifier to output the most recent ice type. Taking both spatial features and time variation into consideration, our method can achieve a high classification accuracy of 95.7&#37; for seven ice types. Our method can automatically produce more objective sea-ice interpretation maps, allowing detailed sea-ice distribution and improving the efficiency of sea-ice monitoring tasks.
WOS:000722170500011
</snippet>
</document>

<document id="274">
<title>Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2797894</url>
<snippet>We propose a novel spatiotemporal fusion method based on deep convolutional neural networks (CNNs) under the application background of massive remote sensing data. In the training stage, we build two five-layer CNNs to deal with the problems of complicated correspondence and large spatial resolution gaps between MODIS and Landsat images. Specifically, we first learn a nonlinear mapping CNN between MODIS and low-spatial-resolution (LSR) Landsat images and then learn a super-resolution CNN between LSR Landsat and original Landsat images. In the prediction stage, instead of directly taking the outputs of CNNs as the fusion result, we design a fusion model consisting of high-pass modulation and a weighting strategy to make full use of the information in prior images. Specifically, we firstmap the input MODIS images to transitional images via the learned nonlinear mapping CNN and further improve the transitional images to LSR Landsat images via the fusion model; then, via the learned SR CNN, the LSR Landsat images are supersolved to transitional images, which are further improved to Landsat images via the fusion model. Compared with the previous learning-based fusion methods, mainly referring to the sparse-representation-based methods, our CNNs-based spatiotemporalmethod has the following advantages: 1) automatically extracting effective image features; 2) learning an end-to-end mapping between MODIS and LSR Landsat images; and 3) generating more favorable fusion results. To examine the performance of the proposed fusion method, we conduct experiments on two representative Landsat-MODIS datasets by comparing with the sparse-representation-based spatiotemporal fusion model. The quantitative evaluations on all possibleprediction dates and the comparison of fusion results on one key date in both visual effect and quantitative evaluationsdemonstrate that the proposed method can generate more accurate fusionresults.
WOS:000427425000012
</snippet>
</document>

<document id="275">
<title>Deep Learning Classification for Crop Types in North Dakota</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2990104</url>
<snippet>Recently, agricultural remote sensing community has endeavored to utilize the power of artificial intelligence (AI). One important topic is using AI to make the mapping of crops more accurate, automatic, and rapid. This article proposed a classification workflow using deep neural network (DNN) to produce high-quality in-season crop maps from Landsat imageries for North Dakota. We use historical crop maps from the agricultural department and North Dakota ground measurements as training datasets. Processing workflows are created to automate the tedious preprocessing, training, testing, and postprocessing workflows. We tested this hybrid solution on new images and received accurate results on major crops such as corn, soybean, barley, spring wheat, dry beans, sugar beets, and alfalfa. The pixelwise overall accuracy in all three test regions is over 82&#37; for all land types (including noncrop land), which is the same level of accuracy as the U.S. Department of Agriculture Cropland Data Layer. The texture of DNN maps is more consistent with fewer noises, which is more comfortable to read. We find DNN is better on recognizing big farmlands than recognizing the scattered wetlands and suburban regions in North Dakota. The model trained on multiple scenes of multiple years and months yields higher accuracy than any of the models trained only on a single scene, a single month, or a single year. These results reflect that DNN can produce reliable in-season maps for major crops in North Dakota big farms and could provide a relatively accurate reference for the minor crops in scattered wetland fields.
WOS:000542949600006
</snippet>
</document>

<document id="276">
<title>Automated Geological Landmarks Detection on Mars Using Deep Domain Adaptation From Lunar High-Resolution Satellite Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3156371</url>
<snippet>The diversity in geological characteristics on the planetary surface, such as distribution (density), size, shapes, floor structures, ages, and availability of various input data types such as optical, thermal images, and digital elevation maps pose numerous challenges for detecting geological landmarks (e.g., rockfalls, craters, etc.). Several automatic detection methods are proposed to identify geological landmarks. However, the insufficiency of the labeled dataset is a challenging problem. It requires exceedingly time-consuming and expensive manual annotation. In this article, we use the domain adaptation technique to transfer deep learning from the planetary surface to another (lunar surface into Martian surface). We test the feasibility of transfer learning of the convolutional neural networks in optical images and elevation maps to distinguish landmarks such as rockfalls and craters from the background. The experimental results demonstrate the effectiveness of the proposed method. It achieves high F1-scores compared to the state-of-the-art methods with 58.32 +/- 2.3 and 57.51 +/- 2.4 in detecting rockfall regions in optical lunar and Martian images. It also achieves 65.32 +/- 1.8, 67.39 +/- 2.4, 77.37 +/- 2.2, and 72.56 +/- 2.3 in detecting crater regions in optical images and digital elevation maps of Moon and Mars. This method can be a potential approach to identify landmarks for coming Mars missions.
WOS:000773262200001
</snippet>
</document>

<document id="277">
<title>Simple-Yet-Effective SRTM DEM Improvement Scheme for Dense Urban Cities Using ANN and Remote Sensing Data: Application to Flood Modeling</title>
<url>http://dx.doi.org/10.3390/w12030816</url>
<snippet>Digital elevation models (DEMs) are crucial in flood modeling as DEM data reflects the actual topographic characteristics where water can flow in the model. However, a high-quality DEM is very difficult to acquire as it is very time consuming, costly, and, often restricted. DEM data from a publicly accessible satellite, Shuttle Radar Topography Mission (SRTM), and Sentinel 2 multispectral imagery are selected and used to train the artificial neural network (ANN) to improve the quality of SRTMs DEM. High-quality DEM is used as target data in the training of ANN. The trained ANN will then be ready to efficiently and effectively generate a high-quality DEM, at low cost, for places where ground truth DEM data is not available. In this paper, the performance of the DEM improvement scheme is evaluated over two dense urban cities, Nice (France) and Singapore; with the performance criteria using various matrices, e.g., visual clarity, scatter plots, root mean square error (RMSE) and flood maps. The DEM resulting from the improved SRTM (iSRTM) showed significantly better results than the original SRTM DEM, with about 38&#37; RMSE reduction. Flood maps from iSRTM DEM show much more reasonable flood patterns than SRTM DEMs flood map.
WOS:000529249500201
</snippet>
</document>

<document id="278">
<title>Exploring multiple crowdsourced data to learn deep convolutional neural networks for road extraction*</title>
<url>http://dx.doi.org/10.1016/j.jag.2021.102544</url>
<snippet>Road extraction from high-resolution remote sensing images (HRSIs) is essential for applications in various areas. Although deep convolutional neural networks (DCNNs) have exhibited remarkable success in road extraction, the performance relies on a large amount of training samples which are hard to obtain. To address this issue, multiple crowdsourced data are used in this study, including OpenStreetMap (OSM), Zmap and GPS. And a multi-map integration model (MMIM) is developed to improve the noise robustness of DCNNs for road extraction tasks. Specifically, rich geographical road information are obtained from multiple crowdsourced data, including main roads, new construction roads, midsize and small roads, which can generate complete road training samples and reduce the label noise. Meanwhile, by exploring the true road label information hidden in different crowdsourced data, the MMIM is used to generate high-quality refined labels for learning DCNNs. In this case, the DCNN-based road extraction methods have more opportunities to learn true road distribution and avoid the overfitting problems of label noise. Experiments based on real road extraction dataset indicate that the proposed method shows great performance, and road extraction results are smoother and more complete.
WOS:000704648000004
</snippet>
</document>

<document id="279">
<title>Box-level segmentation supervised deep neural networks for accurate and real-time multispectral pedestrian detection</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.02.005</url>
<snippet>Effective fusion of complementary information captured by multi-modal sensors (visible and infrared cameras) enables robust pedestrian detection under various surveillance situations (e.g., daytime and nighttime). In this paper, we present a novel box-level segmentation supervised learning framework for accurate and real-time multispectral pedestrian detection by incorporating features extracted in visible and infrared channels. Specifically, our method takes pairs of aligned visible and infrared images with easily obtained bounding box annotations as input and estimates accurate prediction maps to highlight the existence of pedestrians. It offers two major advantages over the existing anchor box based multispectral detection methods. Firstly, it overcomes the hyperparameter setting problem occurred during the training phase of anchor box based detectors and can obtain more accurate detection results, especially for small and occluded pedestrian instances. Secondly, it is capable of generating accurate detection results using small-size input images, leading to improvement of computational efficiency for real-time autonomous driving applications. Experimental results on KAIST multi spectral dataset show that our proposed method outperforms state-of-the-art approaches in terms of both accuracy and speed.
WOS:000464088400006
</snippet>
</document>

<document id="280">
<title>Attention-based multi-modal fusion for improved real estate appraisal: a case study in Los Angeles</title>
<url>http://dx.doi.org/10.1007/s11042-019-07895-5</url>
<snippet>The geographical presentation of a house, which refers to the sightseeing and topography near the house, is a critical factor to a house buyer. The street map is a type of common data in our daily life, which contains natural geographical presentation. This paper sources real estate data and corresponding street maps of houses in the city of Los Angeles. In the case study, we proposed an innovative method, attention-based multi-modal fusion, to incorporate the geographical presentation from street maps into the real estate appraisal model with a deep neural network. We firstly combine the house attribute features and street map imagery features by applying the attention-based neural network. After that, we apply boosted regression trees to estimate the house price from the fused features. This work explored the potential of attention mechanism and data fusion in the applications of real estate appraisal. The experimental results indicate the competitiveness of proposed method among state-of-the-art methods.
WOS:000495400000009
</snippet>
</document>

<document id="281">
<title>Unsupervised Feature Learning to Improve Transferability of Landslide Susceptibility Representations</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3006192</url>
<snippet>A landslide susceptibility map (LSM) is of vital importance for risk recognition and prevention. In the last decade, statistical methods have gradually exerted their impact on mapping the landslide susceptibility to locate the high-risk places of landslide. However, due to the complexity of getting full access to the thematic information in large scenarios, most of these statistical methods generally suffer from overfitting, inadequate representative power, and the inability to transfer the learned representation to other places. To solve these challenges, this study designed an unsupervised representation learning module, which features independence, compactness, robustness, and transferability. Specifically, we first stack restricted Boltzmann machines and denoising autoencoder to unsupervised discover the underlying representations embedded in the thematic maps. Then, we applied the transferring strategy in an adversarial manner to generalize the learned representations to the sample-scarce area. Experimental results and analyses using data in different regions have revealed that the proposed method can be generalized well between different LSM scenarios. In terms of precision, it outperforms other methods by a large margin, e.g., by around 7&#37; compared to multilayer perceptrons with the same configuration, and by 3&#37;-4&#37; to the state of art algorithm random forest. Besides, compared to other methods, the landslide susceptibility map that is predicted by the proposed method featuring smoothness and stableness seems more reliable, and is more according to some prior knowledge that, for example, distance to the drainage, slope, and stratum, should exert dominant effects on the occurrence of a landslide.
WOS:000550641600004
</snippet>
</document>

<document id="282">
<title>A novel Structure from Motion-based approach to underwater pile field documentation</title>
<url>http://dx.doi.org/10.1016/j.jasrep.2021.103120</url>
<snippet>This article presents a novel methodology to the underwater documentation of pile fields in archaeological lakeside settlement sites using Structure from Motion (SfM). Mapping the piles of such sites is an indispensable basis to the exploitation of the high resolution absolute chronological data gained through dendrochronology. In a case study at the underwater site of Plo.ca, Mi.cov Grad at Lake Ohrid, North Macedonia, nine consecutive 10 m(2) strips and a 6 m(2) excavation section were uncovered, the situation documented, and the wood piles sampled. The gained data was vectorized in a geographic information system. During two field campaigns, a total of 794 wooden elements on a surface of 96 m(2) could be documented three-dimensionally with a residual error of less than 2 cm. The exceptionally high number of fishes in the 5 m deep water resulted in a significant covering of potentially important information on the relevant photos. We present a machine learning approach, especially developed and successfully applied to the automatic detection and masking of these fishes in order to eliminate them from the images. The discussed documentation workflow enables an efficient, cost-effective, accurate and reproducible mapping of pile fields. So far, no other method applied to the recording of pile fields has allowed for a comparably high resolution of spatial information.
WOS:000697557400001
</snippet>
</document>

<document id="283">
<title>A Novel CNN-Based CAD System for Early Assessment of Transplanted Kidney Dysfunction</title>
<url>http://dx.doi.org/10.1038/s41598-019-42431-3</url>
<snippet>This paper introduces a deep-learning based computer-aided diagnostic (CAD) system for the early detection of acute renal transplant rejection. For noninvasive detection of kidney rejection at an early stage, the proposed CAD system is based on the fusion of both imaging markers and clinical biomarkers. The former are derived from diffusion-weighted magnetic resonance imaging (DW-MRI) by estimating the apparent diffusion coefficients (ADC) representing the perfusion of the blood and the diffusion of the water inside the transplanted kidney. The clinical biomarkers, namely: creatinine clearance (CrCl) and serum plasma creatinine (SPCr), are integrated into the proposed CAD system as kidney functionality indexes to enhance its diagnostic performance. The ADC maps are estimated for a user-defined region of interest (ROI) that encompasses the whole kidney. The estimated ADCs are fused with the clinical biomarkers and the fused data is then used as an input to train and test a convolutional neural network (CNN) based classifier. The CAD system is tested on DW-MRI scans collected from 56 subjects from geographically diverse populations and different scanner types/image collection protocols. The overall accuracy of the proposed system is 92.9&#37; with 93.3&#37; sensitivity and 92.3&#37; specificity in distinguishing non-rejected kidney transplants from rejected ones. These results demonstrate the potential of the proposed system for a reliable non-invasive diagnosis of renal transplant status for any DW-MRI scans, regardless of the geographical differences and/or imaging protocol.
WOS:000464094200055
</snippet>
</document>

<document id="284">
<title>Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies-Part 2: Recommendations and Best Practices</title>
<url>http://dx.doi.org/10.3390/rs13132591</url>
<snippet>Convolutional neural network (CNN)-based deep learning (DL) has a wide variety of applications in the geospatial and remote sensing (RS) sciences, and consequently has been a focus of many recent studies. However, a review of accuracy assessment methods used in recently published RS DL studies, focusing on scene classification, object detection, semantic segmentation, and instance segmentation, indicates that RS DL papers appear to follow an accuracy assessment approach that diverges from that of traditional RS studies. Papers reporting on RS DL studies have largely abandoned traditional RS accuracy assessment terminology; they rarely reported a complete confusion matrix; and sampling designs and analysis protocols generally did not provide a population-based confusion matrix, in which the table entries are estimates of the probabilities of occurrence of the mapped landscape. These issues indicate the need for the RS community to develop guidance on best practices for accuracy assessment for CNN-based DL thematic mapping and object detection. As a first step in that process, we explore key issues, including the observation that accuracy assessments should not be biased by the CNN-based training and inference processes that rely on image chips. Furthermore, accuracy assessments should be consistent with prior recommendations and standards in the field, should support the estimation of a population confusion matrix, and should allow for assessment of model generalization. This paper draws from our review of the RS DL literature and the rich record of traditional remote sensing accuracy assessment research while considering the unique nature of CNN-based deep learning to propose accuracy assessment best practices that use appropriate sampling methods, training and validation data partitioning, assessment metrics, and reporting standards.
WOS:000671387200001
</snippet>
</document>

<document id="285">
<title>Integrating Google Earth imagery with Landsat data to improve 30-m resolution land cover mapping</title>
<url>http://dx.doi.org/10.1016/j.rse.2019.111563</url>
<snippet>Land use and land cover maps provide fundamental information that has been used in different kinds of studies, ranging from climate change to city planning. However, despite substantial efforts in recent decades, large-scale 30-m land cover maps still suffer from relatively low accuracy in terms of land cover type discrimination (especially for the vegetation and impervious types), due to limits in relation to the data, method, and design of the workflow. In this work, we improved the land cover classification accuracy by integrating free and public high-resolution Google Earth images (HR-GEI) with Landsat Operational Land Imager (OLI) and Enhanced Thematic Mapper Plus (ETM+) imagery. Our major innovation is a hybrid approach that includes three major components: (1) a deep convolutional neural network (CNN)-based classifier that extracts high-resolution features from Google Earth imagery; (2) traditional machine learning classifiers (i.e., Random Forest (RF) and Support Vector Machine (SVM)) that are based on spectral features extracted from 30-m Landsat data; and (3) an ensemble decision maker that takes all different features into account. Experimental results show that our proposed method achieves a classification accuracy of 84.40&#37; on the entire validation dataset in China, improving the previous state-of-the-art accuracies obtained by RF and SVM by 4.50&#37; and 4.20&#37;, respectively. Moreover, our proposed method reduces misclassifications between certain vegetation types, and improves identification of the impervious type. Evaluation applied over an area of around 14,000 km(2) confirms little improvement for land cover types (e.g., forest) of which the classification accuracies are already over 80&#37; when using traditional machine learning approaches, yet improvements in accuracy of 7&#37; for cropland and shrubland, 9&#37; for grassland, 23&#37; for impervious and 25&#37; for wetlands were achieved when compared with traditional machine learning approaches. The results demonstrate the great potential of integrating features of datasets at different resolutions and the possibility to produce more reliable land cover maps.
WOS:000509819300035
</snippet>
</document>

<document id="286">
<title>Comparing Fully Deep Convolutional Neural Networks for Land Cover Classification with High-Spatial-Resolution Gaofen-2 Images</title>
<url>http://dx.doi.org/10.3390/ijgi9080478</url>
<snippet>Land cover is an important variable of the terrestrial ecosystem that provides information for natural resources management, urban sprawl detection, and environment research. To classify land cover with high-spatial-resolution multispectral remote sensing imagery is a difficult problem due to heterogeneous spectral values of the same object on the ground. Fully convolutional networks (FCNs) are a state-of-the-art method that has been increasingly used in image segmentation and classification. However, a systematic quantitative comparison of FCNs on high-spatial-multispectral remote imagery was not yet performed. In this paper, we adopted the three FCNs (FCN-8s, Segnet, and Unet) for Gaofen-2 (GF2) satellite imagery classification. Two scenes of GF2 with a total of 3329 polygon samples were used in the study area and a systematic quantitative comparison of FCNs was conducted with red, green, blue (RGB) and RGB+near infrared (NIR) inputs for GF2 satellite imagery. The results showed that: (1) The FCN methods perform well in land cover classification with GF2 imagery, and yet, different FCNs architectures exhibited different results in mapping accuracy. The FCN-8s model performed best among the Segnet and Unet architectures due to the multiscale feature channels in the upsampling stage. Averaged across the models, the overall accuracy (OA) andKappacoefficient (Kappa) were 5&#37; and 0.06 higher, respectively, in FCN-8s when compared with the other two models. (2) High-spatial-resolution remote sensing imagery with RGB+NIR bands performed better than RGB input at mapping land cover, and yet the advantage was limited; theOAandKappaonly increased an average of 0.4&#37; and 0.01 in the RGB+NIR bands. (3) The GF2 imagery provided an encouraging result in estimating land cover based on the FCN-8s method, which can be exploited for large-scale land cover mapping in the future.
WOS:000565153800001
</snippet>
</document>

<document id="287">
<title>Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/rs8040329</url>
<snippet>The availability of high-resolution remote sensing (HRRS) data has opened up the possibility for new interesting applications, such as per-pixel classification of individual objects in greater detail. This paper shows how a convolutional neural network (CNN) can be applied to multispectral orthoimagery and a digital surface model (DSM) of a small city for a full, fast and accurate per-pixel classification. The predicted low-level pixel classes are then used to improve the high-level segmentation. Various design choices of the CNN architecture are evaluated and analyzed. The investigated land area is fully manually labeled into five categories (vegetation, ground, roads, buildings and water), and the classification accuracy is compared to other per-pixel classification works on other land areas that have a similar choice of categories. The results of the full classification and segmentation on selected segments of the map show that CNNs are a viable tool for solving both the segmentation and object recognition task for remote sensing data.
WOS:000375156500062
</snippet>
</document>

<document id="288">
<title>Nonparametric machine learning for mapping forest cover and exploring influential factors</title>
<url>http://dx.doi.org/10.1007/s10980-020-01046-0</url>
<snippet>Context The contribution of forest ecosystem services to human well-being varies over space following the dynamics in forest cover. Use of machine learning models is increasing in projecting forest cover changes and investigating the drivers, yet references are still lacking for selecting machine learning models for spatial projection of forest cover patterns. Objectives We assessed the ability of nonparametric machine learning techniques to project the spatial distribution of forest cover and identify its drivers using a case study of Tasmania, Australia. Methods We developed, evaluated, and compared the performance of four nonparametric machine learning models: support vector regression (SVR), artificial neural networks (ANN), random forest (RF), and gradient boosted regression trees (GBRT). Results The results demonstrated that RF far outperformed the other three models in both fitting and projection accuracy, and required less computional costs. GBRT outperformed SVR and ANN in projection accuracy. However, RF exhibited serious overfitting due to the full growth of its decision trees. The influence rankings of explanatory variables on spatial patterns of forest cover were different under the four models. Land tenure type and rainfall were identified among the top four most influential variables by all four models. The ranking produced by the RF model was significantly different with topographic factors associated with land clearing and production costs (elevation and distance to timber facilities) being the two most influential variables. Conclusions We encourage practitioners to consider nonparametric machine learning methods, especially RF, when facing problems of complex environmental data modelling.
WOS:000537990300001
</snippet>
</document>

<document id="289">
<title>Evaluation of different DEMs for gully erosion susceptibility mapping using in-situ field measurement and validation</title>
<url>http://dx.doi.org/10.1016/j.ecoinf.2021.101425</url>
<snippet>The spatial variability in any kind of geomorphic studies based on terrain attributes are the most important issues. This terrain attributes and their respective characteristics play a significant role in the formation and expansion of ephemeral gullies. Therefore, nowadays, the accuracy of terrain based geomorphic studies has been mostly dependent on the resolution and quality of the DEM (digital elevation model) data. As the rate of erosional power of water flow is dependent on the terrain characteristics, therefore the extraction of several terrain features from DEM data is necessary in the study of gully erosion. This case study investigates the scaledependence of DEM-derived terrain factors in gully erosion susceptibility (GES). This work on Garhbeta-I C.D. Block has focused on the comparison among the predicted GES maps through five types of DEM i.e. Shuttle Radar Topography Mission (SRTM), Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), Cartosat-1, ALOS World 3D-30 m (AW3D30) and Advanced Land Observation satellite (ALOS) coupled with the machine learning modelling approach of artificial neural network (ANN), convolution neural network (CNN) and deep learning neural network (DLNN) algorithm. A total of sixteen conditioning factors were chosen for GES assessment based on the topographical, hydro-climatological conditions and multi-collinearity analysis. Here, importance variables are measured by mean decrease accuracy (MDA) method of random forest (RF) algorithm and the result is shown that elevation is the most important factor for gully occurrences. Validation result of receiver operating characteristics-area under curve (ROC-AUC) has been indicates that DLNN model in ALOS DEM (AUC = 0.958) gives the most optimal accuracy in GES assessment. The output maps can assist in identifying gully-prone risk areas, and several suitable with sustainable managements should be taken for conservation accordingly.
WOS:000704529300003
</snippet>
</document>

<document id="290">
<title>Artificial intelligence insights into hippocampal processing</title>
<url>http://dx.doi.org/10.3389/fncom.2022.1044659</url>
<snippet>Advances in artificial intelligence, machine learning, and deep neural networks have led to new discoveries in human and animal learning and intelligence. A recent artificial intelligence agent in the DeepMind family, muZero, can complete a variety of tasks with limited information about the world in which it is operating and with high uncertainty about features of current and future space. To perform, muZero uses only three functions that are general yet specific enough to allow learning across a variety of tasks without overgeneralization across different contexts. Similarly, humans and animals are able to learn and improve in complex environments while transferring learning from other contexts and without overgeneralizing. In particular, the mammalian extrahippocampal system (eHPCS) can guide spatial decision making while simultaneously encoding and processing spatial and contextual information. Like muZero, the eHPCS is also able to adjust contextual representations depending on the degree and significance of environmental changes and environmental cues. In this opinion, we will argue that the muZero functions parallel those of the hippocampal system. We will show that the different components of the muZero model provide a framework for thinking about generalizable learning in the eHPCS, and that the evaluation of how transitions in cell representations occur between similar and distinct contexts can be informed by advances in artificial intelligence agents such as muZero. We additionally explain how advances in AI agents will provide frameworks and predictions by which to investigate the expected link between state changes and neuronal firing. Specifically, we will discuss testable predictions about the eHPCS, including the functions of replay and remapping, informed by the mechanisms behind muZero learning. We conclude with additional ways in which agents such as muZero can aid in illuminating prospective questions about neural functioning, as well as how these agents may shed light on potential expected answers.
WOS:000888323700001
</snippet>
</document>

<document id="291">
<title>Unsupervised Change Detection by Cross-Resolution Difference Learning</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3079907</url>
<snippet>Change detection (CD) aims to identify the differences between multitemporal images acquired over the same geographical area at different times. With the advantages of requiring no cumbersome labeled change information, unsupervised CD has attracted extensive attention of researchers. Multitemporal images tend to have different resolutions as they are usually captured at different times with different sensor properties. It is difficult to directly obtain one pixelwise change map for two images with different resolutions, so current methods usually resize multitemporal images to a unified size. However, resizing operations change the original information of pixels, which limits the final CD performance. This article aims to detect changes from multitemporal images in the originally different resolutions without resizing operations. To achieve this, a cross-resolution difference learning method is proposed. Specifically, two cross-resolution pixelwise difference maps are generated for the two different resolution images and fused to produce the final change map. First, the two input images are segmented into individual homogeneous regions separately due to different resolutions. Second, each pixelwise difference map is produced according to two measure distances, the mutual information distance and the deep feature distance, between image regions in which the pixel lies. Third, the final binary change map is generated by fusing and binarizing the two cross-resolution difference maps. Extensive experiments on four datasets demonstrate the effectiveness of the proposed method for detecting changes from different resolution images.
WOS:000732756300001
</snippet>
</document>

<document id="292">
<title>Measuring River Wetted Width From Remotely Sensed Imagery at the Subpixel Scale With a Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1029/2018WR024136</url>
<snippet>River wetted width (RWW) is an important variable in the study of river hydrological and biogeochemical processes. Presently, RWW is often measured from remotely sensed imagery, and the accuracy of RWW estimation is typically low when coarse spatial resolution imagery is used because river boundaries often run through pixels that represent a region that is a mixture of water and land. Thus, when conventional hard classification methods are used in the estimation of RWW, the mixed pixel problem can become a large source of error. To address this problem, this paper proposes a novel approach to measure RWW at the subpixel scale. Spectral unmixing is first applied to the imagery to obtain a water fraction image that indicates the proportional coverage of water in image pixels. A fine spatial resolution river map from which RWW may be estimated is then produced from the water fraction image by superresolution mapping (SRM). In the SRM analysis, a deep convolutional neural network is used to eliminate the negative effects of water fraction errors and reconstruct the geographical distribution of water. The proposed approach is assessed in two experiments, with the results demonstrating that the convolutional neural network-based SRM model can effectively estimate subpixel scale details of rivers and that the accuracy of RWW estimation is substantially higher than that obtained from the use of a conventional hard image classification. The improvement shows that the proposed method has great potential to derive more accurate RWW values from remotely sensed imagery.
WOS:000481444700026
</snippet>
</document>

<document id="293">
<title>MSNet: multispectral semantic segmentation network for remote sensing images</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2101728</url>
<snippet>In the research of automatic interpretation of remote sensing images, semantic segmentation based on deep convolutional neural networks has been rapidly developed and applied, and the feature segmentation accuracy and network model generalization ability have been gradually improved. However, most of the network designs are mainly oriented to the three visible RGB bands of remote sensing images, aiming to be able to directly borrow the mature natural image semantic segmentation networks and pre-trained models, but simultaneously causing the waste and loss of spectral information in the invisible light bands such as near-infrared (NIR) of remote sensing images. Combining the advantages of multispectral data in distinguishing typical features such as water and vegetation, we propose a novel deep neural network structure called the multispectral semantic segmentation network (MSNet) for semantic segmentation of multi-classified feature scenes. The multispectral remote sensing image bands are split into two groups, visible and invisible, and ResNet-50 is used for feature extraction in both coding stages, and cascaded upsampling is used to recover feature map resolution in the decoding stage, and the multi-scale image features and spectral features from the upsampling process are fused layer by layer using the feature pyramid structure to finally obtain semantic segmentation results. The training and validation results on two publicly available datasets show that MSNet has competitive performance. The code is available: https://github.com/taochx/MSNet.
WOS:000836459800001
</snippet>
</document>

<document id="294">
<title>Fully Convolutional Neural Network for Impervious Surface Segmentation in Mixed Urban Environment</title>
<url>http://dx.doi.org/10.14358/PERS.87.2.117</url>
<snippet>The urgency of creating appropriate, high-resolution data products such as impervious cover information has increased as cities face rapid growth as well as climate change and other environmental challenges. This work explores the use of fully convolutional neural networks (FCNNs)-specifically UNet with a ResNet-152 encoder-in mapping impervious surfaces at the pixel level from World View-2 in a mixed urban/residential environment. We investigate three-, four-, and eight-band multispectral inputs to the FCNN. Resulting maps are promising in both qualitative and quantitative assessment when compared to automated land use/land cover products. Accuracy was assessed by F1 and average precision (AP) scores, as well as receiver operating characteristic curves, with area under the curve (AUC) used as an additional accuracy metric. The four-band model shows the highest average test-set accuracies (F1, Al, and AUC of 0.709, 0.82, and 0.807, respectively), with higher AP and AUG than the automated land use/land cover products, indicating the utility of the blue-green-red-infrared channels for the FCNN. Improved performance was seen in residential areas, with worse performance in more densely developed areas.
WOS:000613182400007
</snippet>
</document>

<document id="295">
<title>Evaluation of the Potential of Convolutional Neural Networks and Random Forests for Multi-Class Segmentation of Sentinel-2 Imagery</title>
<url>http://dx.doi.org/10.3390/rs11080907</url>
<snippet>Motivated by the increasing availability of open and free Earth observation data through the Copernicus Sentinel missions, this study investigates the capacity of advanced computational models to automatically generate thematic layers, which in turn contribute to and facilitate the creation of land cover products. In concrete terms, we assess the practical and computational aspects of multi-class Sentinel-2 image segmentation based on a convolutional neural network and random forest approaches. The annotated learning set derives from data that is made available as result of the implementation of European Unions INSPIRE Directive. Since this network of data sets remains incomplete in regard to some geographic areas, another objective of this work was to provide consistent and reproducible ways for machine-driven mapping of these gaps and a potential update of the existing ones. Finally, the performance analysis identifies the most important hyper-parameters, and provides hints on the models deployment and their transferability.
WOS:000467646800017
</snippet>
</document>

<document id="296">
<title>MSFFA: a multi-scale feature fusion and attention mechanism network for crowd counting</title>
<url>http://dx.doi.org/10.1007/s00371-021-02383-0</url>
<snippet>Crowd counting has been a growing hot topic in the computer vision community in recent years due to its extensive applications in the fields of public safety and commercial planning. However, up to now, it has been still a challenging task in realistic scenes owing to large-scale variations and complex background interference. In this paper, we have proposed an efficient end-to-end Multi-Scale Feature Fusion and Attention mechanism CNN network, named as MSFFA. The presented network consists of three parts: the front-end of the low-level feature extractor, the mid-end of the multi-scale feature fusion operator and the back-end of the density map generator. Among them, most significantly, in the mid-end, we stack three MSFF blocks with the residual connection, which on the one hand, makes the network obtain large-scale continuous variations and on the other hand, enhances the information transmission. Meanwhile, a global attention mechanism module is employed to extract effective features in complex background scenes. Our method has been evaluated on three public datasets, including ShanghaiTech, UCF-QNRF and UCF_CC_50. Experimental results show that our method outperforms some existing advanced approaches, indicating its excellent accuracy and stability.
WOS:000741931800001
</snippet>
</document>

<document id="297">
<title>Land cover dynamic change in the Napahai Basin using the optimized random forest model</title>
<url>http://dx.doi.org/10.1117/1.JRS.13.044518</url>
<snippet>Based on the optimized random forest (ORF) model proposed in our research, data of the Landsat-OLI, Landsat-TM images, and digital elevation model were used by us to obtain the land cover maps during several periods in the Napahai Basin. Model performance testing results show that producer accuracy and kappa coefficient of the ORF approach are 0.916 and 0.903, which are higher than the traditional maximum likelihood method. Furthermore, we analyzed the characteristic of land cover dynamic change and seasonal variation of wetlands landscape in 2002 and 2017, and revealed their driving mechanisms and ecological response process. The conclusions are as follows: (1) Lake area expanded in the past 15 years because of increased glacial runoff generated from global warming. Construction land and farmland replaced wetland areas in the middle and downstream regions. Moreover, due to the limitation of hydrothermal conditions in the canyon, few forests in the northeastern basin transferred to shrubs. (2) Dam, which was constructed in northern Napahai lake, controls water storage and made the lake area stable after 2013, so, area change percentage in 2017 from wet to dry season is lower than 2002. The wetland region in the central and downstream basin was synthetically affected by climate change and drainage projects, so its environment condition becomes dryer, and vegetation communities succession shows a reversed process. (3) Ecological problems in the Napahai Basin were mainly reflected at aspects of agricultural nonpoint source pollution, overgrazing, and urbanization. These factors affected lake, soil, and groundwater in the Napahai Basin and destroyed the ecological health of wetland landscapes. Therefore, in order to realize sustainable development environmental resources, local government should propose the effective policies to restore the degraded ecosystem and protect natural wetland. (C) 2019 Society of Photo Optical Instrumentation Engineers (SPIE)
WOS:000506174600001
</snippet>
</document>

<document id="298">
<title>Attribute-Guided Multi-Scale Prototypical Network for Few-Shot SAR Target Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3126688</url>
<snippet>Few-shot synthetic aperture radar (SAR) target classification has received more and more attention in recent years, where most of the existing methods have applied off-the-shelf networks designed for natural images to SAR images, ignoring the special characteristics of SAR data. Therefore, in this article, we propose an attribute-guided multi-scale prototypical network (AG-MsPN) combined with subband decomposition for few-shot SAR target classification, aiming to learn more discriminative features from a few labeled data. Since the SAR images are essentially complex-valued images containing both amplitude and phase information, we implement the subband decomposition of complex-valued SAR images to explore the backscattering variations of targets, thus obtaining more complete descriptions of targets. Then, considering the complementary features extracted by different convolutional layers, based on the prototypical network, a multi-scale prototypical network (MsPN) is proposed to fuse the features of different layers to enhance the discrimination of feature representations, thus relieving the problem of high intra-class diversity and inter-class similarity for the images of SAR targets. Besides, we devise the prior binary attributes of SAR targets and add an extra attribute classification module (ACM) into the MsPN to map the images into the attribute space for classification. During the training phase, the proposed MsPN and the ACM are jointly utilized to realize the target classification in both the feature space and the attribute space, and meanwhile, the model parameters are optimized by the joint loss. Thus, the classification performance of the MsPN is further enhanced under the joint supervision of class label information from a few labeled data and the target attribute information from the prior knowledge. Therefore, we name the proposed method the AG-MsPN. We demonstrate the effectiveness of our proposed AG-MsPN on the Moving and Stationary Target Acquisition and Recognition benchmark dataset, and our method surpasses many other existing methods in the few-shot cases.
WOS:000728924400004
</snippet>
</document>

<document id="299">
<title>A geographic information-driven method and a new large scale dataset for remote sensing cloud/snow detection</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.01.023</url>
<snippet>Geographic information such as the altitude, latitude, and longitude are common but fundamental meta-records in remote sensing image products. In this paper, it is shown that such a group of records provides important priors for cloud and snow detection in remote sensing imagery. The intuition comes from some common geographical knowledge, where many of them are important but are often overlooked. For example, it is generally known that snow is less likely to exist in low-latitude or low-altitude areas, and clouds in different geographic may have various visual appearances. Previous cloud and snow detection methods simply ignore the use of such information, and perform detection solely based on the image data (band reflectance). Due to the neglect of such priors, most of these methods are difficult to obtain satisfactory performance in complex scenarios (e.g., cloud-snow coexistence). In this paper, a novel neural network called "Geographic Information-driven Network (GeoInfoNet)" is proposed for cloud and snow detection. In addition to the use of the image data, the model integrates the geographic information at both training and detection phases. A "geographic information encoder" is specially designed, which encodes the altitude, latitude, and longitude of imagery to a set of auxiliary maps and then feeds them to the detection network. The proposed network can be trained in an end-to-end fashion with dense robust features extracted and fused. A new dataset called "Levir_CS" for cloud and snow detection is built, which contains 4,168 Gaofen-1 satellite images and corresponding geographical records, and is over 20x larger than other datasets in this field. On "Levir_CS", experiments show that the method achieves 90.74&#37; intersection over union of cloud and 78.26&#37; intersection over union of snow. It outperforms other state of the art cloud and snow detection methods with a large margin. Feature visualizations also show that the method learns some important priors which is close to the common sense. The proposed dataset and the code of GeoInfoNet are available in https://github.com/permanentCH5/GeoInfoNet.
WOS:000640987800007
</snippet>
</document>

<document id="300">
<title>Missing Data Reconstruction in Remote Sensing Image With a Unified Spatial-Temporal-Spectral Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/TGRS.2018.2810208</url>
<snippet>Because of the internal malfunction of satellite sensors and poor atmospheric conditions such as thick cloud, the acquired remote sensing data often suffer from missing information, i.e., the data usability is greatly reduced. In this paper, a novel method of missing information reconstruction in remote sensing images is proposed. The unified spatial-temporal-spectral framework based on a deep convolutional neural network (CNN) employs a unified deep CNN combined with spatial-temporal-spectral supplementary information. In addition, to address the fact that most methods can only deal with a single missing information reconstruction task, the proposed approach can solve three typical missing information reconstruction tasks: 1) dead lines in Aqua Moderate Resolution Imaging Spectroradiometer hand 6; 2) the Landsat Enhanced Thematic Mapper Plus scan line corrector-off problem; and 3) thick cloud removal. It should he noted that the proposed model can use multisource data (spatial, spectral, and temporal) as the input of the unified framework. The results of both simulated and real-data experiments demonstrate that the proposed model exhibits high effectiveness in the three missing information reconstruction tasks listed above.
WOS:000439980200003
</snippet>
</document>

<document id="301">
<title>L-GAN: landmark-based generative adversarial network for efficient face de-identification</title>
<url>http://dx.doi.org/10.1007/s11227-022-04954-x</url>
<snippet>A large amount of high-quality data are collected through autonomous vehicles, CCTVs, guidance service robots, and web map services (Google Street View). However, the data collected through them include personal information such as peoples faces and vehicle license plates. Currently, personal information contained in data is de-identified using methods such as face blur, pixelation, and masking. Consequently, it loses value as data for artificial intelligence (AI) learning. Therefore, in this study, we propose a model to generate a fake face that maintains the basic structure of the human face. There are several methods for generating faces. One is to generate them using a generative adversarial network (GAN) model. The GAN is an AI algorithm used for unsupervised learning and is implemented by a system in which two neural networks compete. However, because GAN operates as an input of a random noise vector, it is difficult to obtain results for the desired face angle and shape. Therefore, pre- and post-processing is required to generate a fake face that maintains the basic structure and angles; however, the calculation is complicated, and it is difficult to generate a natural image. To solve this problem, we propose a method for generating a fake face that maintains the basic structure and angle of the real face by applying a facial landmark. Using the proposed method, it was possible to generate a fake face with a different impression while maintaining the basic structure and angle of the face.
WOS:000886899600002
</snippet>
</document>

<document id="302">
<title>Object Semantic Segmentation in Point Clouds-Comparison of a Deep Learning and a Knowledge-Based Method</title>
<url>http://dx.doi.org/10.3390/ijgi10040256</url>
<snippet>Through the power of new sensing technologies, we are increasingly digitizing the real world. However, instruments produce unstructured data, mainly in the form of point clouds for 3D data and images for 2D data. Nevertheless, many applications (such as navigation, survey, infrastructure analysis) need structured data containing objects and their geometry. Various computer vision approaches have thus been developed to structure the data and identify objects therein. They can be separated into model-driven, data-driven, and knowledge-based approaches. Model-driven approaches mainly use the information on the objects contained in the data and are thus limited to objects and context. Among data-driven approaches, we increasingly find deep learning strategies because of their autonomy in detecting objects. They identify reliable patterns in the data and connect these to the object of interest. Deep learning approaches have to learn these patterns in a training stage. Knowledge-based approaches use characteristic knowledge from different domains allowing the detection and classification of objects. The knowledge must be formalized and substitutes the training for deep learning. Semantic web technologies allow the management of such human knowledge. Deep learning and knowledge-based approaches have already shown good results for semantic segmentation in various examples. The common goal but the different strategies of the two approaches engaged our interest in doing a comparison to get an idea of their strengths and weaknesses. To fill this knowledge gap, we applied two implementations of such approaches to a mobile mapping point cloud. The detected object categories are car, bush, tree, ground, streetlight and building. The deep learning approach uses a convolutional neural network, whereas the knowledge-based approach uses standard semantic web technologies such as SPARQL and OWL2to guide the data processing and the subsequent classification as well. The LiDAR point cloud used was acquired by a mobile mapping system in an urban environment and presents various complex scenes, allowing us to show the advantages and disadvantages of these two types of approaches. The deep learning and knowledge-based approaches produce a semantic segmentation with an average F1 score of 0.66 and 0.78, respectively. Further details are given by analyzing individual object categories allowing us to characterize specific properties of both types of approaches.
WOS:000643095200001
</snippet>
</document>

<document id="303">
<title>Two-pathway anti-interference neural network based on the retinal perception mechanism for classification of remote sensing images from unmanned aerial vehicles</title>
<url>http://dx.doi.org/10.1117/1.JRS.14.026511</url>
<snippet>The ultrahigh resolution of unmanned aerial vehicle (UAV) remote sensing images and tilting photography with multiple perspectives provide complete and detailed ground observation data for various engineering applications. However, noise and interference information make learning the typical features of ground objects difficult for current deep learning semantic segmentation networks. The hierarchical cognitive structure of human vision and the information transmission modes of retinal cone and rod cells were used to design a two-pathway anti-interference network for retinal perception mechanism simulation (RPMS). In the first pathway, the hierarchical cognition of cone cells was simulated by a one-to-one connected multiscale dilated convolution structure. In the second pathway, the hierarchical cognition of rod cells was simulated by a multiscale pyramid structure with many-to-one connections. With the one-to-one connection, the ability of RPMS to recognize detailed edges was strengthened. Furthermore, the many-to-one connection helped RPMS resist the disturbance from noise and interference. By combining the feature maps of the two paths, RPMS exhibited stronger noise resistance, better texture recognition, and better detail recognition compared with other semantic segmentation networks in the classification experiments. Thus this technique is suitable for UAV remote sensing image classification and has a broad application potential. (C) 2020 Society of Photo-Optical Instrumentation Engineers (SPIE)
WOS:000531827500001
</snippet>
</document>

<document id="304">
<title>An Uncertainty Aware Method for Geographic Data Conflation</title>
<url>http://dx.doi.org/10.1145/3282834.3282842</url>
<snippet>With a significant amount of spatial data archives online, data conflation is becoming more and more critical in the domain of Geographical Information Science (GIScience) because of its broad applications such as detecting the development of road networks and the change of river course. Existing conflation approaches usually rely on the vector data of corresponding features in multiple sources to have an approximate location. However, they commonly overlook the uncertainty produced during the vector data generation process in the data sources. In previous work, we presented a Convolutional Neural Networks (CNN) recognition system that automatically recognizes areas of geographic features from maps and then generates a centerline representation of the area feature (e.g., from pixels of road areas to a road network). In this paper, we propose a method to systematically quantify the uncertainty generated by an image recognition model and the centerline extraction process. We provide an end-to-end evaluation method that exploits the distance map to calculate the uncertainty value for centerline extraction. Compared with methods that do not consider uncertainty value, our algorithm avoids using a fixed buffer size to identify corresponding features from multiple sources and generate accurate conflation results.
WOS:000471043000004
</snippet>
</document>

<document id="305">
<title>Landslide Detection Using Densely Connected Convolutional Networks and Environmental Conditions</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3079196</url>
<snippet>A complete and accurate landslide map is necessary for landslide susceptibility and risk assessment. Currently, deep learning faces the dilemma of insufficient application, scarce samples, and poor efficiency in landslide recognition. This article utilizes the advantages of dense convolutional networks (DenseNets) and their modified technique to solve the three proposed problems. For this purpose, we created a new landslide sample library. On the original remote sensing image, 12 geological, topographic, hydrological and land cover factors that can directly or indirectly reflect the landslide are superimposed. Then, landslide detection was carried out in the three Gorges reservoir area in China to test the performance of the improved method. The quantitative evaluation of the landslide detection map shows that the combination of environmental factors and DenseNet can improve the accuracy of the detection model. Compared with the optical image, kappa and F1 increased by 9.7&#37; and 9.1&#37; respectively. Compared with other traditional neural networks and machine learning algorithms, DenseNet has the highest kappa and F1 values. Based on the base Densenet, through data augmentation and fine-tuning optimization technology, the kappa and F1 values reach the highest values of 0.9474 and 0.9505, respectively. The proposed method has promising applicability in large area landslide identification scenarios.
WOS:000658340600008
</snippet>
</document>

<document id="306">
<title>Ship Detection Based on Compressive Sensing Measurements of Optical Remote Sensing Scenes</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3209024</url>
<snippet>The compressive sensing (CS)-based optical remote sensing (ORS) imaging system has been verified for the feasibility through numerical simulation experiments. The CS-based ORS imaging system can reduce the demand for sampling equipment, reduce sampling data, save storage space, and reduce transmission costs. However, it needs to reconstruct the original scene when facing the task of ship detection. The scene reconstruction process of CS is computationally expensive, high memory demanding, and time-consuming. In response to this problem, this article proposes an innovation pipeline to perform ship detection tasks, i.e., directly performing ship detection on CS measurements obtained by the imaging system, which avoids the process of scene reconstruction. To achieve the ship detection of CS measurements in the pipeline, we design a convolutional neural network-based algorithm, CS-CenterNet, which jointly optimizes the scene compression sampling phase and the measurements ship detection phase. CS-CenterNet is divided into convolution measurement layer (CML), optimized hourglass network (OHgN), and optimized three-branch head network (OTBHN). First, CML without bias or activation function simulates the block compression sampling process in CS-based ORS imaging system, which performs convolutional coding on the scene to obtain the measurements. Second, OHgN extracts high-resolution feature information of measurements. Finally, OTBHN performs heat-map prediction, center-point offset prediction, and width-height prediction. We test the performance of CS-CenterNet using the HRSC2016 and LEVIR datasets. The experimental results show that the algorithm can achieve high-accuracy ship detection based on CS measurements of ORS scenes.
WOS:000868323600003
</snippet>
</document>

<document id="307">
<title>An Optimized Faster R-CNN Method Based on DRNet and RoI Align for Building Detection in Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/rs12050762</url>
<snippet>In recent years, the increase of satellites and UAV (unmanned aerial vehicles) has multiplied the amount of remote sensing data available to people, but only a small part of the remote sensing data has been properly used; problems such as land planning, disaster management and resource monitoring still need to be solved. Buildings in remote sensing images have obvious positioning characteristics; thus, the detection of buildings can not only help the mapping and automatic updating of geographic information systems but also have guiding significance for the detection of other types of ground objects in remote sensing images. Aiming at the deficiency of traditional building remote sensing detection, an improved Faster R-CNN (region-based Convolutional Neural Network) algorithm was proposed in this paper, which adopts DRNet (Dense Residual Network) and RoI (Region of Interest) Align to utilize texture information and to solve the region mismatch problems. The experimental results showed that this method could reach 82.1&#37; mAP (mean average precision) for the detection of landmark buildings, and the prediction box of building coordinates was relatively accurate, which improves the building detection results. Moreover, the recognition of buildings in a complex environment was also excellent.
WOS:000531559300017
</snippet>
</document>

<document id="308">
<title>Attention-Aware Deep Feature Embedding for Remote Sensing Image Scene Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3229729</url>
<snippet>Due to the wide application of remote sensing (RS) image scene classification, more and more scholars activate great attention to it. With the development of the convolutional neural network (CNN), the CNN-based methods of the RS image scene classification have made impressive progress. In the existing works, most of the architectures just considered the global information of the RS images. However, the global information contains a large number of redundant areas that diminish the classification performance and ignore the local information that reflects more fine spatial details of local objects. Furthermore, most CNN-based methods assign the same weights to each feature vector causing the mode to fail to discriminate the crucial features. In this article, a novel method by Two-branch Deep Feature Embedding (TDFE) with a dual attention-aware (DAA) module for RS image scene classification is proposed. In order to mine more complementary information, we extract global semantic-based features of high level and local object-based features of low level by the TDFE module. Then, to focus selectively on the key global-semantics feature maps as well as the key local regions, we propose a DAA module to attain those key information. We conduct extensive experiments to verify the superiority of our proposed method, and the experimental results obtained on two widely used RS scene classification benchmarks demonstrate the effectiveness of the proposed method.
WOS:000912413700016
</snippet>
</document>

<document id="309">
<title>An Unmixing-Based Network for Underwater Target Detection From Hyperspectral Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3080919</url>
<snippet>Detecting underwater targets from hyperspectral imagery makes a profound impact on marine exploration. Available methods mainly tackle this problem by modifying the land-based detection algorithms with classical bathymetric models, which usually fail to remove the interference of background and ignore the effect of depth information, leading to a poor detection performance. To achieve a more precise result, in this work we propose a novel network based on hyperspectral unmixing (HU) methodology and bathymetric models to detect the desired underwater targets. The proposed network, called underwater target detection network (UTD-Net), first develops a novel joint anomaly detector with classical HU methods to separate out target-water mixed pixels, which is devoted to eliminate the adverse influence of background. Then, we explore a bathymetric model-based autoencoder to unmix the target-water mixed pixels for acquiring the target-associated abundance values and maps. One dimension convolutional neural network is exploited to construct the encoder part of above autoencoder for the sake of addressing spectral variability problem. Moreover, considering the physical meaningless endmembers issue, a particular spectral constraint is imposed on the objective function as a training guidance. In this way, the autoencoder would be capable of generating specific endmembers and their corresponding abundance maps. Finally, according to the physical essence of abundance maps, we figure out the detection result by fusing the outcomes of autoencoder with weight coefficients determined by abundance values. Qualitative and quantitative illustrations demonstrate the effectiveness and efficiency of UTD-Net in comparison with the state-of-the-art underwater target detection methods.
WOS:000660636600013
</snippet>
</document>

<document id="310">
<title>Instance GNN: A Learning Framework for Joint Symbol Segmentation and Recognition in Online Handwritten Diagrams</title>
<url>http://dx.doi.org/10.1109/TMM.2021.3087000</url>
<snippet>Online handwritten diagram recognition (OHDR) has attracted considerable attention for its potential applications in many areas, but it is a challenging task due to the complex 2D structure, writing style variation, and lack of annotated data. Existing OHDR methods often have limitations in modeling and learning complex contextual relationships. To overcome these challenges, we propose an OHDR method based on graph neural networks (GNNs) in this paper. In particular, we formulate symbol segmentation and symbol recognition as node clustering and node classification problems on stroke graphs and solve the problems jointly under a unified learning framework with a GNN model. This GNN model is denoted as Instance GNN since it gives the symbol instance label as well as the semantic label. Extensive experiments on two flowchart datasets and a finite automata dataset show that our method consistently outperforms previous methods with large margins and achieves state-of-the-art performance. In addition, we release a large-scale annotated online handwritten flowchart dataset, CASIA-OHFC, and provide initial experimental results as a baseline.
WOS:000793839600026
</snippet>
</document>

<document id="311">
<title>Integration of Convolutional Neural Networks and Object-Based Post-Classification Refinement for Land Use and Land Cover Mapping with Optical and SAR Data</title>
<url>http://dx.doi.org/10.3390/rs11060690</url>
<snippet>Object-based image analysis (OBIA) has been widely used for land use and land cover (LULC) mapping using optical and synthetic aperture radar (SAR) images because it can utilize spatial information, reduce the effect of salt and pepper, and delineate LULC boundaries. With recent advances in machine learning, convolutional neural networks (CNNs) have become state-of-the-art algorithms. However, CNNs cannot be easily integrated with OBIA because the processing unit of CNNs is a rectangular image, whereas that of OBIA is an irregular image object. To obtain object-based thematic maps, this study developed a new method that integrates object-based post-classification refinement (OBPR) and CNNs for LULC mapping using Sentinel optical and SAR data. After producing the classification map by CNN, each image object was labeled with the most frequent land cover category of its pixels. The proposed method was tested on the optical-SAR Sentinel Guangzhou dataset with 10 m spatial resolution, the optical-SAR Zhuhai-Macau local climate zones (LCZ) dataset with 100 m spatial resolution, and a hyperspectral benchmark the University of Pavia with 1.3 m spatial resolution. It outperformed OBIA support vector machine (SVM) and random forest (RF). SVM and RF could benefit more from the combined use of optical and SAR data compared with CNN, whereas spatial information learned by CNN was very effective for classification. With the ability to extract spatial features and maintain object boundaries, the proposed method considerably improved the classification accuracy of urban ground targets. It achieved overall accuracy (OA) of 95.33&#37; for the Sentinel Guangzhou dataset, OA of 77.64&#37; for the Zhuhai-Macau LCZ dataset, and OA of 95.70&#37; for the University of Pavia dataset with only 10 labeled samples per class.
WOS:000465615300067
</snippet>
</document>

<document id="312">
<title>Detection of Fuchs' Uveitis Syndrome From Slit-Lamp Images Using Deep Convolutional Neural Networks in a Chinese Population</title>
<url>http://dx.doi.org/10.3389/fcell.2021.684522</url>
<snippet>Fuchs uveitis syndrome (FUS) is one of the most under- or misdiagnosed uveitis entities. Many undiagnosed FUS patients are unnecessarily overtreated with anti-inflammatory drugs, which may lead to serious complications. To offer assistance for ophthalmologists in the screening and diagnosis of FUS, we developed seven deep convolutional neural networks (DCNNs) to detect FUS using slit-lamp images. We also proposed a new optimized model with a mixed "attention" module to improve test accuracy. In the same independent set, we compared the performance between these DCNNs and ophthalmologists in detecting FUS. Seven different network models, including Xception, Resnet50, SE-Resnet50, ResNext50, SE-ResNext50, ST-ResNext50, and SET-ResNext50, were used to predict FUS automatically with the area under the receiver operating characteristic curves (AUCs) that ranged from 0.951 to 0.977. Our proposed SET-ResNext50 model (accuracy = 0.930; Precision = 0.918; Recall = 0.923; F1 measure = 0.920) with an AUC of 0.977 consistently outperformed the other networks and outperformed general ophthalmologists by a large margin. Heat-map visualizations of the SET-ResNext50 were provided to identify the target areas in the slit-lamp images. In conclusion, we confirmed that a trained classification method based on DCNNs achieved high effectiveness in distinguishing FUS from other forms of anterior uveitis. The performance of the DCNNs was better than that of general ophthalmologists and could be of value in the diagnosis of FUS.
WOS:000668940700001
</snippet>
</document>

<document id="313">
<title>Corner points localization in electronic topographic maps with deep neural networks</title>
<url>http://dx.doi.org/10.1007/s12145-017-0317-3</url>
<snippet>Digitized topographic maps normally have to go through the geometric calibration before practical utilization. Nowadays, the reference points for the calibration are still manually assigned. Corner points (graticule intersections) in a map are usually good candidates in favor of the reference points. This paper proposes an algorithm for automatically locating the corner points in the electronic topographic maps by detecting the specific rectangle objects in the map corners. It assigns the probabilities to each row and column in the region of interests (RoIs) to provide information regarding the location of the objects. In order to facilitate the object detection with high level visual features, the deep neural networks (DNNs) are employed in the proposed algorithm. For the object proposal, the sliding window scheme is adopted. The experimental results indicate that the proposed approach outperforms the conventional bounding-box regression method in both detection and localization accuracy. For the proposed algorithm, the average F1 score in the object detection is 0.91, which is 12&#37; higher than the conventional model. The mean Euclidean distance between the predicted corner points and the ground-truth by the proposed algorithm is 2.22 pixels, 35.8&#37; lower compared with the regression based model.
WOS:000425550300004
</snippet>
</document>

<document id="314">
<title>DCCP: Deep Convolutional Neural Networks for Cellular Network Positioning</title>
<url>http://dx.doi.org/10.1109/GLOBECOM46510.2021.9685658</url>
<snippet>Although location awareness is prevalent outdoors due to the GPS, we get confused and disoriented in many blocked environments such as in urban canyons and under multi-level flyovers. A straightforward solution is to employ cellular signals for positioning, but the cellular signatures are always sparse and uneven in vast region, and vary among different devices and postures. In this paper, we propose DCCP, a novel cellular network positioning approach that transforms the localization problem into a corresponding object recognition task in geographic space. Specially, we elicit the receptive region of each cellular station via crowdsourced user queries, and exploit neighbour base stations to derive a multi-dimensional feature map. We also devise a CNN model to learn local correlations among nearby map grids, and employ it for cellular positioning. Extensive experiments on two real-world traffic datasets from the DiDi platform have demonstrated our effectiveness compared with the state-of-the-art. This is the lirst approach to use only user queries instead of RF signatures for cellular network positioning, and our system meets requirements of the E911.
WOS:000790747203078
</snippet>
</document>

<document id="315">
<title>Multi-channel convolutional neural network for integration of meteorological and geographical features in solar power forecasting</title>
<url>http://dx.doi.org/10.1016/j.apenergy.2021.117083</url>
<snippet>The forecasting of potential photovoltaic power is essential to investigate suitable regions for power plant installation where high levels of electricity can be produced. However, it remains challenging to integrate the meteorological and geographical features at a regional level into the modeling process of solar forecasting, through which the model trained can be extended to predict at other regions. In particular, regional effects resulting from adjacent topography and weather conditions have rarely been considered in solar energy forecasting. Thus, this paper proposes a multi-channel convolutional neural network that is designed to forecast the monthly photovoltaic power with raster image data representing various regional effects. In particular, the network model with multi-channels allows for training with input data of elevation, solar irradiation, temperature, wind speed, and precipitation in a map format, and output data of corresponding photovoltaic power outputs from 164 sites. The results show that the proposed network model achieves a mean absolute percent error of 8.639&#37;, which outperforms conventional methods such as multiple linear regression (e.g., 16.187&#37;) and artificial neural networks (e.g., 15.991&#37;). This implies that learning regional patterns of both geographical and meteorological features may lead to better performance in solar forecasting, and that the trained model can be applied to other regions-the data of which is not used for the training. Thus, this study may help to identify suitable regions with high electricity potential in a large area.
WOS:000708132400005
</snippet>
</document>

<document id="316">
<title>Analyzing multi-domain learning for enhanced rockfall mapping in known and unknown planetary domains</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.09.018</url>
<snippet>Rockfalls are small-scale mass wasting events that have been observed across the solar system. They provide valuable information about the endo- and exogenic activity of their host body, but are difficult to identify and map in satellite imagery, especially on global scales and in big data sets. Past work implemented convolutional neural networks to automate rockfall mapping on the Moon and Mars with the caveat of (1) achieving sub-optimal performance and (2) requiring substantial manual image labeling efforts. Mixing annotated image data from the Moon and Mars while keeping the total number of labels constant, we show that including a small number (10&#37;) of rockfall labels from a foreign domain (e.g. Moon) during detector training can increase performance in the home domain (e.g. Mars) by up to 6&#37; Average Precision (AP) in comparison to a purely home domain-trained detector. We additionally show that using a large number of foreign domain training examples (90&#37;) in combination with a small number (10&#37;) of home domain labels can be as powerful or more powerful as exclusively (100&#37;) using home labels in the home domain. We further observe that rockfall detectors trained on multiple domains outperform single-domain trained detectors in completely unknown domains by up to 16&#37; AP, using image data from Ceres and comet 67P. We conduct an experiment varying only image resolution on a single planetary body (Mars) to test whether the improvement was due to training on differing resolutions specifically and show that none of the improvement can be explained by this effect alone. This means that the benefits of multi-domain training mostly draw from either variations in lighting condition, differing physical appearance/backgrounds around the target of interest for generalization purposes, or both. Our findings have important applications such as machine learning-enabled science discovery in legacy and new planetary datasets.
WOS:000709811500001
</snippet>
</document>

<document id="317">
<title>Indoor Environment Semantic Topological Mapping Based on Deep Learning</title>
<url>http://dx.doi.org/</url>
<snippet>With the popularity of mobile service robots in the home environment, people have put forward higher requirements on the robots environmental perception. In order to improve the semantic ability of service robot perception of indoor environment, this paper presents a real-time semantic mapping system based on scene learning. Deep convolutional neural network is used to identify indoor typical scenes without training in a specific environment. Then, combining with vision and laser range data to estimate the semantic region, a semantic map layer is building based on a metric map. Furthermore, in order to express the indoor environment concretely, each semantic area in the room is subdivided into several functional areas as semantic topological nodes into the semantic map, thus constructing a semantic topological map. Finally, the feasibility of the semantic topological mapping algorithm is verified through experiments in real indoor environments.
WOS:000459211400089
</snippet>
</document>

<document id="318">
<title>Detecting Post Hurricane House Damage Using Geographic Information Related Multi-Resource Classification Model</title>
<url>http://dx.doi.org/10.1109/ICBASE53849.2021.00098</url>
<snippet>Hurricane, like other natural cataclysms that threaten human life and houses' damage detection after a hurricane, is always a problem that needs to be solved. It is vital to retrieving the building damage status for planning rescue and reconstruction after the cataclysm. In this study, the convolutional neural networks (CNN) were utilized to identify collapsed buildings from post hurricane satellite imagery with the proposed workflow. Test accuracy (TeA), training accuracy (TrA), bootstrap algorithm, Grad-CAM, and feature maps (FM) were used as evaluation metrics. To overcome the imbalance, problems like overfitting, random flip, random sheer and zoom, and early stopping approach were tested on the investigations. The results demonstrated that the building collapsed information can be retrieved by utilizing post-event imagery. Simple convolutional neural network (SCNN) is the standard to compare the other two architectures, which achieved TrA 74.39&#37; and TeA 76.91&#37;, spend 18.22s per epoch. After adding an additional super resolution block specifically designed. The super resolution CNN with up sampling (SRCNN-US) reached lower TrA 78.01&#37; but higher TeA 73.80&#37; by spending nearly 4 times more (78.14s). Moreover, the multi input redesigned SCNN (MICNN) architecture showed better performance, with TrA value from 74.39&#37; to 84.97&#37; and TeA from 76.91&#37; to 78.81&#37; but consumed only 0.22s more per epoch. Combining MICNN and SRCNN-US, the MI-SRCNN-US model achieved the highest accuracy on the test set, 80.36&#37;, and time-consuming, 83.50s/epoch. The 50 times bootstrap investigation shows that the MICNN makes predictions under more certainty with more accuracy. In subsequent evaluations, Grad-CAM and feature maps also prove that MICNN pays more attention to the building's region rather than its surroundings. Therefore, the suitable method to promote classification performance is by using post-hurricane cataclysm satellite imagery together with related geographic coordinates information as the input of CNN.
WOS:000796392500089
</snippet>
</document>

<document id="319">
<title>Multi-scale deep feature learning network with bilateral filtering for SAR image classification</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.07.007</url>
<snippet>Synthetic aperture radar (SAR) image classification using deep neural network has drawn great attention, which generally requires various layers of deep model for feature learning. However, a deeper neural network will result in overfitting with limited training samples. In this paper, a multi-scale deep feature learning network with bilateral filtering (MDFLN-BF) is proposed for SAR image classification, which aims to extract discriminative features and reduce the requirement of labeled samples. In the proposed framework, MDFLN is proposed to extract features from SAR image on multiple scales, where the SAR image is stratified into different scales and a full convolutional network is utilized to extract features from each scale sub-image. Then, features of multiple scales are classified by multiple softmax classifiers and combined by majority vote algorithm. Further, bilateral filtering is developed to optimize the classification map based on spatial relation, which aims to improve the spatial smoothness. Experiments are tested on three SAR images with different sensors, bands, resolutions, and polarizations in order to prove the generalization ability. It is demonstrated that the proposed MDFLN-BF is able to yield superior results than other related deep networks.
WOS:000561346200015
</snippet>
</document>

<document id="320">
<title>Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.01.021</url>
<snippet>In remote sensing images, the absolute orientation of objects is arbitrary. Depending on an objects orientation and on a sensors flight path, objects of the same semantic class can be observed in different orientations in the same image. Equivariance to rotation, in this context understood as responding with a rotated semantic label map when subject to a rotation of the input image, is therefore a very desirable feature, in particular for high capacity models, such as Convolutional Neural Networks (CNNs). If rotation equivariance is encoded in the network, the model is confronted with a simpler task and does not need to learn specific (and redundant) weights to address rotated versions of the same object class. In this work we propose a CNN architecture called Rotation Equivariant Vector Field Network (RotEgNet) to encode rotation equivariance in the network itself. By using rotating convolutions as building blocks and passing only the values corresponding to the maximally activating orientation throughout the network in the form of orientation encoding vector fields, RotEciNet treats rotated versions of the same object with the same filter bank and therefore achieves state-of-the-art performances even when using very small architectures trained from scratch. We test RotaiNet in two challenging sub-decimeter resolution semantic labeling problems, and show that we can perform better than a standard CNN while requiring one order of magnitude less parameters. (C) 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000449125400007
</snippet>
</document>

<document id="321">
<title>Siamese Spectral Attention With Channel Consistency for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3115129</url>
<snippet>Abundant spectral features are the precious wealth of hyperspectral images (HSI). Nevertheless, well-designed spectral feature is still a challenge that affects the performance of the classifier, especially with insufficient number of training samples. To make up the poor discriminability of spectral feature, double-branch methods are proposed by fusing parallel spectral and spatial branches. However, this structure does nothing to improve the quality of spectral feature, which is regarded as the most valuable information for HSI information. In this article, we propose a siamese spectral attention network with channel consistency (SSACC) to focus on obtaining discriminative spectral features, thus improving the generalization ability of the classifier. Two kinds of HSI cubes with different patch sizes are generated as the input of SSACC. The two cubes are divided into top and bottom branches and then be fed into the siamese network to obtain the refined spectral features. Then, self-attention is conducted to interacting with each channel for the spectral features enhancement. Meanwhile, two attention maps are obtained to display the spectral structures of each branch. A channel consistency regularization is performed on the two attention maps by enforcing the two branches to possess similar spectral patterns when identifying the same centric pixel. Extensive experiments conducted on the three HSI datasets verify the superiority of the obtained spectral feature. Furthermore, the proposed method applying convolution only on the spectral domain outperforms the state-of-the-art double-branch methods which integrate the spectral and spatial features simultaneously.
WOS:000709074200006
</snippet>
</document>

<document id="322">
<title>Urban flood mapping with an active self-learning convolutional neural network based on TerraSAR-X intensity and interferometric coherence</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.04.014</url>
<snippet>Synthetic Aperture Radar (SAR) remote sensing has been widely used for flood mapping and monitoring. Nevertheless, flood detection in urban areas still proves to be particularly challenging by using SAR. In this paper, we assess the roles of SAR intensity and interferometric coherence in urban flood detection using multi-temporal TerraSAR-X data. We further introduce an active self-learning convolution neural network (A-SL CNN) framework to alleviate the effect of a limited annotated training dataset. The proposed framework selects informative unlabeled samples based on a temporal-ensembling CNN model. These samples are subsequently pseudo-labeled by a multi-scale spatial filter. Consistency regularization is introduced to penalize incorrect labels caused by pseudo-labeling. We show results for a case study that is centered on flooded areas in Houston, USA, during hurricane Harvey in August 2017. Our experiments show that multi-temporal intensity (pre- and coevent) plays the most important role in urban flood detection. Adding multi-temporal coherence can increase the reliability of the inundation map considerably. Meanwhile, encouraging results are achieved by the proposed A-SL CNN framework: the kappa statistic is improved from 0.614 to 0.686 in comparison to its supervised counterpart.
WOS:000469158200014
</snippet>
</document>

<document id="323">
<title>LidarCSNet: A Deep Convolutional Compressive Sensing Reconstruction Framework for 3D Airborne Lidar Point Cloud</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.08.019</url>
<snippet>Lidar scanning is a widely used surveying and mapping technique ranging across remote-sensing applications involving topological, and topographical information. Typically, lidar point clouds, unlike images, lack inherent consistent structure and store redundant information thus requiring huge processing time. The Compressive Sensing (CS) framework leverages this property to generate sparse representations and accurately reconstructs the signals from very few linear, non-adaptive measurements. The reconstruction is based on valid assumptions on the following parameters- (1) sampling function governed by sampling ratio for generating samples, and (2) measurement function for sparsely representing the data in a low-dimensional subspace. In our work, we address the following motivating scientific questions- Is it possible to reconstruct dense point cloud data from a few sparse measurements? And, what could be the optimal limit for CS sampling ratio with respect to overall classification metrics? Our work proposes a novel Convolutional Neural Network based deep Compressive Sensing Network (named LidarCSNet) for generating sparse representations using publicly available 3D lidar point clouds of the Philippines. We have performed extensive evaluations for analysing the reconstruction for different sampling ratios {4&#37;, 10&#37;, 25&#37;, 50&#37; and 75&#37;} and we observed that our proposed LidarCSNet reconstructed the 3D lidar point cloud with a maximum PSNR of 54.47 dB for a sampling ratio of 75&#37;. We investigate the efficacy of our novel LidarCSNet framework with 3D airborne lidar point clouds for two domains - forests and urban environment on the basis of Peak Signal to Noise Ratio, Haussdorf distance, Pearson Correlation Coefficient and Kolmogorov-Smirnov Test Statistic as evaluation metrics for 3D reconstruction. The results relevant to forests such as Canopy Height Model and 2D vertical profile are compared with the ground truth to investigate the robustness of the LidarCSNet framework. In the urban environment, we extend our work to propose two novel 3D lidar point cloud classification frameworks, LidarNet and LidarNet++, achieving maximum classification accuracy of 90.6&#37; as compared to other prominent lidar classification frameworks. The improved classification accuracy is attributed to ensemble-based learning on the proposed novel 3D feature stack and justifies the robustness of using our proposed LidarCSNet for near-perfect reconstruction followed by classification. We document our classification results for the original dataset along with the point clouds reconstructed by using LidarCSNet for five different measurement ratios - based on overall accuracy and mean Intersection over Union as evaluation metrics for 3D classification. It is envisaged that our proposed deep network based convolutional sparse coding approach for rapid lidar point cloud processing finds huge potential across vast applications, either as a plug-and-play (reconstruction) framework or as an end-to-end (reconstruction followed by classification) system for scalability.
WOS:000697167200021
</snippet>
</document>

<document id="324">
<title>Extracting buildings from high-resolution remote sensing images by deep ConvNets equipped with structural-cue-guided feature alignment</title>
<url>http://dx.doi.org/10.1016/j.jag.2022.102970</url>
<snippet>In surveying, mapping and geographic information systems, building extraction from remote sensing imagery is a common task. However, there are still some challenges in automatic building extraction. First, using only single-scale depth features cannot take into account the uncertainty of features such as the hue and texture of buildings in images, and the results are prone to missed detection. Moreover, extracted high-level features often lose structural information and have scale differences with low-level features, which results in less accurate extraction of boundaries. To simultaneously address these problems, we propose pyramid feature extraction (PFE) to construct multi-scale representations of buildings, which is inspired by the feature extraction of scale-invariant feature transform. We also apply attention modules in channel dimension and spatial dimension to PFE and low-level feature maps. Furthermore, we use the structural-cue-guided feature alignment module to learn the correlation between feature maps at different levels, obtaining high-resolution features with strong semantic representation and ensuring the integrity of high-level features in both structural and semantic dimensions. An edge loss is applied to get a highly accurate building boundary. For the WHU Building Dataset, our method achieves an F1 score of 95.3&#37; and an Intersection over Union (IoU) score of 90.9&#37;; for the Massachusetts Buildings Dataset, our method achieves an F1 score of 85.0&#37; and an IoU score of 74.1&#37;.
WOS:000857298300003
</snippet>
</document>

<document id="325">
<title>Monitoring the Invasive Plant Spartina alterniflora in Jiangsu Coastal Wetland Using MRCNN and Long-Time Series Landsat Data</title>
<url>http://dx.doi.org/10.3390/rs14112630</url>
<snippet>Jiangsu coastal wetland has the largest area of the invasive plant, Spartina alterniflora (S. alterniflora), in China. S. alterniflora has been present in the wetland for nearly 40 years and poses a substantial threat to the safety of coastal wetland ecosystems. There is an urgent need to control the distribution of S. alterniflora. The biological characteristics of the invasion process of S. alterniflora contribute to its multi-scale distribution. However, the current classification methods do not deal successfully with multi-scale problems, and it is also difficult to perform high-precision land cover classification on multi-temporal remote sensing images. In this study, based on Landsat data from 1990 to 2020, a new deep learning multi-scale residual convolutional neural network (MRCNN) model was developed to identify S. alterniflora. In this method, features at different scales are extracted and concatenated to obtain multi-scale information, and residual connections are introduced to ensure gradient propagation. A multi-year data unified training method was adopted to improve the temporal scalability of the MRCNN. The MRCNN model was able to identify the annual S. alterniflora distribution more accurately, overcame the disadvantage that traditional CNNs can only extract feature information at a single scale, and offered significant advantages in spatial characterization. A thematic map of S. alterniflora distribution was obtained. Since it was introduced in 1982, the distribution of S. alterniflora has expanded to approximately 17,400 ha. In Jiangsu, the expansion process of S. alterniflora over time was divided into three stages: the growth period (1982-1994), the outbreak period (1995-2004), and the plateau period (2005-2020). The spatial expansion direction was mainly parallel and perpendicular to the coastline. The hydrodynamic conditions and tidal flat environment on the coast of Jiangsu Province are suitable for the growth of S. alterniflora. Reclamation of tidal flats is the main factor affecting the expansion of S. alterniflora.
WOS:000809111400001
</snippet>
</document>

<document id="326">
<title>Multi-modal shared module that enables the bottom-up formation of map representation and top-down map reading</title>
<url>http://dx.doi.org/10.1080/01691864.2021.1993334</url>
<snippet>Humans create internal models of an environment (i.e. cognitive maps) through subjective sensorimotor experiences and can also understand spatial locations by looking at an external map as a symbol of an environment. We simulate the development of the cognitive map from sensorimotor experiences and grounding of the external map in a single deep neural network model. Our proposed network has a shared module that processes the features of multiple modalities (i.e. vision, hearing, and touch) and even external maps in the same manner. The multiple modalities are encoded into feature vectors by modality-specific encoders, and the encoded features are processed by the same shared module. The proposed network was trained to predict the sensory inputs of a simulated mobile robot. After the predictive learning, the spatial representation was developed in the internal states of the shared module, and the same spatial representation was used for predicting multiple modalities, including the external map. The network can also perform spatial navigation by associating the external map with the cognitive map. This implies that the external maps are grounded in subjective sensorimotor experiences, being bridged through the developed internal spatial representation in the shared module.
WOS:000713897000001
</snippet>
</document>

<document id="327">
<title>An Innovative Intelligent System with Integrated CNN and SVM: Considering Various Crops through Hyperspectral Image Data</title>
<url>http://dx.doi.org/10.3390/ijgi10040242</url>
<snippet>Generation of a thematic map is important for scientists and agriculture engineers in analyzing different crops in a given field. Remote sensing data are well-accepted for image classification on a vast area of crop investigation. However, most of the research has currently focused on the classification of pixel-based image data for analysis. The study was carried out to develop a multi-category crop hyperspectral image classification system to identify the major crops in the Chiayi Golden Corridor. The hyperspectral image data from CASI (Compact Airborne Spectrographic Imager) were used as the experimental data in this study. A two-stage classification was designed to display the performance of the image classification. More specifically, the study used a multi-class classification by support vector machine (SVM) + convolutional neural network (CNN) for image classification analysis. SVM is a supervised learning model that analyzes data used for classification. CNN is a class of deep neural networks that is applied to analyzing visual imagery. The image classification comparison was made among four crops (paddy rice, potatoes, cabbages, and peanuts), roads, and structures for classification. In the first stage, the support vector machine handled the hyperspectral image classification through pixel-based analysis. Then, the convolution neural network improved the classification of image details through various blocks (cells) of segmentation in the second stage. A series of discussion and analyses of the results are presented. The repair module was also designed to link the usage of CNN and SVM to remove the classification errors.
WOS:000643061900001
</snippet>
</document>

<document id="328">
<title>MRSE-Net: Multiscale Residuals and SE-Attention Network for Water Body Segmentation From Satellite Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3185245</url>
<snippet>Automatic extraction of water bodies from various satellite images containing complex targets is a very important and challenging task in remote sensing and image interpretation. In recent years, convolutional neural networks (CNNs) have become an important choice in the field of semantic segmentation of remote sensing images. However, generic CNN models present many problems when performing water body segmentation, such as: 1) blurred water body boundaries; 2) difficulty in accommodating different scales of rivers, often losing information about many small-scale rivers; and 3) a large number of trainable parameters. This article proposes an end-to-end CNN structure based on multiscale residuals and squeeze-and-excitation (SE)-attention for water segmentation, called MRSE-Net. MRSE-Net consists of an encoder-decoder and a skip connection, which captures contextual information at different scales using the encoder, and then passes the encoder feature mapping through the improved skip connection, while localization is achieved by the decoder is implemented. With the multiscale residual module, the number of parameters in our model can be significantly reduced and water pixels can be extracted accurately. The SE-attention module is used to enhance the prediction results, mitigate the blurring effect, and make the segmented water boundaries more continuous. Landsat-8 images are used to train our model and validate our proposed methods performance and effectiveness. In addition, we evaluate our method on Landsat-7 and Sentinel-2 images and obtain the best water segmentation results. Preliminary results on Sentinel-2 images show that the cross-sensor generalization capability of our model is beyond the range of the Landsat sensor family.
WOS:000820509400001
</snippet>
</document>

<document id="329">
<title>POLYGONIZATION OF REMOTE SENSING CLASSIFICATION MAPS BY MESH APPROXIMATION</title>
<url>http://dx.doi.org/</url>
<snippet>The ultimate goal of land mapping from remote sensing image classification is to produce polygonal representations of Earths objects, to be included in geographic information systems. This is most commonly performed by running a pixelwise image classifier and then polygonizing the connected components in the classification map. We here propose a novel polygonization algorithm, which uses a labeled triangular mesh to approximate the input classification maps. The mesh is optimized in terms of an l(1) norm with respect to the classifierss output. We use a rich set of optimization operators, which includes a vertex relocator, and add a topology preservation strategy. The method outperforms current approaches, yielding better accuracy with fewer vertices.
WOS:000428410700113
</snippet>
</document>

<document id="330">
<title>Land use/cover classification in an arid desert-oasis mosaic landscape of China using remote sensed imagery: Performance assessment of four machine learning algorithms</title>
<url>http://dx.doi.org/10.1016/j.gecco.2020.e00971</url>
<snippet>The importance of land use and cover change (LUCC) has gradually attracted more attention due to its influence on the climate and ecosystem. Consequently, the necessity of accurate LUCC mapping has become increasingly apparent. Over the past decades, although a large number of machine learning algorithms have been developed to improve the accuracy and reliability of remote sensing image classification, especially for LUCC classification, there is a lack of studies that assess the performance of machine learning algorithms in arid desert-oasis mosaic landscapes. In this study, the main objective is to provide a reference for the extraction of LUCC information in dryland regions with oasis-desert mosaic landscapes by comparing the performances of the k-nearest neighbor (KNN), random forest (RF), support vector machine (SVM) and artificial neural network (ANN) for the LUCC classification of the Dengkou Oasis, China. Landsat-8 Operational Land Imager (OLI) image data were used with spectral indices and auxiliary variables that were derived from a digital terrain model to classify 7 different land cover categories. The highest overall accuracy was produced by the ANN (97.16&#37;), which was closely followed by the RF (96.92&#37;), SVM (96.20&#37;), and finally KNN (93.98&#37;); statistically similar accuracies were obtained for the ANN, SVM and RF. The RF algorithm performed well across several aspects, such as stability, ease of use and processing time during the parameter tuning. Overall, the random forest algorithm is a good first choice method for land-cover classification in this study area, and the elevation and some spectral indices, such as the NDVI, MSAVI2 and MNDWI, should be used as variables to improve the overall accuracy. (C) 2020 The Authors. Published by Elsevier B.V.
WOS:000539265300007
</snippet>
</document>

<document id="331">
<title>Fusing Multiple Deep Models for In Vivo Human Brain Hyperspectral Image Classification to Identify Glioblastoma Tumor</title>
<url>http://dx.doi.org/10.1109/TIM.2021.3117634</url>
<snippet>Glioblastoma (GBM) tumor is the most common primary brain malignant tumor. The precise identification of GBM tumors is very important for diagnosis and treatment. Hyperspectral imaging is a fast, noncontact, accurate, and safe modern medical detection technology, which is expected to be a new tool of intraoperative diagnosis. In order to make full use of the spectral and spatial information of hyperspectral images (HSIs) to achieve accurate GBM tumor identification, a method based on the fusion of multiple deep models is proposed for in vivo human brain HSI classification. The proposed method includes the following major steps: 1) spectral phasor analysis and data oversampling; 2) 1-D deep neural network (1D-DNN)-based spectral HSI feature extraction and classification; 3) 2-D convolution neural network (2D-CNN)-based spectral-spatial HSI feature extraction and classification; 4) edge-preserving filtering-based classification result fusion and optimization; and 5) fully convolutional network (FCN)-based background segmentation. To verify the capabilities of the proposed method, experiments are performed on two real human brain hyperspectral datasets, including 36 in vivo HSIs captured from 16 different patients. The proposed method can achieve an overall accuracy of 96.69&#37; for four-class classification and overall accuracy of 96.34&#37; for GBM tumor identification. Experimental results demonstrate that the proposed method exhibits competitive classification performance and can generate satisfactory thematic maps of the location of the GBM tumor, which can provide the surgeon with guidance on successful and precise tumor resection.
WOS:000706960500014
</snippet>
</document>

<document id="332">
<title>IMPROVING CNN-BASED BUILDING SEMANTIC SEGMENTATION USING OBJECT BOUNDARIES</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B3-2022-41-2022</url>
<snippet>Semantic segmentation is an active area of research with a wide range of applications including autonomous driving, digital mapping, urban monitoring, land use analysis and disaster management. For the past few years approaches based on Convolutional Neural Networks, especially end-to-end approaches based on architectures like the Fully Convolutional Networks (FCN) and UNet, have made great progress and are considered the current state-of-the-art. Nevertheless, there is still room for improvement as CNN-based supervised-learning models require a very large amount of labelled data in order to generalize effectively to new data and the segmentation results often lack detail, mostly in areas near the boundaries between objects. In this work we leverage the semantic information provided by the objects' boundaries to improve the quality and detail of an encoder-decoder model's semantic segmentation output. We use a UNet-based model with ResNet as an encoder for our backbone architecture in which we incorporate a decoupling module that separates the boundaries from the main body of the objects and thus learns explicit representations for both body and edges of each object. We evaluate our proposed approach on the Inria Aerial Image Labelling dataset and compare the results to a more traditional Unet-based architecture. We show that the proposed approach marginally outperforms the baseline on the mean precision, F1-score and IoU metrics by 1.1 to 1.6&#37;. Finally, we examine certain cases of misclassification in the ground truth data and discuss how the trained models perform in such cases.
WOS:000855647800005
</snippet>
</document>

<document id="333">
<title>Automatic segmentation of nine retinal layer boundaries in OCT images of non-exudative AMD patients using deep learning and graph search</title>
<url>http://dx.doi.org/10.1364/BOE.8.002732</url>
<snippet>We present a novel framework combining convolutional neural networks (CNN) and graph search methods (termed as CNN-GS) for the automatic segmentation of nine layer boundaries on retinal optical coherence tomography (OCT) images. CNN-GS first utilizes a CNN to extract features of specific retinal layer boundaries and train a corresponding classifier to delineate a pilot estimate of the eight layers. Next, a graph search method uses the probability maps created from the CNN to find the final boundaries. We validated our proposed method on 60 volumes (2915 B-scans) from 20 human eyes with non-exudative age-related macular degeneration (AMD), which attested to effectiveness of our proposed technique. (C) 2017 Optical Society of America
WOS:000400500400024
</snippet>
</document>

<document id="334">
<title>An improved lightweight deep neural network with knowledge distillation for local feature extraction and visual localization using images and LiDAR point clouds</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.12.011</url>
<snippet>Visual localization nowadays is a research hotspot in computer vision and photogrammetry. It can provide meter level or higher localization accuracy under the conditions without GPS signals. However, achieving efficient, robust and high-accuracy visual localization under the condition of day-night changes is still challenging. To deal with this problem, we develop an improved lightweight deep neural network with knowledge distillation to efficiently extract deep local features from imagery while maintaining strong robustness for day-night visual localization. Furthermore, to further improve the accuracy of visual localization, we use aligned dense LiDAR point clouds and imagery collected by a new portable camera-LiDAR integrated device to build a prior map, and directly utilize the 2D-3D correspondences between 2D local feature points extracted by our lightweight network and 3D laser points retrieved from the prior map for localization. Moreover, we build our own ground truth point cloud dataset at 5 cm accuracy to evaluate the accuracy of the constructed prior map as well as a day-night dataset including prior map and verification data for the evaluation of the proposed visual localization method. The experimental results prove that our visual localization method achieves a balance between the efficiency and robustness while improving localization accuracy for day-night visual localization. In a comparison with a variety of state-of-the-art local feature extraction methods based on deep neural networks, our lightweight network has the least number of parameters (0.2 million) and reaches the highest feature extraction efficiency (92 frames per second), which is on par with that of the classic real-time ORB feature extraction method. Furthermore, our network remains competitive with other advanced deep local feature extraction networks in feature matching and day-night visual localization. In addition, evaluations performed on our own dataset demonstrate that our visual localization method using images and LiDAR point clouds provides a localization error of 1.2 m under the conditions of day-night changes, which is much smaller than those achieved by a state-of-the-art, purely visual localization method.
WOS:000781623800002
</snippet>
</document>

<document id="335">
<title>Direct generation of level of service maps from images using convolutional and long short-term memory networks</title>
<url>http://dx.doi.org/10.1080/15472450.2018.1563865</url>
<snippet>Congestion in transport stations could result in stampede development and deadly crush situations. Closed circuit television (CCTV) cameras enable station managers to monitor the crowd and reduce overcrowding risks. However, identifying congestion conditions is a very laborious task for a human operator who has to monitor multiple locations at the same time. This paper presents a new approach to automated image-based identification of congestion as measured by level of service (LOS), which is the most widely accepted standard for measuring congestion. Existing methods for measuring LOS based on crowd density estimation from images have the disadvantages that, crowd density cannot be estimated accurately. In addition, the calculation of flow parameters involves a complex process, and consequently these parameters are not indicative of congestion in real-time. This paper proposes a novel method based on machine learning to directly classify LOS without calculating flow parameters. In the proposed method, visual features extracted by a deep convolutional neural network are classified using a support vector machine classifier and the classification results are further refined by using a long short-term memory network. A second contribution of this research is to develop a web-based LOS map visualization platform to monitor pedestrian distribution and variation of distribution in real-time. Experimental evaluation at Flinders Street Station in Melbourne shows that this method can achieve an accuracy of 81.9&#37; and efficiency of 0.40 seconds per frame in LOS classification using CCTV images.
WOS:000463113800007
</snippet>
</document>

<document id="336">
<title>A Novel Framework Based on Mask R-CNN and Histogram Thresholding for Scalable Segmentation of New and Old Rural Buildings</title>
<url>http://dx.doi.org/10.3390/rs13061070</url>
<snippet>Mapping new and old buildings are of great significance for understanding socio-economic development in rural areas. In recent years, deep neural networks have achieved remarkable building segmentation results in high-resolution remote sensing images. However, the scarce training data and the varying geographical environments have posed challenges for scalable building segmentation. This study proposes a novel framework based on Mask R-CNN, named Histogram Thresholding Mask Region-Based Convolutional Neural Network (HTMask R-CNN), to extract new and old rural buildings even when the label is scarce. The framework adopts the result of single-object instance segmentation from the orthodox Mask R-CNN. Further, it classifies the rural buildings into new and old ones based on a dynamic grayscale threshold inferred from the result of a two-object instance segmentation task where training data is scarce. We found that the framework can extract more buildings and achieve a much higher mean Average Precision (mAP) than the orthodox Mask R-CNN model. We tested the novel frameworks performance with increasing training data and found that it converged even when the training samples were limited. This frameworks main contribution is to allow scalable segmentation by using significantly fewer training samples than traditional machine learning practices. That makes mapping Chinas new and old rural buildings viable.
WOS:000651944500001
</snippet>
</document>

<document id="337">
<title>Mapping dead forest cover using a deep convolutional neural network and digital aerial photography</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.07.010</url>
<snippet>Tree mortality is an important forest ecosystem variable having uses in many applications such as forest health assessment, modelling stand dynamics and productivity, or planning wood harvesting operations. Because tree mortality is a spatially and temporally erratic process, rates and spatial patterns of tree mortality are difficult to estimate with traditional inventory methods. Remote sensing imagery has the potential to detect tree mortality at spatial scales required for accurately characterizing this process (e.g., landscape, region). Many efforts have been made in this sense, mostly using pixel- or object-based methods. In this study, we explored the potential of deep Convolutional Neural Networks (CNNs) to detect and map tree health status and functional type over entire regions. To do this, we built a database of around 290,000 photo-interpreted trees that served to extract and label image windows from 20 cm-resolution digital aerial images, for use in CNN training and evaluation. In this process, we also evaluated the effect of window size and spectral channel selection on classification accuracy, and we assessed if multiple realizations of a CNN, generated using different weight initializations, can be aggregated to provide more robust predictions. Finally, we extended our model with 5 additional classes to account for the diversity of landcovers found in our study area. When predicting tree health status only (live or dead), we obtained test accuracies of up to 94&#37;, and up to 86&#37; when predicting functional type only (broadleaf or needleleaf). Channel selection had a limited impact on overall classification accuracy, while window size increased the ability of the CNNs to predict plant functional type. The aggregation of multiple realizations of a CNN allowed us to avoid the selection of suboptimal models and help to remove much of the speckle effect when predicting on new aerial images. Test accuracies of plant functional type and health status were not affected in the extended model and were all above 95&#37; for the 5 extra classes. Our results demonstrate the robustness of the CNN for between-scene variations in aerial photography and also suggest that this approach can be applied at operational level to map tree mortality across extensive territories.
WOS:000487765800002
</snippet>
</document>

<document id="338">
<title>Improving Probabilistic Quantitative Precipitation Forecasts Using Short Training Data through Artificial Neural Networks</title>
<url>http://dx.doi.org/10.1175/JHM-D-22-0021.1</url>
<snippet>Conventional statistical postprocessing techniques offer limited ability to improve the skills of probabilistic guidance for heavy precipitation. This paper introduces two artificial neural network (ANN)-based, geographically aware, and computationally efficient postprocessing schemes, namely, the ANN-multiclass (ANN-Mclass) and the ANN-censored, shifted gamma distribution (ANN-CSGD). Both schemes are implemented to postprocess Global Ensemble Forecast System (GEFS) forecasts to produce probabilistic quantitative precipitation forecasts (PQPFs) over the contiguous United States (CONUS) using a short (60 days), rolling training window. The performances of these schemes are assessed through a set of hindcast experiments, wherein postprocessed 24-h PQPFs from the two ANN schemes were compared against those produced using the benchmark quantile mapping algorithm for lead times ranging from 1 to 8 days. Outcomes of the hindcast experiments show that ANN schemes overall outperform the benchmark as well as the raw forecast over the CONUS in predicting probability of precipitation over a range of thresholds. The relative performance varies among geographic regions, with the two ANN schemes broadly improving upon quantile mapping over the central, south, and southeast, and slightly underperforming along the Pacific coast where skills of raw forecasts are the highest. Between the two schemes, the hybrid ANN-CSGD outperforms at higher rainfall thresholds (i.e., &gt;50 mm day(-1)), though the outperformance comes at a slight expense of sharpness and spatial specificity. Collectively, these results confirm the ability of the ANN algorithms to produce skillful PQPFs with a limited training window and point to the prowess of the hybrid scheme for calibrating PQPFs for rare-to-extreme rainfall events.
WOS:000861689900001
</snippet>
</document>

<document id="339">
<title>Automatic mapping and geomorphometry extraction technique for crevasses in geodetic mass-balance calculations at Haig Glacier, Canadian Rockies</title>
<url>http://dx.doi.org/10.1017/jog.2019.71</url>
<snippet>Finely resolved geodetic data provide an opportunity to assess the extent and morphology of crevasses and their change over time. Crevasses have the potential to bias geodetic measurements of elevation and mass change unless they are properly accounted for. We developed a framework that automatically maps and extracts crevasse geometry and masks them where they interfere with surface mass-balance assessment. Our study examines airborne light detection and ranging digital elevation models (LiDAR DEMs) from Haig Glacier, which is experiencing a transient response in its crevassed upper regions as the glacier thins, using a self-organizing map algorithm. This method successfully extracts and characterizes similar to 1000 crevasses, with an overall accuracy of 94&#37;. The resulting map provides insight into stress and flow conditions. The crevasse mask also enables refined geodetic estimates of summer mass balance. From differencing of September and April LiDAR DEMs, the raw LiDAR DEM gives a 9&#37; overestimate in the magnitude of glacier thinning over the summer: -5.48 m compared with a mean elevation change of -5.02 m when crevasses are masked out. Without identification and removal of crevasses, the LiDAR-derived summer mass balance therefore has a negative bias relative to the glaciological surface mass balance.
WOS:000510631500008
</snippet>
</document>

<document id="340">
<title>Hyperspectral Image Restoration Combining Intrinsic Image Characterization With Robust Noise Modeling</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3046488</url>
<snippet>In hyperspectral image (HSI) processing, a fundamental issue is to restore HSI data from various degradations such as noise corruption and information missing. However, most existing methods more or less ignore the abundant prior knowledge on HSIs and the embedded noise, leading to suboptimal performance in practice. In this article, we propose a novel HSI restoration method by fully considering the intrinsic image structures and the complex noise characteristics. For HSIs, the global correlation is captured by the Kronecker-basis-representation-based tensor low-rankness measure, which integrates the insights delivered by both CP and Tucker decompositions; the local regularity is depicted by a plug-and-play spatial-spectral convolutional neural network with strong fitting ability to complex image features. For realistic noise, its statistical characteristics are encoded by a nonidentical and nonindependent distributed mixture of Gaussians distribution with flexible fitting capability. Then, we incorporate these image and noise priors into a probabilistic model based on the maximum a posteriori principle, and develop a solving scheme by combining expectation-maximization and alternating direction method of multipliers. Extensive experimental results on both simulated and real scenarios demonstrate the effectiveness of the proposed method and its superiority over the compared state-of-the- arts.
WOS:000607810600015
</snippet>
</document>

<document id="341">
<title>RoadVecNet: a new approach for simultaneous road network segmentation and vectorization from aerial and google earth imagery in a complex urban set-up</title>
<url>http://dx.doi.org/10.1080/15481603.2021.1972713</url>
<snippet>In this study, we present a new automatic deep learning-based network named Road Vectorization Network (RoadVecNet), which comprises interlinked UNet networks to simultaneously perform road segmentation and road vectorization. Particularly, RoadVecNet contains two UNet networks. The first network with powerful representation capability can obtain more coherent and satisfactory road segmentation maps even under a complex urban set-up. The second network is linked to the first network to vectorize road networks by utilizing all of the previously generated feature maps. We utilize a loss function called focal loss weighted by median frequency balancing (MFB_FL) to focus on the hard samples, fix the training data imbalance problem, and improve the road extraction and vectorization performance. A new module named dense dilated spatial pyramid pooling, which combines the benefit of cascaded modules with atrous convolution and atrous spatial pyramid pooling, is designed to produce more scale features over a broader range. Two types of high-resolution remote sensing datasets, namely, aerial and Google Earth imagery, were used for road segmentation and road vectorization tasks. Classification results indicate that the RoadVecNet outperforms the state-of-the-art deep learning-based networks with 92.51&#37; and 93.40&#37; F1 score for road surface segmentation and 89.24&#37; and 92.41&#37; F1 score for road vectorization from the aerial and Google Earth road datasets, respectively. In addition, the proposed method outperforms the other comparative methods in terms of qualitative results and produces high-resolution road segmentation and vectorization maps. As a conclusion, the presented method demonstrates that considering topological quality may result in improvement of the final road network, which is essential in various applications, such as GIS database updating.
WOS:000691150000001
</snippet>
</document>

<document id="342">
<title>Deep Transfer Learning for Improved Detection of Keratoconus using Corneal Topographic Maps</title>
<url>http://dx.doi.org/10.1007/s12559-021-09880-3</url>
<snippet>Clinical keratoconus (KCN) detection is a challenging and time-consuming task. In the diagnosis process, ophthalmologists must revise demographic and clinical ophthalmic examinations. The latter include slit-lamb, corneal topographic maps, and Pentacam indices (PI). We propose an Ensemble of Deep Transfer Learning (EDTL) based on corneal topographic maps. We consider four pretrained networks, SqueezeNet (SqN), AlexNet (AN), ShuffleNet (SfN), and MobileNet-v2 (MN), and fine-tune them on a dataset of KCN and normal cases, each including four topographic maps. We also consider a PI classifier. Then, our EDTL method combines the output probabilities of each of the five classifiers to obtain a decision based on the fusion of probabilities. Individually, the classifier based on PI achieved 93.1&#37; accuracy, whereas the deep classifiers reached classification accuracies over 90&#37; only in isolated cases. Overall, the average accuracy of the deep networks over the four corneal maps ranged from 86&#37; (SfN) to 89.9&#37; (AN). The classifier ensemble increased the accuracy of the deep classifiers based on corneal maps to values ranging (92.2&#37; to 93.1&#37;) for SqN and (93.1&#37; to 94.8&#37;) for AN. Including in the ensemble-specific combinations of corneal maps classifiers and PI increased the accuracy to 98.3&#37;. Moreover, visualization of first learner filters in the networks and Grad-CAMs confirmed that the networks had learned relevant clinical features. This study shows the potential of creating ensembles of deep classifiers fine-tuned with a transfer learning strategy as it resulted in an improved accuracy while showing learnable filters and Grad-CAMs that agree with clinical knowledge. This is a step further towards the potential clinical deployment of an improved computer-assisted diagnosis system for KCN detection to help ophthalmologists to confirm the clinical decision and to perform fast and accurate KCN treatment.
WOS:000662103400001
</snippet>
</document>

<document id="343">
<title>Surface Water Mapping by Deep Learning</title>
<url>http://dx.doi.org/10.1109/JSTARS.2017.2735443</url>
<snippet>Mapping of surface water is useful in a variety of remote sensing applications, such as estimating the availability of water, measuring its change in time, and predicting droughts and floods. Using the imagery acquired by currently active Landsat missions, a surface water map can be generated from any selected region as often as every 8 days. Traditional Landsat water indices require carefully selected threshold values that vary depending on the region being imaged and on the atmospheric conditions. They also suffer from many false positives, arising mainly from snow and ice, and from terrain and cloud shadows being mistaken for water. Systems that produce high-quality water maps usually rely on ancillary data and complex rule-based expert systems to overcome these problems. Here, we instead adopt a data-driven, deep-learning-based approach to surface water mapping. We propose a fully convolutional neural network that is trained to segment water on Landsat imagery. Our proposed model, named Deep-WaterMap, learns the characteristics of water bodies from data drawn from across the globe. The trained model separates water from land, snow, ice, clouds, and shadows using only Landsat bands as input. Our code and trained models are publicly available at http://live.ece.utexas.edu/research/deepwatermap/.
WOS:000415719000020
</snippet>
</document>

<document id="344">
<title>Deep learning to map concentrated animal feeding operations</title>
<url>http://dx.doi.org/10.1038/s41893-019-0246-x</url>
<snippet>Enforcement of environmental law depends critically on permitting and monitoring intensive animal agricultural facilities, known in the United States as concentrated animal feeding operations (CAFOs). The current legal landscape in the United States has made it difficult for government agencies, environmental groups and the public to know where such facilities are located. Numerous groups have, as a result, conducted manual, resource-intensive enumerations based on maps or ground investigation to identify facilities. Here we show that applying a deep convolutional neural network to high-resolution satellite images offers an effective, highly accurate and lower cost approach to detecting CAFO locations. In North Carolina, the algorithm is able to detect 589 additional poultry CAFOs, representing an increase of 15&#37; from the baseline that was detected through manual enumeration. We show how the approach scales over geography and time, and can inform compliance and monitoring priorities.
WOS:000463925700016
</snippet>
</document>

<document id="345">
<title>A Novel Method for Hyperspectral Image Classification: Deep Network With Adaptive Graph Structure Integration</title>
<url>http://dx.doi.org/10.1109/TGRS.2022.3150349</url>
<snippet>Hyperspectral image (HSI) classification has always been one of the hot issues in the study of geographic remote sensing information, and graph neural networks have attracted much attention in recent years. Several graph neural network-based approaches have been introduced into HSI study to explore the spatial information of HSI within a constructed graph. However, the existing methods of building HSI-based graphs are always unsuitable and inaccurate due to the complicated spatial variability of spectral signatures. Meanwhile, these graph-based HSI classification methods usually suffer from the over-smoothing problem. Motivated by these, this article presents a novel deep network with adaptive graph structure integration (DNAGSI), which could learn a graph structure of HSI dynamically and promote its discriminative ability with devising a much deeper network architecture. Specifically, dynamic graphs are first built across different layers and adaptively integrated with the initial graph structure to boost the robust graph representation of HSI. Second, the initial residual and identity mapping are employed to significantly increase the depth of the network and obtain more abstract deep features. Finally, a joint loss with center loss is devised to learn the similarity relationship between hyperspectral pixels explicitly, thereby gathering the intraclass graph features. Benefiting from the integration of center loss, initial residual, and identity mapping, the proposed method can alleviate the over-smoothing problem effectively to some extent. Experiments on benchmark HSI datasets demonstrate the superiority of DNAGSI over state-of-the-art methods.
WOS:000772472400020
</snippet>
</document>

<document id="346">
<title>Incorporating Deep Features into GEOBIA Paradigm for Remote Sensing Imagery Classification: A Patch-Based Approach</title>
<url>http://dx.doi.org/10.3390/rs12183007</url>
<snippet>The fast and accurate creation of land use/land cover maps from very-high-resolution (VHR) remote sensing imagery is crucial for urban planning and environmental monitoring. Geographic object-based image analysis methods (GEOBIA) provide an effective solution using image objects instead of individual pixels in VHR remote sensing imagery analysis. Simultaneously, convolutional neural networks (CNN) have been widely used in the image processing field because of their powerful feature extraction capabilities. This study presents a patch-based strategy for integrating deep features into GEOBIA for VHR remote sensing imagery classification. To extract deep features from irregular image objects through CNN, a patch-based approach is proposed for representing image objects and learning patch-based deep features, and a deep features aggregation method is proposed for aggregating patch-based deep features into object-based deep features. Finally, both object and deep features are integrated into a GEOBIA paradigm for classifying image objects. We explored the influences of segmentation scales and patch sizes in our method and explored the effectiveness of deep and object features in classification. Moreover, we performed 5-fold stratified cross validations 50 times to explore the uncertainty of our method. Additionally, we explored the importance of deep feature aggregation, and we evaluated our method by comparing it with three state-of-the-art methods in a Beijing dataset and Zurich dataset. The results indicate that smaller segmentation scales were more conducive to VHR remote sensing imagery classification, and it was not appropriate to select too large or too small patches as the patch size should be determined by imagery and its resolution. Moreover, we found that deep features are more effective than object features, while object features still matter for image classification, and deep feature aggregation is a critical step in our method. Finally, our method can achieve the highest overall accuracies compared with the state-of-the-art methods, and the overall accuracies are 91.21&#37; for the Beijing dataset and 99.05&#37; for the Zurich dataset.
WOS:000580262000001
</snippet>
</document>

<document id="347">
<title>Change Detection From Synthetic Aperture Radar Images Based on Channel Weighting-Based Deep Cascade Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2953128</url>
<snippet>Deep learning methods have recently demonstrated their significant capability for synthetic aperture radar (SAR) image change detection. However, with the increase of network depth, convolutional neural networks often encounter some negative effects, such as overfitting and exploding gradients. In addition, the existing deep networks employed in SAR change detection tend to produce a lot of redundant features that affect the performance of the network. To solve the aforementioned problems, this article proposed a deep cascade network (DCNet) for SAR image change detection. On the one hand, a very DCNet is established to exploit discriminative features, and residual learning is introduced to solve the exploding gradients problem. In addition, a fusion mechanism is employed to combine the outputs of different hierarchical layers to further alleviate the exploding gradient problem. Moreover, a simple yet effective channel weighting-based module is designed for SAR change detection. Average pooling and max pooling are used to aggregate channel-wise information. Meaningful channel-wise features are emphasized and unnecessary ones are suppressed. Therefore, the similarity in feature maps can be reduced, and then, the classification performance of the DCNet is improved. Experimental results on four real SAR datasets demonstrated that the proposed DCNet can obtain better change detection performance than several competitive methods. Our codes are available at https://github.com/summitgao/SAR_CD_DCNet.
WOS:000508437700030
</snippet>
</document>

<document id="348">
<title>DETECTION AND RECOGNITION OF TEXTS FEATURES FROM A TOPOGRAPHIC MAP USING DEEP LEARNING</title>
<url>http://dx.doi.org/</url>
<snippet>The aim of this research is to digitize texts present on topographic maps. Digitization is critical for information processing, hence texts on topographic maps that are in pixel format are digitized. The above is achieved using deep learning technology. Detection and recognition deep learning models are leveraged for first detecting texts on topographic maps and then recognition the detected texts. Detection model is inspired by Faster Region Based Convolutional Neural Networks (Faster R-CNN) to achieve real time text detection using region proposal networks and recognition model is inspired by Convolutional Recurrent Neural Network (CRNN), which is an end to end trainable neural network to achieve image-based sequence recognition that is suitable for recognizing texts on character level. The models have been trained and validated on self-curated dataset of topographic maps. The digitized information may be very helpful for many GIS applications like automated construction of Digital Elevation Model, Digital Surface Model, etc.
WOS:000859152500002
</snippet>
</document>

<document id="349">
<title>Intelligent Map Reader: A Framework for Topographic Map Understanding With Deep Learning and Gazetteer</title>
<url>http://dx.doi.org/10.1109/ACCESS.2018.2823501</url>
<snippet>Text features in topographic maps are important for helping users to locate the area that a map covers and to understand the maps content. Previous works on the optical detection of map text from topographic maps have used geometric features, the Hough transform, and segmentation. However, these approaches still face challenges when detecting map text in complicated contexts, especially when the map text is touching other map features, such as contours or geographical features. Thus, state-of-theart techniques for map text and feature recognition and manual interpretation and correction are always required to produce accurate results when optically converting topographic maps into a readable format. This paper proposes a methodological framework called the intelligent map reader that enables the automatic and accurate optical understanding of the content of a topographic map using deep learning techniques in combination with a gazetteer. The intelligent map reader framework includes the detection of map text via deep learning, the separation of text units via graph-based segmentation and clustering, optical character recognition (OCR) via an OCR engine, and digital-gazetteer-based map content understanding. Experimental results validate the efficiency and robustness of our proposed methodology for map text recognition and map content understanding. We expect the proposed intelligent map reader to contribute to various applications in the GeoAI fIeld.
WOS:000433619200001
</snippet>
</document>

<document id="350">
<title>Evaluation of machine learning-based algorithms for landslide detection across satellite sensors for the 2019 Cyclone Idai event, Chimanimani District, Zimbabwe</title>
<url>http://dx.doi.org/10.1007/s10346-022-01912-9</url>
<snippet>Changes in climatic patterns, manifested as intensified cyclones and torrential rainfalls in a warming world will inevitably impact the frequency of landslides. One such climatic extreme, Cyclone Idai (March 2019), caused significant havoc across southeastern Africa, including Mozambique, Malawi, and eastern Zimbabwe, by triggering thousands of landslides and widespread concurrent flooding, both of which resulted in substantial loss of life. The study was conducted in the Chimanimani District of eastern Zimbabwe to understand the impact of Cyclone Idai on landslide initiation, quantify the volume of mobilized hillslope material during the event, and compare the event-triggered material release against the annual erosional yield across the study area using machine learning and remote sensing techniques. We evaluated satellite imagery of various resolutions, namely, PlanetScope (3m/px),RapidEye (5m/px), and Sentinel-2 (tom/px) and three machine learning algorithms: artificial neural network (ANN), random forest (RF), and support vector machine (SVM) to identify landslides and compared the efficacy of the models. A total of nine predictor variables derived from the satellite imagery and a 3o-m ASTER Global Digital Elevation Model were employed to identify landslides and differentiate them from concurrent hydrological flooding. The models classified the study area into three classes: (i) landslides, (ii) flooding, and (iii) unaffected area. The RF model using PlanetScope satellite data attained the highest prediction accuracy of 97.88&#37;, whereas the accuracy of other machine learning model-satellite data combinations ranged between 94.58 and 97.27&#37;. Subsequently, landslide size thresholds were applied on the initially mapped landslides to eliminate noise and uncertainty from the data before estimating the final Cyclone Idai event-triggered landslide volume. A probability density function, which corresponds to a logarithmic plot of non-cumulative landslide frequency against the mapped landslide area, was employed to calculate landslide size thresholds using divergence and rollover point cutoff values. Finally, a landslide area-to-volume power-law scaling relationship was exploited to derive landslide volumes in the study area that ranged between 6.8 x 10(6) to 14.7 x 10(6) m(3) and 9.0 x 10(6) to 19.6 x 10(6) m(3) for divergence and rollover point thresholds, respectively, across the different combinations of machine learning models and satellite sensors. The estimated landslide volume indicates hillslope material liberated by Cyclone Idai was 269 to 345 times greater than the estimated annual average background denudation of the study area computed from a topography-based local erosion model.
WOS:000840286800001
</snippet>
</document>

<document id="351">
<title>Identification of Crop Type in Crowdsourced Road View Photos with Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/s21041165</url>
<snippet>In situ ground truth data are an important requirement for producing accurate cropland type map, and this is precisely what is lacking at vast scales. Although volunteered geographic information (VGI) has been proven as a possible solution for in situ data acquisition, processing and extracting valuable information from millions of pictures remains challenging. This paper targets the detection of specific crop types from crowdsourced road view photos. A first large, public, multiclass road view crop photo dataset named iCrop was established for the development of crop type detection with deep learning. Five state-of-the-art deep convolutional neural networks including InceptionV4, DenseNet121, ResNet50, MobileNetV2, and ShuffleNetV2 were employed to compare the baseline performance. ResNet50 outperformed the others according to the overall accuracy (87.9&#37;), and ShuffleNetV2 outperformed the others according to the efficiency (13 FPS). The decision fusion schemes major voting was used to further improve crop identification accuracy. The results clearly demonstrate the superior accuracy of the proposed decision fusion over the other non-fusion-based methods in crop type detection of imbalanced road view photos dataset. The voting method achieved higher mean accuracy (90.6-91.1&#37;) and can be leveraged to classify crop type in crowdsourced road view photos.
WOS:000624665100001
</snippet>
</document>

<document id="352">
<title>AUTOMATIC BUILDING FOOTPRINT EXTRACTION FROM UAV IMAGES USING NEURAL NETWORKS</title>
<url>http://dx.doi.org/10.15292/geodetski-vestnik.2020.04.545-561</url>
<snippet>Up-to-date cadastral maps are crucial for urban planning. Creating those maps with the classical geodetic methods is expensive and time-consuming. Emerge of Unmanned Aerial Vehicles (UAV) made a possibility for quick acquisition of data with much more details than it was possible before. The topic of the research refers to the challenges of automatic extraction of building footprints on high-resolution orthophotos. The objectives of this study were as follows: (1) to test the possibility of using different publicly available datasets (Tanzania, AIRS and Inria) for neural network training and then test the generalisation capability of the model on the Area Of Interest (AOI); (2) to evaluate the effect of the normalised digital surface model (nDSM) on the results of neural network training and implementation. Evaluation of the results shown that the models trained on the Tanzania (IoU 36.4&#37;), AIRS (IoU 64.4&#37;) and Inria (IoU 7.4&#37;) datasets doesn't satisfy the requested accuracy to update cadastral maps in study area. Much better results are achieved in the second part of the study, where the training of the neural network was done on tiles (256x256) of the orthophoto of AOI created from data acquired using UAV. A combination of RGB orthophoto with nDSM resulted in a 2&#37; increase of IoU, achieving the final IoU of over 90&#37;.
WOS:000618871100007
</snippet>
</document>

<document id="353">
<title>MACHINE LEARNING-BASED ECONOMIC DEVELOPMENT MAPPING FROM MULTI-SOURCE OPEN GEOSPATIAL DATA</title>
<url>http://dx.doi.org/10.5194/isprs-annals-V-4-2022-259-2022</url>
<snippet>Timely and accurate socioeconomic indicators are the prerequisite for smart social governance. For example, the level of economic development and the structure of population are important statistics for regional or national policy-making. However, the collection of these characteristics usually depends on demographic and social surveys, which are time- and labor-intensive. To address these issues, we propose a machine learning-based approach to estimate and map the economic development from multi-source open available geospatial data, including remote sensing imagery and OpenStreetMap road networks. Specifically, we first extract knowledge-based features from different data sources; then the multi-view graphs are constructed through different perspectives of spatial adjacency and feature similarity; and a multi-view graph neural network (MVGNN) model is built on them and trained in a self-supervised learning manner. Then, the handcrafted features and the learned graph representations are combined to estimate the regional economic development indicators via random forest models. Taking China's county-level gross domestic product (GDP) as an example, extensive experiments have been conducted and the results demonstrate the effectiveness of the proposed method, and the combination of the knowledge-based and learning-based features can significantly outperform baseline methods. Our proposed approach can advance the goal of acquiring timely and accurate socioeconomic variables through widely accessible geospatial data, which has the potential to extend to more social indicators and other geographic regions to support smart governance and policy-making in the future.
WOS:000855205000032
</snippet>
</document>

<document id="354">
<title>A Spectral-Spatial Feature Extraction Method With Polydirectional CNN for Multispectral Image Compression</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3158281</url>
<snippet>Convolutional neural networks (CNN) has been widely used in the research of multispectral image compression, but they still face the challenge of extracting spectral feature effectively while preserving spatial feature with integrity. In this article, a novel spectral-spatial feature extraction method is proposed with polydirectional CNN (SSPC) for multispectral image compression. First, the feature extraction network is divided into three parallel modules. The spectral module is employed to obtain spectral features along the spectral direction independently, and simultaneously, with two spatial modules extracting spatial features along two different spatial directions. Then all the features are fused together, followed by downsampling to reduce the size of the feature maps. To control the tradeoff between the rate loss and the distortion, the rate-distortion optimizer is added to the network. In addition, quantization and entropy encoding are applied in turn, converting the data into bit stream. The decoder is structurally symmetric to the encoder, which is convenient for structuring the framework to recover the image. For comparison, SSPC is tested along with JPEG2000 and three-dimensional (3-D) SPIHT on the multispectral datasets of Landsat-8 and WorldView-3 satellites. Besides, to further validate the effectiveness of polydirectional CNN, SSPC is also compared with a normal CNN-based algorithm. The experimental results show that SSPC outperforms other methods at the same bit rates, which demonstrates the validity of the spectral-spatial feature extraction method with polydirectional CNN.
WOS:000784198000001
</snippet>
</document>

<document id="355">
<title>Automated assessment of gear wear mechanism and severity using mould images and convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.triboint.2020.106280</url>
<snippet>A novel methodology for automated wear mechanism and severity assessment combining surface replication, imaging and deep learning is proposed. A large dataset of images of gear teeth moulds was built and covers abrasive wear, macropitting and scuffing, and three severity levels for each mechanism, i.e., mild, moderate and severe. A two-level inference methodology was implemented, based on a first convolutional neural network (CNN), which contains multiple convolutional layers and is commonly used for image classification, for wear mechanism identification, followed by three CNNs for wear severity estimation. The first level obtained a test classification accuracy of 98.22&#37; and the second of 95.16&#37; on average. The two-level system was also applied to full tooth flank mould images to generate wear mechanism and severity maps showing the geographical distribution of wear.
WOS:000528273000025
</snippet>
</document>

<document id="356">
<title>LISU: Low-light indoor scene understanding with joint learning of reflectance restoration</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.11.010</url>
<snippet>Semantic segmentation using convolutional neural networks (CNNs) achieves higher accuracy than traditional methods, but it fails to yield satisfactory results under illumination variants when the training set is limited. In this paper we present a new data set containing both real and rendered images and a novel cascade network to study semantic segmentation in low-light indoor environments. Specifically, the network decomposes a low-light image into illumination and reflectance components, and then a multi-tasking learning scheme is built. One branch learns to reduce noise and restore information on the reflectance (reflectance restoration branch). Another branch learns to segment the reflectance map (semantic segmentation branch). The CNN features from two tasks are concatenated together so as to improve the segmentation accuracy by embedding the illumination invariant features. We compare our approach with other CNN-based segmentation frameworks, including the state-of-the-art DeepLab v3+, on the proposed real data set, and our approach achieves the highest mIoU (47.6&#37;). The experimental results also show that the semantic information supports the restoration of a sharper reflectance map, thus further improving the segmentation. Besides, we pre-train a model with the proposed large-scale rendered images and then fine-tune it on the real images. The pre-training results in an improvement of mIoU by 7.2&#37;. Our models and data set are publicly available for research. This research is part of the EU project INGENIOUS(1). Our data sets and models are available on our website(2).
WOS:000782441900002
</snippet>
</document>

<document id="357">
<title>Hyperspectral Image Classification Method Based on CNN Architecture Embedding With Hashing Semantic Feature</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2911987</url>
<snippet>Deep convolutional neural networks (CNN) have led to a successful breakthrough for hyperspectral image (HSI) classification. In this paper, a CNN system embedded with an extracted hashing feature is proposed for HSI classification that utilizes the semantic information of the HSI. First, a series of hash functions are constructed to enhance the presentation of the locality and discriminability of classes. Then, the sparse binary hash codes calculated by the discriminative learning algorithm are combined into the original HSI. Next, we design a CNN framework with seven hidden layers to obtain the hierarchical feature maps with both spectral and spatial information for classification. A deconvolution layer aims to improve the robustness of the proposed CNN network and is used to enhance the expression of deep features. The proposed CNN classification architecture achieves powerful distinguishing ability from different classes. The extensive experiments on real hyperspectral images results demonstrate that the proposed CNN network can effectively improve the classification accuracy after the embedding of the extracted semantic features.
WOS:000476807300021
</snippet>
</document>

<document id="358">
<title>Comparing Deep Neural Networks, Ensemble Classifiers, and Support Vector Machine Algorithms for Object-Based Urban Land Use/Land Cover Classification</title>
<url>http://dx.doi.org/10.3390/rs11141713</url>
<snippet>With the advent of high-spatial resolution (HSR) satellite imagery, urban land use/land cover (LULC) mapping has become one of the most popular applications in remote sensing. Due to the importance of context information (e.g., size/shape/texture) for classifying urban LULC features, Geographic Object-Based Image Analysis (GEOBIA) techniques are commonly employed for mapping urban areas. Regardless of adopting a pixel- or object-based framework, the selection of a suitable classifier is of critical importance for urban mapping. The popularity of deep learning (DL) (or deep neural networks (DNNs)) for image classification has recently skyrocketed, but it is still arguable if, or to what extent, DL methods can outperform other state-of-the art ensemble and/or Support Vector Machines (SVM) algorithms in the context of urban LULC classification using GEOBIA. In this study, we carried out an experimental comparison among different architectures of DNNs (i.e., regular deep multilayer perceptron (MLP), regular autoencoder (RAE), sparse, autoencoder (SAE), variational autoencoder (AE), convolutional neural networks (CNN)), common ensemble algorithms (Random Forests (RF), Bagging Trees (BT), Gradient Boosting Trees (GB), and Extreme Gradient Boosting (XGB)), and SVM to investigate their potential for urban mapping using a GEOBIA approach. We tested the classifiers on two RS images (with spatial resolutions of 30 cm and 50 cm). Based on our experiments, we drew three main conclusions: First, we found that the MLP model was the most accurate classifier. Second, unsupervised pretraining with the use of autoencoders led to no improvement in the classification result. In addition, the small difference in the classification accuracies of MLP from those of other models like SVM, GB, and XGB classifiers demonstrated that other state-of-the-art machine learning classifiers are still versatile enough to handle mapping of complex landscapes. Finally, the experiments showed that the integration of CNN and GEOBIA could not lead to more accurate results than the other classifiers applied.
WOS:000480527800083
</snippet>
</document>

<document id="359">
<title>Water Quality Detection Based on FCN and Embedded System</title>
<url>http://dx.doi.org/10.2112/JCR-SI104-013.1</url>
<snippet>In this paper, the convolutional neural network (CNN) based on deep learning is applied to embedded systems. After summarizing the shortcomings of traditional algorithms, the image processing technology of full convolutional neural network (FCN) is adopted to study a method for online water quality measurement, which solves the key problem of image recognition. In view of the complex nonlinear characteristics of water quality related data, the full convolutional neural network is used to train water quality data, determine the mapping relationship between input and output, and input the obtained relationship into the embedded system to form a water quality detection system, so as to quickly and efficiently detect water pollution. The experimental results show that the accuracy of the predicted grade and the measured grade of water quality in the test area can be as high as 90&#37;.
WOS:000584510900013
</snippet>
</document>

<document id="360">
<title>Detection of Precipitation Cloud over the Tibet Based on the Improved U-Net</title>
<url>http://dx.doi.org/10.32604/cmc.2020.011526</url>
<snippet>Aiming at the problem of radar base and ground observation stations on the Tibet is sparsely distributed and cannot achieve large-scale precipitation monitoring. UNet, an advanced machine learning (ML) method, is used to develop a robust and rapid algorithm for precipitating cloud detection based on the new-generation geostationary satellite of FengYun-4A (FY-4A). First, in this algorithm, the real-time multi-band infrared brightness temperature from FY-4A combined with the data of Digital Elevation Model (DEM) has been used as predictor variables for our model. Second, the efficiency of the feature was improved by changing the traditional convolution layer serial connection method of U-Net to residual mapping. Then, in order to solve the problem of the network that would produce semantic differences when directly concentrated with low-level and high-level features, we use dense skip pathways to reuse feature maps of different layers as inputs for concatenate neural networks feature layers from different depths. Finally, according to the characteristics of precipitation clouds, the pooling layer of U-Net was replaced by a convolution operation to realize the detection of small precipitation clouds. It was experimentally concluded that the Pixel Accuracy (PA) and Mean Intersection over Union (MIoU) of the improved U-Net on the test set could reach 0.916 and 0.928, the detection of precipitation clouds over Tibet were well actualized.
WOS:000595341400005
</snippet>
</document>

<document id="361">
<title>A deep learning approach for mapping and dating burned areas using temporal sequences of satellite images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.12.014</url>
<snippet>Over the past decades, methods for burned areas mapping and dating from remote sensing imagery have been the object of extensive research. The limitations of current methods, together with the heavy pre-processing of input data they require, make them difficult to improve or apply to different satellite sensors. Here, we explore a deep learning approach based on daily sequences of multi-spectral images, as a promising and flexible technique that can be applicable to observations with various spatial and spectral resolutions. We test the proposed model for five regions around the globe using input data from VIIRS 750 m bands resampled to a 0.01 degrees spatial resolution grid. The derived burned areas are validated against higher resolution reference maps and compared with the MCD64A1 Collection 6 and FireCCI51 global burned area datasets. We show that the proposed methodology achieves competitive results in the task of burned areas mapping, despite using lower spatial resolution observations than the two global datasets. Furthermore, we improve the task of burned areas dating for the considered regions of study when compared with state-of-the-art products. We also show that our model can be used to map burned areas for low burned fraction levels and that it can operate in near-real-time, converging to the final solution in only a few days. The obtained results are a strong indication of the advantage of deep learning approaches for the problem of mapping and dating of burned areas and provide several routes for future research.
WOS:000510525500018
</snippet>
</document>

<document id="362">
<title>UNBALANCED GEOLOGIC BODY CLASSIFICATION OF HYPERSPECTRAL DATA BASED ON SQUEEZE AND EXCITATION NETWORKS AT TIANSHAN AREA</title>
<url>http://dx.doi.org/10.1109/IGARSS39084.2020.9323795</url>
<snippet>Hyperspectral data contains abundant spectral do main information, which is of great significance to classification of objects. However, due to the lack of labeled data, it is difficult to get an acceptable result by just using the small number of labeled data. We propose a semi-supervised classification model based on convolutional neural network and introduce the attention mechanism to balance the sample weight. After the convolution of the multi-layer network, more information is concentrated on the channels, so we use the Squeeze-and-Excitation block, which can adaptively recalibrates the channel-wise characteristic response by explicitly modelling the inter-channel dependencies. At the same time, we used focal loss to reduce the problem of poor training caused by uneven samples. We test our model on hyperspectral data at Tianshan area. From the result, we can find that our method can get a great result on the mineral classification task, which can be used for making geological map.
WOS:000664335306152
</snippet>
</document>

<document id="363">
<title>Learning Topometric Semantic Maps from Occupancy Grids</title>
<url>http://dx.doi.org/</url>
<snippet>Todays mobile robots are expected to operate in complex environments they share with humans. To allow intuitive human-robot collaboration, robots require a human-like understanding of their surroundings in terms of semantically classified instances. In this paper, we propose a new approach for deriving such instance-based semantic maps purely from occupancy grids. We employ a combination of deep learning techniques to detect, segment and extract door hypotheses from a random-sized map. The extraction is followed by a post-processing chain to further increase the accuracy of our approach, as well as place categorization for the three classes room, door and corridor. All detected and classified entities are described as instances specified in a common coordinate system, while a topological map is derived to capture their spatial links. To train our two neural networks used for detection and map segmentation, we contribute a simulator that automatically creates and annotates the required training data. We further provide insight into which features are learned to detect doorways, and how the simulated training data can be augmented to train networks for the direct application on real-world grid maps. We evaluate our approach on several publicly available real-world data sets. Even though the used networks are solely trained on simulated data, our approach demonstrates high robustness and effectiveness in various real-world indoor environments.
WOS:000544658403063
</snippet>
</document>

<document id="364">
<title>Large-Scale Semantic 3-D Reconstruction: Outcome of the 2019 IEEE GRSS Data Fusion Contest-Part A</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3032221</url>
<snippet>In this article, we present the scientific outcomes of the 2019 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2019 Contest addressed the problem of 3-D reconstruction and 3-D semantic understanding on a large scale. Several competitions were organized to assess specific issues, such as elevation estimation and semantic mapping from a single view, two views, or multiple views. In Part A, we report the results of the best-performing approaches for semantic 3-D reconstruction according to these various setups, whereas 3-D point cloud semantic mapping is discussed in Part B.
WOS:000607413900029
</snippet>
</document>

<document id="365">
<title>Lightweight Oriented Object Detection Using Multiscale Context and Enhanced Channel Attention in Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3079968</url>
<snippet>Object detection is a focal point in remote sensing applications. Remote sensing images typically contain a large number of small objects and a wide range of orientations across objects. This results in great challenges to small object detection approaches based on remote sensing images. Methods directly employ channel relations with equal weights to construct information features leads to inadequate feature representation in complex image small object detection tasks. Multiscale detection methods improve the speed and accuracy of detection, while small objects themselves contain limited information, and the features are easily lost following down-sampling. During the detection, the feature images are independent across scales, resulting in a discontinuity at the detection scale. In this article, we propose the multiscale context and enhanced channel attention (MSCCA) model. MSCCA employs PeleeNet as the backbone network. In particular, the feature image channel attention is enhanced and the multiscale context information is fused with multiscale detection methods to improve the characterization ability of the convolutional neural network. The proposed MSCCA method is evaluated on two real datasets. Results show that for 512 x 512 input images, MSCCA was able to achieve 80.4&#37; and 94.4&#37; mAP on the DOTA and NWPU VHR-10, respectively. Meanwhile, the model size of MSCCA is 21&#37; smaller than that of its predecessor. MSCCA can be considered as a practical lightweight oriented object detection model in remote sensing images.
WOS:000663535500012
</snippet>
</document>

<document id="366">
<title>Estimation of building height using a single street view image via deep neural networks</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.08.006</url>
<snippet>Building smart cities requires three-dimensional (3D) modelling to facilitate the planning and management of built environments. This requirement leads to high demand for data on vertical dimensions, such as building height, that are critical for the construction of 3D city models. Despite increasing recognition of the importance of such data, their acquisition in a low-cost and efficient manner remains a daunting task. Big data, particularly street view images (SVIs), provide an opportunity to efficiently solve this problem. In this study, we aim to derive information on building height from openly available SVIs by using single view metrology. Unlike other methods using multisource inputs, our method capitalizes on deep neural networks to extract a set of features - such as vanishing points, line segments, and semantic segmentation maps - for single view measurement and then es-timates the height from single SVIs. The minimal input required by the method increases its competitiveness in large-scale estimation of building heights, especially in areas with difficulty to obtain the conventional remote sensing data. In addition to experiments that demonstrate the effectiveness and efficiency of the proposed method, we also conduct a thorough analysis of uncertainties and errors brought by the method, thereby providing guidance for its future applications.
WOS:000848879100002
</snippet>
</document>

<document id="367">
<title>Robust graticule intersection localization for rotated topographic maps</title>
<url>http://dx.doi.org/10.1007/s00138-019-01025-9</url>
<snippet>Graticule intersections in topographic maps are usually considered to be suitable candidates for reference points in geometric calibration because the corresponding geographical information can be directly retrieved from the maps or derived from sheet numbers. Previous research on automatic corner point detection relies on the assumption that scanned maps are not rotated, which is rarely practical. To address this issue, a semantic segmentation approach for accurate graticule intersection localization is proposed in this paper. A fully convolutional network is utilized to provide pixel level information about the locations of specific rectangular objects at the corners of map frames by dense classification in regions of interest. The globally optimal segmentation of the foreground rotated object is obtained by the graph cuts technique. The bounding box of the rotated object is further retrieved with the minimum-area enclosing rectangle algorithm. Finally, the coordinates of graticule intersections are derived in accordance with the positions of the sliding windows and the relative locations of the vertices of the objects. The proposed method reduces the average localization error to 1.5 pixels, which is 32.4&#37; lower than that of the baseline model. The standard deviation of localization error is 0.91 pixels, which aligns with an average of 52&#37; improvements to the baseline model in the location variance metric.
WOS:000469483000014
</snippet>
</document>

<document id="368">
<title>An Experimental Research on the Use of Recurrent Neural Networks in Landslide Susceptibility Mapping</title>
<url>http://dx.doi.org/10.3390/ijgi8120578</url>
<snippet>Natural hazards have a great number of influencing factors. Machine-learning approaches have been employed to understand the individual and joint relations of these factors. However, it is a challenging process for a machine learning algorithm to learn the relations of a large parameter space. In this circumstance, the success of the model is highly dependent on the applied parameter reduction procedure. As a state-of-the-art neural network model, representative learning assumes full responsibility of learning from feature extraction to prediction. In this study, a representative learning technique, recurrent neural network (RNN), was applied to a natural hazard problem. To that end, it aimed to assess the landslide problem by two objectives: Landslide susceptibility and inventory. Regarding the first objective, an empirical study was performed to explore the most convenient parameter set. In landslide inventory studies, the capability of the implemented RNN on predicting the subsequent landslides based on the events before a certain time was investigated respecting the resulting parameter set of the first objective. To evaluate the behavior of implemented neural models, receiver operating characteristic analysis was performed. Precision, recall, f-measure, and accuracy values were additionally measured by changing the classification threshold. Here, it was proposed that recall metric be utilized for an evaluation of landslide mapping. Results showed that the implemented RNN achieves a high estimation capability for landslide susceptibility. By increasing the network complexity, the model started to predict the exact label of the corresponding landslide initiation point instead of estimating the susceptibility level.
WOS:000518041800057
</snippet>
</document>

<document id="369">
<title>IMG2nDSM: Height Estimation from Single Airborne RGB Images with Deep Learning</title>
<url>http://dx.doi.org/10.3390/rs13122417</url>
<snippet>Estimating the height of buildings and vegetation in single aerial images is a challenging problem. A task-focused Deep Learning (DL) model that combines architectural features from successful DL models (U-NET and Residual Networks) and learns the mapping from a single aerial imagery to a normalized Digital Surface Model (nDSM) was proposed. The model was trained on aerial images whose corresponding DSM and Digital Terrain Models (DTM) were available and was then used to infer the nDSM of images with no elevation information. The model was evaluated with a dataset covering a large area of Manchester, UK, as well as the 2018 IEEE GRSS Data Fusion Contest LiDAR dataset. The results suggest that the proposed DL architecture is suitable for the task and surpasses other state-of-the-art DL approaches by a large margin.
WOS:000666728000001
</snippet>
</document>

<document id="370">
<title>Deep Learning Based Land Cover and Crop Type Classification: A Comparative Study</title>
<url>http://dx.doi.org/10.1109/ICoDT252288.2021.9441483</url>
<snippet>Remote sensing data is available free of cost with an ever-increase in the number of satellites. This satellite imagery can be used as raw input from which cultivated/non-cultivated and crop fields can be mapped. Previous trends included the use of traditional ML techniques and standard CNN, RNN for such mappings. In this paper, we investigate the segmentation models for the task of Landcover and Crop type Classification. We investigate the UNet, SegNet, and DeepLabv3+ in the data-rich states of Nebraska, Mid-West, United States. We acquire dataset from Cropland data Layer provided by USDA National Agricultural Statistics Service. Our Experimental results show that cultivated and non-cultivated landcover is classified with an accuracy of 90&#37; and crop types are classified around 70&#37; ensuring the models trained on one geographical area can be used for accurate classification in other geographical areas, which makes it more reliable for real-time application in agricultural business. [GitHub]
WOS:000760235700011
</snippet>
</document>

<document id="371">
<title>An Oriented SAR Ship Detector With Mixed Convolution Channel Attention Module and Geometric Nonmaximum Suppression</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3206247</url>
<snippet>Benefiting from deep learning, synthetic aperture radar (SAR) ship detection based on convolutional neural network (CNN) has developed rapidly and corresponding performance is getting better. Nevertheless, most of the existing methods still cannot achieve a good balance between precision and recall in scenes with complex background interferences, or in a scene where two or more ships dock side by side. To address these problems, this article proposes a novel oriented SAR ship detector, which uses oriented bounding boxes (OBBs) to describe ships. For the purpose of reducing missed ships (aiming to improve recall) while suppressing false alarms (aiming to maintain precision), the proposed detector embeds a mixed convolution channel attention (MCCA) module into the backbone network, which highlights the important feature channels to enhance ship representation features by reweighting all channels of the feature map. In addition, we consider the geometric position relationship of neighbor ships and propose geometric nonmaximum suppression (G-NMS) to remove the redundant ship candidates or possible false alarms. Extensive experiments conducted on the SSDD and HRSIDs datasets demonstrate the effectiveness of MCCA and G-NMS, the proposed detector also achieves better performance compared to state-of-the-art OBB-based detectors.
WOS:000861443200002
</snippet>
</document>

<document id="372">
<title>Improved Land Cover Classification of VHR Optical Remote Sensing Imagery Based Upon Detail Injection Procedure</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3032423</url>
<snippet>Development of very-high-resolution (VHR) remote sensing imaging platforms have resulted in a requirement for developing refined land cover classification maps for various applications. Therefore, aiming at exploring the accurate boundary and complex interior texture retrieval in VHR optical remote sensing images, a novel detail injection network (DI-Net) is proposed in this article, which is composed of three aspects. First, the decoupling refinement module embedded with a multiscale representation is designed to improve the feature extraction capabilities that precede the encoding-to-decoding process. Second, we pay attention to the hard examples of boundary and complex interior texture in land cover classification and design two detail injection attention modules to solve the feature inactivation phenomenon in gradually convolutional encoding-to-decoding process. Third, a specific stage grading loss is proposed to adaptively regulate the structural-level weights of the encoding and decoding stages, which facilitates the details retrieval and produce refined land cover classification results. Finally, various datasets [&lt;italic&gt;incl.&lt;/italic&gt; International Society for Photogrammetry and Remote Sensing (ISPRS) and Gaofen Image Dataset (GID)] are employed to demonstrate that the proposed DI-Net achieves better performance than state-of-the-art methods. DI-Net provides more accurate boundaries and more consistent interior textures, and it achieves 86.86x0025; &lt;italic&gt;PA&lt;/italic&gt; and 68.37x0025; &lt;italic&gt;mIoU&lt;/italic&gt; on ISPRS dataset as well as 77.04x0025; &lt;italic&gt;PA&lt;/italic&gt; and 64.38x0025; &lt;italic&gt;mIoU&lt;/italic&gt; on GID dataset, respectively.
WOS:000679956300001
</snippet>
</document>

<document id="373">
<title>A Deep Convolutional Neural Network With Multiscale Feature Dynamic Fusion for InSAR Phase Filtering</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3199118</url>
<snippet>Interferometric phase filtering is a crucial step in the interferometric synthetic aperture radar (InSAR) data processing, which is also important for improving the accuracy of topography mapping and deformation monitoring. Most of the commonly used phase filtering methods perform windowing computations based on the statistical characteristics of a single interferogram in the spatial or frequency domain. However, the difficulty in taking into account the diversity and complexity of the phase image results in filtering methods with weak denoising, limited detail preservation, and poor generalization ability. At the same time, regardless of the spatial or frequency domain, improved phase filtering performance inevitably leads to the problem of declining effectiveness. This article proposes a phase filtering method based on the deep convolution neural network with multiscale feature dynamic fusion (MSFF). Unlike the traditional feedforward neural networks, the proposed method adopts a strategy of multiscale feature dynamic fusion that accounts for the deep and shallow features of the interferometric phase while also taking into account image detail preservation and noise suppression during phase filtering. Based on both subjective and objective evaluations, the experimental results using the simulated data prove that the proposed method has better noise suppression and detail preservation than the commonly used methods and that the filtering performance is less dependent on noise level. Experiments using the real data confirm that the proposed method has better generalization ability and can meet the precision requirements of practical applications. The method presented in this article can provide a new approach for research in high-precision InSAR data processing technology while also offering technical support for practical InSAR applications.
WOS:000845070100003
</snippet>
</document>

<document id="374">
<title>A New Method to Evaluate Gold Mineralisation-Potential Mapping Using Deep Learning and an Explainable Artificial Intelligence (XAI) Model</title>
<url>http://dx.doi.org/10.3390/rs14184486</url>
<snippet>Geoscientists have extensively used machine learning for geological mapping and exploring the mineral prospect of a province. However, the interpretation of results becomes challenging due to the complexity of machine learning models. This study uses a convolutional neural network (CNN) and Shapley additive explanation (SHAP) to estimate potential locations for gold mineralisation in Rengali Province, a tectonised mosaic of volcano-sedimentary sequences juxtaposed at the interface of the Archaean cratonic segment in the north and the Proterozoic granulite provinces of the Eastern Ghats Belt in Eastern India. The objective is to integrate multi-thematic data involving geological, geophysical, mineralogical and geochemical surveys on a 1:50 K scale with the aim of prognosticating gold mineralisation. The available data utilised during the integration include aero-geophysical (aeromagnetic and aerospectrometric), geochemical (national geochemical mapping), ground geophysical (gravity), satellite gravity, remote sensing (multispectral) and National Geomorphology and Lineament Project structural lineament maps obtained from the Geological Survey of India Database. The CNN model has an overall accuracy of 90&#37;. The SHAP values demonstrate that the major contributing factors are, in sequential order, antimony, clay, lead, arsenic content and a magnetic anomaly in CNN modelling. Geochemical pathfinders, including geophysical factors, have high importance, followed by the shear zones in mineralisation mapping. According to the results, the central parts of the study area, including the river valley, have higher gold prospects than the surrounding areas. Gold mineralisation is possibly associated with intermediate metavolcanics along the shear zone, which is later intruded by quartz veins in the northern part of the Rengali Province. This work intends to model known occurrences with respect to multiple themes so that the results can be replicated in surrounding areas.
WOS:000859069000001
</snippet>
</document>

<document id="375">
<title>An Optimized Deep Neural Network Detecting Small and Narrow Rectangular Objects in Google Earth Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2975606</url>
<snippet>Object detection is an important task for rapidly localizing target objects using high-resolution satellite imagery (HRSI). Although deep learning has been shown an efficient means of detection, object detection in HRSI remains problematic due to variations in object scale and size. In this article, we present a novel deep neural network (DNN) that combines double-shot neural network with misplaced localization strategy that adapts to object detection tasks in satellite images. This novel architecture optimizes the localization of small and narrow rectangular objects, which frequently appear in HRSI images, without accuracy loss on other size and width/height ratio objects. This method outperforms other state-of-art methods. We evaluated our proposed method on the NWPU VHR-10 public dataset and a new benchmark dataset (seven classes of small and narrow rectangular objects, SNRO-7). The NWPU VHR-10 dataset built a dataset for multiclass object detection; however, most labels are assigned in normal size and width/height ratios. SNRO-7 focuses on multiscale and multisize object detection and includes many small-size and narrow rectangular objects. We also evaluated the accuracy difference on DNN training and testing between gray scale and RGB datasets. The results of the experiment on object detection reveal that the mean average precision (MaP) of our method is 82.6&#37; in NWPU VHR-10 and 79.3&#37; in SNRO-7, which exceeds the MaPs of other state-of-the-art object detection neural networks. The model trained with the RGB dataset can achieve similar accuracy (around 79.0&#37; MIoU) testing in both RGB and gray scale datasets. When training the model by mixing RGB and gray scale datasets in different ratios, the accuracy in the RGB channel significantly decreases with increasing gray scale images, but this does not influence the accuracy in the gray scale dataset.
WOS:000528932700001
</snippet>
</document>

<document id="376">
<title>Multi-Feature Learning by Joint Training for Handwritten Formula Symbol Recognition</title>
<url>http://dx.doi.org/10.1109/ACCESS.2020.2979346</url>
<snippet>Given the similarity of handwritten formula symbols and various handwriting styles, this paper proposes a squeeze-extracted multi-feature convolution neural network (SE-MCNN) to improve the recognition rate of handwritten formula symbols. The system proposed in this paper integrates the eight-directional feature of the original sequence in the convolutional layer, which significantly compensates for the lost dynamic trajectory information in the handwritten formula symbol. Meanwhile, the joint loss is constructed to improve the discriminability of features in the way of supervised learning, which enlarges the inter-class difference and decreases inner-class similarity. The standard mathematical formula symbol library provided by the Competition Organization on Recognition of Online Handwritten Mathematical Expression (CROHME) is used to verify the effectiveness of the proposed algorithm. Experiments show that the proposed SE-MCNN approach outperforms the state-of-the-art methods even at the condition of without using the data augmentation.
WOS:000524689500008
</snippet>
</document>

<document id="377">
<title>Automatic 3D building reconstruction from multi-view aerial images with deep learning</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.11.011</url>
<snippet>The study presented in this paper introduced a new fully automatic three-dimensional building reconstruction method that can generate first level of detail (LoD 1) building models from multi-view aerial images without any assistance from other data. The accuracy and completeness of our reconstructed models have approached that of manually delineated models to a large extent. The presented method consists of three parts: (1) efficient dense matching and Earth surface reconstruction, (2) reliable building footprint extraction and polygon regularization, and (3) highly accurate height inference of building roofs and bases. First, our novel deep learning-based multi-view matching method, composed of a convolutional neural network, gated recurrent convolutions, and a multi-scale pyramid matching structure, is used to reconstruct the digital surface model (DSM) and digital orthophoto map (DOM) efficiently without generating epipolarly rectified images. Second, our three-stage 2D building extraction method is introduced to deliver reliable and accurate building contours. Deep-learning based segmentation, assisted with DSM, is used to segment buildings from backgrounds; and the generated building maps are fused with a terrain classification algorithm to reach better segmentation results. A polygon regularization algorithm and a level set algorithm are thereafter employed to transfer the binary segmentation maps to structured vector-form building polygons. Third, a novel method is introduced to infer the height of building roofs and bases using adaptive local terrain filtering and neighborhood buffer analysis. We tested our method on a large experimental area that covered 2284 aerial images and 782 various types of buildings. Our results as far as correctness and completeness exceeded the results of other similar methods in a between-method comparison by at least 15&#37; for individual 3D building models with many of them comparable to manual delineation results.
WOS:000604406500011
</snippet>
</document>

<document id="378">
<title>Modified Deep Reinforcement Learning with Efficient Convolution Feature for Small Target Detection in VHR Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.3390/ijgi10030170</url>
<snippet>Small object detection in very-high-resolution (VHR) optical remote sensing images is a fundamental but challenaging problem due to the latent complexities. To tackle this problem, the MdrlEcf model is proposed by modifying deep reinforcement learning (DRL) and extracting the efficient convolution feature. Firstly, an efficient attention network is constructed by introducing the local attention into the convolutional neural network. Combining the shallow low-level features with rich detail descriptions and high-level features with more semantic meanings effectively, efficient convolution features can be obtained. By this, the attention network can effectively enhance the ability to extract small target features and suppressing useless features. Secondly, the efficient feature map is sent to the region proposal network constructed by modified DRL. Using the modified reward function, this model can accumulate more rewards to conduct the search process, and potentially generate effective subsequent proposals and classification scores. It also can increase the effectiveness of object locations and classifications for small targets. Quantitative and qualitative experiments are conducted to verify the detection performance of different models. The results show that the proposed MdrlEcf can effectively and accurately locate and identify related small objects.
WOS:000633704600001
</snippet>
</document>

<document id="379">
<title>Satellite Image Classification with Deep Learning</title>
<url>http://dx.doi.org/</url>
<snippet>Satellite imagery is important for many applications including disaster response, law enforcement, and environmental monitoring. These applications require the manual identification of objects and facilities in the imagery. Because the geographic expanses to be covered are great and the analysts available to conduct the searches are few, automation is required. Yet traditional object detection and classification algorithms are too inaccurate and unreliable to solve the problem. Deep learning is a family of machine learning algorithms that have shown promise for the automation of such tasks. It has achieved success in image understanding by means of convolutional neural networks. In this paper we apply them to the problem of object and facility recognition in high-resolution, multi-spectral satellite imagery. We describe a deep learning system for classifying objects and facilities from the IARPA Functional Map of the World (fMoW) dataset into 63 different classes. The system consists of an ensemble of convolutional neural networks and additional neural networks that integrate satellite metadata with image features. It is implemented in Python using the Keras and TensorFlow deep learning libraries and runs on a Linux server with an NVIDIA Titan X graphics card. At the time of writing the system is in 2nd place in the fMoW TopCoder competition. Its total accuracy is 83&#37; the F-1 score is 0.797, and it classifies 15 of the classes with accuracies of 95&#37; or better.
WOS:000454739700035
</snippet>
</document>

<document id="380">
<title>Mapping trees along urban street networks with deep learning and street-level imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.01.016</url>
<snippet>Planning and managing urban forests for livable cities remains a challenge worldwide owing to sparse information on the spatial distribution, structure and composition of urban trees and forests. National and municipal sources of tree inventory remain limited due to a lack of detailed, consistent and frequent inventory assessments. Despite advancements in research on the automation of urban tree mapping using Light Detection and Ranging (LiDAR) or high-resolution satellite imagery, in practice most municipalities still perform labor-intensive field surveys to collect and update tree inventories. We present a robust, affordable and rapid method for creating tree inventories in any urban region where sufficient street-level imagery is readily available. Our approach is novel in that we use a Mask Regional Convolutional Neural Network (Mask R-CNN) to detect and locate separate tree instances from street-level imagery, thereby successfully creating shape masks around unique fuzzy urban objects like trees. The novelty of this method is enhanced by using monocular depth estimation and triangulation to estimate precise tree location, relying only on photographs and images taken from the street. Experiments across four cities show that our method is transferable to different image sources (Google Street View, Mapillary) and urban ecosystems. We successfully detect &gt;70&#37; of all public and private trees recorded in a ground-truth campaign across Metro Vancouver. The accuracy of geolocation is also promising. We automatically locate public and private trees with a mean error in the absolute position ranging from 4 to 6 m, which is comparable to ground-truth measurements in conventional manual urban tree inventory campaigns.
WOS:000644695700011
</snippet>
</document>

<document id="381">
<title>HOWFAR SHOULD I LOOK? A NEURAL ARCHITECTURE SEARCH STRATEGY FOR SEMANTIC SEGMENTATION OF REMOTE SENSING IMAGES</title>
<url>http://dx.doi.org/10.5194/isprs-annals-V-3-2022-17-2022</url>
<snippet>Neural architecture search (NAS) is a subset of automated machine learning that tries to find the best neural network to perform a given task. In this article, a network search space is defined and applied to perform the semantic segmentation of satellite imagery. Due to the spatial nature of the data, the search space uses cells that group parallel operations with kernels of different sizes, providing options to accommodate the neighborhood information required to perform a better classification. The architecture search space follows a UNet-like network. The proposed approach uses scaled sigmoid gates, a strategy for network pruning that was adapted to search for the best operations on the cell search space. The architecture achieved by the proposed approach uses wider kernels on lower resolution feature maps, which leads to the interpretation that some pixels required information from pixels farther away than expected. The resulting network was compared to a very similar UNet-like network that only used 3x3 convolutions. The resulting network shows slightly better results on the test set.
WOS:000855203200004
</snippet>
</document>

<document id="382">
<title>Towards a new image archive for the built environment</title>
<url>http://dx.doi.org/10.1177/23998083211011474</url>
<snippet>The ever-growing online corpus of images of the built environment, on social media and mapping platforms, offers a new kind of archive of the built environment. Recent advances in computer vision, specifically convolutional neural networks, offer new ways of querying and analyzing large image corpuses. In this paper, we propose a new method by which historians of the built environment can use these vast image corpuses in their study, enabling new research questions. To demonstrate proof of need, we report on an ongoing case study in Tel Aviv that attempts to show the feasibility of our proposed method for enabling a Historic Urban Landscapes (HUL)-based approach to the study of the built environment. In so doing, we show how such image corpuses could potentially form a new type of archive for architectural and urban history.
WOS:000650021500001
</snippet>
</document>

<document id="383">
<title>A Deep Learning Method for Near-Real-Time Cloud and Cloud Shadow Segmentation from Gaofen-1 Images</title>
<url>http://dx.doi.org/10.1155/2020/8811630</url>
<snippet>In this study, an essential application of remote sensing using deep learning functionality is presented. Gaofen-1 satellite mission, developed by the China National Space Administration (CNSA) for the civilian high-definition Earth observation satellite program, provides near-real-time observations for geographical mapping, environment surveying, and climate change monitoring. Cloud and cloud shadow segmentation are a crucial element to enable automatic near-real-time processing of Gaofen-1 images, and therefore, their performances must be accurately validated. In this paper, a robust multiscale segmentation method based on deep learning is proposed to improve the efficiency and effectiveness of cloud and cloud shadow segmentation from Gaofen-1 images. The proposed method first implements feature map based on the spectral-spatial features from residual convolutional layers and the cloud/cloud shadow footprints extraction based on a novel loss function to generate the final footprints. The experimental results using Gaofen-1 images demonstrate the more reasonable accuracy and efficient computational cost achievement of the proposed method compared to the cloud and cloud shadow segmentation performance of two existing state-of-the-art methods.
WOS:000590888100001
</snippet>
</document>

<document id="384">
<title>Data-driven polyline simplification using a stacked autoencoder-based deep neural network</title>
<url>http://dx.doi.org/10.1111/tgis.12965</url>
<snippet>Automatic simplification of polylines is an important issue in spatial database and mapping. Traditional rule-based methods are usually limited in performance, especially when the man-made rules have to be adapted to different polylines with different shapes and structures. Compared to the existing neural network methods focusing only on the output layer or the code layers for classification or regression, our proposed method generates multi-level abstractions of polylines by extracting features from multiple hidden layers. Specifically, we first organize the cartographic polylines into the form of feature vectors acceptable to the neural network model. Then, a stacked autoencoder-based deep neural network model is trained to learn the pattern features of polyline bends and omit unimportant details layer by layer. Finally, the multi-level abstractions of input polylines are generated from different hidden layers of a single model. The experimental results demonstrate that, compared with the classic Douglas-Peucker and Wang and Muller algorithms, the proposed method is able to properly simplify the polylines while representing their essential shapes smoothly and reducing areal displacement.
WOS:000811583700001
</snippet>
</document>

<document id="385">
<title>Bounding Box-Free Instance Segmentation Using Semi-Supervised Iterative Learning for Vehicle Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3169128</url>
<snippet>Vehicle classification is a hot computer vision topic, with studies ranging from ground-view to top-view imagery. Top-view images allow understanding city patterns, traffic management, among others. However, there are some difficulties for pixel-wise classification: most vehicle classification studies use object detection methods, and most publicly available datasets are designed for this task, creating instance segmentation datasets is laborious, and traditional instance segmentation methods underperform on this task since the objects are small. Thus, the present research objectives are as follows: first, propose a novel semisupervised iterative learning approach using the geographic information system software, second, propose a box-free instance segmentation approach, and third, provide a city-scale vehicle dataset. The iterative learning procedure considered the following: first, labeling a few vehicles from the entire scene, second, choosing training samples near those areas, third, training the deep learning model (U-net with efficient-net-B7 backbone), fourth, classifying the whole scene, fifth, converting the predictions into shapefile, sixth, correcting areas with wrong predictions, seventh, including them in the training data, eighth repeating until results are satisfactory. We considered vehicle interior and borders to separate instances using a semantic segmentation model. When removing the borders, the vehicle interior becomes isolated, allowing for unique object identification. Our procedure is very efficient and accurate for generating data iteratively, which resulted in 122 567 mapped vehicles. Metrics-wise, our method presented higher intersection over union when compared to box-based methods (82&#37; against 72&#37;), and per-object metrics surpassed 90&#37; for precision and recall.
WOS:000797454600001
</snippet>
</document>

<document id="386">
<title>Generating annual high resolution land cover products for 28 metropolises in China based on a deep super-resolution mapping network using Landsat imagery</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2142727</url>
<snippet>High resolution of global land cover dynamic is indicative for understanding the influence of anthropogenic activity on environmental change. However, most of the land cover products are based on Landsat image that only has 30 m resolution, which is insufficient to distinguish the heterogenous urban structure; while very high spatial resolution image usually has low temporal resolution, which is difficult to monitor the urban dynamic. Deep-learning-driven super-resolution mapping is a prevailing way of achieving very-high-resolution land cover dynamic products in aspect of alleviating the mixed pixel problem of Landsat image. However, two limitations are obvious: 1) the fixed grid of kernel during the upsampling process favors spatial homogeneity and suppresses the learning of spatial heterogeneity of urban composition and 2) geometric or radiation variation over large spatial and long temporal extent in remote sensing images makes the super-resolution mapping approach difficult to transfer for application. Here, we attempt to solve these two limitations: 1) a progressive edge-guided super-resolution architecture is designed to allow nonuniformed kernel specific at the low-confidence edge region and intensify the learning of heterogenous compositions patterns and 2) an alternating optimization strategy is designed to minimize the resultant entropy and modulate the classification hyperplane to accommodate to the manifold of the discrepant region. Validation experiments are investigated based on a fine-grained and large-extent super-resolution (FLAS) dataset constructed in this study, and it is found that our approach remarkably enhances rich detailed patterns of heterogenous region and outperforms other state-of-the-art algorithms. Besides, we applied DETNet to the large spatial extent of 28 metropolises in China (&gt;40,000 km(2)) and the large temporal extent of continuous 21-year (2000-2020) in Wuhan city to examine transferability. From the land cover areas variation, we find that the expansion rate of cropland is faster than the urban expansion over the past 10 years, which are gradually becoming the principal source for the encroachment of forest and lakes. From detailed urban dynamic reflected by the 21-year products, we find that urban-villages between the old city zone and the outer high-tech development zone are gradually disappeared. The captured dynamic is consistence with the urban-village renovation policy during this period, which is meant to redistribute the spatial configuration of the city for a more sustainable urban structure. We believe that the proposed method can facilitate a seamless and fine-grained observation system that can fill the weakness of the existing land cover activities and provide a brand-new insight into the urban dynamic and its underlying mechanism.
WOS:000884630400001
</snippet>
</document>

<document id="387">
<title>The Outcome of the 2021 IEEE GRSS Data Fusion Contest-Track MSD: Multitemporal Semantic Change Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3144318</url>
<snippet>We present here the scientific outcomes of the 2021 Data Fusion Contest (DFC2021) organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. DFC2021 was dedicated to research on geospatial artificial intelligence (AI) for social good with a global objective of modeling the state and changes of artificial and natural environments from multimodal and multitemporal remotely sensed data toward sustainable developments. DFC2021 included two challenge tracks: "Detection of settlements without electricity" and "Multitemporal semantic change detection." This article mainly focuses on the outcome of the multitemporal semantic change detection track. We describe in this article the DFC2021 dataset that remains available for further evaluation of corresponding approaches and report the results of the best-performing methods during the contest.
WOS:000756804000002
</snippet>
</document>

<document id="388">
<title>Machine Learning Approaches for Quantifying Margalefidinium Polykrikoides Bloom from Airborne Hyperspectral Imagery</title>
<url>http://dx.doi.org/10.2112/SI102-025.1</url>
<snippet>The ichthyotoxic mixotrophic dinoflagellate Margalefidinium polykrikoides is well known for killing fish in aquaculture cages via gill clogging at high cell abundance. In order to establish a countermeasure against M. polykrikoides blooms, it is essential to monitor and quantify the spatial abundance of red tide cells with high accuracy. In this study, machine-learning (ML) approaches were applied to investigate the spectral features of M. polykrikoides cell abundance on the south coast of Korea. Four ML models were developed to retrieve the M polykrikoides cell abundance in optically complex waters from airborne hyperspectral imagery (HSI). The models included the feed forward neural network (FFN), support vector machine (SVM), ensemble bagged tree (EBT), and Gaussian process regression (GPR). Paired data of in situ spectra and M. polykrikoides cell abundance were used to train and validate the ML models. The performances of ML models were evaluated using the linear regression value (R-2) and root mean squared error (RMSE), considering the predicted cell abundances calculated from HSI and ground truth. The GPR model utilized the validation data and showed the best performance (R-2 = 0.57 and RMSE = 659.18 cells mL(-1)). GPR was also able to estimate the cell abundance with over 3,000 cell mL(-1), indicating viable results. Therefore, using GPR is suggested to obtain more accurate red tide cell abundance maps. Local-based ML models for red tide cell abundance retrieval from HSI data can support the monitoring and surveillance of red tide blooms on the south coast of Korea.
WOS:000600072400026
</snippet>
</document>

<document id="389">
<title>A Skeleton-Line-Based Graph Convolutional Neural Network for Areal Settlements' Shape Classification</title>
<url>http://dx.doi.org/10.3390/app121910001</url>
<snippet>Among the geographic elements, shape recognition and classification is one of the im portant elements of map cartographic generalization, and the shape classification of an areal settlement is an important part of geospatial vector data. However, there is currently no relatively simple and efficient way to achieve areal settlement classification. Therefore, we combined the skeleton line vector data of an areal settlement and the graph convolutional neural network to propose an areal settlement shape classification method that (1) extracts the skeleton line of the areal settlement to form a dual graph with nodes as edges, (2) extracts multiple features to obtain a graph representation of the shape, (3) extracts and aggregates the shape information represented by the areal settlement skeleton line using the graph convolutional neural network for multiple rounds to extract high-dimensional shape information, and (4) completes the shape classification of the high-dimensional shape information. The experiment used 240 samples, and the classification accuracy was 93.3&#37;, with areal settlement shapes of E-, F-, and H-type achieving F-measures of 96.5&#37;, 92.3&#37;, and 100&#37;, respectively. The result shows that the classification method of the areal settlement shape has high accuracy.
WOS:000866719400001
</snippet>
</document>

<document id="390">
<title>GIS-Supervised Building Extraction With Label Noise-Adaptive Fully Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/LGRS.2019.2963065</url>
<snippet>Automatic building extraction from aerial or satellite images is a dense pixel prediction task for many applications. It demands a large number of clean label data to train a deep neural network for building extraction. But it is labor expensive to collect such pixel-wise annotated data manually. Fortunately, the building footprint data of geographic information system (GIS) maps provide a cheap way of generating building label data, but these labels are imperfect due to misalignment between the GIS maps and images. In this letter, we consider the task of learning a deep neural network to label images pixel-wise from such noisy label data for building extraction. To this end, we propose a general label noise-adaptive (NA) neural network framework consisting of a base network followed by an additional probability transition modular (PTM) which is introduced to capture the relationship between the true label and the noisy label. The parameters of the PTM can be estimated as part of the training process of the whole network by the off-the-shelf backpropagation algorithm. We conduct experiments on real-world data set to demonstrate that our proposed PTM can better handle noisy labels and improve the performance of convolutional neural networks (CNNs) trained on the noisy label data generated by GIS maps for building extraction. The experimental results indicate that being armed with our proposed PTM for fully CNN, it provides a promising solution to reduce manual annotation effort for the labor-expensive object extraction tasks from remote sensing images.
WOS:000594634800023
</snippet>
</document>

<document id="391">
<title>Airborne LiDAR point cloud classification with global-local graph attention convolution neural network</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.01.007</url>
<snippet>Airborne light detection and ranging (LiDAR) plays an increasingly significant role in urban planning, topographic mapping, environmental monitoring, power line detection and other fields thanks to its capability to quickly acquire large-scale and high-precision ground information. To achieve point cloud classification, previous studies proposed point cloud deep learning models that can directly process raw point clouds based on PointNet-like architectures. And some recent works proposed graph convolution neural network based on the inherent topology of point clouds. However, the above point cloud deep learning models only pay attention to exploring local geometric structures, yet ignore global contextual relationships among all points. In this paper, we present a global-local graph attention convolution neural network (GACNN) that can be directly applied to the classification of unstructured 3D point clouds obtained by airborne LiDAR. Specifically, we first introduce a graph attention convolution module that incorporates global contextual information and local structural features. The global attention module examines spatial relationships among all points, while the local attention module can dynamically learn convolution weights with regard to the spatial position of the local neighboring points and reweight the convolution weights by inspecting the density of each local region. Based on the proposed graph attention convolution module, we further design an end-to-end encoder-decoder network, named GACNN, to capture multiscale features of the point clouds and therefore enable more accurate airborne point cloud classification. Experiments on the ISPRS 3D labeling dataset show that the proposed model achieves a new state-of-the-art performance in terms of average F1 score (71.5&#37;) and a satisfying overall accuracy (83.2&#37;). Additionally, experiments further conducted on the 2019 Data Fusion Contest Dataset by comparing with other prevalent point cloud deep learning models demonstrate the favorable generalization capability of the proposed model.
WOS:000640986100012
</snippet>
</document>

<document id="392">
<title>Computer-aided recognition of myopic tilted optic disc using deep learning algorithms in fundus photography</title>
<url>http://dx.doi.org/10.1186/s12886-020-01657-w</url>
<snippet>BackgroundIt is necessary to consider myopic optic disc tilt as it seriously impacts normal ocular parameters. However, ophthalmologic measurements are within inter-observer variability and time-consuming to get. This study aimed to develop and evaluate deep learning models that automatically recognize a myopic tilted optic disc in fundus photography.MethodsThis study used 937 fundus photographs of patients with normal or myopic tilted disc, collected from Samsung Medical Center between April 2016 and December 2018. We developed an automated computer-aided recognition system for optic disc tilt on color fundus photographs via a deep learning algorithm. We preprocessed all images with two image resizing techniques. GoogleNet Inception-v3 architecture was implemented. The performances of the models were compared with the human examiners results. Activation map visualization was qualitatively analyzed using the generalized visualization technique based on gradient-weighted class activation mapping (Grad-CAM++).ResultsNine hundred thirty-seven fundus images were collected and annotated from 509 subjects. In total, 397 images from eyes with tilted optic discs and 540 images from eyes with non-tilted optic discs were analyzed. We included both eye data of most included patients and analyzed them separately in this study. For comparison, we conducted training using two aspect ratios: the simple resized dataset and the original aspect ratio (AR) preserving dataset, and the impacts of the augmentations for both datasets were evaluated. The constructed deep learning models for myopic optic disc tilt achieved the best results when simple image-resizing and augmentation were used. The results were associated with an area under the receiver operating characteristic curve (AUC) of 0.9780.008, an accuracy of 0.960 +/- 0.010, sensitivity of 0.937 +/- 0.023, and specificity of 0.963 +/- 0.015. The heatmaps revealed that the model could effectively identify the locations of the optic discs, the superior retinal vascular arcades, and the retinal maculae.Conclusions We developed an automated deep learning-based system to detect optic disc tilt. The model demonstrated excellent agreement with the previous clinical criteria, and the results are promising for developing future programs to adjust and identify the effect of optic disc tilt on ophthalmic measurements.
WOS:000578602000003
</snippet>
</document>

<document id="393">
<title>The visual quality of streets: A human-centred continuous measurement based on machine learning algorithms and street view images</title>
<url>http://dx.doi.org/10.1177/2399808319828734</url>
<snippet>This study proposes a workable approach for quantitatively measuring the perceptual-based visual quality of streets, which has often relied on subjective impressions or feelings. With the help of recently emerged street view images and machine learning algorithms, an evaluation model has been trained to assess the perceived visual quality with accuracy similar to that of experienced urban designers, to provide full coverage and detailed results for a citywide area. The town centre of Shanghai was selected for the site. Around 140,000 screenshots from Baidu Street View were processed and a machine learning algorithm, SegNet, was applied to intelligently extract the pixels representing key elements affecting the visual quality of streets, including the building frontage, greenery, sky view, pedestrian space, motorisation, and diversity. A Java-based program was then produced to automatically collect the preferences of experienced urban designers on representative sample images. Another machine learning algorithm, i.e. an artificial neural network, was used to train an evaluation model to achieve a citywide, high-resolution evaluation of the visual quality of the streets. Further validation through different approaches shows this evaluation model obtains a satisfactory accuracy. The results from the artificial neural network also help to explore the high or low effects of various key elements on visual quality. In short, this study contributes to the development of human-centred planning and design by providing continuous measurements of an unmeasurable quality across large-scale areas. Meanwhile, insights on the perceptual-based visual quality and detailed mapping of various key elements in streets can assist in more efficient street renewal by providing accurate design guidance.
WOS:000485946000005
</snippet>
</document>

<document id="394">
<title>Multisensor Land Cover Classification With Sparsely Annotated Data Based on Convolutional Neural Networks and Self-Distillation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3119191</url>
<snippet>Extensive research studies have been conducted in recent years to exploit the complementarity among multisensor (or multimodal) remote sensing data for prominent applications such as land cover mapping. In order to make a step further with respect to previous studies, which investigate multitemporal SAR and optical data or multitemporal/multiscale optical combinations, here, we propose a deep learning framework that simultaneously integrates all these input sources, specifically multitemporal SAR/optical data and fine-scale optical information at their native temporal and spatial resolutions. Our proposal relies on a patch-based multibranch convolutional neural network (CNN) that exploits different per-source encoders to deal with the specificity of the input signals. In addition, we introduce a new self-distillation strategy to boost the per-source analyses and exploit the interplay among the different input sources. This new strategy leverages the final prediction of the multisource framework to guide the learning of the per-source CNN encoders supporting the network to learn from itself. Experiments are carried out on two real-world benchmarks, namely, the Reunion island (a French overseas department) and the Dordogne study site (a southwest department in France), where the annotated reference data were collected under operational constraints (sparsely annotated ground-truth data). Obtained results providing an overall classification accuracy of about 94&#37; (respectively, 88&#37;) on the Reunion island (respectively, the Dordogne) study site highlight the effectiveness of our framework based on CNNs and self-distillation to combine heterogeneous multisensor remote sensing data and confirm the benefit of multimodal analysis for downstream tasks such as land cover mapping.
WOS:000720519100015
</snippet>
</document>

<document id="395">
<title>BUILDING EXTRACTION FROM HIGH-RESOLUTION REMOTE SENSING IMAGERY BASED ON MULTI-SCALE FEATURE FUSION AND ENHANCEMENT</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B3-2022-55-2022</url>
<snippet>The accurate detection and mapping of buildings from high-resolution remote sensing (HRRS) images have attracted extensive attention. However, as an artificial target, buildings not only have various types, but also have multi-scale characteristics and complex context, which brings great challenges to the accurate identification of buildings. To deal with this problem, a semantic segmentation model based on multi-scale feature fusion and enhancement (MSFFE) is proposed for building extraction from HRRS images. Specifically, the proposed model uses the network structure of encoder and decoder. In the encoding stage, densely connected convolutional neural network is used as an encoder to extract multi-level spatial and semantic features. To effectively use the multi-scale features of buildings, a multi-scale feature fusion (MSFF) module between encoder and decoder is designed to distinguish buildings of different scales in complex scenes. In the decoding stage, an attention weighted semantic enhancement (AWSE) module is introduced into the decoder to assist the up-sampling process. It not only makes full use of the multi-level features output by the encoder, but also highlights the key local semantic information of the building. To verify the effectiveness of the proposed model, experiments were conducted on two building segmentation data sets, WHU and INRIA. The preliminary results show that the proposed model can effectively identify buildings with different scales in complex scenes, and has better performance than the current representative networks including FCN, U-net, DeeplabV3+ and MA-FCN.
WOS:000855647800007
</snippet>
</document>

<document id="396">
<title>Deep-learning-based workflow for boundary and small target segmentation in digital rock images using UNet plus plus and IK-EBM</title>
<url>http://dx.doi.org/10.1016/j.petrol.2022.110596</url>
<snippet>Three-dimensional (3D) X-ray micro-computed tomography (mu CT) has been widely used in petroleum engineering because it can provide detailed pore structural information for a reservoir rock, which can be imported into a pore-scale numerical model to simulate the transport and distribution of multiple fluids in the pore space. The partial volume blurring (PVB) problem is a major challenge in segmenting raw mu CT images of rock samples, which impacts boundaries and small targets near the resolution limit. We developed a deep-learning (DL)-based workflow for accurate and fast partial volume segmentation. The DL models performance depends primarily on the training data quality and model architecture. This study employed the entropy-based-masking indicator kriging (IK-EBM) to segment 3D Berea sandstone images as training datasets. The comparison between IK-EBM and manual segmentation using a 3D synthetic sphere pack, which had a known ground truth, showed that IKEBM had higher accuracy on partial volume segmentation. We then trained and tested the UNet++ model, a state-of-the-art supervised encoder-decoder model, for binary (i.e., void and solid) and four-class segmentation. We compared the UNet++ with the commonly used U-Net and wide U-Net models and showed that the UNet++ had the best performance in terms of pixel-wise and physics-based evaluation metrics. Specifically, boundaryscaled accuracy demonstrated that the UNet++ architecture outperformed the regular U-Net architecture in the segmentation of pixels near boundaries and small targets, which were subjected to the PVB effect. Feature map visualization illustrated that the UNet++ bridged the semantic gaps between the feature maps extracted at different depths of the network, thereby enabling faster convergence and more accurate extraction of fine-scale features. The developed workflow significantly enhances the performance of supervised encoder-decoder models in partial volume segmentation, which has extensive applications in fundamental studies of subsurface energy, water, and environmental systems.
WOS:000800555100001
</snippet>
</document>

<document id="397">
<title>A deep dive into understanding tumor foci classification using multiparametric MRI based on convolutional neural network</title>
<url>http://dx.doi.org/10.1002/mp.14255</url>
<snippet>Purpose Deep learning models have had a great success in disease classifications using large data pools of skin cancer images or lung X-rays. However, data scarcity has been the roadblock of applying deep learning models directly on prostate multiparametric MRI (mpMRI). Although model interpretation has been heavily studied for natural images for the past few years, there has been a lack of interpretation of deep learning models trained on medical images. In this paper, an efficient convolutional neural network (CNN) was developed and the model interpretation at various convolutional layers was systematically analyzed to improve the understanding of how CNN interprets multimodality medical images and the predictive powers of features at each layer. The problem of small sample size was addressed by feeding the intermediate features into a traditional classification algorithm known as weighted extreme learning machine (wELM), with imbalanced distribution among output categories taken into consideration. Methods The training data collection used a retrospective set of prostate MR studies, from SPIE-AAPM-NCI PROSTATEx Challenges held in 2017. Three hundred twenty biopsy samples of lesions from 201 prostate cancer patients were diagnosed and identified as clinically significant (malignant) or not significant (benign). All studies included T2-weighted (T2W), proton density-weighted (PD-W), dynamic contrast enhanced (DCE) and diffusion-weighted (DW) imaging. After registration and lesion-based normalization, a CNN with four convolutional layers were developed and trained on tenfold cross validation. The features from intermediate layers were then extracted as input to wELM to test the discriminative power of each individual layer. The best performing model from the tenfolds was chosen to be tested on the holdout cohort from two sources. Feature maps after each convolutional layer were then visualized to monitor the trend, as the layer propagated. Scatter plotting was used to visualize the transformation of data distribution. Finally, a class activation map was generated to highlight the region of interest based on the model perspective. Results Experimental trials indicated that the best input for CNN was a modality combination of T2W, apparent diffusion coefficient (ADC) and DWIb50. The convolutional features from CNN paired with a weighted extreme learning classifier showed substantial performance compared to a CNN end-to-end training model. The feature map visualization reveals similar findings on natural images where lower layers tend to learn lower level features such as edges, intensity changes, etc, while higher layers learn more abstract and task-related concept such as the lesion region. The generated saliency map revealed that the model was able to focus on the region of interest where the lesion resided and filter out background information, including prostate boundary, rectum, etc. Conclusions This work designs a customized workflow for the small and imbalanced dataset of prostate mpMRI where features were extracted from a deep learning model and then analyzed by a traditional machine learning classifier. In addition, this work contributes to revealing how deep learning models interpret mpMRI for prostate cancer patient stratification.
WOS:000539766200001
</snippet>
</document>

<document id="398">
<title>Spatial Mapping of the Groundwater Potential of the Geum River Basin Using Ensemble Models Based on Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/rs11192285</url>
<snippet>This study analyzed the Groundwater Productivity Potential (GPP) of Okcheon city, Korea, using three different models. Two of these three models are data mining models: Boosted Regression Tree (BRT) model and Random Forest (RF) model. The other model is the Logistic Regression (LR) model. The three models are based on the relationship between groundwater-productivity data (specific capacity (SPC) and transmissivity (T)) and the related hydro-geological factors from thematic maps, such as topography, lineament, geology, land cover, and etc. The thematic maps which are generated from the remote sensing images. Groundwater productivity data were collected from 86 wells locations. The resulting GPP maps were validated through area-under-the-curve (AUC) analysis using wells data that had not been used for training the model. When T was used in the BRT, RF, and LR models, the obtained GPP maps had 81.66&#37;, 80.21&#37;, and 85.04&#37; accuracy, respectively, and when SPC was used, the maps had 81.53&#37;, 78.57&#37;, and 82.22&#37; accuracy, respectively. The LR model, which is a statistical model, showed the highest verification accuracy, also the other two models showed high accuracies. These observations indicate that all three models can be useful for groundwater resource development.
WOS:000496827100099
</snippet>
</document>

<document id="399">
<title>A neural network regression model for estimating maximum daily air temperature using Landsat-8 data</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B3-2022-1273-2022</url>
<snippet>Urban Heat Islands (UM) phenomenon is a pressing problem for highly industrialized areas with serious risks for public health. Weather stations guarantee long-term accurate observations of weather parameters, such Air Temperature (AT), but lack appropriate spatial coverage. Numerous studies have argued that satellite Land Surface Temperature (LST) is a relevant parameter for estimating AT maps, exploring both linear regression and Machine Learning algorithms. This study proposes a Neural Network (NN) regression model for estimating the maximum AT from Landsat-8 data. The approach has been tested in a variegated morphological region (Puglia, Italy) using a large stack of data acquired from 2018 to 2020. The algorithm uses the median values of LST and Normalized Difference Vegetation Index (NDVI) computed using different buffer radius around the location of each reference weather station (250 m, 1000 m, and 2000 m) to train the NN model with a K-fold cross-validation strategy. The reference dataset was split into three sets using a stratified sampling approach considering the different station categories: rural, High- and Low-density Urban areas respectively. The algorithm was tested with different learning rates (LR) (0.001 and 0.005). The results show that our NN model accuracy improves with the increase of the buffer radius, minimizing the difference in terms of R boolean AND 2 between training and evaluation data, with an overall accuracy consistently higher than 0.84. Future research could investigate more input variables in the NN model such as morphology or climate variables and test the algorithm on larger areas.
WOS:000855647800180
</snippet>
</document>

<document id="400">
<title>A dual-branch hybrid dilated CNN model for the AI-assisted segmentation of meningiomas in MR images</title>
<url>http://dx.doi.org/10.1016/j.compbiomed.2022.106279</url>
<snippet>Background and objective: Treatment for meningiomas usually includes surgical removal, radiation therapy, and chemotherapy. Accurate segmentation of tumors significantly facilitates complete surgical resection and precise radiotherapy, thereby improving patient survival. In this paper, a deep learning model is constructed for magnetic resonance T1-weighted Contrast Enhancement (T1CE) images to develop an automatic processing scheme for accurate tumor segmentation.Methods: In this paper, a novel Convolutional Neural Network (CNN) model is proposed for the accurate meningioma segmentation in MR images. It can extract fused features in multi-scale receptive fields of the same feature map based on MR image characteristics of meningiomas. The attention mechanism is added as a helpful addition to the model to optimize the feature information transmission.Results and conclusions: The results were evaluated on two internal testing sets and one external testing set. Mean Dice Similarity Coefficient (DSC) values of 0.886, 0.851, and 0.874 are demonstrated, respectively. In this paper, a deep learning approach is proposed to segment tumors in T1CE images. Multi-center testing sets validated the effectiveness and generalization of the method. The proposed model demonstrates state-of-the-art tumor segmentation performance.
WOS:000900142700001
</snippet>
</document>

<document id="401">
<title>Localization of Mobile Robots with Topological Maps and Classification with Reject Option using Convolutional Neural Networks in Omnidirectional Images</title>
<url>http://dx.doi.org/</url>
<snippet>In this paper, we propose a new localization and navigation approach for mobile robots using topological maps and classification with reject option applying convolutional neural networks (CNN) for feature extraction in omnidirectional images. The use of CNN as feature extractor is based on the concept of Transfer Learning. Reject option is used to improve the task of the classifiers, querying information from the topological map. With the objective of evidencing the high performance of the technique considered, an analysis is made between several feature extractors and classifiers, established in the literature. Parameters such as processing time and accuracy are calculated to prove the credibility and effectiveness of the approach, since these properties are fundamental in the analysis of embedded systems. Considering the proposed approach, CNN stands out among the other feature extractors, as it generated the best results in extraction time and accuracy. It obtained an average accuracy of 99.86&#37; and an extraction time of 0.1517s, proving to be a relevant method for the localization and navigation activities.
WOS:000585967401007
</snippet>
</document>

<document id="402">
<title>Performance comparison of deep learning and machine learning methods in determining wetland water areas using EuroSAT dataset</title>
<url>http://dx.doi.org/10.1007/s11356-021-17177-z</url>
<snippet>Wetlands are critical to the ecology because they maintain biodiversity and provide home for a variety of species. Researching, mapping, and conservation of wetlands is a challenging and time-consuming process. Because they produce temporal and geographical information, remote sensing and photogrammetric approaches are useful tools for analyzing and managing wetlands. In this study, the water areas of five different wetlands obtained with Sentinel-2 images in Turkey were classified. Although obtaining large amounts of high-dimensional dataset labeled for various land types is costly, it is a significant advantage to use it after model training in a wide range of applications. In this paper, the EuroSAT dataset was used in the validation process. Proposed deep learning-based 1D convolutional neural networks (CNN) and traditional machine learning methods (i.e., support vector machine, linear discriminant analysis, K-nearest neighborhood, canonical correlation forests, and AdaBoost.M1) were compared quantitatively (i.e., accuracy, recall, precision, specificity, F-score, and image quality assessment metrics) and qualitatively. Finally, pairwise comparison was made with chi-square-based McNemars test. There is a statistical difference between 1D CNN and machine learning method (except the support vector machine vs linear discriminant analysis in Test 1 area). CNN models outperform machine learning algorithms in terms of non-linear function approximation and the ability to extract and articulate data features. Since 1D CNNs can process data in a highly complex and unique feature space, they are very successful in segmenting strongly related and highly correlated discrete signals. It also has advantages over machine learning methods for water body extraction in that it can be integrated with sophisticated image pre-processing and standardization tools, is less susceptible to low-level random noise, and provides shift in variations and contrast-invariant image local transforms.
WOS:000715190000002
</snippet>
</document>

<document id="403">
<title>Scale-Robust Deep-Supervision Network for Mapping Building Footprints From High-Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3109237</url>
<snippet>Building footprint information is one of the key factors for sustainable urban planning and environmental monitoring. Mapping building footprints from remote sensing images is an important and challenging task in the earth observation field. Over the years, convolutional neural networks have shown outstanding improvements in the building extraction field due to their ability to automatically extract hierarchical features and make building predictions. However, as buildings are various in different sizes, scenes, and roofing materials, it is hard to precisely depict buildings of varied sizes, especially in large areas (e.g., nationwide). To tackle these limitations, we propose a novel deep-supervision convolutional neural network (denoted as DS-Net) for extracting building footprints from high-resolution remote sensing images. In the proposed network, we applied deep supervision with an extra lightweight encoder, which enables the network to learn representative building features of different scales. Furthermore, a scale attention module is designed to aggregate multiscale features and generate the final building prediction. Experiments on two publicly available building datasets, including the WHU Building Dataset and the Massachusetts Building Dataset, show the effectiveness of the proposed method. With only a 0.22-M increment of parameters compared with U-Net, the proposed DS-Net achieved an IoU of 90.4&#37; on the WHU Building Dataset and 73.8&#37; on the Massachusetts Dataset. DS-Net also outperforms the state-of-the-art building extraction methods on the two datasets, indicating the effectiveness of the proposed deep supervision and scale attention.
WOS:000707442300011
</snippet>
</document>

<document id="404">
<title>GeoRec: Geometry-enhanced semantic 3D reconstruction of RGB-D indoor scenes</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.02.014</url>
<snippet>Semantic indoor 3D modeling with multi-task deep neural networks is an efficient and low-cost way for reconstructing an indoor scene with geometrically complete room structure and semantic 3D individuals. Challenged by the complexity and clutter of indoor scenarios, the semantic reconstruction quality of current methods is still limited by the insufficient exploration and learning of 3D geometry information. To this end, this paper proposes an end-to-end multi-task neural network for geometry-enhanced semantic 3D reconstruction of RGB-D indoor scenes (termed as GeoRec). In the proposed GeoRec, we build a geometry extractor that can effectively learn geometry-enhanced feature representation from depth data, to improve the estimation accuracy of layout, camera pose and 3D object bounding boxes. We also introduce a novel object mesh generator that strengthens the reconstruction robustness of GeoRec to indoor occlusion with geometry-enhanced implicit shape embedding. With the parsed scene semantics and geometries, the proposed GeoRec reconstructs an indoor scene by placing reconstructed object mesh models with 3D object detection results in the estimated layout cuboid. Extensive experiments conducted on two benchmark datasets show that the proposed GeoRec yields outstanding performance with 5.19 x 10(-3) mean chamfer distance error for object reconstruction on the challenging Pix3D dataset, 70.45&#37; mAP for 3D object detection and 77.1&#37; 3D mIoU for layout estimation on the commonly-used SUN RGB-D dataset. Especially, the mesh reconstruction sub-network of GeoRec trained on Pix3D can be directly transferred to SUN RGB-D without any fine-tuning, manifesting a high generalization ability.
WOS:000782581700003
</snippet>
</document>

<document id="405">
<title>A new method for pixel classification for rice variety identification using spectral and time series data from Sentinel-2 satellite imagery</title>
<url>http://dx.doi.org/10.1016/j.compag.2022.106731</url>
<snippet>In the agriculture sector food productivity, security, and sustainability, imposed challenges on farmers, regulatory bodies, and policymakers due to increasing demand and depleting natural resources and environmental concerns. Rice crop holds a prominent place in Pakistans agriculture sector, it is not only consumed locally but also exported to many countries including China. Gathering crop information such as variety maps, yield estimation, or etc. can help farmers, regulatory bodies, policymakers, and rice mills in decision-making. In Pakistan, crop information is collected through manual field surveys that require a lot of human labor, are costly, and are time-consuming. One cannot ignore human error and bias in the process. A new framework for pixel classification is proposed that uses both spectral and time-series data of Sentinal-2 satellite for mapping two rice varieties "Basmati" and "IRRI, grown in Pakistan. The data were collected from twelve rice fields (approx. 307 acres) of different geographical locations at 16-time instances to cover the complete rice-growing season (May-October) in 2019. A linear spectral unmixing model is used to determine sub-pixel information of water, soil, and vegetation content, which is used for labeling each pixel for supervised learning. The input to our classifier is a 16 x 15 image formed using 15 spectral features (12 spectral bands and 3 radiometric indices) of 16 carefully selected different time instances for each pixel. The output is a pixel-level classification (semantic segmentation) of each pixel into Basmati, IRRI, and others (soil, water, etc.). Experimental results have exhibited an excellent overall accuracy of 98.6&#37; with the proposed approach. The Basmati rice obtained higher accuracy of 99.7&#37; as compared to IRRI rice with an accuracy of 95.2&#37;.
WOS:000754268300001
</snippet>
</document>

<document id="406">
<title>HA-MPPNet: Height Aware-Multi Path Parallel Network for High Spatial Resolution Remote Sensing Image Semantic Seg-Mentation</title>
<url>http://dx.doi.org/10.3390/ijgi10100672</url>
<snippet>Semantic segmentation of remote sensing images (RSI) plays a significant role in urban management and land cover classification. Due to the richer spatial information in the RSI, existing convolutional neural network (CNN)-based methods cannot segment images accurately and lose some edge information of objects. In addition, recent studies have shown that leveraging additional 3D geometric data with 2D appearance is beneficial to distinguish the pixels category. However, most of them require height maps as additional inputs, which severely limits their applications. To alleviate the above issues, we propose a height aware-multi path parallel network (HA-MPPNet). Our proposed MPPNet first obtains multi-level semantic features while maintaining the spatial resolution in each path for preserving detailed image information. Afterward, gated high-low level feature fusion is utilized to complement the lack of low-level semantics. Then, we designed the height feature decode branch to learn the height features under the supervision of digital surface model (DSM) images and used the learned embeddings to improve semantic context by height feature guide propagation. Note that our module does not need a DSM image as additional input after training and is end-to-end. Our method outperformed other state-of-the-art methods for semantic segmentation on publicly available remote sensing image datasets.
WOS:000711631000001
</snippet>
</document>

<document id="407">
<title>PolSAR Image Classification Based on Low-Frequency and Contour Subbands-Driven Polarimetric SENet</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3015520</url>
<snippet>In order to more efficiently mine the features of PolSAR images and build a more suitable classification model that combines the features of the polarimetric domain and the spatial domain, this article proposes a PolSAR image classification method, called low-frequency and contour subbands-driven polarimetric squeeze-and-excitation network (LC-PSENet). First, the proposed LC-PSENet introduces the nonsubsampled Laplacian pyramid to decompose polarimetric feature maps, so as to construct a multichannel PolSAR image based on the low-frequency subband and contour subband of these maps. It guides the network to perform feature mining and selection in the subbands of each polarimetric map in a supervised way, automatically balancing the contributions of polarimetric features and their subbands and the influence of interference information such as noise, making the network learning more efficient. Second, the method introduces squeeze-and-excitation operation in the convolutional neural network (CNN) to perform channel modeling on the polarimetric feature subbands. It strengthens the learning of the contributions of local maps of the polarimetric features and subbands, thereby, effectively combining the features of the polarimetric domain and the spatial domain. Experiments on the datasets of Flevoland, The Netherlands, and Oberpfaffenhofen show that the proposed LC-PSENet achieves overall accuracies of 99.66&#37;, 99.72&#37;, and 95.89&#37;, which are 0.87&#37;, 0.27&#37;, and 1.42&#37; higher than the baseline CNN, respectively. The isolated points in the classification results are obviously reduced, and the distinction between boundary and nonboundary is more clear and delicate. Also, the method performs better than many current state-of-the-art methods in terms of classification accuracy.
WOS:000564195900010
</snippet>
</document>

<document id="408">
<title>Constrained fixation point based segmentation via deep neural network</title>
<url>http://dx.doi.org/10.1016/j.neucom.2019.08.051</url>
<snippet>It is an explicit mode to use the clicking points by the mouse in the interactive image segmentation, while an implicit interaction mode is to use the fixation points from the eye-tracking device. Both modes can provide a series of points. Inspired by the similarity between these two interaction modes, we propose a novel human visual system (HVS) based neural network for transferring the constrained fixation point based segmentation to the clicking point based interactive segmentation. Briefly speaking, the sequence of information transmission and processing in our model is RGB image, VGG-16 backbone, LGN-like module (LGNL) and ConvLSTM block, which correspond to the pathway of stimulus transmission and processing, i.e. stimulus, retina, lateral geniculate nucleus (LGN) and visual cortex in the HVS. First, the RGB image is fed to the VGG-16 backbone to obtain the multiple-layer feature maps. Then the LGNL is adopted to effectively incorporate edge-aware features and semantic features from different layers of the VGG-16 backbone in multiple resolutions, so as to produce rich contextual features. Finally, with the guidance of the fixation density map transformed from the fixation points, the output feature maps of LGNL are utilized to generate the segmentation map via a stack of ConvLSTM blocks in a coarse-to-fine manner. Comprehensive experiments demonstrate that the proposed HVS based neural network achieves a higher segmentation performance and outperforms seven state-of-the-art methods, and prove that the transfer from constrained fixation points to clicking points is reasonable and valid. (C) 2019 Elsevier B.V. All rights reserved.
WOS:000489905200016
</snippet>
</document>

<document id="409">
<title>BubblEX: An Explainable Deep Learning Framework for Point-Cloud Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3195200</url>
<snippet>Point-cloud data are nowadays one of the major data sources for describing our environment. Recently, deep architectures have been proposed as a key step in understanding and retrieving semantic information. Despite the great contribution of deep learning in this field, the explainability of these models for 3-D data is still fairly unexplored. Explainability, identified as a potential weakness of deep neural networks (DNNs), can help researchers against skepticism, considering that these models are far from being self-explanatory. Although literature provides many examples on the exploitation of explainable artificial intelligence approaches with 2-D data, only a few studies have attempted to investigate it for 3-D DNNs. To overcome these limitations, BubblEX is proposed here, a novel multimodal fusion framework to learn the 3-D point features. BubblEX framework comprises two stages: "Visualization Module" for the visualization of features learned from the network in its hidden layers and "Interpretability Module," which aims at describing how the neighbor points are involved in the feature extraction. For our experiments, dynamic graph convolutional neural network has been used, trained on Modelnet40 dataset. The developed framework extends a method for obtaining saliency maps from image data, to deal with 3-D point-cloud data, allowing the analysis, comparison, and contrasting of multiple features. Besides, it permits the generation of visual explanations from any DNN-based network for 3-D point-cloud classification without requiring architectural changes or retraining. Our findings will be extremely useful for both scientists and nonexperts in understanding and improving future AI-based models.
WOS:000842061200007
</snippet>
</document>

<document id="410">
<title>Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system</title>
<url>http://dx.doi.org/10.1080/15481603.2018.1426091</url>
<snippet>Deep learning networks have shown great success in several computer vision applications, but its implementation in natural land cover mapping in the context of object-based image analysis (OBIA) is rarely explored area especially in terms of the impact of training sample size on the performance comparison. In this study, two representatives of deep learning networks including fully convolutional networks (FCN) and patch-based deep convolutional neural networks (DCNN), and two conventional classifiers including random forest and support vector machine were implemented within the framework of OBIA to classify seven natural land cover types. We assessed the deep learning classifiers using different training sample sizes and compared their performance with traditional classifiers. FCN was implemented using two types of training samples to investigate its ability to utilize object surrounding information.Our results indicate that DCNN may produce inferior performance compared to conventional classifiers when the training sample size is small, but it tends to show substantially higher accuracy than the conventional classifiers when the training sample size becomes large. The results also imply that FCN is more efficient in utilizing the information in the training sample than DCNN and conventional classifiers, with higher if not similar achieved accuracy regardless of sample size. DCNN and FCN tend to show similar performance for the large sample size when the training samples used for training the FCN do not contain object surrounding label information. However, with the ability of utilizing surrounding label information, FCN always achieved much higher accuracy than all the other classification methods regardless of the number of training samples.
WOS:000427855000006
</snippet>
</document>

<document id="411">
<title>A New Deep-Learning-Based Approach for Earthquake-Triggered Landslide Detection From Single-Temporal RapidEye Satellite Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3028855</url>
<snippet>Accurate landslide detection and mapping are essential for land use planning, managementassessment, and geo-disaster risk mitigation as well as post-disaster reconstructions. Till now, visual interpretation and field survey are still the most widely adopted techniques for landslide mapping, which are often criticized labor-intensive, time-consuming, and costly. With the rapid advancement of artificial intelligence, deep-learning-based approach for landslide detection and mapping has drawn great attention for its significant advantages over the traditional techniques. However, lack of sufficient training samples has constrained the application of deep-learning-based approach in landslide detection from satellite images for a long time. The present study aimed to examine the feasibility of a new deep-learning-based approach to intelligently detect and map earthquake-triggered landslides from single-temporal RapidEye satellite images. Specifically, the proposed approach consists of three steps. First of all, a standard data preprocessing workflow to automatically generate training samples was designed and some data augmentation strategies were implemented to alleviate the lack of training samples. Then, a cascaded end-to-end deep learning network, namely LandsNet, was constructed to learn various features of landslides. Finally, the identified landslide maps were further optimized with morphological processing. Experiments in two spatially independent earthquake-affected regions showed our proposed approach yielded the best F1 value of about 86.89, which was about 7 and 8 higher than that obtained by ResUNet and DeepUNet, respectively. Comparative studies on the feasibility and robustness of the proposed approach with ResUNet and DeepUNet demonstrated its strong application potentials in the emergency response of natural disasters.
WOS:000583501200004
</snippet>
</document>

<document id="412">
<title>A review and evaluation of the state-of-the-art in PV solar power forecasting: Techniques and optimization</title>
<url>http://dx.doi.org/10.1016/j.rser.2020.109792</url>
<snippet>Integration of photovoltaics into power grids is difficult as solar energy is highly dependent on climate and geography; often fluctuating erratically. This causes penetrations and voltage surges, system instability, inefficient utilities planning and financial loss. Forecast models can help; however, time stamp, forecast horizon, input correlation analysis, data pre and post-processing, weather classification, network optimization, uncertainty quantification and performance evaluations need consideration. Thus, contemporary forecasting techniques are reviewed and evaluated. Input correlational analyses reveal that solar irradiance is most correlated with Photovoltaic output, and so, weather classification and cloud motion study are crucial. Moreover, the best data cleansing processes: normalization and wavelet transforms, and augmentation using generative adversarial network are recommended for network training and forecasting. Furthermore, optimization of inputs and network parameters, using genetic algorithm and particle swarm optimization, is emphasized. Next, established performance evaluation metrics MAE, RMSE and MAPE are discussed, with suggestions for including economic utility metrics. Subsequently, modelling approaches are critiqued, objectively compared and categorized into physical, statistical, artificial intelligence, ensemble and hybrid approaches. It is determined that ensembles of artificial neural networks are best for forecasting short term photovoltaic power forecast and online sequential extreme learning machine superb for adaptive networks; while Bootstrap technique optimum for estimating uncertainty. Additionally, convolutional neural network is found to excel in eliciting a models deep underlying non-linear input-output relationships. The conclusions drawn impart fresh insights in photovoltaic power forecast initiatives, especially in the use of hybrid artificial neural networks and evolutionary algorithms.
WOS:000521976000021
</snippet>
</document>

<document id="413">
<title>Easy-to-use spatial random-forest-based downscaling-calibration method for producing precipitation data with high resolution and high accuracy</title>
<url>http://dx.doi.org/10.5194/hess-25-5667-2021</url>
<snippet>Precipitation data with high resolution and high accuracy are significantly important in numerous hydrological applications. To enhance the spatial resolution and accuracy of satellite-based precipitation products, an easy-to-use downscaling-calibration method based on a spatial random forest (SRF-DC) is proposed in this study, where the spatial autocorrelation of precipitation measurements between neighboring locations is considered. SRF-DC consists of two main stages. First, the satellite-based precipitation is downscaled by the SRF with the incorporation of high-resolution variables including latitude, longitude, normalized difference vegetation index (NDVI), digital elevation model (DEM), terrain slope, aspect, relief and land surface temperatures. Then, the downscaled precipitation is calibrated by the SRF with rain gauge observations and the aforementioned high-resolution variables. The monthly Integrated MultisatellitE Retrievals for Global Precipitation Measurement (IMERG) over Sichuan Province, China, from 2015 to 2019 was processed using SRF-DC, and its results were compared with those of classical methods including geographically weighted regression (GWR), artificial neural network (ANN), random forest (RF), kriging interpolation only on gauge measurements, bilinear interpolation-based downscaling and then SRF-based calibration (Bi-SRF), and SRF-based downscaling and then geographical difference analysis (GDA)-based calibration (SRF-GDA). Comparative analyses with respect to root mean square error (RMSE), mean absolute error (MAE) and correlation coefficient (CC) demonstrate that (1) SRF-DC outperforms the classical methods as well as the original IMERG; (2) the monthly based SRF estimation is slightly more accurate than the annually based SRF fraction disaggregation method; (3) SRF-based downscaling and calibration perform better than bilinear downscaling (Bi-SRF) and GDA-based calibration (SRF-GDA); (4) kriging is more accurate than GWR and ANN, whereas its precipitation map loses detailed spatial precipitation patterns; and (5) based on the variable-importance rank of the RF, the precipitation interpolated by kriging on the rain gauge measurements is the most important variable, indicating the significance of incorporating spatial autocorrelation for precipitation estimation.
WOS:000715749100001
</snippet>
</document>

<document id="414">
<title>A CBAM Based Multiscale Transformer Fusion Approach for Remote Sensing Image Change Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3198517</url>
<snippet>Change detection methods play an indispensable role in remote sensing. Some change detection methods have obtained a fairly good performance by introducing attention mechanism on the basis of the convolutional neural network (CNN), but identifying intricate changes remains difficult. In response to these problems, this article proposes a new model for detecting changes in remote sensing, namely, MTCNet, which combines the advantages of multiscale transformer with the convolutional block attention module (CBAM) to improve the detection quality of different remote sensing images. On the basis of traditional convolutions, the transformer module is introduced to extract bitemporal image features by modeling contextual information. Based on the transformer module, a multiscale module is designed to form a multiscale transformer, which can obtain features at different scales in bitemporal images, thereby identifying the changes we are interested in. Based on the multiscale transformer module, the CBAM is introduced. The CBAM is split into a spatial attention module and a channel attention module, which are applied to the front and back ends of the multiscale transformer, respectively. Spatial information and channel information of feature maps are modeled separately. In this article, the validity and efficiency of the method are verified by a large number of experiments on the LEVIR-CD dataset and the WHU-CD dataset.
WOS:000845070100011
</snippet>
</document>

<document id="415">
<title>A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3165005</url>
<snippet>Driven by the rapid development of Earth observation sensors, semantic segmentation using multimodal fusion of remote sensing data has drawn substantial research attention in recent years. However, existing multimodal fusion methods based on convolutional neural networks cannot capture long-range dependencies across multiscale feature maps of remote sensing data in different modalities. To circumvent this problem, this work proposes a crossmodal multiscale fusion network (CMFNet) by exploiting the transformer architecture. In contrast to the conventional early, late, or hybrid fusion networks, the proposed CMFNet fuses information of different modalities at multiple scales using the cross-attention mechanism. More specifically, the CMFNet utilizes a novel cross-modal attention architecture to fuse multiscale convolutional feature maps of optical remote sensing images and digital surface model data through a crossmodal multiscale transformer (CMTrans) and a multiscale context augmented transformer (MCATrans). The CMTrans can effectively model long-range dependencies across multiscale feature maps derived from multimodal data, while the MCATrans can learn discriminative integrated representations for semantic segmentation. Extensive experiments on two large-scale fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam, confirm the excellent performance of the proposed CMFNet as compared to other multimodal fusion methods.
WOS:000795117800003
</snippet>
</document>

<document id="416">
<title>Deep Convolutional Neural Networks for Automated Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery</title>
<url>http://dx.doi.org/10.3390/rs10091487</url>
<snippet>The microtopography associated with ice-wedge polygons governs many aspects of Arctic ecosystem, permafrost, and hydrologic dynamics from local to regional scales owing to the linkages between microtopography and the flow and storage of water, vegetation succession, and permafrost dynamics. Wide-spread ice-wedge degradation is transforming low-centered polygons into high-centered polygons at an alarming rate. Accurate data on spatial distribution of ice-wedge polygons at a pan-Arctic scale are not yet available, despite the availability of sub-meter-scale remote sensing imagery. This is because the necessary spatial detail quickly produces data volumes that hamper both manual and semi-automated mapping approaches across large geographical extents. Accordingly, transforming big imagery into science-ready insightful analytics demands novel image-to-assessment pipelines that are fueled by advanced machine learning techniques and high-performance computational resources. In this exploratory study, we tasked a deep-learning driven object instance segmentation method (i.e., the Mask R-CNN) with delineating and classifying ice-wedge polygons in very high spatial resolution aerial orthoimagery. We conducted a systematic experiment to gauge the performances and interoperability of the Mask R-CNN across spatial resolutions (0.15 m to 1 m) and image scene contents (a total of 134 km(2) ) near Nuiqsut, Northern Alaska. The trained Mask R-CNN reported mean average precisions of 0.70 and 0.60 at thresholds of 0.50 and 0.75, respectively. Manual validations showed that approximately 95&#37; of individual ice-wedge polygons were correctly delineated and classified, with an overall classification accuracy of 79&#37;. Our findings show that the Mask R-CNN is a robust method to automatically identify ice-wedge polygons from fine-resolution optical imagery. Overall, this automated imagery-enabled intense mapping approach can provide a foundational framework that may propel future pan-Arctic studies of permafrost thaw, tundra landscape evolution, and the role of high latitudes in the global climate system.
WOS:000449993800167
</snippet>
</document>

<document id="417">
<title>Mapping freshwater marsh species in the wetlands of Lake Okeechobee using very high-resolution aerial photography and lidar data</title>
<url>http://dx.doi.org/10.1080/01431161.2018.1455242</url>
<snippet>Accurate marsh species maps are needed to study their abundance, distribution, habitat change, and to support the ongoing and future restoration activities in Lake Okeechobee, Florida, U.S.A. In this study, we integrated very high-resolution aerial photography with a light detection and ranging (LiDAR)-derived digital elevation model (DEM) for the discrimination of six freshwater marsh species (Salix caroliniana, Spartina bakeri, Polygonum spp., Typha spp., Phragmites australis, and Cladium jamaicense). Four techniques were combined in the mapping procedure, including object-based image analysis, machine learning classifiers, texture analysis, and ensemble analysis. The results showed that both texture and topography features were invaluable for marsh species mapping. A synergy of spatial, spectral, and topographical features achieved an overall accuracy (OA) of 85.3&#37; and kappa coefficient of 0.83 using the Random Forest (RF) classifier. Ensemble analysis of the outputs from Support Vector Machine (SVM), RF, and Artificial Neural Network (ANN) did not increase the classification accuracy, but produced an uncertainty map to identify regions with a robust classification and areas difficult to map. Ensemble analysis was beneficial in getting further insight into marsh species mapping. The developed digital procedure is a promising alternative to traditional field survey and manual interpretation methods for generating marsh maps to support the lake restoration.
WOS:000449510000002
</snippet>
</document>

<document id="418">
<title>De Novo Molecular Design by Combining Deep Autoencoder Recurrent Neural Networks with Generative Topographic Mapping</title>
<url>http://dx.doi.org/10.1021/acs.jcim.8b00751</url>
<snippet>Here we show that Generative Topographic Mapping (GTM) can be used to explore the latent space of the SMILES-based autoencoders and generate focused molecular libraries of interest. We have built a sequence-to-sequence neural network with Bidirectional Long Short-Term Memory layers and trained it on the SMILES strings from ChEMBL23. Very high reconstruction rates of the test set molecules were achieved (&gt;98&#37;), which are comparable to the ones reported in related publications. Using GTM, we have visualized the autoencoder latent space on the two-dimensional topographic map. Targeted map zones can be used for generating novel molecular structures by sampling associated latent space points and decoding them to SMILES. The sampling method based on a genetic algorithm was introduced to optimize compound properties "on the fly". The generated focused molecular libraries were shown to contain original and a priori feasible compounds which, pending actual synthesis and testing, showed encouraging behavior in independent structure-based affinity estimation procedures (pharmacophore matching, docking).
WOS:000462943700022
</snippet>
</document>

<document id="419">
<title>Localizing and quantifying infrastructure damage using class activation mapping approaches</title>
<url>http://dx.doi.org/10.1007/s13278-019-0588-4</url>
<snippet>Traditional post-disaster assessment of damage heavily relies on expensive geographic information system (GIS) data, especially remote sensing image data. In recent years, social media have become a rich source of disaster information that may be useful in assessing damage at a lower cost. Such information includes text (e.g., tweets) or images posted by eyewitnesses of a disaster. Most of the existing research explores the use of text in identifying situational awareness information useful for disaster response teams. The use of social media images to assess disaster damage is limited. We have recently proposed a novel approach, based on convolutional neural networks and class activation mapping, to locate building damage in a disaster image and to quantify the degree of the damage. In this paper, we study the usefulness of the proposed approach for other categories of infrastructure damage, specifically bridge and road damage, and compare two-class activation mapping approaches in this context. Experimental results show that our proposed approach enables the use of social network images for post-disaster infrastructure damage assessment and provides an inexpensive and feasible alternative to the more expensive GIS approach.
WOS:000480469200001
</snippet>
</document>

<document id="420">
<title>Identifying wetland areas in historical maps using deep convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.ecoinf.2022.101557</url>
<snippet>The local environment and land usages have changed a lot during the past one hundred years. Historical documents and materials are crucial in understanding and following these changes. Historical documents are, therefore, an important piece in the understanding of the impact and consequences of land usage change. This, in turn, is important in the search of restoration projects that can be conducted to turn and reduce harmful and unsustainable effects originating from changes in the land-usage.&amp; nbsp;This work extracts information on the historical location and geographical distribution of wetlands, from hand-drawn maps. This is achieved by using deep learning (DL), and more specifically a convolutional neural network (CNN). The CNN model is trained on a manually pre-labelled dataset on historical wetlands in the area of Jonkoping county in Sweden. These are all extracted from the historical map called "Generalstabskartan ".&amp; nbsp;The presented CNN performs well and achieves a F-1-score of 0.886 when evaluated using a 10-fold cross validation over the data.The trained models are additionally used to generate a GIS layer of the presumable historical geographical distribution of wetlands for the area that is depicted in the southern collection in Generalstabskartan, which covers the southern half of Sweden. This GIS layer is released as an open resource and can be freely used.&amp; nbsp;To summarise, the presented results show that CNNs can be a useful tool in the extraction and digitalisation of non-textual information in historical documents, such as historical maps. A modern GIS material that can be used to further understand the past land-usage change is produced within this research. Previously, no material of this detail and extent have been available, due to the large effort needed to manually create such. However, with the presented resource better quantifications and estimations of historical wetlands that have been lost can be made.
WOS:000792769800006
</snippet>
</document>

<document id="421">
<title>IMPROVING THE BINARY CLASSIFICATION OF PEAT LOCALITIES FROM MULTI-SOURCE REMOTELY-SENSED DATA USING CNN</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B3-2022-983-2022</url>
<snippet>Neural networks were explored to achieve a binary classification for determining land corresponding to peat for a study area in the boreal forest of northern Ontario, Canada. Environmental covariates were employed as predictors and obtained from multiple sources, which included multispectral imagery, LiDAR, SAR, and aeromagnetic data. A dense neural network (DNN), as well as a convolutional neural network (CNN), were each implemented. Logistic regression, support vector machine (SVM) and random forest (RF) approaches were also modelled. Neighboring pixels surrounding the soil sampling sites were incorporated as input into the CNN, that permitted training on additional information that was not exploited by other methods. Preliminary results indicate that a CNN can attain improved accuracies for peat classification, when compared against other approaches.
WOS:000855647800137
</snippet>
</document>

<document id="422">
<title>Generating Sentinel-2 all-band 10-m data by sharpening 20/60-m bands: A hierarchical fusion network</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.12.017</url>
<snippet>Earth observations from the Sentinel-2 mission have been extensively accepted in a variety of land services. The thirteen spectral bands of Sentinel-2, however, are collected at three spatial resolutions of 10/20/60 m, and such a difference brings difficulties to analyze multispectral imagery at a uniform resolution. To address this problem, we developed a hierarchical fusion network (HFN) to sharpen 20/60-m bands and generate Sentinel-2 all-band 10-m data. The deep learning architecture is used to learn the complex mapping between multi-resolution input and output data. Given the deficiency of previous studies in which the spatial information is inferred only from the fine-resolution bands, the proposed hierarchical fusion framework simultaneously leverages the self-similarity information from coarse-resolution bands and the spatial structure information from fine-resolution bands, to enhance the sharpening performance. Technically, the coarse-resolution bands are super-resolved by exploiting the information from themselves and then sharpened by fusing with the fine-resolution bands. Both 20-m and 60-m bands can be sharpened via the developed approach. Experimental results regarding visual comparison and quantitative assessment demonstrate that HFN outperforms the other benchmarking models, including pan-sharpening-based, model-based, geostatistical-based, and other deep-learning-based approaches, showing remarkable performance in reproducing explicit spatial details and maintaining original spectral fea-tures. Moreover, the developed model works more effectively than the other models over the heterogeneous landscape, which is usually considered a challenging application scenario. To sum up, the fusion model can sharpen Sentinel-2 20/60-m bands, and the created all-band 10-m data allows image analysis and geoscience applications to be authentically carried out at the 10-m resolution.
WOS:000917494300001
</snippet>
</document>

<document id="423">
<title>MC-UNet: Martian Crater Segmentation at Semantic and Instance Levels Using U-Net-Based Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3390/rs15010266</url>
<snippet>Crater recognition on Mars is of paramount importance for many space science applications, such as accurate planetary surface age dating and geological mapping. Such recognition is achieved by means of various image-processing techniques employing traditional CNNs (convolutional neural networks), which typically suffer from slow convergence and relatively low accuracy. In this paper, we propose a novel CNN, referred to as MC-UNet (Martian Crater U-Net), wherein classical U-Net is employed as the backbone for accurate identification of Martian craters at semantic and instance levels from thermal-emission-imaging-system (THEMIS) daytime infrared images. Compared with classical U-Net, the depth of the layers of MC-UNet is expanded to six, while the maximum number of channels is decreased to one-fourth, thereby making the proposed CNN-based architecture computationally efficient while maintaining a high recognition rate of impact craters on Mars. For enhancing the operation of MC-UNet, we adopt average pooling and embed channel attention into the skip-connection process between the encoder and decoder layers at the same network depth so that large-sized Martian craters can be more accurately recognized. The proposed MC-UNet is adequately trained using 2 &amp; SIM;32 km radii Martian craters from THEMIS daytime infrared annotated images. For the predicted Martian crater rim pixels, template matching is subsequently used to recognize Martian craters at the instance level. The experimental results indicate that MC-UNet has the potential to recognize Martian craters with a maximum radius of 31.28 km (136 pixels) with a recall of 0.7916 and F1-score of 0.8355. The promising performance shows that the proposed MC-UNet is on par with or even better than other classical CNN architectures, such as U-Net and Crater U-Net.
WOS:000909721200001
</snippet>
</document>

<document id="424">
<title>Interband Retrieval and Classification Using the Multilabeled Sentinel-2 BigEarthNet Archive</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3112209</url>
<snippet>Conventional remote sensing data analysistechniques have a significant bottleneck of operating on a selectively chosen small-scale dataset. Availability of an enormous volume of data demands handling large-scale, diverse data, which have been made possible with neural network-based architectures. This article exploits the contextual information capturing ability of deep neural networks, particularly investigating multispectral band properties from Sentinel-2 image patches. Besides, an increase in the spatial resolution often leads to nonlinear mixing of land-cover types within a target resolution cell. We recognize this fact and group the bands according to their spatial resolutions, and propose a classification and retrieval framework. We design a representation learning framework for classifying the multispectral data by first utilizing all the bands and then using the grouped bands according to their spatial resolutions. We also propose a novel triplet-loss function for multilabeled images and use it to design an interband group retrieval framework. We demonstrate its effectiveness over the conventional triplet-loss function. Finally, we present a comprehensive discussion of the obtained results. We thoroughly analyze the performance of the band groups on various land-cover and land-use areas from agro-forestry regions, water bodies, and human-made structures. Experimental results for the classification and retrieval framework on the benchmarked BigEarthNet dataset exhibit marked improvements over existing studies.
WOS:000709074200001
</snippet>
</document>

<document id="425">
<title>Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.05.013</url>
<snippet>Robust and reliable automatic building detection and segmentation from aerial images/point clouds has been a prominent field of research in remote sensing, computer vision and point cloud processing for a number of decades. One of the largest issues associated with deep learning methods is the high quantity of data required for training. To help address this we present a method to improve public GIS building footprint labels by using Morphological Geodesic Active Contours (MorphGACs). We demonstrate by improving the quality of building footprint labels for detection and semantic segmentation, more robust and reliable models can be obtained. We evaluate these methods over a large UK-based dataset of 24556 images containing 169835 building instances. This is achieved by training several Mask/Faster R-CNN and RetinaNet deep convolutional neural networks. Networks are supplied with both RGB and fused RGB-lidar data. We offer quantitative analysis on the benefits of the inclusion of depth data for building segmentation. By employing both methods we achieve a detection accuracy of 0.92 (mAP@0.5) and segmentation f1 scores of 0.94 over a 4911 test images ranging from urban to rural scenes.
WOS:000480671500006
</snippet>
</document>

<document id="426">
<title>Building outline delineation: From aerial images to polygons with an improved end-to-end learning framework</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.02.014</url>
<snippet>Deep learning methods based upon convolutional neural networks (CNNs) have demonstrated impressive performance in the task of building outline delineation from very high resolution (VHR) remote sensing (RS) imagery. In this paper, we introduce an improved method that is able to predict regularized building outline in a vector format within an end-to-end deep learning framework. The main idea of our framework is to learn to predict the location of key vertices of the buildings and connect them in sequence. The proposed method is based on PolyMapper. We upgrade the feature extraction by introducing global context and boundary refinement blocks and add channel and spatial attention modules to improve the effectiveness of the detection module. In addition, we introduce stacked conv-GRU to further preserve the geometric relationship between vertices and accelerate inference. We tested our method on two large-scale VHR-RS building extraction dataset. The results on both COCO and PoLiS metrics demonstrate better performance compared with Mask R-CNN and PolyMapper. Specifically, we achieve 4.2 mask mean average precision (mAP) and 3.7 mean average recall (mAR) absolute improvements compared to PolyMapper. Also, the qualitative comparison shows that our method significantly improves the instance segmentation of buildings of various shapes.
WOS:000644695700009
</snippet>
</document>

<document id="427">
<title>F-UNet plus plus : Remote Sensing Image Fusion Based on Multipurpose Adaptive Shuffle Attention and Composite Multi-Input Reconstruction Network</title>
<url>http://dx.doi.org/10.1109/TIM.2022.3229725</url>
<snippet>The fusion of multispectral (MS) and panchromatic (PAN) images is of great significance for the construction of high-resolution remote sensing images. Because of differences in sensors, no single MS or PAN image can express the complete information of a scene. Therefore, it is a key issue to fuse MS images containing rich spectral content and PAN images with spatial information to construct a high-resolution MS image. In this work, an adaptive shuffle attention (ASA) module and an optimized UNet++ are combined in a fusion-UNet++ (F-UNet++) framework for the problem of MS and PAN image fusion. This ASA module can focus on important information in the mixed domain and adjust the dimensions of tensors. F-UNet++ includes a multiscale feature extraction module, multiscale feature fusion module, and image reconstruction module. The multiscale feature extraction module obtains spectral and spatial information, the multiscale feature fusion module fuses spectral and spatial information, and a composite multi-input image reconstruction module (CMI-UNet++) reconstructs the final image. By combining the ASA attention module, the loss of feature information can be reduced to enhance the fidelity of the spectral and spatial information of the fused image. Experiments show that F-UNet++ is qualitatively and quantitatively superior to current image fusion methods. (The code is available at https://github.com/Josephing/F-UNet).
WOS:000928235800088
</snippet>
</document>

<document id="428">
<title>Semantic Segmentation for High Spatial Resolution Remote Sensing Images Based on Convolution Neural Network and Pyramid Pooling Module</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2860989</url>
<snippet>Semantic segmentation provides a practical way to segment remotely sensed images into multiple ground objects simultaneously, which can be potentially applied to multiple remote sensed related aspects. Current classification algorithms in remotely sensed images are mostly limited by different imaging conditions, the multiple ground objects are difficult to be separated from each other due to high intraclass spectral variances and interclass spectral similarities. In this study, we propose an end-to-end framework to semantically segment high-resolution aerial images without postprocessing to refine the segmentation results. The framework provides a pixel-wise segmentation result, comprising convolutional neural network structure and pyramid pooling module, which aims to extract feature maps at multiple scales. The proposed model is applied to the ISPRS Vaihingen benchmark dataset from the ISPRS 2D Semantic Labeling Challenge. Its segmentation results are compared with previous state-of-the-art method UZ _1, UPB and three other methods that segment images into objects of all the classes (including clutter/background) based on true orthophoto tiles, and achieve the highest overall accuracy of 87.8&#37; over the published performances, to the best of our knowledge. The results validate the efficiency of the proposed model in segmenting multiple ground objects from remotely sensed images simultaneously.
WOS:000444485100023
</snippet>
</document>

<document id="429">
<title>Method for Mapping Rice Fields in Complex Landscape Areas Based on Pre-Trained Convolutional Neural Network from HJ-1 A/B Data</title>
<url>http://dx.doi.org/10.3390/ijgi7110418</url>
<snippet>Accurate and timely information about rice planting areas is essential for crop yield estimation, global climate change and agricultural resource management. In this study, we present a novel pixel-level classification approach that uses convolutional neural network (CNN) model to extract the features of enhanced vegetation index (EVI) time series curve for classification. The goal is to explore the practicability of deep learning techniques for rice recognition in complex landscape regions, where rice is easily confused with the surroundings, by using mid-resolution remote sensing images. A transfer learning strategy is utilized to fine tune a pre-trained CNN model and obtain the temporal features of the EVI curve. Support vector machine (SVM), a traditional machine learning approach, is also implemented in the experiment. Finally, we evaluate the accuracy of the two models. Results show that our model performs better than SVM, with the overall accuracies being 93.60&#37; and 91.05&#37;, respectively. Therefore, this technique is appropriate for estimating rice planting areas in southern China on the basis of a pre-trained CNN model by using time series data. And more opportunity and potential can be found for crop classification by remote sensing and deep learning technique in the future study.
WOS:000451313900005
</snippet>
</document>

<document id="430">
<title>Investigating the Impact of Using IR Bands on Early Fire Smoke Detection from Landsat Imagery with a Lightweight CNN Model</title>
<url>http://dx.doi.org/10.3390/rs14133047</url>
<snippet>Smoke plumes are the first things seen from space when wildfires occur. Thus, fire smoke detection is important for early fire detection. Deep Learning (DL) models have been used to detect fire smoke in satellite imagery for fire detection. However, previous DL-based research only considered lower spatial resolution sensors (e.g., Moderate-Resolution Imaging Spectroradiometer (MODIS)) and only used the visible (i.e., red, green, blue (RGB)) bands. To contribute towards solutions for early fire smoke detection, we constructed a six-band imagery dataset from Landsat 5 Thematic Mapper (TM) and Landsat 8 Operational Land Imager (OLI) with a 30-metre spatial resolution. The dataset consists of 1836 images in three classes, namely "Smoke", "Clear", and "Other_aerosol". To prepare for potential on-board-of-small-satellite detection, we designed a lightweight Convolutional Neural Network (CNN) model named "Variant Input Bands for Smoke Detection (VIB_SD)", which achieved competitive accuracy with the state-of-the-art model SAFA, with less than 2&#37; of its number of parameters. We further investigated the impact of using additional Infra-Red (IR) bands on the accuracy of fire smoke detection with VIB_SD by training it with five different band combinations. The results demonstrated that adding the Near-Infra-Red (NIR) band improved prediction accuracy compared with only using the visible bands. Adding both Short-Wave Infra-Red (SWIR) bands can further improve the model performance compared with adding only one SWIR band. The case study showed that the model trained with multispectral bands could effectively detect fire smoke mixed with cloud over small geographic extents.
WOS:000824285200001
</snippet>
</document>

<document id="431">
<title>Improved Spatial Information Based Semisupervised Classification of Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2961985</url>
<snippet>Motivation in the use of semisupervised learning method is because of its ability to strategically explore and use abundantly available unlabeled samples along with the limited number of labeled samples, as seen in the remote sensing (RS) imagery. In this direction, the present article proposes a semisupervised classification model with spatial information based self-learning methodology to classify land covers in RS images. The model uses granular neural network (GNN) as the base classifier because of its customizable network architecture that is functionally interpretable and costs less computational complexity. Architecture of GNN is governed by fuzzy if-then rules that are generated from fuzzy granulation of input feature space. We have used an improved spatial neighborhood learning method for better understanding of data distribution in a semisupervised framework. The method collects the information with collaborative opinions of two independent information extraction approaches, i.e., based on mutual neighborhood criteria and class map of unlabeled samples. Superiority of the proposed model with existing methods are established with different RS images in terms of various performance measurement indexes.
WOS:000526639900027
</snippet>
</document>

<document id="432">
<title>CNN-Based Target Detection and Classification When Sparse SAR Image Dataset is Available</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3093645</url>
<snippet>Synthetic aperture radar (SAR) is an earth observation technology that can obtain high-resolution image in allweather and all-time conditions,and hence, has been widely used in civil and military applications. SAR target detection and classification are the key processes for the detailed feature information extraction of the interested target. Compared with traditional matched filtering (MF) recovered result, sparse SAR image has lower sidelobes, noise, and clutter. Thus, it will theoretically has better performance in target detection and classification. In this article, we propose a novel sparse SAR image based target detection and classification framework. This novel framework first obtains the sparse SAR image dataset by complex approximate message passing (CAMP), which is an L-1-norm regularization sparse imaging method. Different from other regularization recovery algorithms, CAMP can output not only a sparse solution, but also a nonsparse estimation of considered scene that well preserves the statistical characteristic of the image when protruding the target. Then, we detect and classify the targets by using the convolutional neural network based technologies from the sparse SAR image datasets constructed by the sparse and nonsparse solutions of CAMP, respectively. For clarify, these two kinds of sparse SAR image datasets are named as D-sp, and D-Nsp. Experimental results show that under standard operating conditions, the proposed framework can obtain 92.60&#37; and 99.29&#37; mAP on Faster RCNN and YOLOv3 by using the D-Nsp sparse SAR image dataset. Under extended operating conditions, the mAP value of Faster RCNN and YOLOv3 are 95.69&#37; and 89.91&#37; mAP, respectively. These values based on the D-Nsp dataset are much higher than the classified result based on the corresponding MF dataset.
WOS:000673623400004
</snippet>
</document>

<document id="433">
<title>Towards Image and Video Super-Resolution for Improved Analytics from Overhead Imagery</title>
<url>http://dx.doi.org/10.1117/12.2518179</url>
<snippet>In this work, we address the problem of losing details in the overhead remote sensing image acquisition and generation process due to sensor resolution and distance to target by leveraging state-of-the-art deep neural network architectures. The goal is to recover such details by super-resolving the images acquired by overhead imaging sensors in order for human analysts to interpret data more accurately, and consequentially, for automated visual exploitation algorithms to be applied more effectively. We have developed a super-resolution framework operating on overhead full motion video (FMV) and still imagery (e.g. satellite images). Our framework consists of a neural network capable of learning the mapping between low and high resolution images in order to produce plausible details about the scene. Our framework combines Generative Adversarial Networks (GANs) and Recurrent Neural Networks (RNNs) to process low resolution signals both spatially and, in the case of FMV, temporally. We have applied the output of our system to several visual perception tasks, including object detection, object tracking, and semantic segmentation. We have also applied our methods to data from different geographical areas, sensors, and even modalities to demonstrate broad and generalized applicability.
WOS:000502037900002
</snippet>
</document>

<document id="434">
<title>Intelligent High-Resolution Geological Mapping Based on SLIC-CNN</title>
<url>http://dx.doi.org/10.3390/ijgi9020099</url>
<snippet>High-resolution geological mapping is an important supporting condition for mineral and energy exploration. However, high-resolution geological mapping work still faces many problems. At present, high-resolution geological mapping is still generated by expert interpretation of survey lines, compasses, and field data. The work in the field is constrained by the weather, terrain, and personnel, and the working methods need to be improved. This paper proposes a new method for high-resolution mapping using Unmanned Aerial Vehicle (UAV) and deep learning algorithms. This method uses the UAV to collect high-resolution remote sensing images, cooperates with some groundwork to anchor the lithology, and then completes most of the mapping work on high-resolution remote sensing images. This method transfers a large amount of field work into the room and provides an automatic mapping process based on the Simple Linear Iterative Clustering-Convolutional Neural Network (SLIC-CNN) algorithm. It uses the convolutional neural network (CNN) to identify the image content and confirms the lithologic distribution, the simple linear iterative cluster (SLIC) algorithm can be used to outline the boundary of the rock mass and determine the contact interface of the rock mass, and the mode and expert decision method is used to clarify the results of the fusion and mapping. The mapping method was applied to the Taili waterfront in Xingcheng City, Liaoning Province, China. In this study, the Area Under the Curve (AUC) of the mapping method was 0.937. The Kappa test result was k = 0.8523, and a high-resolution geological map was obtained.
WOS:000522449700039
</snippet>
</document>

<document id="435">
<title>Prediction of drought-induced reduction of agricultural productivity in Chile from MODIS, rainfall estimates, and climate oscillation indices</title>
<url>http://dx.doi.org/10.1016/j.rse.2018.10.006</url>
<snippet>Global food security is negatively affected by drought. Climate projections show that drought frequency and intensity may increase in different parts of the globe. These increases are particularly hazardous for developing countries. Early season forecasts on drought occurrence and severity could help to better mitigate the negative consequences of drought. The objective of this study was to assess if interannual variability in agricultural productivity in Chile can be accurately predicted from freely-available, near real-time data sources. As the response variable, we used the standard score of seasonal cumulative NDVI (zcNDVI), based on 2000-2017 data from Moderate Resolution Imaging Spectroradiometer (MODIS), as a proxy for anomalies of seasonal primary productivity. The predictions were performed with forecast lead times from one- to six-month before the end of the growing season, which varied between census units in Chile. Predictor variables included the zcNDVI obtained by cumulating NDVI from season start up to prediction time; standardised precipitation indices derived from satellite rainfall estimates, for time-scales of 1, 3, 6, 12 and 24 months; the Pacific Decadal Oscillation and the Multivariate ENSO oscillation indices; the length of the growing season, and latitude and longitude. For each of the 758 census units considered, the time series of the response and the predictor variables were averaged for agricultural areas resulting in a 17-season time series per unit for each variable. We used two prediction approaches: (i) optimal linear regression (OLR) whereby for each census unit the single predictor was selected that best explained the interannual zcNDVI variability, and (ii) a multi-layer feedforward neural network architecture, often called deep learning (DL), where all predictors for all units were combined in a single spatio-temporal model. Both approaches were evaluated with a leave-one-year-out cross-validation procedure. Both methods showed good prediction accuracies for small lead times and similar values for all lead times. The mean R-cv(2) values for OLR were 0.95, 0.83, 0.68, 0.56, 0.46 and 0.37, against 0.96, 0.84, 0.65, 0.54, 0.46 and 0.38 for DL, for one, two, three, four, five, and six months lead time, respectively. Given the wide range of climates and vegetation types covered within the study area, we expect that the presented models can contribute to an improved early warning system for agricultural drought in different geographical settings around the globe.
WOS:000450379200002
</snippet>
</document>

<document id="436">
<title>Time series of remote sensing and water deficit to predict the occurrence of soil water repellency in New Zealand pastures</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.09.024</url>
<snippet>Soil water repellency (SWR) is a natural phenomenon occurring in soils throughout the world, which impacts upon ecosystem services at multiple temporal and spatial scales (nano to ecosystem scale). In pastures, the development of SWR is primarily determined by the cycling of hydrophobic materials at the soil surface, and is controlled by climate, soil and water management, and soil properties. The complex interactions between these factors make it an intricate system to understand and model. Detailed spatiotemporal characterization of the surface moisture and biomass in pastoral ecosystems would allow for a better understanding of this phenomenon. Normalized Difference Vegetation Index (NDVI) and Synthetic Aperture Radar (SAR) backscatter are good predictors for surface biomass and soil moisture, respectively. Machine learning on remote sensing time series (TS) data shows promise to predict the occurrence of SWR in pastures. This study evaluates the ability of remote sensing TS data to predict the occurrence of SWR in New Zealand pastures, using three machine learning al- gorithms. Soil water repellency data were collected from 58 pastoral sites. Machine learning models were trained and cross-validated on a monthly aggregated remote sensing and water deficit TS data to predict SWR level. Prediction output from artificial neural networks (ANN), random forest (RF), and support vector machine (SVM) were compared using root mean squared error (RMSE). When using NDVI TS data from 58 site as predictors of SWR, SVM and RF (RMSE = 0.82 and 0.87, respectively) outperformed ANN (RMSE = 1.23). Random forest was used to map SWR magnitude over Hawkes Bay region in the North Island of New Zealand, and the overall accuracy was equal to 86&#37;. This study is the first investigation implicating remote sensing TS data to predict the occurrence of SWR at the regional scale. Mapping the potential SWR will aid in identifying critical zones of SWR, to attenuate its effect on pastures through adapted management.
WOS:000584231200023
</snippet>
</document>

<document id="437">
<title>Correction of Low Vegetation Impact on UAV-Derived Point Cloud Heights With U-Net Networks</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3057272</url>
<snippet>This study presents an approach to the problem of minimizing the impact of low vegetation on the accuracy of a UAV-derived DEM, based on the use of a deep neural network (DNN). It is proposed to use the U-Net network to determine corrections to the height of the raw point cloud so that the processed data reflect the actual earthx2019;s surface. The implemented solution is therefore based on regression, not classification. As a result of the proposed processing method, the expected value of the land surface height is determined for each point of the unified point cloud. In addition, a second U-Net network is trained, enabling the uncertainty of the corrected heights of the land surface to be determined for each point of the unified cloud. The training set includes data from different seasons, which makes the models more resistant and allows for assessment of the impact of the season and more generally the related vegetation status on the model accuracy. The processing results can be used in DEM generation, and also for determining the vertical displacements of the terrain surface associated with underground mining, as well as natural phenomena such as landslides. A key advantage of the proposed processing method is the ability to predict the uncertainty of the results.
WOS:000728266600020
</snippet>
</document>

<document id="438">
<title>Optimal CNN-based semantic segmentation model of cutting slope images</title>
<url>http://dx.doi.org/10.1007/s11709-021-0797-6</url>
<snippet>This paper utilizes three popular semantic segmentation networks, specifically DeepLab v3+, fully convolutional network (FCN), and U-Net to qualitively analyze and identify the key components of cutting slope images in complex scenes and achieve rapid image-based slope detection. The elements of cutting slope images are divided into 7 categories. In order to determine the best algorithm for pixel level classification of cutting slope images, the networks are compared from three aspects: a) different neural networks, b) different feature extractors, and c) 2 different optimization algorithms. It is found that DeepLab v3+ with Resnet18 and Sgdm performs best, FCN 32s with Sgdm takes the second, and U-Net with Adam ranks third. This paper also analyzes the segmentation strategies of the three networks in terms of feature map visualization. Results show that the contour generated by DeepLab v3+ (combined with Resnet18 and Sgdm) is closest to the ground truth, while the resulting contour of U-Net (combined with Adam) is closest to the input images.
WOS:000817053000003
</snippet>
</document>

<document id="439">
<title>Light-Weight Semantic Segmentation Network for UAV Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3104382</url>
<snippet>Semantic segmentation for unmanned aerial vehicle (UAV) remote sensing images has become one of the research focuses in the field of remote sensing at present, which could accurately analyze the ground objects and their relationships. However, conventional semantic segmentation methods based on deep learning require large-scale models that are not suitable for resource-constrained UAV remote sensing tasks. Therefore, it is important to construct a light-weight semantic segmentation method for UAV remote sensing images. With this motivation, we propose a light-weight neural network model with fewer parameters to solve the problem of semantic segmentation of UAV remote sensing images. The network adopts an encoder-decoder architecture. In the encoder, we build a light-weight convolutional neural network model with fewer channels of each layer to reduce the number of model parameters. Then, feature maps of different scales from the encoder are concatenated together after resizing to carry out the multiscale fusion. Moreover, we employ two attention modules to capture the global semantic information from the context and the correlation among channels in UAV remote sensing images. In the decoder part, the model obtains predictions of each pixel through the softmax function. We conducted experiments on the ISPRS Vaihingen dataset, UAVid dataset, and UDD6 dataset to verify the effectiveness of the light-weight network. Our method obtains quality semantic segmentation results evaluated on UAV remote sensing datasets with only 9 M parameters the model owns, which is competitive among popular methods with the same level of parameters.
WOS:000692210900015
</snippet>
</document>

<document id="440">
<title>BUILDING FOOTPRINT EXTRACTION FROM SPACE- BORNE IMAGERY USING DEEP NEURAL NETWORKS</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B2-2022-641-2022</url>
<snippet>One of the important and high-level detailing contained within basemaps is the 'building feature'. Though pre-trained Deep Learning (DL) models are available for Building Feature Extraction (BFE), they are not efficient in predicting the buildings in other locations. This study explores the need and the major issue of implementing DL models for BFE from Very High Resolution Remote Sensing (VHRS) satellite data for any given area. Though advanced DL models are invented, in order to implement them, huge amount of potential training data is demanded for feed in. the building typologies are highly subjected to the context of study area including soil characteristics, culture/lifestyle/economy, architectural style and the building byelaws. The study believes that availability of enough training data of contextual buildings as one of the concern for effective model training. The study aims to extract the buildings present in the study area from Pleiades 1A (2019) RGB VHRS data using simple Mask R-CNN instance segmentation model which is training on the native contextual buildings. Here, an automated method of generating the location-specific training data for a given area is followed using Google Maps API (2021). The generated training data when trained on a deep learning architecture and predicted by the input data yielded promising results. The prediction accuracy of about 98.41&#37; specificity, 96.20&#37; predictive accuracy and 0.89 F1 score are achieved. The methods adopted assist the planning/governing bodies to accelerate the qualitative urban map preparation.
WOS:000855635300088
</snippet>
</document>

<document id="441">
<title>Automated Detection of Greenhouse Structures Using Cascade Mask R-CNN</title>
<url>http://dx.doi.org/10.3390/app12115553</url>
<snippet>Automated detection of the content of images remains a challenging problem in artificial intelligence. Hence, continuous manual monitoring of restricted development zones is critical to maintaining territorial integrity and national security. In this regard, local governments of the Republic of Korea conduct four periodic inspections per year to preserve national territories from illegal encroachments and unauthorized developments in restricted zones. The considerable expense makes responding to illegal developments difficult for local governments. To address this challenge, we propose a deep-learning-based Cascade Mask region-based convolutional neural network (R-CNN) algorithm designed to perform automated detection of greenhouses in aerial photographs for efficient and continuous monitoring of restricted development zones in the Republic of Korea. Our proposed model is regional-based because it was optimized for the Republic of Korea via transfer learning and hyperparameter tuning, which improved the efficiency of the automated detection of greenhouse facilities. The experimental results demonstrated that the mAP value of the proposed Cascade Mask R-CNN model was 83.6, which was 12.83 higher than baseline mask R-CNN, and 0.9 higher than Mask R-CNN with hyperparameter tuning and transfer learning considered. Similarly, the F1-score of the proposed Cascade Mask R-CNN model was 62.07, which outperformed those of the baseline mask R-CNN and the Mask R-CNN with hyperparameter tuning and transfer learning considered (i.e., the F1-score 52.33 and 59.13, respectively). The proposed improved Cascade Mask R-CNN model is expected to facilitate efficient and continuous monitoring of restricted development zones through routine screening procedures. Moreover, this work provides a baseline for developing an integrated management system for national-scale land-use planning and development infrastructure by synergizing geographical information systems, remote sensing, and deep learning models.
WOS:000808740800001
</snippet>
</document>

<document id="442">
<title>Application of Convolutional Neural Networks With Object-Based Image Analysis for Land Cover and Land Use Mapping in Coastal Areas: A Case Study in Ain Temouchent, Algeria</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3185185</url>
<snippet>Land use and land cover (LULC) information is a fundamental component of environmental research relating to urban planning, agricultural sustainability, and natural hazards assessment. In particular, remote sensing technology has demonstrated a powerful capacity for LULC modeling with a corresponding increase in sensor number and type. Here, an advanced convolutional neural network (CNN) deep learning model was developed in combination with object-based image analysis (OBIA) to map LULC in Ain Temouchent coastal area, western Algeria, using sentinel-2 and Pleiades imagery data. First, the CNN model was constructed based on convolution, hidden, and max pooling layers. The parameters of CNN architecture were optimized to improve the model for further processing. Then, based on high levels of CNN feature extraction, the OBIA was applied to classify the segmented objects, and detect the LULC features. Furthermore, machine learning methods, including random forest and support vector machines were tested for comparison. The proposed method achieved a high overall accuracy (93.5&#37;) using Pleiades imagery, revealing significant improvements compared to other machine learning techniques. Accordingly, it was concluded that the method proposed here is useful for LULC detection, and can be applied at larger scales in coastal areas. The derived maps can also inform regional and national-level decision making.
WOS:000821505900001
</snippet>
</document>

<document id="443">
<title>Oil Spill Detection of Kerch Strait in November 2007 from Dual-Polarized TerraSAR-X Image Using Artificial and Convolutional Neural Network Regression Models</title>
<url>http://dx.doi.org/10.2112/SI102-017.1</url>
<snippet>The oil spill is the main marine disaster. It is known that the data mining based method performs better in detecting oil than the traditional SAR based method to distinguish from lookalikes. Recently, artificial neural networks were employed to detect the oil spill. For that synthetic aperture radar intensity map, intensity texture map, co-polarized coherence map, co-polarized phase difference texture map, and digital elevation model was employed as input. The detection performance based on data mining largely depends on the used data, the characteristics of the study area, and the used model. Improving detection performance through various existing data mining models is a very important task since minor improvements in detection can have a significant impact on disaster damage mitigation. Artificial and convolutional neural network regression models were applied to detect the oil spill of Kerch strait in November 2007. The two models showed F1-scores of 0.832 and 0.823 respectively. The results of this study would contribute to monitor oil spill dynamic and mitigate damages caused by the oil spill.
WOS:000600072400018
</snippet>
</document>

<document id="444">
<title>Deep learning for land use and land cover classification from the Ecuadorian Paramo.</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2088872</url>
<snippet>The paramo, plays an important role in our ecosystems as They balance the water resources and can retain substantial quantities of carbon. This research was carried out in the province of Tungurahua, specifically the Quero district. The aim is to develop a classification of the land use land cover (LULC) in the paramo using satellite imagery using several classifiers and determine which one obtains the best performance, for which three different approaches were applied: Pixel-Based Image Analysis (PBIA), Geographic Object-Based Image Analysis (GEOBIA), and a Deep Neural Network (DNN). Various parameters were used, such as the Normalized Difference Vegetation Index (NDVI), the Bare Soil Index (BSI), texture, altitude, and slope. Seven classes were used: paramo, pasture, crops, herbaceous vegetation, urban, shrubrainland, and forestry plantations. The data was obtained with the help of onsite technical experts, using geo-referencing and reference maps. Among the models used the highest-ranked was DNN with an overall precision of 87.43&#37;, while for the paramo class specifically, GEOBIA reached a precision of 95&#37;.
WOS:000811917600001
</snippet>
</document>

<document id="445">
<title>Replay in minds and machines</title>
<url>http://dx.doi.org/10.1016/j.neubiorev.2021.08.002</url>
<snippet>Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.
WOS:000692239200009
</snippet>
</document>

<document id="446">
<title>Climate-Based Regionalization and Inclusion of Spectral Indices for Enhancing Transboundary Land-Use/Cover Classification Using Deep Learning and Machine Learning</title>
<url>http://dx.doi.org/10.3390/rs13245054</url>
<snippet>Accurate land use and cover data are essential for effective land-use planning, hydrological modeling, and policy development. Since the Okavango Delta is a transboundary Ramsar site, managing natural resources within the Okavango Basin is undoubtedly a complex issue. It is often difficult to accurately map land use and cover using remote sensing in heterogeneous landscapes. This study investigates the combined value of climate-based regionalization and integration of spectral bands with spectral indices to enhance the accuracy of multi-temporal land use/cover classification using deep learning and machine learning approaches. Two experiments were set up, the first entailing the integration of spectral bands with spectral indices and the second involving the combined integration of spectral indices and climate-based regionalization based on Koppen-Geiger climate zones. Landsat 5 TM and Landsat 8 OLI images, machine learning classifiers (random forest and extreme gradient boosting), and deep learning (neural network and deep neural network) classifiers were used in this study. Supervised classification using a total of 5140 samples was conducted for the years 1996, 2004, 2013, and 2020. Average overall accuracy and Kappa coefficients were used to validate the results. The study found that the integration of spectral bands with indices improves the accuracy of land use/cover classification using machine learning and deep learning. Post-feature selection combinations yield higher accuracies in comparison to combinations of bands and indices. A combined integration of spectral indices with bands and climate-based regionalization did not significantly improve the accuracy of land use/cover classification consistently for all the classifiers (p &lt; 0.05). However, post-feature selection combinations and climate-based regionalization significantly improved the accuracy for all classifiers investigated in this study. Findings of this study will improve the reliability of land use/cover monitoring in complex heterogeneous TDBs.
WOS:000737598700001
</snippet>
</document>

<document id="447">
<title>Developing an integrated approach based on geographic object-based image analysis and convolutional neural network for volcanic and glacial landforms mapping</title>
<url>http://dx.doi.org/10.1038/s41598-022-26026-z</url>
<snippet>Rapid detection and mapping of landforms are crucially important to improve our understanding of past and presently active processes across the earth, especially, in complex and dynamic volcanoes. Traditional landform modeling approaches are labor-intensive and time-consuming. In recent years, landform mapping has increasingly been digitized. This study conducted an in-depth analysis of convolutional neural networks (CNN) in combination with geographic object-based image analysis (GEOBIA), for mapping volcanic and glacial landforms. Sentinel-2 image, as well as predisposing variables (DEM and its derivatives, e.g., slope, aspect, curvature and flow accumulation), were segmented using a multi-resolution segmentation algorithm, and relevant features were selected to define segmentation scales for each landform category. A set of object-based features was developed based on spectral (e.g., brightness), geometrical (e.g., shape index), and textural (grey level co-occurrence matrix) information. The landform modelling networks were then trained and tested based on labelled objects generated using GEOBIA and ground control points. Our results show that an integrated approach of GEOBIA and CNN achieved an ACC of 0.9685, 0.9780, 0.9614, 0.9767, 0.9675, 0.9718, 0.9600, and 0.9778 for dacite lava, caldera, andesite lava, volcanic cone, volcanic tuff, glacial circus, glacial valley, and suspended valley, respectively. The quantitative evaluation shows the highest performance (Accuracy &gt; 0.9600 and cross-validation accuracy &gt; 0.9400) for volcanic and glacial landforms and; therefore, is recommended for regional and large-scale landform mapping. Our results and the provided automatic workflow emphasize the potential of integrated GEOBIA and CNN for fast and efficient landform mapping as a first step in the earths surface management.
WOS:000898277000025
</snippet>
</document>

<document id="448">
<title>Spatial prediction of sparse events using a discrete global grid system; a case study of hate crimes in the USA</title>
<url>http://dx.doi.org/10.1080/17538947.2021.1886356</url>
<snippet>Spatial prediction of any geographic phenomenon can be an intractable problem. Predicting sparse and uncertain spatial events related to many influencing factors necessitates the integration of multiple data sources. We present an innovative approach that combines data in a Discrete Global Grid System (DGGS) and uses machine learning for analysis. A DGGS provides a structured input for multiple types of spatial data, consistent over multiple scales. This data framework facilitates the training of an Artificial Neural Network (ANN) to map and predict a phenomenon. Spatial lag regression models (SLRM) are used to evaluate and rank the outputs of the ANN. In our case study, we predict hate crimes in the USA. Hate crimes get attention from mass media and the scientific community, but data on such events is sparse. We trained the ANN with data ingested in the DGGS based on a 50&#37; sample of hate crimes as identified by the Southern Poverty Law Center (SPLC). Our spatial prediction is up to 78&#37; accurate and verified at the state level against the independent FBI hate crime statistics with a fit of 80&#37;. The derived risk maps are a guide to action for policy makers and law enforcement.
WOS:000618991600001
</snippet>
</document>

<document id="449">
<title>Local Similarity Siamese Network for Urban Land Change Detection on Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3069242</url>
<snippet>Change detection is an important task in the field of remote sensing. Various change detection methods based on convolutional neural networks (CNNs) have recently been proposed for remote sensing using satellite or aerial images. However, existing methods allow only the partial use of content information in images during change detection because they adopt simple feature similarity measurements or pixel-level loss functions to construct their network architectures. Therefore, when these methods are applied to complex urban areas, their performance in terms of change detection tends to be limited. In this article, a novel CNN-based change detection approach, referred to as a local similarity Siamese network (LSS-Net), with a cosine similarity measurement, was proposed for better urban land change detection in remote sensing images. To use content information on two sequential images, a new change attention map-based content loss function was developed in this study. In addition, to enhance the performance of the LSS-Net in terms of change detection, a suitable feature similarity measurement method, incorporated into a local similarity attention module, was determined through systemic experiments. To verify the change detection performance of the LSS-Net, it was compared with other state-of-the-art methods. The experimental results show that the proposed method outperforms the state-of-the-art methods in terms of the F1 score (0.9630, 0.9377, and 0.7751) and kappa (0.9581, 0.9351, and 0.7646) on the three test datasets, thus suggesting its potential for various remote sensing applications.
WOS:000645081200011
</snippet>
</document>

<document id="450">
<title>PersonalizedQuery Auto-Completion for Large-Scale POI Search at Baidu Maps</title>
<url>http://dx.doi.org/10.1145/3394137</url>
<snippet>Query auto-completion (QAC) is a featured function that has been widely adopted by many sub-domains of search. It can dramatically reduce the number of typed characters and avoid spelling mistakes. These merits of QAC are highlighted to improve user satisfaction, especially when users intend to type in a query on mobile devices. In this article, we will present our industrial solution to the personalized QAC for the point of interest (POI) search at Baidu Maps, a well-known Web mapping service on mobiles in China. The industrial solution makes a good tradeoff between the offline effectiveness of a novel neural learning model that we devised for feature generation and the online efficiency of an off-the-shelf learning to rank (LTR) approach for the real-time suggestion. Besides some practical lessons from how a real-world QAC system is built and deployed in Baidu Maps to facilitate a large number of users in searching tens of millions of POIs, we mainly explore two specific features for the personalized QAC function of the POI search engine: the spatial-temporal characteristics of POIs and the historically queried POIs of individual users. We leverage the large-volume POI search logs in Baidu Maps to conduct offline evaluations of our personalized QAC model measured by multiple metrics, including Mean Reciprocal Rank (MRR), Success Rate (SR), and normalized Discounted Cumulative Gain (nDCG). Extensive experimental results demonstrate that the personalized model enhanced by the proposed features can achieve substantial improvements (i.e., +3.29&#37; MRR, +3.78&#37; SR@1, +5.17&#37; SR@3, +1.96&#37; SR@5, and +3.62&#37; nDCG@5). After deploying this upgraded model into the POI search engine at Baidu Maps for A/B testing online, we observe that some other critical indicators, such as the average number of keystrokes and the average typing speed at keystrokes in a QAC session, which are also related to user satisfaction, decrease as well by 1.37&#37; and 1.69&#37;, respectively. So the conclusion is that the two kinds of features contributed by us are quite helpful in personalized mapping services for industrial practice.
WOS:000582617800010
</snippet>
</document>

<document id="451">
<title>Detection of Banana Plants Using Multi-Temporal Multispectral UAV Imagery</title>
<url>http://dx.doi.org/10.3390/rs13112123</url>
<snippet>Unoccupied aerial vehicles (UAVs) have become increasingly commonplace in aiding planning and management decisions in agricultural and horticultural crop production. The ability of UAV-based sensing technologies to provide high spatial (&lt;1 m) and temporal (on-demand) resolution data facilitates monitoring of individual plants over time and can provide essential information about health, yield, and growth in a timely and quantifiable manner. Such applications would be beneficial for cropped banana plants due to their distinctive growth characteristics. Limited studies have employed UAV data for mapping banana crops and to our knowledge only one other investigation features multi-temporal detection of banana crowns. The purpose of this study was to determine the suitability of multiple-date UAV-captured multi-spectral data for the automated detection of individual plants using convolutional neural network (CNN), template matching (TM), and local maximum filter (LMF) methods in a geographic object-based image analysis (GEOBIA) software framework coupled with basic classification refinement. The results indicate that CNN returns the highest plant detection accuracies, with the developed rule set and model providing greater transferability between dates (F-score ranging between 0.93 and 0.85) than TM (0.86-0.74) and LMF (0.86-0.73) approaches. The findings provide a foundation for UAV-based individual banana plant counting and crop monitoring, which may be used for precision agricultural applications to monitor health, estimate yield, and to inform on fertilizer, pesticide, and other input requirements for optimized farm management.
WOS:000660595800001
</snippet>
</document>

<document id="452">
<title>Employing a Multi-Input Deep Convolutional Neural Network to Derive Soil Clay Content from a Synergy of Multi-Temporal Optical and Radar Imagery Data</title>
<url>http://dx.doi.org/10.3390/rs12091389</url>
<snippet>Earth observation (EO) has an immense potential as being an enabling tool for mapping spatial characteristics of the topsoil layer. Recently, deep learning based algorithms and cloud computing infrastructure have become available with a great potential to revolutionize the processing of EO data. This paper aims to present a novel EO-based soil monitoring approach leveraging open-access Copernicus Sentinel data and Google Earth Engine platform. Building on key results from existing data mining approaches to extract bare soil reflectance values the current study delivers valuable insights on the synergistic use of open access optical and radar images. The proposed framework is driven by the need to eliminate the influence of ambient factors and evaluate the efficiency of a convolutional neural network (CNN) to effectively combine the complimentary information contained in the pool of both optical and radar spectral information and those form auxiliary geographical coordinates mainly for soil. We developed and calibrated our multi-input CNN model based on soil samples (calibration = 80&#37; and validation 20&#37;) of the LUCAS database and then applied this approach to predict soil clay content. A promising prediction performance (R-2 = 0.60, ratio of performance to the interquartile range (RPIQ) = 2.02, n = 6136) was achieved by the inclusion of both types (synthetic aperture radar (SAR) and laboratory visible near infrared-short wave infrared (VNIR-SWIR) multispectral) of observations using the CNN model, demonstrating an improvement of more than 5.5&#37; in RMSE using the multi-year median optical composite and current state-of-the-art non linear machine learning methods such as random forest (RF; R-2 = 0.55, RPIQ = 1.91, n = 6136) and artificial neural network (ANN; R-2 = 0.44, RPIQ = 1.71, n = 6136). Moreover, we examined post-hoc techniques to interpret the CNN model and thus acquire an understanding of the relationships between spectral information and the soil target identified by the model. Looking to the future, the proposed approach can be adopted on the forthcoming hyperspectral orbital sensors to expand the current capabilities of the EO component by estimating more soil attributes with higher predictive performance.
WOS:000543394000036
</snippet>
</document>

<document id="453">
<title>Toward a Yearly Country-Scale CORINE Land-Cover Map without Using Images: A Map Translation Approach</title>
<url>http://dx.doi.org/10.3390/rs13061060</url>
<snippet>CORINE Land-Cover (CLC) and its by-products are considered as a reference baseline for land-cover mapping over Europe and subsequent applications. CLC is currently tediously produced each six years from both the visual interpretation and the automatic analysis of a large amount of remote sensing images. Observing that various European countries regularly produce in parallel their own land-cover country-scaled maps with their own specifications, we propose to directly infer CORINE Land-Cover from an existing map, therefore steadily decreasing the updating time-frame. No additional remote sensing image is required. In this paper, we focus more specifically on translating a country-scale remote sensed map, OSO (France), into CORINE Land Cover, in a supervised way. OSO and CLC not only differ in nomenclature but also in spatial resolution. We jointly harmonize both dimensions using a contextual and asymmetrical Convolution Neural Network with positional encoding. We show for various use cases that our method achieves a superior performance than the traditional semantic-based translation approach, achieving an 81&#37; accuracy over all of France, close to the targeted 85&#37; accuracy of CLC.
WOS:000651981800001
</snippet>
</document>

<document id="454">
<title>Mapping Forested Wetland Inundation in the Delmarva Peninsula, USA Using Deep Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/rs12040644</url>
<snippet>The Delmarva Peninsula in the eastern United States is partially characterized by thousands of small, forested, depressional wetlands that are highly sensitive to weather variability and climate change, but provide critical ecosystem services. Due to the relatively small size of these depressional wetlands and their occurrence under forest canopy cover, it is very challenging to map their inundation status based on existing remote sensing data and traditional classification approaches. In this study, we applied a state-of-the-art U-Net semantic segmentation network to map forested wetland inundation in the Delmarva area by integrating leaf-off WorldView-3 (WV3) multispectral data with fine spatial resolution light detection and ranging (lidar) intensity and topographic data, including a digital elevation model (DEM) and topographic wetness index (TWI). Wetland inundation labels generated from lidar intensity were used for model training and validation. The wetland inundation map results were also validated using field data, and compared to the U.S. Fish and Wildlife Service National Wetlands Inventory (NWI) geospatial dataset and a random forest output from a previous study. Our results demonstrate that our deep learning model can accurately determine inundation status with an overall accuracy of 95&#37; (Kappa = 0.90) compared to field data and high overlap (IoU = 70&#37;) with lidar intensity-derived inundation labels. The integration of topographic metrics in deep learning models can improve the classification accuracy for depressional wetlands. This study highlights the great potential of deep learning models to improve the accuracy of wetland inundation maps through use of high-resolution optical and lidar remote sensing datasets.
WOS:000519564600055
</snippet>
</document>

<document id="455">
<title>Wet-GC: A Novel Multimodel Graph Convolutional Approach for Wetland Classification Using Sentinel-1 and 2 Imagery With Limited Training Samples</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3177579</url>
<snippet>Wetland is one of the most productive resources on earth, and it provides vital habitats for several unique species of flora and fauna. Over the last decade, mapping and monitoring wetlands by utilizing deep learning (DL) models and remote sensing data gained popularity due to the importance of wetland preservation. In general, DL-based methods have shown astonishing achievement in wetland classification, but some practical issues, such as limited training samples, still need to be addressed. Moreover, the performance of most of the DL approaches is decreased when moderate-resolution images with few features are used as input data. One solution to breaking the performance bottleneck of a single model is to fuse two or more of them. To this end, we strive to investigate and develop a multimodel DL algorithm for wetland classification based on the combination of a graph convolutional network (GCN) and a shallow convolutional neural network (CNN), which is called the Wet-GC algorithm hereinafter. In doing this, moderate-resolution Sentinel-1 (S1) synthetic aperture radar (SAR) and Sentinel-2 (S2) multispectral optical imagery are fed into the GCN and CNN models, respectively. As we know from the literature, the synergistic use of S1 SAR and S2 optical imagery can be used to extract different types of wetland features and increase the class discrimination possibility. Hence, wetland mapping by jointly using GCN and CNN has the ability to boost the wetland classification task. Findings indicate that the efficiency of Wet-GC with an overall accuracy (OA) of 88.68&#37; outperforms the results obtained from random forest (OA = 84.88&#37;), support vector machine (OA = 82.86&#37;), extreme gradient boosting (OA = 86.55&#37;), and ResNet50 (OA = 86.93&#37;). The outcomes reveal that the Wet-GC architecture proposed in this article has an excellent capability to be applied over large areas with minimal need for training samples and can perform acceptably in supporting regional wetland mapping.
WOS:000825968600001
</snippet>
</document>

<document id="456">
<title>Polycentric Circle Pooling in Deep Convolutional Networks for High-Resolution Remote Sensing Image Recognition</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2968564</url>
<snippet>Most existing deep learning-based methods use feature maps extracted from convolutional neural networks (CNNs) for classification and detection of high-resolution remote sensing images (HRSIs). However, directly applying these features to classification and object detection in HRSI is problematic because of rotational variations. In this article, we design networks using the polycentric circle pooling (PCP) strategy to alleviate the abovementioned problem. The PCP network (PCP-net) structure can generate a fixed-length representation for different input image sizes and encode rotation-invariant information. With these advantages, PCP-net should in general improve the CNN-based HRSI classification methods. Specifically, on the basis of the concentric circle pooling network structure, we improve the structure using multiple concentric circle centers to generate more robust rotation-invariant information. Using two challenging HRSI scene datasets, we prove that PCP-net improves the accuracy of CNN architectures for a scene classification tasks. PCP-net can be conveniently applied to object detection because the output size is fixed regardless of image size. Experiments applying the faster region-CNN to a publicly available ten-class object detection dataset demonstrate that our proposed PCP can achieve accuracy higher than that of a region of interest pooling in the HRSI object detection task.
WOS:000526635300006
</snippet>
</document>

<document id="457">
<title>Sentinel SAR-optical fusion for crop type mapping using deep learning and Google Earth Engine</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.02.018</url>
<snippet>Accurate crop type mapping provides numerous benefits for a deeper understanding of food systems and yield prediction. Ever-increasing big data, easy access to high-resolution imagery, and cloud-based analytics platforms like Google Earth Engine have drastically improved the ability for scientists to advance data-driven agriculture with improved algorithms for crop type mapping using remote sensing, computer vision, and machine learning. Crop type mapping techniques mainly relied on standalone SAR and optical imagery, few studies investigated the potential of SAR-optical data fusion, coupled with virtual constellation, and 3-dimensional (3D) deep learning networks. To this extent, we use a deep learning approach that utilizes the denoised backscatter and texture information from multi-temporal Sentinel-1 SAR data and the spectral information from multi-temporal optical Sentinel-2 data for mapping ten different crop types, as well as water, soil and urban area. Multi-temporal Sentinel-1 data was fused with multi-temporal optical Sentinel-2 data in an effort to improve classification accuracies for crop types. We compared the results of the 3D U-Net to the state-of-the-art deep learning networks, including SegNet and 2D U-Net, as well as commonly used machine learning method such as Random Forest. The results showed (1) fusing multi-temporal SAR and optical data yields higher training overall accuracies (OA) (3D U-Net 0.992, 2D U-Net 0.943, SegNet 0.871) and testing OA (3D U-Net 0.941, 2D U-Net 0.847, SegNet 0.643) for crop type mapping compared to standalone multi-temporal SAR or optical data (2) optical data fused with denoised SAR data via a denoising convolution neural network (OA 0.912) performed better for crop type mapping compared to optical data fused with boxcar (OA 0.880), Lee (OA 0.881), and median (OA 0.887) filtered SAR data and (3) 3D convolutional neural networks perform better than 2D convolutional neural networks for crop type mapping (SAR OA 0.912, optical OA 0.937, fused OA 0.992).
WOS:000644695700016
</snippet>
</document>

<document id="458">
<title>Geographically and temporally weighted neural networks for satellite-based mapping of ground-level PM2.5</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.06.019</url>
<snippet>The integration of satellite-derived aerosol optical depth (AOD) and station-measured PM2.5 (particulate matter with an aerodynamic diameter of less than 2.5 mu m) provides a promising approach for the monitoring of PM2.5. Previous models have generally only considered either the spatiotemporal heterogeneities of the AOD-PM2.5 relationship or the nonlinear relationship between AOD and PM2 5. In this paper, to simultaneously allow for the nonlinearity and spatiotemporal heterogeneities of the AOD-PM2.5 relationship, the geographically and temporally weighted neural network (GTWNN) model is proposed for the satellite-based estimation of ground-level PM2.5. The GTWNN model represents the nonlinear AOD-PM2.5 relationship via a generalized regression neural network, and is separately established for individual locations and times, to address the spatiotemporal heterogeneities of the AOD-PM2.5 relationship. Meanwhile, a spatiotemporal weighting scheme is incorporated in the GTWNN model to capture the local relations of samples for the training of the AOD-PM2.5 relationship. By the use of the Moderate Resolution Imaging Spectroradiometer (MODIS) AOD product, meteorological data, and MODIS normalized difference vegetation index (NDVI) data as input, the GTWNN model was verified using ground station PM2.5 measurements from China in 2015. The GTWNN model achieved sample-based cross-validation (CV) and site-based CV R-2 values of 0.80 and 0.79, respectively, and it outperformed the geographically and temporally weighted regression model (CV R-2: 0.75 and 0.73) and the daily geographically weighted regression model (CV R-2: 0.72 and 0.72). The proposed model implements the combination of geographical law and a neural network, and will be of great use for remote sensing retrieval of environmental variables.
WOS:000561346200012
</snippet>
</document>

<document id="459">
<title>A Novel VBM Framework of Fiber Recognition Based on Image Segmentation and DCNN</title>
<url>http://dx.doi.org/10.1109/TIM.2019.2912238</url>
<snippet>Manual approaches of fiber recognition are often time-consuming, laborious, and subjective. In this paper, a novel vision-based measurement (VBM) framework of fiber recognition based on image segmentation and deep convolutional neural networks (DCNNs) is proposed. The proposed segmentation method can be applied to the segmentation of overlapping and adhering translucent fibers and has better segmentation performance. At the same time, the segmentation method provides a large number of single-fiber training samples and test samples for DCNN and provides a basis for multifiber map recognition using voting strategies. Finally, four frequently used kinds of fibers, i.e., cotton, camel hair, viscose, and yak cashmere, are selected for experiments. In order to decrease the chance of overfitting, the data augmentation methods are utilized to enlarge the data sets formed by the segmented single-fiber images. In the experimental phase, performance differences are evaluated between the four network architectures, namely, AlexNet, cashmere and wool classification-Net, VGG-Net-16, and GoogLeNet. The GoogLeNet &amp; x2019;s single-fiber classification accuracy rate reaches 96.6 &amp; x0025;, and the average accuracy rate of multifiber identification strategy reaches 99.5 &amp; x0025;. The results show that the proposed VBM framework of fiber recognition is effective.
WOS:000521163500003
</snippet>
</document>

<document id="460">
<title>RSI-CB: A Large-Scale Remote Sensing Image Classification Benchmark Using Crowdsourced Data</title>
<url>http://dx.doi.org/10.3390/s20061594</url>
<snippet>Image classification is a fundamental task in remote sensing image processing. In recent years, deep convolutional neural networks (DCNNs) have experienced significant breakthroughs in natural image recognition. The remote sensing field, however, is still lacking a large-scale benchmark similar to ImageNet. In this paper, we propose a remote sensing image classification benchmark (RSI-CB) based on massive, scalable, and diverse crowdsourced data. Using crowdsourced data, such as Open Street Map (OSM) data, ground objects in remote sensing images can be annotated effectively using points of interest, vector data from OSM, or other crowdsourced data. These annotated images can, then, be used in remote sensing image classification tasks. Based on this method, we construct a worldwide large-scale benchmark for remote sensing image classification. This benchmark has large-scale geographical distribution and large total image number. It contains six categories with 35 sub-classes of more than 24,000 images of size 256x256 pixels. This classification system of ground objects is defined according to the national standard of land-use classification in China and is inspired by the hierarchy mechanism of ImageNet. Finally, we conduct numerous experiments to compare RSI-CB with the SAT-4, SAT-6, and UC-Merced data sets. The experiments show that RSI-CB is more suitable as a benchmark for remote sensing image classification tasks than other benchmarks in the big data era and has many potential applications.
WOS:000529139700053
</snippet>
</document>

<document id="461">
<title>A Multi-Scale Water Extraction Convolutional Neural Network (MWEN) Method for GaoFen-1 Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/ijgi9040189</url>
<snippet>Automatic water body extraction method is important for monitoring floods, droughts, and water resources. In this study, a new semantic segmentation convolutional neural network named the multi-scale water extraction convolutional neural network (MWEN) is proposed to automatically extract water bodies from GaoFen-1 (GF-1) remote sensing images. Three convolutional neural networks for semantic segmentation (fully convolutional network (FCN), Unet, and Deeplab V3+) are employed to compare with the water bodies extraction performance of MWEN. Visual comparison and five evaluation metrics are used to evaluate the performance of these convolutional neural networks (CNNs). The results show the following. (1) The results of water body extraction in multiple scenes using the MWEN are better than those of the other comparison methods based on the indicators. (2) The MWEN method has the capability to accurately extract various types of water bodies, such as urban water bodies, open ponds, and plateau lakes. (3) By fusing features extracted at different scales, the MWEN has the capability to extract water bodies with different sizes and suppress noise, such as building shadows and highways. Therefore, MWEN is a robust water extraction algorithm for GaoFen-1 satellite images and has the potential to conduct water body mapping with multisource high-resolution satellite remote sensing data.
WOS:000539535700004
</snippet>
</document>

<document id="462">
<title>Detecting Large-Scale Urban Land Cover Changes from Very High Resolution Remote Sensing Images Using CNN-Based Classification</title>
<url>http://dx.doi.org/10.3390/ijgi8040189</url>
<snippet>The study investigates land use/cover classification and change detection of urban areas from very high resolution (VHR) remote sensing images using deep learning-based methods. Firstly, we introduce a fully Atrous convolutional neural network (FACNN) to learn the land cover classification. In the FACNN an encoder, consisting of full Atrous convolution layers, is proposed for extracting scale robust features from VHR images. Then, a pixel-based change map is produced based on the classification map of current images and an outdated land cover geographical information system (GIS) map. Both polygon-based and object-based change detection accuracy is investigated, where a polygon is the unit of the GIS map and an object consists of those adjacent changed pixels on the pixel-based change map. The test data covers a rapidly developing city of Wuhan (8000 km(2)), China, consisting of 0.5 m ground resolution aerial images acquired in 2014, and 1 m ground resolution Beijing-2 satellite images in 2017, and their land cover GIS maps. Testing results showed that our FACNN greatly exceeded several recent convolutional neural networks in land cover classification. Second, the object-based change detection could achieve much better results than a pixel-based method, and provide accurate change maps to facilitate manual urban land cover updating.
WOS:000467499300030
</snippet>
</document>

<document id="463">
<title>Embedding topological features into convolutional neural network salient object detection</title>
<url>http://dx.doi.org/10.1016/j.neunet.2019.09.009</url>
<snippet>Salient object detection can be applied as a critical preprocessing step in many computer vision tasks. Recent studies of salient object detection mainly employed convolutional neural networks (CNNs) for mining high-level semantic properties. However, the existing methods can still be improved to find precise semantic information in different scenarios. In particular, in the two main methods employed for salient object detection, the patchwise detection models might ignore spatial structures among regions and the fully convolution-based models mainly consider semantic features in a global manner. In this paper, we proposed a salient object detection framework by embedding topological features into a deep neural network for extracting semantics. We segment the input image and compute weight for each region with low-level features. The weighted segmentation result is called a topological map and it provides an additional channel for the CNN to emphasize the structural integrity and locality during the extraction of semantic features. We also utilize the topological map for saliency refinement based on a conditional random field at the end of our model. Experimental results on six benchmark datasets demonstrated that our proposed framework achieves competitive performance compared to other state-of-the-art methods. (C) 2019 Elsevier Ltd. All rights reserved.
WOS:000500922700024
</snippet>
</document>

<document id="464">
<title>Enhancement of Urban Floodwater Mapping From Aerial Imagery With Dense Shadows via Semisupervised Learning</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3215730</url>
<snippet>Timely and accurate mapping of floodwater in urban areas from aerial imagery is critical to support emergency response and rescue work. However, massive shadows cast by buildings and trees over dense built-up urban areas can cause a significant underestimation of flood outcomes, and few studies for flood monitoring explore this in current state-of-the-art approaches. Meanwhile, recent deep learning (DL) algorithms have reported superior performance in flood mapping over conventional machine learning methods. Nevertheless, acquiring a large amount of training data remains challenging in the DL paradigm. In this study, to exploit the potential of the DL algorithm in detecting all visible (including shadowed and nonshadowed) floodwater with limited training samples from aerial imagery, we designed a modified fully convolutional network and combined it with a deep semisupervised learning framework integrating consistency regularization and RandMix strategy into the floodwater detection workflow. Besides, the test-time augmentation technique was leveraged to improve the model performance in the evaluation phase. Extensive experiments on the 2013 Calgary flood demonstrated the effectiveness of our approach on extracting visible floodwater in urban areas with dense shadows. Notably, our method could accurately detect more floodwater with a largely reduced number of labeled training samples, which is a considerable enhancement in the applicability and availability of DL algorithms for flood monitoring in densely shadowed urban areas.
WOS:000878197300001
</snippet>
</document>

<document id="465">
<title>Using deep learning to map retrogressive thaw slumps in the Beiluhe region (Tibetan Plateau) from CubeSat images</title>
<url>http://dx.doi.org/10.1016/j.rse.2019.111534</url>
<snippet>Retrogressive thaw slumps (RTSs) are among the most dynamic landforms in permafrost areas, and their formation can be attributed to the thawing of ice-rich permafrost. The spatial distribution and impacts of RTSs on the Tibetan Plateau are poorly understood due to their remote location and the technical challenges of automatic mapping. In this study, we innovatively applied DeepLabv3 +, a cutting-edge deep learning algorithm for semantic segmentation, to Planet CubeSat images, which are satellite images with high spatial and temporal resolution. Our method allows us to automatically delineate 220 RTSs within an area of 5200 km(2) with an average precision of 0.541. The corresponding precision, recall, and F1 score are 0.863, 0.833, and 0.848 respectively, when the threshold of intersection over union is 0.5. Moreover, approximately 100 experiments on k-fold cross-validation (k = 3, 5, and 10) and data augmentation show that our method is robust. And a test in a different geographic area shows that the generalization of the trained model is very good. We find that (1) most of the RTSs are small (areas &lt; eight ha and perimeters &lt; 2000 m) and (2) RTSs preferentially develop at locations with gentle slopes (four to eight degrees), and in areas lower than the surroundings (the mean topographic position index is - 0.17) and receiving less solar radiation (i.e., north-facing slopes). The results show that the method can map RTSs automatically from Planet CubeSat images and can potentially be applied to larger areas.
WOS:000509819300007
</snippet>
</document>

<document id="466">
<title>Combining Deep Semantic Edge and Object Segmentation for Large-Scale Roof-Part Polygon Extraction from Ultrahigh-Resolution Aerial Imagery</title>
<url>http://dx.doi.org/10.3390/rs14194722</url>
<snippet>The roofscape plays a vital role in the support of sustainable urban planning and development. However, availability of detailed and up-to-date information on the level of individual roof-part topology remains a bottleneck for reliable assessment of its present status and future potential. Motivated by the need for automation, the current state-of-the-art focuses on applying deep learning techniques for roof-plane segmentation from light-detection-and-ranging (LiDAR) point clouds, but fails to deliver on criteria such as scalability, spatial predictive continuity, and vectorization for use in geographic information systems (GISs). Therefore, this paper proposes a fully automated end-to-end workflow capable of extracting large-scale continuous polygon maps of roof-part instances from ultra-high-resolution (UHR) aerial imagery. In summary, the workflow consists of three main steps: (1) use a multitask fully convolutional network (FCN) to infer semantic roof-part edges and objects, (2) extract distinct closed shapes given the edges and objects, and (3) vectorize to obtain roof-part polygons. The methodology is trained and tested on a challenging dataset comprising of UHR aerial RGB orthoimagery (0.03 m GSD) and LiDAR-derived digital elevation models (DEMs) (0.25 m GSD) of three Belgian urban areas (including the famous touristic city of Bruges). We argue that UHR optical imagery may provide a competing alternative for this task over classically used LiDAR data, and investigate the added value of combining these two data sources. Further, we conduct an ablation study to optimize various components of the workflow, reaching a final panoptic quality of 54.8&#37; (segmentation quality = 87.7&#37;, recognition quality = 62.6&#37;). In combination with human validation, our methodology can provide automated support for the efficient and detailed mapping of roofscapes.
WOS:000867182300001
</snippet>
</document>

<document id="467">
<title>RepDarkNet: A Multi-Branched Detector for Small-Target Detection in Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/ijgi11030158</url>
<snippet>Recent years have seen rapid progress in target-detection missions, whereas small targets, dense target distribution, and shadow occlusion continue to hinder progress in the detection of small targets, such as cars, in remote sensing images. To address this shortcoming, we propose herein a backbone feature-extraction network called "RepDarkNet" that adds several convolutional layers to CSPDarkNet53. RepDarkNet considerably improves the overall network accuracy with almost no increase in inference time. In addition, we propose a multi-scale cross-layer detector that significantly improves the capability of the network to detect small targets. Finally, a feature fusion network is proposed to further improve the performance of the algorithm in the AP@0.75 case. Experiments show that the proposed method dramatically improves detection accuracy, achieving AP = 75.53&#37; for the Dior-vehicle dataset and mAP = 84.3&#37; for the Dior dataset, both of which exceed the state-of-the-art level. Finally, we present a series of improvement strategies that justifies our improvement measures.
WOS:000775407200001
</snippet>
</document>

<document id="468">
<title>DeepSSN: A deep convolutional neural network to assess spatial scene similarity</title>
<url>http://dx.doi.org/10.1111/tgis.12915</url>
<snippet>Spatial-query-by-sketch is an intuitive tool to explore human spatial knowledge about geographic environments and to support communication with scene database queries. However, traditional sketch-based spatial search methods perform inadequately due to their inability to find hidden multiscale map features from mental sketches. In this research, we propose a deep convolutional neural network, namely the Deep Spatial Scene Network (DeepSSN), to better assess the spatial scene similarity. In DeepSSN, a triplet loss function is designed as a comprehensive distance metric to support the similarity assessment. A positive and negative example mining strategy is designed to ensure a consistently increasing distinction of triplets during the training process. Moreover, we develop a prototype spatial scene search system using the proposed DeepSSN, in which the users input spatial queries via sketch maps and the system can automatically augment the sketch training data. The proposed model is validated using multisource conflated map data including 131,300 labeled scene samples after data augmentation. The empirical results demonstrate that the DeepSSN outperforms baseline methods including k-nearest neighbors, the multilayer perceptron, AlexNet, DenseNet, and ResNet using mean reciprocal rank and precision metrics. This research advances geographic information retrieval studies by introducing a novel deep learning method tailored to spatial scene queries.
WOS:000776485100001
</snippet>
</document>

<document id="469">
<title>Object-Level Semantic Segmentation on the High-Resolution Gaofen-3 FUSAR-Map Dataset</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3063797</url>
<snippet>Land cover classification with SAR images mainly focuses on the utilization of fully polarimetric SAR (PolSAR) images. The conventional task of PolSAR classification is single-pixel-based region-level classification using polarimetric target decomposition. In recent years, a large number of high-resolution SAR images have become available, most of which are single-polarization. This article explores the potential of object-level semantic segmentation of high-resolution single-pol SAR images, in particular tailored for the Gaofen-3 (GF-3) sensor. First, a well-annotated GF-3 segmentation dataset "FUSAR-Map" is presented for SAR semantic segmentation. It is based on four data sources: GF-3 single-pol SAR images, Google Earth optical remote sensing images, Google Earth digital maps, and building footprint vector data. It consists of 610 high-resolution GF-3 single-pol SAR images with the size of 1024 x 1024. Second, an encoder-decoder network based on transfer learning is employed to implement semantic segmentation of GF-3 SAR images. For the FUSAR-Map dataset, an optical image pretrained deep convolution neural network (DCNN) is fine-tuned with the SAR training dataset. Experiments on the FUSAR-Map dataset demonstrate the feasibility of object-level semantic segmentation with high-resolution GF-3 single-pol SAR images. Also, our algorithm obtains fourth place about the PolSAR image semantic segmentation on the "2020 Gaofen Challenge on Automated High-Resolution Earth Observation Image Interpretation." The new dataset and the encoder-decoder network are intended as the benchmark data and baseline algorithm for further development of semantic segmentation with high-resolution SAR images. The FUSAR-Map and our algorithm are available at github.com/fudanxu/FUSAR-Map/.
WOS:000634496000003
</snippet>
</document>

<document id="470">
<title>Spatiotemporal modelling of soil moisture in an Atlantic forest through machine learning algorithms</title>
<url>http://dx.doi.org/10.1111/ejss.13123</url>
<snippet>Understanding the spatiotemporal behaviour of soil moisture in tropical forests is fundamental because it mediates processes such as infiltration, groundwater recharge, runoff and evapotranspiration. This study aims to model the spatiotemporal dynamics of soil moisture in an Atlantic forest remnant (AFR) through four machine learning algorithms, as these dynamics represent an important knowledge gap under tropical conditions. Random forest (RF), support vector machine, average neural network and weighted k-nearest neighbour were studied. The abilities of the models were evaluated by means of root mean square error, mean absolute error, coefficient of determination (R-2) and Nash-Sutcliffe efficiency (NS) for two calibration approaches: (a) chronological and (b) randomized. The models were further compared with a multilinear regression (MLR). The study period spans from September 2012 to November 2019 and relies on variables representing the weather, geographical location, forest structure, soil physics and morphology. RF was the best algorithm for modelling the spatiotemporal dynamics of the soil moisture with an NS of 0.77 and R-2 of 0.51 in the randomized approach. This finding highlights the ability of RF to generalize a dataset with contrasting weather conditions. Kriging maps highlighted the suitability of RF to track the spatial distribution of soil moisture in the AFR. Throughfall (TF), potential evapotranspiration (ETo), longitude (Long), diameter at breast height (DBH) and species diversity (H) were the most important variables controlling soil moisture. MLR performed poorly in modelling the spatiotemporal dynamics of soil moisture due to the highly nonlinear condition of this process. Highlights Modelling soil moisture in an Atlantic forest through machine learning. Machine learning algorithms are powerful tools to address the spatiotemporal dynamics of soil moisture. Climate, position and forest variables drive the spatiotemporal pattern of soil moisture. Random forest is the best algorithm to simulate soil moisture dynamics.
WOS:000647116100001
</snippet>
</document>

<document id="471">
<title>Deep Neural Network for Automatic Image Recognition of Engineering Diagrams</title>
<url>http://dx.doi.org/10.3390/app10114005</url>
<snippet>Piping and instrument diagrams (P&amp;IDs) are a key component of the process industry; they contain information about the plant, including the instruments, lines, valves, and control logic. However, the complexity of these diagrams makes it difficult to extract the information automatically. In this study, we implement an object-detection method to recognize graphical symbols in P&amp;IDs. The framework consists of three parts-region proposal, data annotation, and classification. Sequential image processing is applied as the region proposal step for P&amp;IDs. After getting the proposed regions, the unsupervised learning methods, k-means, and deep adaptive clustering are implemented to decompose the detected dummy symbols and assign negative classes for them. By training a convolutional network, it becomes possible to classify the proposed regions and extract the symbolic information. The results indicate that the proposed framework delivers a superior symbol-recognition performance through dummy detection.
WOS:000543385900342
</snippet>
</document>

<document id="472">
<title>Assessing Performance of Convolutional Features for Terrain Classification Using Remote Sensing Data</title>
<url>http://dx.doi.org/</url>
<snippet>Advancements in satellite imagery and image processing techniques has enabled computerized solutions for various geographical and ecological monitoring and analysis problems. This has assisted geographical experts in topographic mapping, disaster monitoring and analysis of urban sprawl. Automated terrain segmentation and classification is a significant preliminary step in any Geographic Information System (GIS) based application. Nevertheless it is the most challenging task as well. In this paper, we present a complete schematic for assessing the effectiveness of various features and classifiers that are popularly employed in the literature for landform classification. A subset of images from DeepSat SAT-6 dataset is reconstructed using the RGB spectral band information and evaluations are performed using all six terrain classes. A number of color and texture features are then extracted from samples of each class and fed to a number of classifiers. In addition to conventional features, the effectiveness of convolutional features for terrain classification is also assessed in this study. A light convolutional neural network is proposed and trained on the employed dataset. The study also highlights the effect of color, texture and convolutional features on classification of each type of terrain under consideration. Experimental results show that convolutional features outperform both texture and color features by achieveing an overall accuracy of 93&#37;.
WOS:000469783600034
</snippet>
</document>

<document id="473">
<title>CNN-based generation of high-accuracy urban distribution maps utilising SAR satellite imagery for short-term change monitoring</title>
<url>http://dx.doi.org/10.1080/19479832.2018.1491897</url>
<snippet>Urban areas in developing countries are experiencing rapid growth, and monitoring short-term changes has become increasingly important. For short-term monitoring, constant observation and generation of high-accuracy urban distribution maps without noise disturbance are key issues. Synthetic aperture radar (SAR) satellite images are suitable for day and night regardless of atmospheric weather condition observations for monitoring changes. We propose a method to generate high-accuracy urban distribution maps for urban change detection via SAR satellite images based using a convolutional neural network (CNN). To increase accuracy, several improvements relative to SAR polarisation combinations and dataset construction are considered in the proposed method. In addition, digital surface model (DSM) data, which are useful in the classification of land cover, were included to improve accuracy. The results demonstrate that high-accuracy urban distribution maps suitable for short-term monitoring were generated. In an evaluation, urban change data were extracted by taking the difference of urban distribution maps. A change analysis with time-series images revealed the locations of short-term urban change, and comparisons with optical satellite images validated the analysis results.
WOS:000446359800003
</snippet>
</document>

<document id="474">
<title>Deep learning in remote sensing applications: A meta-analysis and review</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.04.015</url>
<snippet>Deep learning (DL) algorithms have seen a massive rise in popularity for remote-sensing image analysis over the past few years. In this study, the major DL concepts pertinent to remote-sensing are introduced, and more than 200 publications in this field, most of which were published during the last two years, are reviewed and analyzed. Initially, a meta-analysis was conducted to analyze the status of remote sensing DL studies in terms of the study targets, DL model(s) used, image spatial resolution(s), type of study area, and level of classification accuracy achieved. Subsequently, a detailed review is conducted to describe/discuss how DL has been applied for remote sensing image analysis tasks including image fusion, image registration, scene classification, object detection, land use and land cover (LULC) classification, segmentation, and object-based image analysis (OBIA). This review covers nearly every application and technology in the field of remote sensing, ranging from pre-processing to mapping. Finally, a conclusion regarding the current state-of-the art methods, a critical conclusion on open challenges, and directions for future research are presented.
WOS:000469158200013
</snippet>
</document>

<document id="475">
<title>Mapping Human Activity Volumes Through Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3023730</url>
<snippet>The spatial concentration of the human activity is a crucial indication of socioeconomic vitality. Accurately mapping activity volumes is fundamental to support the regional sustainable development. Current approaches rely on mobile positioning data, which record information about human daily activity but are inaccessible in most cities due to privacy and data sharing concerns. Alternative methods are needed to provide more generalized predictions on extensive areas while maintaining low cost. This study demonstrates how remote sensing imagery can be used through an end-to-end deep learning framework for reliable estimates of human activity volumes. The neighbor effect, representing the inherent nature of spatial autocorrelation in the volumes, is incorporated to improve the network. The proposed model exhibits strong predictive power and demonstrates great explainability of physical environment on variations of activity volumes. Landscape interpretations based on hierarchical features provide both object-based and region-based insights into the coevolvement of landscape and human activity. Our findings indicate the possibility of extensively predicting activity volumes, especially in areas with limited access to mobile data, and provide support for the promising framework to better comprehend broad aspects of the human society from observable physical environments.
WOS:000576263500007
</snippet>
</document>

<document id="476">
<title>Classification of Different Irrigation Systems at Field Scale Using Time-Series of Remote Sensing Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3222884</url>
<snippet>Maps of irrigation systems are of critical value for a better understanding of the human impact on the water cycle, while they also present a very useful tool at the administrative level to monitor changes and optimize irrigation practices. This study proposes a novel approach for classifying different irrigation systems at field level by using remotely sensed data at subfield scale as inputs of different supervised machine learning (ML) models for time-series classification. The ML models were trained using ground-truth data from more than 300 fields collected during a field campaign in 2020 across an intensely cultivated region in Catalunya, Spain. Two hydrological variables retrieved from satellite data, actual evapotranspiration (ETa) and soil moisture (SM), showed the best results when used for classification, especially when combined together, retrieving a final accuracy of 90.1 +/- 2.7&#37;. All the three ML models employed for the classification showed that they were able to distinguish different irrigation systems, regardless of the different crops present in each field. For all the different tests, the best performances were reached by ResNET, the only deep neural network model among the three tested. The resulting method enables the creation of maps of irrigation systems at field level and for large areas, delivering detailed information on the status and evolution of irrigation practices.
WOS:000914641600006
</snippet>
</document>

<document id="477">
<title>Estimating subpixel turbulent heat flux over leads from MODIS thermal infrared imagery with deep learning</title>
<url>http://dx.doi.org/10.5194/tc-15-2835-2021</url>
<snippet>The turbulent heat flux (THF) over leads is an important parameter for climate change monitoring in the Arctic region. THF over leads is often calculated from satellite-derived ice surface temperature (IST) products, in which mixed pixels containing both ice and open water along lead boundaries reduce the accuracy of calculated THF. To address this problem, this paper proposes a deep residual convolutional neural network (CNN)-based framework to estimate THF over leads at the subpixel scale (DeepSTHF) based on remotely sensed images The proposed DeepSTHF provides an IST image and the corresponding lead map with a finer spatial resolution than the input IST image so that the subpixel-scale THF can be estimated from them. The proposed approach is verified using simulated and real Moderate Resolution Imaging Spectroradiometer images and compared with the conventional cubic interpolation and pixelbased methods. The results demonstrate that the proposed CNN-based method can effectively estimate subpixel-scale information from the coarse data and performs well in producing fine-spatial-resolution IST images and lead maps, thereby providing more accurate and reliable THF over leads.
WOS:000667954200001
</snippet>
</document>

<document id="478">
<title>Semantic Segmentation of Sentinel-2 Imagery for Mapping Irrigation Center Pivots</title>
<url>http://dx.doi.org/10.3390/rs12233937</url>
<snippet>Estimating the number and size of irrigation center pivot systems (CPS) from remotely sensed data, using artificial intelligence (AI), is a potential information source for assessing agricultural water use. In this study, we identified two technical challenges in the neural-network-based classification: Firstly, an effective reduction of the feature space of the remote sensing data to shorten training times and increase classification accuracy is required. Secondly, the geographical transferability of the AI algorithms is a pressing issue if AI is to replace human mapping efforts one day. Therefore, we trained the semantic image segmentation algorithm U-NET on four spectral channels (U-NET SPECS) and the first three principal components (U-NET principal component analysis (PCA)) of ESA/Copernicus Sentinel-2 images on a study area in Texas, USA, and assessed the geographic transferability of the trained models to two other sites: the Duero basin, in Spain, and South Africa. U-NET SPECS outperformed U-NET PCA at all three study areas, with the highest f1-score at Texas (0.87, U-NET PCA: 0.83), and a value of 0.68 (U-NET PCA: 0.43) in South Africa. At the Duero, both models showed poor classification accuracy (f1-score U-NET PCA: 0.08; U-NET SPECS: 0.16) and segmentation quality, which was particularly evident in the incomplete representation of the center pivot geometries. In South Africa and at the Duero site, a high rate of false positive and false negative was observed, which made the model less useful, especially at the Duero test site. Thus, geographical invariance is not an inherent model property and seems to be mainly driven by the complexity of land-use pattern. We do not consider PCA a suited spectral dimensionality reduction measure in this. However, shorter training times and a more stable training process indicate promising prospects for reducing computational burdens. We therefore conclude that effective dimensionality reduction and geographic transferability are important prospects for further research towards the operational usage of deep learning algorithms, not only regarding the mapping of CPS.
WOS:000597522700001
</snippet>
</document>

<document id="479">
<title>Breaking Limits of Remote Sensing by Deep Learning From Simulated Data for Flood and Debris-Flow Mapping</title>
<url>http://dx.doi.org/10.1109/TGRS.2020.3035469</url>
<snippet>We propose a framework that estimates the inundation depth (maximum water level) and debris-flow-induced topographic deformation from remote sensing imagery by integrating deep learning and numerical simulation. A water and debris-flow simulator generates training data for various artificial disaster scenarios. We show that regression models based on Attention U-Net and LinkNet architectures trained on such synthetic data can predict the maximum water level and topographic deformation from a remote sensing-derived change detection map and a digital elevation model. The proposed framework has an inpainting capability, thus mitigating the false negatives that are inevitable in remote sensing image analysis. Our framework breaks limits of remote sensing and enables rapid estimation of inundation depth and topographic deformation, essential information for emergency response, including rescue and relief activities. We conduct experiments with both synthetic and real data for two disaster events that caused simultaneous flooding and debris flows and demonstrate the effectiveness of our approach quantitatively and qualitatively. Our code and data sets are available at https://github.com/nyokoya/dlsim.
WOS:000726094900004
</snippet>
</document>

<document id="480">
<title>Mapping essential urban land use categories with open big data: Results for five metropolitan areas in the United States of America</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.06.010</url>
<snippet>Urban land-use maps outlining the distribution, pattern, and composition of various land use types are critically important for urban planning, environmental management, disaster control, health protection, and biodiversity conservation. Recent advances in remote sensing and social sensing data and methods have shown great potentials in mapping urban land use categories, but they are still constrained by mixed land uses, limited predictors, non-localized models, and often relatively low accuracies. To inform these issues, we proposed a robust and cost-effective framework for mapping urban land use categories using openly available multi-source geo-spatial "big data". With street blocks generated from OpenStreetMap (OSM) data as the minimum classification unit, we integrated an expansive set of multi-scale spatially explicit information on land surface, vertical height, socio-economic attributes, social media, demography, and topography. We further proposed to apply the automatic ensemble learning that leverages a bunch of machine learning algorithms in deriving optimal urban land use classification maps. Results of block-level urban land use classification in five metropolitan areas of the United States found the overall accuracies of major-class (Level-I) and minor-class (Level-II) classification could be high as 91&#37; and 86&#37;, respectively. A multi-model comparison revealed that for urban land use classification with high-dimensional features, the multi-layer stacking ensemble models achieved better performance than base models such as random forest, extremely randomized trees, LightGBM, CatBoost, and neural networks. We found without very-high-resolution National Agriculture Imagery Program imagery, the classification results derived from Sentinel-1, Sentinel-2, and other open big data based features could achieve plausible overall accuracies of Level-I and Level-II classification at 88&#37; and 81&#37;, respectively. We also found that model transferability depended highly on the heterogeneity in characteristics of different regions. The methods and findings in this study systematically elucidate the role of data sources, classification methods, and feature transferability in block-level land use classifications, which have important implications for mapping multi-scale essential urban land use categories.
WOS:000669954900015
</snippet>
</document>

<document id="481">
<title>Towards a Deep Learning Powered Query Engine for Urban Planning</title>
<url>http://dx.doi.org/</url>
<snippet>Urban planning is crucial to sustainable growth. In order for the planners to make informed decisions, data from multiple sources have to he retrieved and cross-referenced efficiently. We discuss the implementation of a query engine which accepts natural language as input, using machine learning and NLP techniques namely word embed cling, CNN, rule-based system and NER to produce accurate output enriched with geographical insights to facilitate the planning process. The query engine classifies the query into one of the planning domains, as well as determines the category, location and the size of buffer. Processed results are presented on the ePlamier, which is a map service on the GIS implemented by the Urban Redevelopment Authority (URA) of Singapore.
WOS:000428370700024
</snippet>
</document>

<document id="482">
<title>Few-Shot Ship Classification in Optical Remote Sensing Images Using Nearest Neighbor Prototype Representation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3066539</url>
<snippet>With the development of ship detection in optical remote sensing images, it is convenient to obtain accurate detection results and ship images. Owing to the superior performance of convolutional neural networks (CNNs), one way to acquire the category of ship is to train a classifier using numerous ship images. However, the classification performance of CNN may degrade in the case of a small number of training samples. To solve this problem, we propose a metric-based few-shot method to generate novel concept (class) representation using nearest neighbor prototype. Different from image-to-image measure in common few-shot methods, we use an image-to-feature measure. We map small number of samples to the feature space through CNN, and generate prototypes by computing nearest neighbor value on each dimension of the feature separately. Our method is validated on patch-level ship image dataset, a reproduced ship classification dataset based on HRSC2016. The experimental results demonstrate the accuracy and robustness of our method for ship classification with a small amount of labeled data.
WOS:000638400600009
</snippet>
</document>

<document id="483">
<title>Sentinel 2 Time Series Analysis with 3D Feature Pyramid Network and Time Domain Class Activation Intervals for Crop Mapping</title>
<url>http://dx.doi.org/10.3390/ijgi10070483</url>
<snippet>In this paper, we provide an innovative contribution in the research domain dedicated to crop mapping by exploiting the of Sentinel-2 satellite images time series, with the specific aim to extract information on "where and when" crops are grown. The final goal is to set up a workflow able to reliably identify (classify) the different crops that are grown in a given area by exploiting an end-to-end (3+2)D convolutional neural network (CNN) for semantic segmentation. The method also has the ambition to provide information, at pixel level, regarding the period in which a given crop is cultivated during the season. To this end, we propose a solution called Class Activation Interval (CAI) which allows us to interpret, for each pixel, the reasoning made by CNN in the classification determining in which time interval, of the input time series, the class is likely to be present or not. Our experiments, using a public domain dataset, show that the approach is able to accurately detect crop classes with an overall accuracy of about 93&#37; and that the network can detect discriminatory time intervals in which crop is cultivated. These results have twofold importance: (i) demonstrate the ability of the network to correctly interpret the investigated physical process (i.e., bare soil condition, plant growth, senescence and harvesting according to specific cultivated variety) and (ii) provide further information to the end-user (e.g., the presence of crops and its temporal dynamics).
WOS:000676437600001
</snippet>
</document>

<document id="484">
<title>Building Extraction at Scale Using Convolutional Neural Network: Mapping of the United States</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2835377</url>
<snippet>Establishing up-to-date large scale building maps is essential to understand the urban dynamics, such asestimating population, urban planning, and many other applications. Although many computer vision tasks have been successfully carried out with deep convolutional neural networks, there is a growing need to understand their large scale impact on building mapping with remote sensing imagery. Taking advantage of the scalability of convolutional neural networks (CNNs) and using only few areas with the abundance of building footprints, for the first time we conduct a comparative analysis of four state-of-the-art CNNs for extracting building footprints across the entire continental United States. The four CNN architectures namely: Branch-out CNN, fully convolutional network (FCN), conditional random field as recurrent neural network (CRFasRNN), and SegNet, support semantic pixelwise labeling and focus on capturing textural information at multiscale. We use 1-meter resolution aerial images from National Agriculture Imagery Program as the test-bed, and compare the extraction results across the four methods. In addition, we propose to combine signed-distance labels with SegNet, the preferred CNN architecture identified by our extensive evaluations, to advance building extraction results to instance level. We further demonstrate the usefulness of fusing additional near IR information into the building extraction framework. Large scale experimental evaluations are conducted and reported using metrics that include: Precision, recall rate, intersection over union, and the number of buildings extracted. With the improved CNN model and no requirement of further postprocessing, we have generated building maps for the United States with an average processing time less than oneminute for an area of size similar to 56 km(2). The quality of extracted buildings and processing time demonstrated that the proposed CNN based framework fits the need of building extraction at scale.
WOS:000443755000004
</snippet>
</document>

<document id="485">
<title>Digital Soil Mapping of Soil Organic Matter with Deep Learning Algorithms</title>
<url>http://dx.doi.org/10.3390/ijgi11050299</url>
<snippet>Digital soil mapping has emerged as a new method to describe the spatial distribution of soils economically and efficiently. In this study, a lightweight soil organic matter (SOM) mapping method based on a deep residual network, which we call LSM-ResNet, is proposed to make accurate predictions with background covariates. ResNet not only integrates spatial background information around the observed environmental covariates, but also reduces problems such as information loss, which undermines the integrity of information and reduces prediction uncertainty. To train the model, rectified linear units, mean squared error, and adaptive momentum estimation were used as the activation function, loss/cost function, and optimizer, respectively. The method was tested with Landsat5, the meteorological data from WorldClim, and the 1602 sampling points set from Xinxiang, China. The performance of the proposed LSM-ResNet was compared to a traditional machine learning algorithm, the random forest (RF) algorithm, and a training set (80&#37;) and a test set (20&#37;) were created to test both models. The results showed that the LSM-ResNet (RMSE = 6.40, R-2 = 0.51) model outperformed the RF model in both the roots mean square error (RMSE) and coefficient of determination (R-2), and the training accuracy was significantly improved compared to RF (RMSE = 6.81, R-2 = 0.46). The trained LSM-ResNet model was used for SOM prediction in Xinxiang, a district of plain terrain in China. The prediction maps can be deemed an accurate reflection of the spatial variability of the SOM distribution.
WOS:000801768200001
</snippet>
</document>

<document id="486">
<title>Personalized tourist route recommendation model with a trajectory understanding via neural networks</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2130456</url>
<snippet>Travel recommendations form a major part of tourism service. Traditional collaborative filtering and Markov model are not appropriate for expressing the trajectory features, for travel preferences of tourists are dynamic and affected by previous behaviors. Inspired by the success of deep learning in sequence learning, a personalized recurrent neural network (P-RecN) is proposed for tourist route recommendation. It is data-driven and adaptively learns the unknown mapping of historical trajectory input to recommended route output. Specifically, a trajectory encoding module is designed to mine the semantic information of trajectory data, and LSTM neural networks are used to capture the sequence travel patterns of tourists. In particular, a temporal attention mechanism is integrated to emphasize the main behavioral intention of tourists. We retrieve a geotagged photo dataset in Shanghai, and evaluate our model in terms of accuracy and ranking ability. Experimental results illustrated that P-RecN outperforms other baseline approaches and can effectively understand the travel patterns of tourists.
WOS:000876027800001
</snippet>
</document>

<document id="487">
<title>Detecting Land Abandonment in Lodz Voivodeship Using Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/land9030082</url>
<snippet>The wide availability of multispectral satellite imagery through projects such as Landsat and Sentinel, combined with the introduction of deep learning in general and Convolutional Neural Networks (CNNs) in particular, has allowed for the rapid and effective analysis of multiple classes of problems pertaining to land coverage. Taking advantage of the two phenomena, we propose a machine learning model for the classification of land abandonment. We designed a Convolutional Neural Network architecture that outputs a classification probability for the presence of land abandonment in a given 15-25 ha grid element by using multispectral imaging data obtained through Sentinel Hub. For both the training and validation of the model, we used imagery of the Lodz Voivodeship in central Poland. The main source of truth was a 2009 orthophoto study available from the WMS (Web Map Service) of the Geoportal site. The model achieved 0.855 auc (area under curve), 0.47 loss, and 0.78 accuracy for the test dataset. Using the classification results and the Getis-Ord Gi* statistic, we prepared a map of cold- and hotspots with individual areas that exceed 50 km(2). This thresholded heatmap allowed for an analysis of contributing factors for both low and intense land abandonment, demonstrating that common trends are identifiable through the interpretation of the classification results of the chosen model. We additionally performed a comparative field study on two selected cold- and hotspots. The study, along with the high-accuracy results of the models validation, confirms that CNN-type models are an effective tool for the automatic detection of land abandonment.
WOS:000523663000032
</snippet>
</document>

<document id="488">
<title>Self-attention for raw optical Satellite Time Series Classification</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.06.006</url>
<snippet>The amount of available Earth observation data has increased dramatically in recent years. Efficiently making use of the entire body of information is a current challenge in remote sensing; it demands lightweight problemagnostic models that do not require region- or problem-specific expert knowledge. End-to-end trained deep learning models can make use of raw sensory data by learning feature extraction and classification in one step, solely from data. Still, many methods proposed in remote sensing research require implicit feature extraction through data preprocessing or explicit design of features. In this work, we compare recent deep learning models on crop type classification on raw and preprocessed Sentinel 2 data. We concentrate on the common neural network architectures for time series, i.e., 1D-convolutions, recurrence, and the novel self-attention architecture. Our central findings are that data preprocessing still increased the overall classification performance for all models while the choice of model was less crucial. Self-attention and recurrent neural networks, by their architecture, outperformed convolutional neural networks on raw satellite time series. We explore this by a feature importance analysis based on gradient backpropagation that exploits the differentiable nature of deep learning models. Further, we qualitatively show how self-attention scores focus selectively on a few classification-relevant observations.
WOS:000584231200032
</snippet>
</document>

<document id="489">
<title>Inception U-Net Architecture for Semantic Segmentation to Identify Nuclei in Microscopy Cell Images</title>
<url>http://dx.doi.org/10.1145/3376922</url>
<snippet>With the increasing applications of deep learning in biomedical image analysis, in this article we introduce an inception U-Net architecture for automating nuclei detection in microscopy cell images of varying size and modality to help unlock faster cures, inspired from Kaggle Data Science Bowl Challenge 2018 (KDSB18). This study follows from the fact that most of the analysis requires nuclei detection as the starting phase for getting an insight into the underlying biological process and further diagnosis. The proposed architecture consists of a switch normalization layer, convolution layers, and inception layers (concatenated 1x1, 3x3, and 5x5 convolution and the hybrid of a max and Hartley spectral pooling layer) connected in the U-Net fashion for generating the image masks. This article also illustrates the model perception of image masks using activation maximization and filter map visualization techniques. A novel objective function segmentation loss is proposed based on the binary cross entropy, dice coefficient, and intersection over union loss functions. The intersection over union score, loss value, and pixel accuracy metrics evaluate the model over the KDSB18 dataset. The proposed inception U-Net architecture exhibits quite significant results as compared to the original U-Net and recent U-Net++ architecture.
WOS:000583712100011
</snippet>
</document>

<document id="490">
<title>Deep Semantic-Visual Alignment for zero-shot remote sensing image scene classification</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2023.02.012</url>
<snippet>Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Those class embeddings may not be visually detectable and the annotation process is time-consuming and labor-intensive. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. We predict attributes for each class by depicting the semantic-visual similarity between attributes and images. In this way, the attribute annotation process is accomplished by machine instead of human as in other methods. Moreover, we propose a Deep Semantic -Visual Alignment (DSVA) that take advantage of the self-attention mechanism in the transformer to associate local image regions together, integrating the background context information for prediction. The DSVA model further utilizes the attribute attention maps to focus on the informative image regions that are essential for knowledge transfer in ZSL, and maps the visual images into attribute space to perform ZSL classification. With extensive experiments, we show that our model outperforms other state-of-the-art models by a large margin on a challenging large-scale RS scene classification benchmark. Moreover, we qualitatively verify that the attributes annotated by our network are both class discriminative and semantic related, which benefits the zero-shot knowledge transfer.
WOS:000952301200001
</snippet>
</document>

<document id="491">
<title>A Spatial Ensemble Model for Rockfall Source Identification From High Resolution LiDAR Data and GIS</title>
<url>http://dx.doi.org/10.1109/ACCESS.2019.2919977</url>
<snippet>Rockfall source identification is the most challenging task in rockfall hazard and risk assessment. This difficulty rises in the areas where there is a presence of other types of the landslide, such as shallow landslide and debris flow. The aim of this paper is to develop and test a hybrid model that can accurately identify the source areas. High-resolution light detection and ranging data (LiDAR) was employed to derive the digital terrain model (DTM), from which several conditioning factors were extracted. These conditioning factors were optimized utilizing the ant colony optimization (ACO). Different machine learning algorithms, namely, logistic regression (LR), random tree (RT), random forest (RF), support vector machine (SVM), and artificial neural network (ANN), in addition to their ensemble models (stacking, bagging, and voting), were examined. This is based on the selected best subset of conditioning factors and inventory dataset. Stacking LR-RT (the best fit model) was then utilized to produce the probabilities of different landslide types. On the other hand, the Gaussian mixture model (GMM) was optimized and applied for automatically identifying the slope threshold of the occurrence of the different landslide types. In order to reduce the model sensitivity to the alteration in various conditioning factors and to improve the model computations performance, land use probability area was formed. The rockfall sources were identified by integrating the probability maps and the reclassified slope raster based on the GMM results. The accuracy assessment reveals that the developed hybrid model can identify the probable rockfall regions with an accuracy of 0.95 based on the validation dataset and 0.94 on based the training dataset. The slope thresholds calculated by GMM were found to be &gt; 58 degrees, 22 degrees-58 degrees, and 9 degrees-22 degrees for rockfall, shallow landslide, and debris flow, respectively. This indicates that the model can be generalized and replicated in different regions, and the proposed method can be applied in various landslides studies.
WOS:000472651700001
</snippet>
</document>

<document id="492">
<title>3D EdgeSegNET: a deep neural network framework for simultaneous edge detection and segmentation of medical images</title>
<url>http://dx.doi.org/10.1007/s11760-023-02518-x</url>
<snippet>Deep learning has been a mainstream choice for computer-aided medical diagnosis in recent years. Medical practitioners need accurate and fast diagnosis results to monitor the extent of the disease and devise an efficient treatment plan. This article proposes a deep neural network-based 3D EdgeSegNET architectural framework for simultaneous segmentation and edge detection of brain tumors. It is essential to extract and analyze the critical information about the lesion's shape and volume from the brain's magnetic resonance imaging protocol for accurate tumor segmentation. A radiologist keeps a mental map of both edges and segmented regions while performing the brain tumor segmentation task, so shapes and segmented regions are essential parameters of any segmentation task. By recognizing the importance of boundary and area, an automated 3D EdgeSegNET model is devised by combining edge detection and semantic segmentation tasks in a single model architecture with a twofold output of interest of edges and tumor volume. The proposed model achieves more accurate and robust performance on the benchmark dataset provided by Brain Tumor Segmentation Challenge (BraTS 2020) compared to a few top-ranked submissions. The edge detection task obtains an F-measure at optimal dataset scale of 0.7704. An average dice score of 0.89595 and a Hausdorff distance (95th percentile) of 4.375 is achieved on the whole tumor for the semantic segmentation task.
WOS:000935498700003
</snippet>
</document>

<document id="493">
<title>Detection method of tunnel lining voids based on guided anchoring mechanism</title>
<url>http://dx.doi.org/10.1016/j.compeleceng.2021.107462</url>
<snippet>In tunnel construction engineering, the form of tunnel void diseases are complex and easily affected by the geographical environment. The traditional manual interpretation of image data has the characteristics of heavy workload, high probability of missing, and misjudgment. This paper constructs a convolution neural network that integrates the mechanism of guiding anchoring to detect tunnel voids. The network is composed of four parts: Feature extraction network extracts disease features from the enriched samples; Region proposal by guided anchoring join the generalized intersection over union (GIoU) evaluation criteria, and predict the shape of the anchor point through learning; The obtained feature maps are fixed in the region of interest pooling; Finally, the disease features are classified and bounding box regression. Compared with the existing target detection algorithm, the experimental results show that the improved network achieves an average classification accuracy of 92.74&#37;, and the trained model has good generalization ability and robustness.
WOS:000702661400010
</snippet>
</document>

<document id="494">
<title>A Novel Deep Structure U-Net for Sea-Land Segmentation in Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2925841</url>
<snippet>Sea-land segmentation is an important process for many key applications in remote sensing. Proper operative sea-land segmentation for remote sensing images remains a challenging issue due to complex and diverse transition between sea and land. Although several convolutional neural networks (CNNs) have been developed for sea-land segmentation, the performance of these CNNs is far from the expected target. This paper presents a novel deep neural network structure for pixel-wise sea-land segmentation, a residual Dense U-Net (RDU-Net), in complex and high-density remote sensing images. RDU-Net is a combination of both downsampling and upsampling paths to achieve satisfactory results. In each downsampling and upsampling path, in addition to the convolution layers, several densely connected residual network blocks are proposed to systematically aggregate multiscale contextual information. Each dense network block contains multilevel convolution layers, short-range connections, and an identity mapping connection, which facilitates features reuse in the network and makes full use of the hierarchical features from the original images. These proposed blocks have a certain number of connections that are designed with shorter distance backpropagation between the layers and can significantly improve segmentation results while minimizing computational costs. We have performed extensive experiments on two real datasets, Google-Earth and ISPRS, and compared the proposed RDU-Net against several variations of dense networks. The experimental results show that RDU-Net outperforms the other state-of-the-art approaches on the sea-land segmentation tasks.
WOS:000489785800007
</snippet>
</document>

<document id="495">
<title>Densely Based Multi-Scale and Multi-Modal Fully Convolutional Networks for High-Resolution Remote-Sensing Image Semantic Segmentation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2906387</url>
<snippet>Automatic and accurate semantic segmentation from high-resolution remote-sensing images plays an important role in the field of aerial images analysis. The task of dense semantic segmentation requires that semantic labels be assigned to each pixel in the image. Recently, convolutional neural networks (CNNs) have proven to be powerful tools for image classification, and they have been adopted in the remote-sensing community. But many limitations still exist when modern CNN architectures are directly applied to remote-sensing images, such as gradient explosion when the depth of the network increases, over-fitting with limited labeled remote-sensing data, and special differences between remote-sensing images and natural images. In this paper, we present a novel architecture that combines the thought of dense connection and fully convolutional networks, referred as DFCN, to automatically provide fine-grained semantic segmentation maps. In addition, we improve DFCN with multi-scale filters to widen the network and to increase the richness and diversity of extracted information, making the network more powerful and expressive than the naive convolution layer. Furthermore, we investigate a multi-modal network that incorporates digital surface models (DSMs) into a DFCN structure, and then we propose dual-path densely convolutional networks where the encoder consists of two paths that, respectively, extract features from spectral data and DSMs data and then fuse them. Finally, through conducting comprehensive experimental evaluations on two remote sensing benchmark datasets, we test our proposed models and compare them with other deep networks. The results demonstrate the effectiveness of proposed approaches; they can achieve competitive performance compared with the current state-of-the-art methods.
WOS:000487530100004
</snippet>
</document>

<document id="496">
<title>INTRODUCING EUROSAT: A NOVEL DATASET AND DEEP LEARNING BENCHMARK FOR LAND USE AND LAND COVER CLASSIFICATION</title>
<url>http://dx.doi.org/</url>
<snippet>In this paper, we address the challenge of land use and land cover classification using Sentinel-2 satellite images. The key contributions are as follows. We present a novel dataset based on Sentinel-2 satellite images covering 13 different spectral bands and consisting of 10 classes with in total 27,000 labeled images. We evaluate state-of-the-art deep Convolutional Neural Networks (CNNs) on this novel dataset with its different spectral bands. We also evaluate deep CNNs on existing remote sensing datasets and compare the obtained results. With the proposed novel dataset, we achieved an overall classification accuracy of 98.57&#37;. The classification system resulting from the proposed research opens a gate towards various Earth observation applications. We demonstrate how the classification system can assist in improving geographical maps.
WOS:000451039800052
</snippet>
</document>

<document id="497">
<title>Improving spatial accuracy of urban growth simulation models using ensemble forecasting approaches</title>
<url>http://dx.doi.org/10.1016/j.compenvurbsys.2019.04.005</url>
<snippet>This paper aims to improve the spatial accuracy of urban growth simulation models and clarify any associated uncertainties. Artificial Neural Networks (ANNs), Random Forest (RF), and Logistic Regression (LR) were implemented to simulate urban growth in the megacity of Tehran, Iran, as a case study. Model calibration was performed using data between 1985 and 1999 whereas the data between 1999 and 2014 was used for model validation. First of all, Transition Index Maps (TIMs) were computed by means of each model to assess the potential of urban growth for each cell. Using the standard deviation, consensus between the TIMs was evaluated. Because the TIMs of the individual models manifested discrepancies, they were combined using a number of ensemble forecasting approaches including median, mathematical average, principle component analysis, and weighted area under the total operating characteristic. The individual and combined TIMs were then put into Cellular Automata (CA) to simulate the future pattern of urban growth in Tehran. The results were evaluated in two stages. At first, the TIMs were evaluated by means of Total Operating Characteristics (TOC), and then a set of statistical indices was used to evaluate the spatial accuracy of the simulated urban growth maps. The best result was obtained by median ensemble forecasting, whereas the LR model showed the lowest level of accuracy. In similar studies, it is recommended to implement and compare different ensemble methods when integrating individual models.
WOS:000471087600008
</snippet>
</document>

<document id="498">
<title>Sentinel-1-Based Water and Flood Mapping: Benchmarking Convolutional Neural Networks Against an Operational Rule-Based Processing Chain</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3152127</url>
<snippet>In this study, the effectiveness of several convolutional neural network architectures (AlbuNet-34/FCN/DeepLabV3+/U-Net/U-Net++) for water and flood mapping using Sentinel-1 amplitude data is compared to an operational rule-based processor (S-1FS). This comparison is made using a globally distributed dataset of Sentinel-1 scenes and the corresponding ground truth water masks derived from Sentinel-2 data to evaluate the performance of the classifiers on a global scale in various environmental conditions. The impact of using single versus dual-polarized input data on the segmentation capabilities of AlbuNet-34 is evaluated. The weighted cross entropy loss is combined with the Lovasz loss and various data augmentation methods are investigated. Furthermore, the concept of atrous spatial pyramid pooling used in DeepLabV3+ and the multiscale feature fusion inherent in U-Net++ are assessed. Finally, the generalization capacity of AlbuNet-34 is tested in a realistic flood mapping scenario by using additional data from two flood events and the Sen1Floods11 dataset. The model trained using dual polarized data outperforms the S-1FS significantly and increases the intersection over union (IoU) score by 5&#37;. Using a weighted combination of the cross entropy and the Lovasz loss increases the IoU score by another 2&#37;. Geometric data augmentation degrades the performance while radiometric data augmentation leads to better testing results. FCN/DeepLabV3+/U-Net/U-Net++ perform not significantly different to AlbuNet-34. Models trained on data showing no distinct inundation perform very well in mapping the water extent during two flood events, reaching IoU scores of 0.96 and 0.94, respectively, and perform comparatively well on the Sen1Floods11 dataset.
WOS:000766623900002
</snippet>
</document>

<document id="499">
<title>Exploiting deep learning and volunteered geographic information for mapping buildings in Kano, Nigeria</title>
<url>http://dx.doi.org/10.1038/sdata.2018.217</url>
<snippet>Buildings in the developing world are inadequately mapped. Lack of such critical geospatial data adds unnecessary challenges to locating and reaching a large segment of the worlds most vulnerable population, impeding sustainability goals ranging from disaster relief to poverty reduction. Use of volunteered geographic information (VGI) has emerged as a widely accepted source to fill such voids. Despite its promise, availability of building maps for developing countries significantly lags behind demand. We present a new approach, coupling deep convolutional neural networks (CNNs) with VGI for automating building map generation from high-resolution satellite images for Kano state, Nigeria. Specifically, we trained a CNN with VGI building outlines of limited quality and quantity and generated building maps for a 50,000 km(2) area. Resulting maps are in strong agreement with existing settlement maps and require a fraction of the manual input needed for the latter. The VGI-based maps will provide support across multiple facets of socioeconomic development in Kano state, and demonstrates potential advancements in current mapping capabilities in resource constrained countries.
WOS:000448055100001
</snippet>
</document>

<document id="500">
<title>Modelling soil organic carbon stock distribution across different land-uses in South Africa: A remote sensing and deep learning approach</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.04.026</url>
<snippet>Soil organic carbon (SOC) is a critical measure for ecosystem health and offers opportunities to understand carbon fluxes and associated implications. However, SOC can be significantly influenced by anthropogenic land use change, with intensive and extensive disturbances resulting in considerable SOC loss. Consequently, understanding the spatial distribution of SOC across different land uses, particularly at national level characterised by different biomes, is vital for integrated land-use planning and climate change mitigation. Remote sensing and deep learning (DL) offer a reliable largescale mapping of SOC by leveraging on their big data provision and powerful analytical prowess, respectively. This study modelled SOC stocks across South Africas major land uses using Deep Neural Networks (DNN) and Sentinel-3 satellite data. Based on 1936 soil samples and 31 spectral predictors, results show a relatively high accuracy with an R-2 and RMSE value of 0.685 and 10.15 t/h (26&#37; of the mean), respectively. From the seven land uses evaluated, grasslands (31.36&#37;) contributed the most to the overall SOC stocks while urban vegetation (0.04&#37;) contributed the least. Moreover, although SOC stock was found to be relatively proportional to land coverage, commercial (46.06 t/h) and natural (44.34 t/h) forests showed a higher carbon sequestration capacity. These findings provide an important guideline to managing SOC stocks in South Africa, useful in climate change mitigation through sustainable land-use practices. Whereas landscape restoration, and other relevant interventions are encouraged to improve SOC storage, care must be taken within land use decision making to maintain an appropriate balance between carbon sequestration, biodiversity, and general ecosystem functions.
WOS:000799679800001
</snippet>
</document>

<document id="501">
<title>Improving synthetic 3D model-aided indoor image localization via domain adaptation</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.10.005</url>
<snippet>Although the deep learning-based indoor image localization has made significant improvement in terms of accuracy, efficiency, and storage requirement of large indoor scenes, the need for collecting huge labeled training data severely limits its practical application. Recently, the synthetic images rendered from widely available 3D models have shown promising potential to relieve the data collection problem. However, due to the dramatic differences between the synthetic and real images, the localization accuracy of approaches trained on synthetic images is not comparable to the methods trained on real images. In this paper, we propose a domain adaptation-based approach to address this issue. Specifically, the proposed approach mainly contains a model consisting of a multi-level constrained pose regression network and a feature-level discriminator network. The discriminator network forces the pose regression network to generate domain-invariant features from real and synthetic images by adversarial learning and thus reduces the performance gaps. In addition, the multi-level constraints further enhance the localization accuracy of pose regression. We perform extensive experiments on open-source rendering images in different settings. The results show that the proposed method significantly improves the performance. The code for the proposed work is available at https://github.com/lqing900205/ BIM_domainadaptation.
WOS:000719574500008
</snippet>
</document>

<document id="502">
<title>Learning Spatial-Spectral-Temporal EEG Features With Recurrent 3D Convolutional Neural Networks for Cross-Task Mental Workload Assessment</title>
<url>http://dx.doi.org/10.1109/TNSRE.2018.2884641</url>
<snippet>Mental workload assessment is essential for maintaining human health and preventing accidents. Most research on this issue is limited to a single task. However, cross-task assessment is indispensable for extending a pre trained model to new workload conditions. Because brain dynamics are complex across different tasks, it is difficult to propose efficient human-designed features based on prior knowledge. Therefore, this paper proposes a concatenated structure of deep recurrent and 3D convolutional neural networks (R3DCNNs) to learn EEG features across different tasks without prior knowledge. First, this paper adds frequency and time dimensions to EEG topographic maps based on a Morlet wavelet transformation. Then, R3DCNN is proposed to simultaneously learn EEG features from the spatial, spectral, and temporal dimensions. The proposed model is validated based on the EEG signals collected from 20 subjects. This paper employs a binary classification of low and high mental workload across spatial n-back and arithmetic tasks. The results show that the R3DCNN achieves an average accuracy of 88.9&#37;, which is a significant increase compared with that of the state-of-the-art methods. In addition, the visualization of the convolutional layers demonstrates that the deep neural network can extract detailed features. These results indicate that R3DCNN is capable of identifying the mental workload levels for cross-task conditions.
WOS:000456141300004
</snippet>
</document>

<document id="503">
<title>Crop Mapping Using Sentinel Full-Year Dual-Polarized SAR Data and a CPU-Optimized Convolutional Neural Network With Two Sampling Strategies</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3094973</url>
<snippet>Although optical remote sensing can capture the Earths environment with visible and infrared sensors, it is limited by weather conditions. Often, only a few sets of cloud-free optical imagery are available in cloudy regions, where many agricultural towns are located. On the other hand, radar remote sensing can capture imagery under cloudy conditions. In this study, we examined the capability of Sentinel-1 multitemporal dual-polarized synthetic aperture radar (SAR) imagery in a whole year from Google Earth Engine in crop mapping in two study sites in Chongqing, China, and Landivisiau, France. Results show that it is possible to produce better crop classification maps using multitemporal SAR imagery, but the performance is limited by local terrain. Flat agricultural regions, such as Western Europe, are expected to benefit from the multitemporal SAR information. Mountain agricultural regions, such as Southwestern China, will encounter difficulties due to the undulate terrain. We also tested two sampling strategies, i.e., random sampling and regional sampling, and observed high variation in overall accuracy: the former led to a higher accuracy. The gap is caused by the diversity of training sets examined using tSNE visualization. The importance of SAR channels in each month are correlated with their entropy. Data from the growing season are important in distinguishing crop types. The 3-D convolutional neural network (CNN) achieved similar results under a huge computation cost compared with 2-D CNNs. Based on the experiments, we recommend to use a lightweight 2-D CNN that can run on the CPU for real-world crop mapping with SAR data.
WOS:000678338200009
</snippet>
</document>

<document id="504">
<title>Deep Localization of Static Scans in Mobile Mapping Point Clouds</title>
<url>http://dx.doi.org/10.3390/rs13020219</url>
<snippet>Mobile laser scanning (MLS) systems are often used to efficiently acquire reference data covering a large-scale scene. The terrestrial laser scanner (TLS) can easily collect high point density data of local scene. Localization of static TLS scans in mobile mapping point clouds can afford detailed geographic information for many specific tasks especially in autonomous driving and robotics. However, large-scale MLS reference data often have a huge amount of data and many similar scene data; significant differences may exist between MLS and TLS data. To overcome these challenges, this paper presents a novel deep neural network-based localization method in urban environment, divided by place recognition and pose refinement. Firstly, simple, reliable primitives, cylinder-like features were extracted to describe the global features of a local urban scene. Then, a probabilistic framework is applied to estimate a similarity between TLS and MLS data, under a stable decision-making strategy. Based on the results of a place recognition, we design a patch-based convolution neural network (CNN) (point-based CNN is used as kernel) for pose refinement. The input data unit is the batch consisting of several patches. One patch goes through three main blocks: feature extraction block (FEB), the patch correspondence search block and the pose estimation block. Finally, a global refinement was proposed to tune the predicted transformation parameters to realize localization. The research aim is to find the most similar scene of MLS reference data compared with the local TLS scan, and accurately estimate the transformation matrix between them. To evaluate the performance, comprehensive experiments were carried out. The experiments demonstrate that the proposed method has good performance in terms of efficiency, i.e., the runtime of processing a million points is 5 s, robustness, i.e., the success rate of place recognition is 100&#37; in the experiments, accuracy, i.e., the mean rotation and translation error is (0.24 deg, 0.88 m) and (0.03 deg, 0.06 m) on TU Delft campus and Shanghai urban datasets, respectively, and outperformed some commonly used methods (e.g., iterative closest point (ICP), coherent point drift (CPD), random sample consensus (RANSAC)-based method).
WOS:000611550100001
</snippet>
</document>

<document id="505">
<title>Incremental Learning of Hand Symbols Using Event-Based Cameras</title>
<url>http://dx.doi.org/10.1109/JETCAS.2019.2951062</url>
<snippet>Conventional cameras create redundant output especially when the frame rate is high. Dynamic vision sensors (DVSs), on the other hand, generate asynchronous and sparse brightness change events only when an object in the field of view is in motion. Such event-based output can be processed as a 1D time sequence, or it can be converted to 2D frames that resemble conventional camera frames. Frames created, e.g., by accumulating a fixed number of events, can be used as input for conventional deep learning algorithms, thus upgrading existing computer vision pipelines through low-power, low-redundancy sensors. This paper describes a hand symbol recognition system that can quickly be trained to incrementally learn new symbols recorded with an event-based camera, without forgetting previously learned classes. By using the iCaRL incremental learning algorithm, we show that we can learn up to 16 new symbols using only 4000 samples for each symbol and achieving a final symbol accuracy of over 80. The system achieves latency of under 0.5s and training requires 3 minutes for 5 epochs on an NVIDIA 1080TI GPU.
WOS:000502993500010
</snippet>
</document>

<document id="506">
<title>Mineral prospectivity mapping using a joint singularity-based weighting method and long short-term memory network</title>
<url>http://dx.doi.org/10.1016/j.cageo.2021.104974</url>
<snippet>The successful application of geographic information system (GIS)-based mineral prospectivity mapping (MPM) essentially relies on two factors: one is reasonable evidential layers that conform to geological cognition, and the other is excellent models that can extract critical prospecting information from evidential layers. Geological features in MPM are usually discretized by categorizing them into classes and assigning the same or linear weights to each class, which suffer from bias in the interpretation of geological processes under ambiguous knowledge. Moreover, either unsupervised or supervised MPM models are constructed based on the assumption that the variables are relatively independent or identically distributed. In terms of these two issues, this study develops a joint workflow that combines a rational evidence layer weighting method and a deep learning MPM model, considering both the spatial and genetic associations of geological features. A data-driven singularitybased weighting method is first applied to evaluate the relative importance of geological features for mineralization and assign continuous weights to evidential layers using a nonlinear function that is consistent with existing geological models. Then, a more recent deep learning model, namely the long short-term memory network, is employed to extract and integrate the deep-level geological prospecting information among the weighted evidence layers. This joint approach was demonstrated with the help of a case study targeting Fe mineralization in southwestern Fujian Province, China. The mineral potential map obtained using this approach revealed that almost all the known Fe mineral deposits developed in the delineated high prospective regions, indicating that the proposed workflow is reasonable and meaningful for MPM.
WOS:000719327900001
</snippet>
</document>

<document id="507">
<title>Detection of Expanded Reformed Geographical Area in Bi-temporal Multispectral Satellite Images Using Machine Intelligence Neural Network</title>
<url>http://dx.doi.org/10.1007/s12524-021-01476-6</url>
<snippet>Detection of changes using bi-temporal multispectral images is an important processing tool and widely used to predict changes over the earth's surface and for various applications in agricultural and environmental monitoring, natural hazards assessment, urban drafting, and map redrafting. All the traditional methods used to detect changes are not realistic to predict real changes. Therefore, finding actual changes in satellite images is still a challenging task. In the solution, the real changes occurring on the Earth's surface are being explored using artificial intelligence techniques. To complete this challenging task, we recommend a novel machine intelligence learning model based on terrestrial prediction for multispectral satellite image change detection. The main objective of our algorithm is to discover the changes without any additional computations, which has the maximum learning characteristics from bi-temporal satellite images. Machine intelligence learning model has been trained with seven transposed features of training data set and used to predict for bi-temporal image target data set and ultimately obtained good quality difference image. The accuracy of the designed machine intelligence learning model is obtained 99.94&#37; with a Kappa coefficient of 0.9985. The water expanded reformed geographical area near Poyang Lake is calculated with 1749.918628 km(2). Due to the sudden rise of water, there was a lot of damage near Poyang Lake, which can be estimated from the enlarged water area. Such a good accuracy has been obtained from the proposed method, which is much better than other previous methods. This proposed method can be applied to correctly estimate any unexpected damage or change.
WOS:000750277900001
</snippet>
</document>

<document id="508">
<title>High-Resolution Remote Sensing Image Segmentation Framework Based on Attention Mechanism and Adaptive Weighting</title>
<url>http://dx.doi.org/10.3390/ijgi10040241</url>
<snippet>Semantic segmentation has been widely used in the basic task of extracting information from images. Despite this progress, there are still two challenges: (1) it is difficult for a single-size receptive field to acquire sufficiently strong representational features, and (2) the traditional encoder-decoder structure directly integrates the shallow features with the deep features. However, due to the small number of network layers that shallow features pass through, the feature representation ability is weak, and noise information will be introduced to affect the segmentation performance. In this paper, an Adaptive Multi-Scale Module (AMSM) and Adaptive Fuse Module (AFM) are proposed to solve these two problems. AMSM adopts the idea of channel and spatial attention and adaptively fuses three-channel branches by setting branching structures with different void rates, and flexibly generates weights according to the content of the image. AFM uses deep feature maps to filter shallow feature maps and obtains the weight of deep and shallow feature maps to filter noise information in shallow feature maps effectively. Based on these two symmetrical modules, we have carried out extensive experiments. On the ISPRS Vaihingen dataset, the F1-score and Overall Accuracy (OA) reached 86.79&#37; and 88.35&#37;, respectively.
WOS:000643075200001
</snippet>
</document>

<document id="509">
<title>Intra-urban land use maps for a global sample of cities from Sentinel-2 satellite imagery and computer vision</title>
<url>http://dx.doi.org/10.1016/j.compenvurbsys.2022.101917</url>
<snippet>Intra-urban land use maps provide important information to urban planners and policymakers, but these maps are costly, time consuming to generate and are often unavailable in developing countries where most urban growth is now occurring. This paper reports on machine learning methods to automate the production of land use maps from cloud-free mosaics of Sentinel-2 imagery. We have trained a novel neural network architecture to produced 5 meter resolution land use maps for a global stratified sample of 200 cities. The sample includes all world regions, 78 countries, and a range of population sizes. The model architecture is roughly 1 to 2 orders of magnitude smaller than similar architectures such as UNet (Ronneberger, Fischer, &amp; Brox, 2015) and Deep-labV3+ (Chen, Zhu, Papandreou, Schroff, &amp; Adam, 2018), significantly lowering the cost and computational requirements to produce maps. We are in the process of generating land use maps for all 4,000 + cities and metropolitan areas in the world with populations exceeding 100,000. The resulting product will be the first, regularly updated, freely available, global intra-urban land use maps at 5 meter resolution.We present a 4-tier land use taxonomy which at its root distinguishes open-space from built-up area. At the second tier, it subdivides the built-up category into nonresidential and residential areas. The third tier distin-guishes formal from informal residential land use, and the fourth tier further subdivides formal and informal residential land uses into more detailed categories. Accuracy scores at tier-1 and tier-2 were 86&#37; and 79&#37; respectively. Tiers 3 and 4 had an accuracy scores of 75&#37; and 71&#37; respectively. Additionally, we train a roads -only model and compare its output to the Atlas of Urban Expansions Arterial Roads dataset and Open Street Map. As an example use case, we train an Informal Settlement Classifier, correctly classifying 87&#37; of the settlements.
WOS:000900037300003
</snippet>
</document>

<document id="510">
<title>A Lightweight Object Detection Method in Aerial Images Based on Dense Feature Fusion Path Aggregation Network</title>
<url>http://dx.doi.org/10.3390/ijgi11030189</url>
<snippet>In recent years, significant progress has been obtained in object detection using Convolutional Neural Networks (CNNs). However, owing to the particularity of Remote Sensing Images (RSIs), common object detection methods are not well suited for RSIs. Aiming at the difficulties in RSIs, this paper proposes an object detection method based on the Dense Feature Fusion Path Aggregation Network (DFF-PANet). Firstly, for better improving the detection performance of small and medium-sized instances, we propose Feature Reuse Module (FRM), which can integrate semantic and location information contained in feature maps; this module can reuse feature maps in the backbone to enhance the detection capability of small and medium-sized instances. After that, we design the DFF-PANet, which can help feature information extracted from the backbone to be fused more efficiently, and thus cope with the problem of external interference factors. We performed experiments on the Dataset of Object deTection in Aerial images (DOTA) dataset and the HRSC2016 dataset; the accuracy reached 71.5&#37; mAP, which exceeds most object detectors of one-stage and two-stages at present. Meanwhile, the size of our model is only 9.2 M, which satisfies the requirement of being lightweight. The experimental results demonstrate that our method not only has better detection accuracy but also maintains high efficiency in RSIs.
WOS:000775356600001
</snippet>
</document>

<document id="511">
<title>Landslide Detection Mapping Employing CNN, ResNet, and DenseNet in the Three Gorges Reservoir, China</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3117975</url>
<snippet>Landslide detection mapping (LDM) is the basis of the field of landslide disaster prevention; however, it has faced certain difficulties. The Three Gorges Reservoir area of the Yangtze River has been one of the most intensively evaluated areas for landslide prevention in the world, due to the high frequency of landslide disasters here. In this article, we constructed an accurate LDM model based on convolutional neural networks, residual neural networks, and dense convolutional neural networks (DenseNets) that considers "ZY-3" high spatial resolution (HSR) data and conditioning factors (CFs). In this article, 19 factors based on remote sensing (RS) images, topographical and geological data associated with historical landslide locations were randomly divided into training (70&#37; of total) and testing (30&#37;) datasets. The experimental results show that the accuracy (ACC) of these three LDM models is above 0.95, indicating that the deep neural networks aimed at landslide detection performed well. Furthermore, DenseNet with RS images and CFs can accurately detect landslides. Specifically, DenseNet with RS images and CFs outperforms the other five models by considering the evaluation metrics, which exhibited Kappa coefficient improvements of 0.01-0.04 and ACC improvements of 0.02-0.3&#37;. Among all the factors, elevation factor has a high importance of 0.727, which is the most important factors found in this landslide model construction experiment.
WOS:000720519100011
</snippet>
</document>

<document id="512">
<title>Best Representation Branch Model for Remote Sensing Image Scene Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3114404</url>
<snippet>Remote sensing image scene classification is an important method for understanding the high-resolution remote sensing images. Based on convolutional neural network, various classification methods have been applied into this field and achieved remarkable results. These methods mainly rely on the semantic information to improve the classification performance. However, as the network goes deeper, the highly abstract and global semantic information makes it difficult for the network to accurately classify scene images with similar layout and structures, limiting further improvement of classification accuracy. Relying on the semantic information only is not sufficient to effectively classify these similar scene images and the network needs spatial information to enhance the classification capability. To solve this dilemma, this article proposes a best representation branch model, which reaches the optimal balance point where the network can make use of both the semantic information and spatial information to improve the final classification accuracy. In the proposed method, ResNet50 pretrained on the ImageNet dataset is first divided into four branches with different depths to extract feature maps and a capsule network is used as the classifier. The Grad-CAM algorithm is adopted to explain the mechanism of the optimal balance point from the perspective of attention and guide the further feature fusion. In addition, ablation studies are conducted to prove the effectiveness of our method and extensive experiments are conducted on three public benchmark remote sensing datasets. The results demonstrate that the proposed method can achieve competitive classification performance compared to the state-of-the-art methods.
WOS:000704824700009
</snippet>
</document>

<document id="513">
<title>Towards Detecting Building Facades with Graffiti Artwork Based on Street View Images</title>
<url>http://dx.doi.org/10.3390/ijgi9020098</url>
<snippet>As a recognized type of art, graffiti is a cultural asset and an important aspect of a citys aesthetics. As such, graffiti is associated with social and commercial vibrancy and is known to attract tourists. However, positional uncertainty and incompleteness are current issues of open geo-datasets containing graffiti data. In this paper, we present an approach towards detecting building facades with graffiti artwork based on the automatic interpretation of images from Google Street View (GSV). It starts with the identification of geo-tagged photos of graffiti artwork posted on the photo sharing media Flickr. GSV images are then extracted from the surroundings of these photos and interpreted by a customized, i.e., transfer learned, convolutional neural network. The compass heading of the GSV images classified as containing graffiti artwork and the possible positions of their acquisition are considered for scoring building facades according to their potential of containing the artwork observable in the GSV images. More than 36,000 GSV images and 5000 facades from buildings represented in OpenStreetMap were processed and evaluated. Precision and recall rates were computed for different facade score thresholds. False-positive errors are caused mostly by advertisements and scribblings on the building facades as well as by movable objects containing graffiti artwork and obstructing the facades. However, considering higher scores as threshold for detecting facades containing graffiti leads to the perfect precision rate. Our approach can be applied for identifying previously unmapped graffiti artwork and for assisting map contributors interested in the topic. Furthermore, researchers interested on the spatial correlations between graffiti artwork and socio-economic factors can profit from our open-access code and results.
WOS:000522449700038
</snippet>
</document>

<document id="514">
<title>Object Detection in Aerial Images Using a Multiscale Keypoint Detection Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3044733</url>
<snippet>Automatic object detection in aerial imagery is being increasingly adopted in many applications, such as traffic monitoring, smart cities, and disaster assistance. In keypoint-based detectors, the predictionmodules are usually generated froma fixed featuremap scale. This configuration significantly limits the ability to detect multiscale objects in aerial scenes. The corner selection module in these detectors often ignores that a category in an aerial image is relatively unitary. In this article, a novel network, called the multiscale keypoint detection network (MKD-Net), is proposed to address these challenges. MKD-Net fuses multiscale layers to generate multiple feature maps for objects of different sizes. During the inference phase, both feature maps can be exploited for predicting corners. Moreover, a category attention module is designed to reduce the channel noise for a single-category scene. Experiments on benchmarks PASCAL VOC and DOTA show promising performance ofMKD-Net compared with the baseline network. The code is available on https://github.com/jason-su/MKD-NET.
WOS:000659056800001
</snippet>
</document>

<document id="515">
<title>Land Cover Classification from fused DSM and UAV Images Using Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/rs11121461</url>
<snippet>In recent years, remote sensing researchers have investigated the use of different modalities (or combinations of modalities) for classification tasks. Such modalities can be extracted via a diverse range of sensors and images. Currently, there are no (or only a few) studies that have been done to increase the land cover classification accuracy via unmanned aerial vehicle (UAV)-digital surface model (DSM) fused datasets. Therefore, this study looks at improving the accuracy of these datasets by exploiting convolutional neural networks (CNNs). In this work, we focus on the fusion of DSM and UAV images for land use/land cover mapping via classification into seven classes: bare land, buildings, dense vegetation/trees, grassland, paved roads, shadows, and water bodies. Specifically, we investigated the effectiveness of the two datasets with the aim of inspecting whether the fused DSM yields remarkable outcomes for land cover classification. The datasets were: (i) only orthomosaic image data (Red, Green and Blue channel data), and (ii) a fusion of the orthomosaic image and DSM data, where the final classification was performed using a CNN. CNN, as a classification method, is promising due to hierarchical learning structure, regulating and weight sharing with respect to training data, generalization, optimization and parameters reduction, automatic feature extraction and robust discrimination ability with high performance. The experimental results show that a CNN trained on the fused dataset obtains better results with Kappa index of 0.98, an average accuracy of 0.97 and final overall accuracy of 0.98. Comparing accuracies between the CNN with DSM result and the CNN without DSM result for the overall accuracy, average accuracy and Kappa index revealed an improvement of 1.2&#37;, 1.8&#37; and 1.5&#37;, respectively. Accordingly, adding the heights of features such as buildings and trees improved the differentiation between vegetation specifically where plants were dense.
WOS:000473794600067
</snippet>
</document>

<document id="516">
<title>Automatic Mapping of Thermokarst Landforms from Remote Sensing Images Using Deep Learning: A Case Study in the Northeastern Tibetan Plateau</title>
<url>http://dx.doi.org/10.3390/rs10122067</url>
<snippet>Thawing of ice-rich permafrost causes thermokarst landforms on the ground surface. Obtaining the distribution of thermokarst landforms is a prerequisite for understanding permafrost degradation and carbon exchange at local and regional scales. However, because of their diverse types and characteristics, it is challenging to map thermokarst landforms from remote sensing images. We conducted a case study towards automatically mapping a type of thermokarst landforms (i.e., thermo-erosion gullies) in a local area in the northeastern Tibetan Plateau from high-resolution images by the use of deep learning. In particular, we applied the DeepLab algorithm (based on Convolutional Neural Networks) to a 0.15-m-resolution Digital Orthophoto Map (created using aerial photographs taken by an Unmanned Aerial Vehicle). Here, we document the detailed processing flow with key steps including preparing training data, fine-tuning, inference, and post-processing. Validating against the field measurements and manual digitizing results, we obtained an F1 score of 0.74 (precision is 0.59 and recall is 1.0), showing that the proposed method can effectively map small and irregular thermokarst landforms. It is potentially viable to apply the designed method to mapping diverse thermokarst landforms in a larger area where high-resolution images and training data are available.
WOS:000455637600209
</snippet>
</document>

<document id="517">
<title>Exploring semantic elements for urban scene recognition: Deep integration of high-resolution imagery and OpenStreetMap (OSM)</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.03.019</url>
<snippet>Urban scenes refer to city blocks which are basic units of megacities, they play an important role in citizens welfare and city management. Remote sensing imagery with largescale coverage and accurate target descriptions, has been regarded as an ideal solution for monitoring the urban environment. However, due to the heterogeneity of remote sensing images, it is difficult to access their geographical content at the object level, let alone understanding urban scenes at the block level. Recently, deep learning-based strategies have been applied to interpret urban scenes with remarkable accuracies. However, the deep neural networks require a substantial number of training samples which are hard to satisfy, especially for high-resolution images. Meanwhile, the crowed-sourced Open Street Map (OSM) data provides rich annotation information about the urban targets but may encounter the problem of insufficient sampling (limited by the places where people can go). As a result, the combination of OSM and remote sensing images for efficient urban scene recognition is urgently needed. In this paper, we present a novel strategy to transfer existing OSM data to high-resolution images for semantic element determination and urban scene understanding. To be specific, the object-based convolutional neural network (OCNN) can be utilized for geographical object detection by feeding it rich semantic elements derived from OSM data. Then, geographical objects are further delineated into their functional labels by integrating points of interest (POIs), which contain rich semantic terms, such as commercial or educational labels. Lastly, the categories of urban scenes are easily acquired from the semantic objects inside. Experimental results indicate that the proposed method has an ability to classify complex urban scenes. The classification accuracies of the Beijing dataset are as high as 91&#37; at the object-level and 88&#37; at the scene level. Additionally, we are probably the first to investigate the object level semantic mapping by incorporating high resolution images and OSM data of urban areas. Consequently, the method presented is effective in delineating urban scenes that could further boost urban environment monitoring and planning with high-resolution images.
WOS:000469306300017
</snippet>
</document>

<document id="518">
<title>A Super-Resolution Convolutional-Neural-Network-Based Approach for Subpixel Mapping of Hyperspectral Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2941089</url>
<snippet>A new subpixel mapping (SPM) method based on a super-resolution convolutional neural network (SRCNN) is proposed to generate subpixel land cover maps for hyperspectral images. The SRCNN is used to restore the image spatial resolution from a coarse input image, which is equivalent to interpolation. First, an efficient subpixel convolutional neural network, which is a state-of-the-art SRCNN, is utilized to calculate the subpixel soft class value via a transfer learning strategy. Then, a classifier is used to transform the subpixel soft class values to hard-classified land cover maps with the constraint of fraction images. Experiments on three different hyperspectral images demonstrate that the SPM accuracy of the proposed SRCNN-based method is significantly better than those of three traditional SPM methods. In addition, the SRCNN-based SPM method has a simplified calculation process, does not require training data, and is less time consuming. This article provides a new solution for SPM of hyperspectral images.
WOS:000515698700020
</snippet>
</document>

<document id="519">
<title>BSSNet: Building Subclass Segmentation From Satellite Images Using Boundary Guidance and Contrastive Learning</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3202524</url>
<snippet>Building subclass segmentation, aimed at predicting classes of buildings (high-rise zone, low-rise zone, single high-rise, and single low-rise) from satellite images, is beneficial in numerous applications, including human geography, urban planning, and humanitarian aid. However, problems, such as complex scenes and similar characteristics of different building categories make it difficult for general models to balance the accuracy of localization and classification in building subclass segmentation. Therefore, this article proposes a novel network for building subclass segmentation called building subclass segmentation network (BSSNet), which uses two subnetworks to divide and conquer the problem. The first network guides the building locations through binary building segmentation, called localization network. The spatial gradient fusion module in the localization network improves the binary segmentation result by supervising the spatial gradient map of prediction. The second network is a classification network, which predicts building subclasses. Intermediate features of the second network are optimized by contrastive learning loss to improve feature consistency. Finally, predictions of the two networks are combined to obtain the final result. The experimental results demonstrate that our BSSNet can perform significant improvements on the Hainan dataset we produced and the xBD dataset. In particular, the BSSNet achieves the best performance compared to current methods on the Hainan dataset.
WOS:000854637400011
</snippet>
</document>

<document id="520">
<title>Spatial modelling of soil salinity: deep or shallow learning models?</title>
<url>http://dx.doi.org/10.1007/s11356-021-13503-7</url>
<snippet>Understanding the spatial distribution of soil salinity is required to conserve land against degradation and desertification. Against this background, this study is the first attempt to predict soil salinity in the Jaghin basin, in southern Iran, by applying and comparing the performance of four deep learning (DL) models (deep convolutional neural networks-DCNNs, dense connected deep neural networks-DenseDNNs, recurrent neural networks-long short-term memory-RNN-LSTM and recurrent neural networks-gated recurrent unit-RNN-GRU) and six shallow machine learning (ML) models (bagged classification and regression tree-BCART, cforest, cubist, quantile regression with LASSO penalty-QR-LASSO, ridge regression-RR and support vectore machine-SVM). To do this, 49 environmental landsat8-derived variables including digital elevation model (DEM)-extracted covariates, soil-salinity indices, and other variables (e.g., soil order, lithology, land use) were mapped spatially. For assessing the relationships between soil salinity (EC) and factors controlling EC, we collected 319 surficial (0-5 cm depth) soil samples for measuring soil salinity on the basis of electrical conductivity (EC). We then selected the most important features (covariates) controlling soil salinity by applying a MARS model. The performance of the DL and shallow ML models for generating soil salinity spatial maps (SSSMs) was assessed using a Taylor diagram and the Nash Sutcliff coefficient (NSE). Among all 10 predictive models, DL models with NSE &gt;= 0.9 (DCNNs was the most accurate model with NSE = 0.96) were selected as the four best models, and performed better than the six shallow ML models with NSE &lt;= 0.83 (QR-LASSO was the weakest predictive model with NSE = 0.50). Based on DCNNs-, the values of the EC ranged between 0.67 and 14.73 dS/m, whereas for QR-LASSO the corresponding EC values were 0.37 to 19.6 dS/m. Overall, DL models performed better than shallow ML models for production of the SSSMs and therefore we recommend applying DL models for prediction purposes in environmental sciences.
WOS:000631811800004
</snippet>
</document>

<document id="521">
<title>Effectiveness assessment of Keras based deep learning with different robust optimization algorithms for shallow landslide susceptibility mapping at tropical area</title>
<url>http://dx.doi.org/10.1016/j.catena.2020.104458</url>
<snippet>This research aims at investigating the capability of Kerass deep learning models with three robust optimization algorithms (stochastic gradient descent, root mean square propagation, and adaptive moment optimization) and two-loss functions for spatial modeling of landslide hazard at a regional scale. Shallow landslides at the Ha Long area (Vietnam) were selected as a case study. For this regard, set of ten influencing factors (slope, aspect, curvature, topographic wetness index, landuse, distance to road, distance to river, soil type, distance to fault, and lithology) and 193 landslide polygons were prepared to construct a Geographic Information System (GIS) database for the study area. Using the collected database, the DNN with its potential of realizing complex functional mapping hidden in the data is used to generalize a decision boundary that separates the learning space into two distinct categories: landslide (a positive class) and non-landslide (a negative class). Experimental results point out that the utilized the Kerass deep learning model with the Adam optimization and the mean squared error lost function is the best with the prediction performance of 84.0&#37;. The performance is better than those of the employed benchmark approaches of random forest, J48 decision tree, classification tree, and logistic model tree. We conclude that the Kerass deep learning model is a new tool for shallow susceptibility mapping at landslide-prone areas.
WOS:000518488500035
</snippet>
</document>

<document id="522">
<title>POI Detection of High-Rise Buildings Using Remote Sensing Images: A Semantic Segmentation Method Based on Multitask Attention Res-U-Net</title>
<url>http://dx.doi.org/10.1109/TGRS.2022.3174399</url>
<snippet>A point-of-interest (POI) represents a specific point location that may be useful or interesting for people, and therefore, each and every building footprint in a topographic map can be recognized as a POI. Automatic extraction of building footprints using remote sensing images has become a challenging and important research topic, which is in demand for urban planning and development. Extensive studies have explored a variety of semantic segmentation methods using deep learning algorithms to achieve better performance in building footprint extraction; however, the existing algorithms were shown to have some limitations, which lead to poor segmentation results. Building roofs were recognized as building footprints in the previous studies. This is prone to error especially for high-rise buildings due to different sensor view angles. In this article, we propose a multitask Res-U-Net model with an attention mechanism for the extraction of the building roofs and the whole building shapes from remote sensing images and then use an offset vector method to detect the footprints of the high-rise buildings based on the boundaries of the corresponding building roofs and shapes. We also apply the online food delivery (OFD) data to parse the POI name of every building footprint. Several strategies are also developed in combination with the proposed model, including data augmentation and postprocessing. We conduct numerical experiments using real data of remote sensing images and OFD historical order data. Results demonstrate that our proposed model achieves a total F1-score of 77.05&#37; and an intersection over union (IoU) of 63.55&#37; in terms of the building roof segmentation, and an overall F1-score of 79.02&#37; and an IoU of 66.05&#37; for the whole building shape segmentation, which both achieve the best performance among all baseline models.
WOS:000804647900005
</snippet>
</document>

<document id="523">
<title>Deep Convolutional Neural Network Framework for Subpixel Mapping</title>
<url>http://dx.doi.org/10.1109/TGRS.2020.3032475</url>
<snippet>Subpixel mapping (SPM) is an effective way to solve the mixed pixel problem, which is a ubiquitous phenomenon in remotely sensed imagery, by characterizing subpixel distribution within the mixed pixels. In fact, the majority of the classical and state-of-the-art SPM algorithms can be viewed as a convolution process, but these methods rely heavily on fixed and handcrafted kernels that are insufficient in characterizing a geographically realistic distribution image. In addition, the traditional SPM approach is based on the prerequisite of abundance images derived from spectral unmixing (SU), during which process uncertainty inherently exists and is propagated to the SPM. In this article, a kernel-learnable convolutional neural network (CNN) framework for subpixel mapping (SPMCNN-F) is proposed. In SPMCNN-F, the kernel is learnable during the training stage based on the given training sample pairs of low- and high-resolution patches for learning a geographically realistic prior, instead of fixed priors. The end-to-end mapping structure enables direct subpixel information extraction from the original coarse image, avoiding the uncertainty propagation from the SU. In the experiments undertaken in this study, two state-of-the-art super-resolution networks were selected as application demonstrations of the proposed SPMCNN-F method. In experiment part, three hyperspectral image data sets were adopted, two in a synthetic coarse image approach and one in a real coarse image approach, for the validation. Additionally, a new data set with pairs of Moderate-resolution Imaging Spectroradiometer (MODIS) and Landsat images were adopted in a real coarse image approach, for further validation of SPMCNN-F in large-scale area. The restored fine distribution images obtained in all the experiments showed a perceptually better reconstruction quality, both qualitatively and quantitatively, confirming the superiority of the proposed SPM framework.
WOS:000711850900046
</snippet>
</document>

<document id="524">
<title>An ANN Based Optimization Algorithm for Diffracted Laser Beam Shaping</title>
<url>http://dx.doi.org/10.2112/JCR-SI104-046.1</url>
<snippet>To overcome the lack of flexibility in laser beam shaping in current industrial applications, a new improved artificial neural network algorithm for diffracted laser beam shaping is proposed. Aiming at the existing problems in laser beam shaping, the Unet neural network algorithm is improved from the label image and convolution operation. By clarifying its training and application steps, the improved neural network algorithm is pre-trained firstly and then formally trained (full training). The result shows that the UNet neural network algorithm can gradually realize the laser beam shaping with the spatial light modulator and find the mapping relationship between the input image (phase diagram) and the output image (laser contour diagram).
WOS:000584510900046
</snippet>
</document>

<document id="525">
<title>Geographic Semantic Network for Cross-View Image Geo-Localization</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3121337</url>
<snippet>The task of cross-view image geo-localization aims to determine the geo-location (Global Positioning System (GPS) coordinates) of a query ground-view image by matching the image with GPS-tagged aerial (or satellite) images in the reference dataset. Due to the dramatic domain gap between the ground and aerial images, the problem is challenging. The existing approaches mainly adopt convolutional neural networks (CNNs) to learn discriminative features. However, these CNN-based methods mainly leverage appearance and semantic information but fail to jointly model the appearance, positional, and orientation properties of scene objects, which belong to the spatial hierarchy. Since spatial hierarchy information is crucial for cross-view feature correspondence, in this article, we propose an end-to-end network architecture, dubbed GeoNet. GeoNet consists of a ResNetX module and a GeoCaps module. On the one hand, the ResNetX module is developed to learn powerful intermediate feature maps and allows the stable propagation of gradients in deep CNNs. On the other hand, the GeoCaps module utilizes the capsule network to encapsulate the intermediate feature maps into several capsules, whose length and orientation represent the existence probability and spatial hierarchy information of scene objects, respectively. Moreover, by using a dynamic routing-by-agreement mechanism, the GeoCaps module is capable of modeling parts-to-whole relationships between scene objects, which is viewpoint invariant and capable of bridging the cross-view domain gap. In addition to GeoNet, we introduce a simple yet effective metric learning method, based on which two weighted soft margin loss functions with online batch hard sample mining are devised. These functions not only speed up convergence but also improve the generalization ability of the network. Extensive experiments on three well-known datasets demonstrate that our GeoNet not only achieves state-of-the-art results for the ground-to-aerial and aerial-to-ground geo-localization tasks but also outperforms competing approaches for the few-shot geo-localization task.
WOS:000754264200019
</snippet>
</document>

<document id="526">
<title>Discriminative Sketch Topic Model With Structural Constraint for SAR Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3024002</url>
<snippet>Synthetic aperture radar (SAR) image classification is an important part in the understanding and interpretation of SAR images. Each patch in SAR images has a scene category, but usually contains multiple land-cover classes or latent properties, which can be represented by topics in the probabilistic topic model (PTM). The representation and selection of discriminative features in PTM have a large impact on the classification results. Most of the existing feature learning methods do not make full use of high-level structure feature and the feature correlation within similar images to mine discriminative features. Therefore, this article proposes a discriminative sketch topic model with structural constraint (C-SSTM) for SAR image classification. In the proposed model, each image patch is characterized by structural and texture features. In particular, the sketch structural feature is based on the sketch map to represent the image local structure pattern. Then, the local image manifold information is preserved in terms of structure and texture. In the structural constraint, the texture and structure of each image patch are combined to learn discriminative latent semantic topics between image patches. Finally, each image patch is quantified by discriminative latent semantic topics instead of low-level representation. The experimental results tested on synthetic and real SAR images demonstrate that the proposed C-SSTM is able to learn effective structural feature representation from SAR images. Compared with other related approaches, C-SSTM produces competitive classification accuracies with high time efficiency.
WOS:000576263500012
</snippet>
</document>

<document id="527">
<title>Deep Learning Approach To Update Road Network using VGI Data</title>
<url>http://dx.doi.org/</url>
<snippet>In our earlier work, we worked on extraction of the total width of road by agents traversing in the direction guided by Volunteered Geographic Information (VGI). The only downfall of VGI approach is its inability to update the new road developments. In this paper, we introduce deep learning approach to update the road network. We make use of the output of our previous work which forms as an input to train the Convolutional Neural Network (CNN). Then, further post processing is performed to remove non- road segments (such as buildings, vegetation, etc) on the output of CNN and finally, obtain the updated road map.
WOS:000468528200016
</snippet>
</document>

<document id="528">
<title>DoMars16k: A Diverse Dataset for Weakly Supervised Geomorphologic Analysis on Mars</title>
<url>http://dx.doi.org/10.3390/rs12233981</url>
<snippet>Mapping planetary surfaces is an intricate task that forms the basis for many geologic, geomorphologic, and geographic studies of planetary bodies. In this work, we present a method to automate a specific type of planetary mapping, geomorphic mapping, taking machine learning as a basis. Additionally, we introduce a novel dataset, termed DoMars16k, which contains 16,150 samples of fifteen different landforms commonly found on the Martian surface. We use a convolutional neural network to establish a relation between Mars Reconnaissance Orbiter Context Camera images and the landforms of the dataset. Afterwards, we employ a sliding-window approach in conjunction with a Markov Random field smoothing to create maps in a weakly supervised fashion. Finally, we provide encouraging results and carry out automated geomorphological analyses of Jezero crater, the Mars2020 landing site, and Oxia Planum, the prospective ExoMars landing site.
WOS:000597556200001
</snippet>
</document>

<document id="529">
<title>A Frustum-based probabilistic framework for 3D object detection by fusion of LiDAR and camera data</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.10.015</url>
<snippet>This paper presents a real-time 3D object detector based on LiDAR based Simultaneous Localization and Mapping (LiDAR-SLAM). The 3D point clouds acquired by mobile LiDAR systems, within the environment of buildings, are usually highly sparse, irregularly distributed, and often contain occlusion and structural ambiguity. Existing 3D object detection methods based on Convolutional Neural Networks (CNNs) rely heavily on both the stability of the 3D features and a large amount of labelling. A key challenge is efficient detection of 3D objects in point clouds of large-scale building environments without pre-training the 3D CNN model. To project image-based object detection results and LiDAR-SLAM results onto a 3D probability map, we combine visual and range information into a frustum-based probabilistic framework. As such, we solve the sparse and noise problem in LiDAR-SLAM data, in which any point cloud descriptor can hardly be applied. The 3D object detection results, obtained using both backpack LiDAR dataset and the well-known KITTI Vision Benchmark Suite, show that our method outperforms the state-of-the-art methods for object localization and bounding box estimation.
WOS:000508739800008
</snippet>
</document>

<document id="530">
<title>Anchor-Free Arbitrary-Oriented Object Detector Using Box Boundary-Aware Vectors</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3158905</url>
<snippet>Characterized by complicated backgrounds, various types, large size variations, and arbitrary orientations, the detection and recognition of arbitrary-oriented objects in remote sensing images are challenging. To address the aforementioned problem, an anchor-free arbitrary-oriented object detector using box boundary-aware vectors is proposed. With the idea of CenterNet to detect objects as points, oriented object detection is achieved by predicting the center, the box boundary-aware vectors, the size, and the type of the bounding box. In the feature extraction stage of the designed architecture, Res2Net, a multiscale convolutional neural network, is used to extract feature maps of different scales and adaptively spatial feature fusion is adopted to improve the detector's adaptability to objects of different sizes. In the detector, a context enhancement module with a multibranch network is designed to enhance the contextual information of the objects and improve the detector's robustness to the complicated backgrounds. Experiments are carried on three challenging benchmarks (i.e., HRSC2016, UCAS-AOD, and DOTA) and our method achieves state-of-the-art performance with 90.30&#37;, 89.70&#37;, and 77.18&#37; mAP, respectively.
WOS:000778928500004
</snippet>
</document>

<document id="531">
<title>Automated crater detection on Mars using deep learning</title>
<url>http://dx.doi.org/10.1016/j.pss.2019.03.008</url>
<snippet>Impact crater cataloging is an important tool in the study of the geological history of planetary bodies in the Solar System, including dating of surface features and geologic mapping of surface processes. Catalogs of impact craters have been created by a diverse set of methods over many decades, including using visible or near infra-red imagery and digital terrain models. I present an automated system for crater detection and cataloging using a digital terrain model (DTM) of Mars - In the algorithm craters are first identified as rings or disks on samples of the DTM image using a convolutional neural network with a UNET architecture, and the location and size of the features are determined using a circle matching algorithm. I describe the crater detection algorithm (CDA) and compare its performance relative to an existing crater dataset. I further examine craters missed by the CDA as well as potential new craters found by the algorithm. I show that the CDA can find three-quarters of the resolvable craters in the Mars DTMs, with a median difference of 5-10&#37; in crater diameter compared to an existing database. A version of this CDA has been used to process DTM data from the Moon and Mercury (Silburt et al., 2019). The source code for the complete CDA is available at https://github.com/silburt/DeepMoon, and Martian crater datasets generated using this CDA are available at https://doi.org/10.5683/SP2/MDKPC8.
WOS:000467510500002
</snippet>
</document>

<document id="532">
<title>Prediction of Molecular Properties Using Molecular Topographic Map</title>
<url>http://dx.doi.org/10.3390/molecules26154475</url>
<snippet>Prediction of molecular properties plays a critical role towards rational drug design. In this study, the Molecular Topographic Map (MTM) is proposed, which is a two-dimensional (2D) map that can be used to represent a molecule. An MTM is generated from the atomic features set of a molecule using generative topographic mapping and is then used as input data for analyzing structure-property/activity relationships. In the visualization and classification of 20 amino acids, differences of the amino acids can be visually confirmed from and revealed by hierarchical clustering with a similarity matrix of their MTMs. The prediction of molecular properties was performed on the basis of convolutional neural networks using MTMs as input data. The performance of the predictive models using MTM was found to be equal to or better than that using Morgan fingerprint or MACCS keys. Furthermore, data augmentation of MTMs using mixup has improved the prediction performance. Since molecules converted to MTMs can be treated like 2D images, they can be easily used with existing neural networks for image recognition and related technologies. MTM can be effectively utilized to predict molecular properties of small molecules to aid drug discovery research.
WOS:000682160700001
</snippet>
</document>

<document id="533">
<title>EO plus Morphometrics: Understanding cities through urban morphology at large scale</title>
<url>http://dx.doi.org/10.1016/j.landurbplan.2023.104691</url>
<snippet>Earth Observation (EO)-based mapping of cities has great potential to detect patterns beyond the physical ones. However, EO combined with the surge of machine learning techniques to map non-physical, such as socioeco-nomic, aspects directly, goes to the expense of reproducibility and interpretability, hence scientific validity. In this paper, we suggest shifting the focus from the direct detection of socioeconomic status from raw images through image features, to the mapping of interpretable urban morphology of basic urban elements as an in-termediate step, to which socioeconomic patterns can then be related. This shift is profound, in that, rather than abstract image features, it allows to capture the morphology of real urban objects, such as buildings and streets, and use this to then interpret other patterns, including socioeconomic ones. Because socioeconomic patterns are not derived from raw image data, the mapping of these patterns is less data demanding and more replicable. Specifically, we propose a 2-step approach: (1) extraction of fundamental urban elements from satellite imagery, and (2) derivation of meaningful urban morphological patterns from the extracted elements. We refer to this 2 -step approach as "EO + Morphometrics". Technically, EO consists of applying deep learning through a reengi-neered U-Net shaped convolutional neural network to publicly accessible Google Earth imagery for building extraction. Methods of urban morphometrics are then applied to these buildings to compute semantically explicit and interpretable metrics of urban form. Finally, clustering is applied to these metrics to obtain morphological patterns, or urban types. The "EO + Morphometrics" approach is applied to the city of Nairobi, Kenya, where 15 different urban types are identified. To test whether this outcome meaningfully describes current urbanization patterns, we verified whether selected types matched locally designated informal settlements. We observe that four urban types, characterized by compact and organic urban form, were recurrent in such settlements. The proposed "EO + Morphometrics" approach paves the way for the large-scale identification of interpretable urban form patterns and study of associated dynamics across any region in the world.
WOS:000925678400001
</snippet>
</document>

<document id="534">
<title>AFSNet: attention-guided full-scale feature aggregation network for high-resolution remote sensing image change detection</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2142626</url>
<snippet>Using change detection technique precisely analyzes remote sensing images, it has a broad range of applications in resource surveys, surveillance systems, and map updating. In recent years, deep learning-based methods have become a focus area owing to their excellent feature extraction and representation ability. The fusion of multi-scale features is the key to improving change detection performance in fully convolutional network-based structural methods, primarily based on an architecture with skip connections or nested and dense skip connections. However, these methods only fuse features on the same scale and lack sufficient information from multiple scales to generate appropriate results. Traditional feature fusion with redundant and unsupervised information leads to poor model fitting. To solve these problems, we proposed a novel attention-guided full-scale feature aggregation network (AFSNet). The proposed method used a Siamese structure as the backbone network to extract features, which were then aggregated using full-scale skip connections, and an attention mechanism to avoid feature redundancy. Finally, to obtain a highly accurate final change map, a multiple side-outputs fusion strategy was used to fuse the change maps at different scales. To check the reliability of AFSNet, we tested it on two public datasets, the LEVIR-CD and SVCD datasets. The F1-Score/IoU scores improved by 0.57&#37;/0.95&#37; and 1.14&#37;/2.07&#37; in the two datasets, respectively, compared to those obtained using the methods that achieved suboptimal values. The results showed that AFSNet outperforms other mainstream methods while maintaining a good balance between computational costs and model parameters.
WOS:000879589200001
</snippet>
</document>

<document id="535">
<title>Construction of Human Behavior Cognitive Map for Robots</title>
<url>http://dx.doi.org/10.3390/app9235026</url>
<snippet>With the advancement of robotics, the importance of service robots in society is increasing. It is crucial for service robots to understand their environment so that they can offer suitable responses to humans. To realize the use of space, robots primarily use an environment model. This paper is focused on the development of an environment model based on human behaviors. In this model, a new neural network structure called dynamic highway networks is applied to recognize humans behaviors. In addition, a two-dimensional pose estimator, Laban movement analysis, and the fuzzy integral are employed. With these methods, two new behavior-recognition algorithms are developed, and a method to record the relationship between behavior and environment is proposed. Based on the proposed environmental model, robots can identify abnormal behavior, provide an appropriate response and guide a person toward the desired normal behavior by identifying abnormal behavior. Simulations and experiments justify the proposed method with satisfactory results.
WOS:000509476600048
</snippet>
</document>

<document id="536">
<title>Small Manhole Cover Detection in Remote Sensing Imagery with Deep Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/ijgi8010049</url>
<snippet>With the development of remote sensing technology and the advent of high-resolution images, obtaining data has become increasingly convenient. However, the acquisition of small manhole cover information still has shortcomings including low efficiency of manual surveying and high leakage rate. Recently, deep learning models, especially deep convolutional neural networks (DCNNs), have proven to be effective at object detection. However, several challenges limit the applications of DCNN in manhole cover object detection using remote sensing imagery: (1) Manhole cover objects often appear at different scales in remotely sensed images and DCNNs' fixed receptive field cannot match the scale variability of such objects; (2) Manhole cover objects in large-scale remotely-sensed images are relatively small in size and densely packed, while DCNNs have poor localization performance when applied to such objects. To address these problems, we propose an effective method for detecting manhole cover objects in remotely-sensed images. First, we redesign the feature extractor by adopting the visual geometry group (VGG), which can increase the variety of receptive field size. Then, detection is performed using two sub-networks: a multi-scale output network (MON) for manhole cover object-like edge generation from several intermediate layers whose receptive fields match different object scales and a multi-level convolution matching network (M-CMN) for object detection based on fused feature maps, which combines several feature maps that enable small and densely packed manhole cover objects to produce a stronger response. The results show that our method is more accurate than existing methods at detecting manhole covers in remotely-sensed images.
WOS:000458582700048
</snippet>
</document>

<document id="537">
<title>A coarse-to-fine weakly supervised learning method for green plastic cover segmentation using high-resolution remote sensing images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.04.012</url>
<snippet>Green plastic cover (GPC) is a kind of green plastic fine mesh primarily used for covering construction sites and mitigating large amounts of dust during construction. Accurate GPC detection is vital for monitoring urban environment and understanding urban development. Convolutional neural network (CNN)-based segmentation methods are widely used for detecting object extents, while they rely on high-quality pixel-level labels with high acquisition cost. In this regard, weakly supervised learning can achieve pixel-level segmentation using only image-level labels, by first generating the class activation map (CAM) to obtain initial pixel-level labels and then applying the CNN-based segmentation methods to detect object extents. However, these initial labels are usually incomplete and noisy, caused by the local high response property of CAM. Moreover, the CNN-based segmentation methods often lead to blurry object boundaries due to the gradual down-sampling of feature maps, and meanwhile suffer from the class imbalance problem in real scenarios. Given these problems, we introduce weakly supervised learning into GPC detection to lower the label acquisition cost. Furthermore, to improve the completeness and correctness of initial labels and mitigate the blurry boundary problem, we propose a coarse-tofine weakly supervised segmentation method (called CFWS), consisting of three steps: 1) object-based label extraction; 2) noisy label correction; and 3) boundary-aware semantic segmentation. Moreover, to alleviate the class imbalance problem, we propose a classification-then-segmentation strategy and integrate it into the CFWS to detect GPC. We test the CFWS on two datasets from Google Earth and Gaofen-2 high-resolution images, respectively. The results show that the CFWS obtains more complete GPCs and effectively retains boundaries on both datasets compared to existing state-of-the-art methods. In real scenarios, the classification-thensegmentation strategy significantly reduces a large number of false alarms generated by direct segmentation. These findings confirm that the CFWS holds great potentials for large-scale GPC detection and urban environmental monitoring. The source code will be available at https://github.com/lauraset/Coarse-to-fine-weaklysupervised-GPC-segmentation.
WOS:000796651200001
</snippet>
</document>

<document id="538">
<title>Forest road extraction from orthophoto images by convolutional neural networks</title>
<url>http://dx.doi.org/10.1080/10106049.2022.2060319</url>
<snippet>Continuous monitoring of the forest road infrastructure and keeping track of the changes occurred are important for forestry practices, map updating, forest fire and forest transport decision support systems. In this context, the most of up to date data can be obtained by automatic forest road extraction from satellite images via machine learning (ML). Acquiring sufficient data is one of the most important factors which affect the success of ML and deep learning (DL). DL architectures yield more consistent results for complex data sets compared with ML algorithms. In the present study, three different deep learning (Resnet-18, MobileNet-V2 and Xception) architectures with semantic segmentation architecture were compared for extracting the forest road network from high-resolution orthophoto images and the results were analyzed. The architectures were evaluated through a multiclass statistical analysis based precision, recall, F1 score, intersection over union and overall accuracy (OA). The results present significant values obtained by the Resnet-18 architecture, with 99.72&#37; of OA and 98.87&#37; of precision and by the MobileNet-V2 architecture, with 97.76&#37; of OA and 98.28&#37; of precision. Also the results show that Resnet-18, MobileNet-V2 semantic segmentation architectures can be used efficiently for forest road extraction.
WOS:000797461600001
</snippet>
</document>

<document id="539">
<title>Application of Artificial Neural Networks for Natural Gas Consumption Forecasting</title>
<url>http://dx.doi.org/10.3390/su12166409</url>
<snippet>The present research study explores three types of neural network approaches for forecasting natural gas consumption in fifteen cities throughout Greece; a simple perceptron artificial neural network (ANN), a state-of-the-art Long Short-Term Memory (LSTM), and the proposed deep neural network (DNN). In this research paper, a DNN implementation is proposed where variables related to social aspects are introduced as inputs. These qualitative factors along with a deeper, more complex architecture are utilized for improving the forecasting ability of the proposed approach. A comparative analysis is conducted between the proposed DNN, the simple ANN, and the advantageous LSTM, with the results offering a deeper understanding the characteristics of Greek cities and the habitual patterns of their residents. The proposed implementation shows efficacy on forecasting daily values of energy consumption for up to four years. For the evaluation of the proposed approach, a real-life dataset for natural gas prediction was used. A detailed discussion is provided on the performance of the implemented approaches, the ANN and the LSTM, that are characterized as particularly accurate and effective in the literature, and the proposed DNN with the inclusion of the qualitative variables that govern human behavior, which outperforms them.
WOS:000578990400001
</snippet>
</document>

<document id="540">
<title>Vehicle Detection in Very-High-Resolution Remote Sensing Images Based on an Anchor-Free Detection Model with a More Precise Foveal Area</title>
<url>http://dx.doi.org/10.3390/ijgi10080549</url>
<snippet>Vehicle detection in aerial images is a challenging task. The complexity of the background information and the redundancy of the detection area are the main obstacles that limit the successful operation of vehicle detection based on anchors in very-high-resolution (VHR) remote sensing images. In this paper, an anchor-free target detection method is proposed to solve the problems above. First, a multi-attention feature pyramid network (MA-FPN) was designed to address the influence of noise and background information on vehicle target detection by fusing attention information in the feature pyramid network (FPN) structure. Second, a more precise foveal area (MPFA) is proposed to provide better ground truth for the anchor-free method by determining a more accurate positive sample selection area. The proposed anchor-free model with MA-FPN and MPFA can predict vehicles accurately and quickly in VHR remote sensing images through direct regression and predict the pixels in the feature map. A detailed evaluation based on remote sensing image (RSI) and vehicle detection in aerial imagery (VEDAI) data sets for vehicle detection shows that our detection method performs well, the network is simple, and the detection is fast.
WOS:000689133000001
</snippet>
</document>

<document id="541">
<title>Urban climate zone classification using convolutional neural network and ground-level images</title>
<url>http://dx.doi.org/10.1177/0309133319837711</url>
<snippet>Urban climate risks have a wide range of impacts on the health of more than 50&#37; of the worlds population, which is a critical issue relating to climate change. To support urban climate study and categorise different urban environments and their atmospheric impacts in a consistent way, the Local Climate Zone (LCZ) classification scheme has been developed. The World Urban Database and Access Portal Tools project aims to map the LCZ of cities across the globe. However, previous classification approaches based on satellite images have limitations regarding the characterisation of three-dimensional features such as building heights. This study aims to apply convolutional neural networks to classify LCZ types based on ground-level images, which can provide more detail of the urban environments. Validation results have shown an overall accuracy of 69.6&#37;. The new method outperformed previous satellite-based studies for classifying the LCZ types Compact Mid-rise, Sparsely Built, Heavy Industry, and Bare Rock or Paved.
WOS:000469873500008
</snippet>
</document>

<document id="542">
<title>Activity landscape image analysis using convolutional neural networks</title>
<url>http://dx.doi.org/10.1186/s13321-020-00436-5</url>
<snippet>Activity landscapes (ALs) are graphical representations that combine compound similarity and activity data. ALs are constructed for visualizing local and global structure-activity relationships (SARs) contained in compound data sets. Three-dimensional (3D) ALs are reminiscent of geographical maps where differences in landscape topology mirror different SAR characteristics. 3D AL models can be stored as differently formatted images and are thus amenable to image analysis approaches, which have thus far not been considered in the context of graphical SAR analysis. In this proof-of-concept study, 3D ALs were constructed for a variety of compound activity classes and 3D AL image variants of varying topology and information content were generated and classified. To these ends, convolutional neural networks (CNNs) were initially applied to images of original 3D AL models with color-coding reflecting compound potency information that were taken from different viewpoints. Images of 3D AL models were transformed into variants from which one-dimensional features were extracted. Other machine learning approaches including support vector machine (SVM) and random forest (RF) algorithms were applied to derive models on the basis of such features. In addition, SVM and RF models were trained using other features obtained from images through edge filtering. Machine learning was able to accurately distinguish between 3D AL image variants with different topology and information content. Overall, CNNs which directly learned feature representations from 3D AL images achieved highest classification accuracy. Predictive performance for CNN, SVM, and RF models was highest for image variants emphasizing topological elevation. In addition, SVM models trained on rudimentary images from edge filtering classified such images with high accuracy, which further supported the critical role of altitude-dependent topological features for image analysis and predictions. Taken together, the findings of our proof-of-concept investigation indicate that image analysis has considerable potential for graphical SAR exploration to systematically infer different SAR characteristics from topological features of 3D ALs.
WOS:000536703400001
</snippet>
</document>

<document id="543">
<title>Indoor Topological Localization Using a Visual Landmark Sequence</title>
<url>http://dx.doi.org/10.3390/rs11010073</url>
<snippet>This paper presents a novel indoor topological localization method based on mobile phone videos. Conventional methods suffer from indoor dynamic environmental changes and scene ambiguity. The proposed Visual Landmark Sequence-based Indoor Localization (VLSIL) method is capable of addressing problems by taking steady indoor objects as landmarks. Unlike many feature or appearance matching-based localization methods, our method utilizes highly abstracted landmark sematic information to represent locations and thus is invariant to illumination changes, temporal variations, and occlusions. We match consistently detected landmarks against the topological map based on the occurrence order in the videos. The proposed approach contains two components: a convolutional neural network (CNN)-based landmark detector and a topological matching algorithm. The proposed detector is capable of reliably and accurately detecting landmarks. The other part is the matching algorithm built on the second order hidden Markov model and it can successfully handle the environmental ambiguity by fusing sematic and connectivity information of landmarks. To evaluate the method, we conduct extensive experiments on the real world dataset collected in two indoor environments, and the results show that our deep neural network-based indoor landmark detector accurately detects all landmarks and is expected to be utilized in similar environments without retraining and that VLSIL can effectively localize indoor landmarks.
WOS:000457935600073
</snippet>
</document>

<document id="544">
<title>An Improved Deep Learning Model for High-Impact Weather Nowcasting</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3203398</url>
<snippet>Accurate nowcasting (short-term prediction, 0-6 h) of high-impact weather, such as landfalling hurricanes and extreme convective precipitation, plays a critical role in natural disaster monitoring and mitigation. A number of nowcasting approaches have been developed in the past few decades, such as optical flow and the tracking radar echoes by correlation system. Most of these mainstream operational techniques are based on radar echo map extrapolation, which determines the velocity and direction of precipitation systems using historical and current radar observations. However, the skill of the traditional extrapolation method decreases rapidly within the first hour. In order to improve nowcasting skill, recent studies have proposed using deep learning methods, such as convolutional recurrent neural network and trajectory gate recurrent unit. But none of these methods focuses on high-impact weather events, and the deep learning models trained based on general precipitation events cannot meet the demand of accurate warnings and decision-making at the scales required for high-impact weather events, such as hurricanes. Using multiradar observations, this article introduces the idea of self-attention and develops a self-attention-based gate recurrent unit (SaGRU) to enhance its generalization capability and scalability in predicting high-impact weather events. In particular, two types of high-impact weather systems, namely, landfalling hurricanes and extreme convective precipitation events, are investigated. Three models are trained based on hurricane events, heavy rainfall (i.e., nonhurricane) events, and all events combined in the southeast United States during 2015 and 2020. The impacts of different data sources on the nowcasting performance are quantified. The evaluation results of nowcasting products show that our SaGRU performs very well in predicting hurricane-induced rainfall. In the new methodology, the data from nonhurricane events are shown to provide useful information in enhancing the nowcasting performance during hurricane events as the model trained by combining all the hurricane and nonhurricane events has the best performance. In addition, this article quantifies the impact of the sequence length of input radar observations on the nowcasting performance, which shows that five consecutive observations are sufficient to obtain a stable model, and even two consecutive observations can produce reasonable results.
WOS:000853871300006
</snippet>
</document>

<document id="545">
<title>Decision Fusion With Multiple Spatial Supports by Conditional Random Fields</title>
<url>http://dx.doi.org/10.1109/TGRS.2018.2797316</url>
<snippet>Classification of remotely sensed images into land cover or land use is highly dependent on geographical information at least at two levels. First, land cover classes are observed in a spatially smooth domain separated by sharp region boundaries. Second, land classes and observation scale are also tightly intertwined: they tend to be consistent within areas of homogeneous appearance, or regions, in the sense that all pixels within a roof should be classified as roof, independently on the spatial support used for the classification. In this paper, we follow these two observations and encode them as priors in an energy minimization framework based on conditional random fields (CRFs), where classification results obtained at pixel and region levels are probabilistically fused. The aim is to enforce the final maps to be consistent not only in their own spatial supports (pixel and region) but also across supports, i.e., by getting the predictions on the pixel lattice and on the set of regions to agree. To this end, we define an energy function with three terms: 1) a data term for the individual elements in each support (support-specific nodes); 2) spatial regularization terms in a neighborhood for each of the supports (support-specific edges); and 3) a regularization term between individual pixels and the region containing each of them (intersupports edges). We utilize these priors in a unified energy minimization problem that can be optimized by standard solvers. The proposed 2L (sic) CRF model consists of a CRF defined over a bipartite graph, i.e., two interconnected layers within a single graph accounting for interlattice connections. 2L (sic) CRF is tested on two very high-resolution data sets involving submetric satellite and subdecimeter aerial data. In all cases, 2L (sic) CRF improves the result obtained by the independent base model (either random forests or convolutional neural networks) and by standard CRF models enforcing smoothness in the spatial domain.
WOS:000433328400022
</snippet>
</document>

<document id="546">
<title>DEEP CONTEXTUAL DESCRIPTION OF SUPERPIXELS FOR AERIAL URBAN SCENES CLASSIFICATION</title>
<url>http://dx.doi.org/</url>
<snippet>This paper proposes a new approach for contextual feature extraction from superpixels in aerial urban scenes. Our method extracts features with many levels of context from superpixels by exploiting different layers of a pre-trained convolutional neural network. Experimental results show the effectiveness of the proposed approach, which outperforms traditional methods based on handcrafted feature extraction algorithms.
WOS:000426954603031
</snippet>
</document>

<document id="547">
<title>Features denoising-based learning for porosity classification</title>
<url>http://dx.doi.org/10.1007/s00521-019-04165-1</url>
<snippet>Reservoir characterization is one of the most challenging tasks that help in modeling different lithological properties like porosity, permeability and fluid saturation using seismic readings like velocity profile, impedance, etc. Such a model is required for field development, placing new wells and prediction management. Seismic attributes are being progressively utilized for the tasks of model building, exploration and properties estimation from the data. However, these tasks become very complex due to the nonlinear and heterogeneous nature of subsurface properties. In this context, present work proposes a recurrent neural network-based learning framework to classify porosity using seismic attributes as predictor variables. The approach begins by calculating different seismic attributes from the data. From the initially calculated attribute set, features that are to be used for classification are selected by using two different strategies. Firstly, the seismic attributes having good correlation strength with reservoir porosity are extracted. Subsequently, generative topographic map is utilized to select the significant features. The final reduced features set obtained by the integrated result of above two strategies is then fed as an input to the empirical mode decomposition (EMD) algorithm. The denoised features resulting from the EMD algorithm are used to train the classification models. Further, a comparison is carried out between the proposed classification framework(EMD+RNN)and other supervised classifiers to show the performance of the proposed framework.
WOS:000577694800013
</snippet>
</document>

<document id="548">
<title>Deep Object-Centric Pooling in Convolutional Neural Network for Remote Sensing Scene Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3100330</url>
<snippet>Remote sensing imagery typically comprises successive background contexts and complex objects. Global average pooling is a popular choice to connect the convolutional and fully connected (FC) layers for the deep convolution network. This article equips the networks with another pooling strategy, namely the deep object-centric pooling (DOCP), to pool convolutional features considering the location of an object within the scene image. The proposed DOCP network structure consists of the following two steps: inferring object's location and separately pooling the foreground and background features to generate an object-level representation. Specifically, a spatial context module is presented to learn the location of the object of interest in the scene image. Then, the convolutional feature maps are pooled separately in the foreground and background of the object. Finally, the FC layer concatenates these pooled features and is followed by a batch normalization layer, a dropout layer, and a softmax layer. Two challenging datasets are employed to validate our approach. The experimental results demonstrate that the proposed DOCP-net can outperform the corresponding pooling methods and achieve a better classification performance than other pretrained convolutional neural network-based scene classification methods.
WOS:000686757500006
</snippet>
</document>

<document id="549">
<title>EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2918242</url>
<snippet>In this paper, we present a patch-based land use and land cover classification approach using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible, and are provided in the earth observation program Copernicus. We present a novel dataset, based on these images that covers 13 spectral bands and is comprised of ten classes with a total of 27 000 labeled and geo-referenced images. Benchmarks are provided for this novel dataset with its spectral bands using state-of-the-art deep convolutional neural networks. An overall classification accuracy of 98.57&#37; was achieved with the proposed novel dataset. The resulting classification system opens a gate toward a number of earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes, and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.
WOS:000480354800020
</snippet>
</document>

<document id="550">
<title>Brain-Inspired Cognitive Model With Attention for Self-Driving Cars</title>
<url>http://dx.doi.org/10.1109/TCDS.2017.2717451</url>
<snippet>The perception-driven approach and end-to-end system are two major vision-based frameworks for self-driving cars. However, it is difficult to introduce attention and historical information into the autonomous driving process, which are essential for achieving human-like driving in these two methods. In this paper, we propose a novel model for self-driving cars called the brain-inspired cognitive model with attention. This model comprises three parts: 1) a convolutional neural network for simulating the human visual cortex; 2) a cognitive map to describe the relationships between objects in a complex traffic scene; and 3) a recurrent neural network, which is combined with the real-time updated cognitive map to implement the attention mechanism and long-short term memory. An advantage of our model is that it can accurately solve three tasks simultaneously: 1) detecting the free space and boundaries for the current and adjacent lanes; 2) estimating the distances to obstacles and vehicle attitude; and 3) learning the driving behavior and decision-making process of a human driver. Importantly, the proposed model can accept external navigation instructions during an end-to-end driving process. To evaluate the model, we built a large-scale road-vehicle dataset containing over 40 000 labeled road images captured by three cameras placed on our self-driving car. Moreover, human driving activities and vehicle states were recorded at the same time.
WOS:000461254800002
</snippet>
</document>

<document id="551">
<title>Cross-Layer Attention Network for Small Object Detection in Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3046482</url>
<snippet>In recent years, despite the tremendous progresses of object detection, small object detection has always been a challenge in the field of remote sensing. The main reason is that small objects cover few features that are easily lost during down-sampling. In this article, we propose a cross-layer attention network aiming to obtain stronger features of small objects for better detection. Specifically, we designed an up-sampling and down-sampling feature pyramid to obtain richer context information by bidirectionally fusing deep and shallow features, as well as skipping connections. Moreover, a cross-layer attention module is designed to obtain the nonlocal association of small objects in each layer, and further strengthen its representation ability through cross-layer integration and balance. Extensive experiments on the publicly available datasets (DIOR dataset and NWPUVHR-10 dataset) and the self-assembled datasets (SDOTA dataset and SDD dataset) show the excellent performance of our method compared with other detectors. Moreover, our method achieved 74.3&#37; mAP on the public DIOR dataset without any tricks.
WOS:000616306700002
</snippet>
</document>

<document id="552">
<title>Generating pre-harvest crop maps by applying convolutional neural network on multi-temporal Sentinel-1 data</title>
<url>http://dx.doi.org/10.1080/01431161.2022.2030072</url>
<snippet>Pre-harvest crop mapping, the fundamental requirement for many of the crop management decisions, continues to be challenging either due to cloud cover in satellite images or due to spectral separability issues. These limitations are overcome in this study by using Synthetic Aperture Radar (SAR) data and deep learning technique. Two-dimensional convolutional neural network (2D-CNN) architecture is applied on multi-temporal SAR data of Sentinel-1 to classify soybean, jowar, cotton and sugarcane crops in a large geographic area located in a Central Indian state. Classification experiments are conducted with full season data from nine overpasses in three modes namely VH alone, VH and VH/VV ratio, and VH and VV, and these experimental results reveal that VH and VV combination performed better with an overall accuracy of 91.75&#37; as compared to 84.96&#37; and 88.75&#37; respectively by others. Classification performances with three sets of temporal data, covering part of the season, reveal that crop map can be generated as early as 27th August (i.e. roughly 45 days prior harvesting) with an accuracy of 89.15&#37; slightly less than the mapping accuracy achieved with full season data (i.e. till 14th October). 2D-CNN algorithm has performed better than SVM and RF techniques. This methodology can be extended to similar agro-ecological regions of the country. Richly available SAR data from Sentinel-1 and the potential of deep learning techniques for recognising complex phenological patterns offer immense opportunities for early-season crop mapping even during monsoon season in tropical countries like India.
WOS:000752274200001
</snippet>
</document>

<document id="553">
<title>Topological Mapping for Manhattan-like Repetitive Environments</title>
<url>http://dx.doi.org/</url>
<snippet>We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.
WOS:000712319504042
</snippet>
</document>

<document id="554">
<title>Landslide detection from an open satellite imagery and digital elevation model dataset using attention boosted convolutional neural networks</title>
<url>http://dx.doi.org/10.1007/s10346-020-01353-2</url>
<snippet>Convolution neural network (CNN) is an effective and popular deep learning method which automatically learns complicated non-linear mapping from original inputs to given labels or ground truth through a series of convolutional layers. This study focuses on detecting landslides from high-resolution optical satellite images using CNN-based methods, providing opportunities for recognizing latent landslides and updating large-scale landslide inventory with high accuracy and time efficiency. Considering the variety of landslides and complicated backgrounds, attention mechanisms originated from the human visual system are developed for boosting the CNN to extract more distinctive feature representations of landslides from backgrounds. As deep learning needs a large number of labeled data to train a learning model, we manually prepared a landslide dataset which is located in the Bijie city, China. In the dataset, 770 landslides, including rock falls, rock slides, and a few debris slides, were interpreted by geologists from the satellite images and digital elevation model (DEM) data and further checked by fieldwork. The landslide data was separated into a training set that trains the attention boosted CNN model and a testing set that evaluates the performance of the model with a ratio of 2:1. The experimental results showed that the best F-1-score of landslide detection reached 96.62&#37;. The results also proved that the performance of our spatial-channel attention mechanism was fairly over other recent attention mechanisms. Additionally, the effectiveness of predicting new potential landslides with high efficiency based on our dataset is demonstrated.
WOS:000515732200001
</snippet>
</document>

<document id="555">
<title>Calibrated Focal Loss for Semantic Labeling of High-Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3197937</url>
<snippet>Currently, the most advanced high-resolution remote sensing image (HRRSI) semantic labeling methods rely on deep neural networks. However, HRRSIs naturally have a serious class imbalance problem, which is not yet well solved by the current method. The cross-entropy loss is often used to guide the training of semantic labeling neural networks for HRRSIs, but it is essentially dominated by the major classes in the image, resulting in poor predictions for the minority class. Based on the prediction results, focal loss (FL) effectively suppresses the negative impact of class imbalance in dense object detection by redistributing the loss of each sample. In this article, we thoroughly analyze the inadequacy of FL for semantic labeling, which inevitably introduces confusing-classified examples that are more difficult to classify while suppressing the loss of well-classified examples. Therefore, following the core idea of FL, we redefine the hard examples in semantic labeling of HRRSIs and propose the prediction confusion map to measure the classification difficulty. Based on this, we further propose the calibrated focal loss (CFL) for the semantic labeling of HRRSIs. Finally, we conduct complete experiments on the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam datasets to analyze the semantic labeling performance, model uncertainty, and confidence calibration of different loss functions. Experimental results show that CFL can achieve outstanding results compared with other commonly used loss functions without increasing model parameters and training iterations, demonstrating the effectiveness of our method. In the end, combined with our previously proposed HCANet, we further verify the effectiveness of CFL on state-of-the-art network structures.
WOS:000842061200004
</snippet>
</document>

<document id="556">
<title>A Residual Network and FPGA Based Real-Time Depth Map Enhancement System</title>
<url>http://dx.doi.org/10.3390/e23050546</url>
<snippet>Depth maps obtained through sensors are often unsatisfactory because of their low-resolution and noise interference. In this paper, we propose a real-time depth map enhancement system based on a residual network which uses dual channels to process depth maps and intensity maps respectively and cancels the preprocessing process, and the algorithm proposed can achieve real-time processing speed at more than 30 fps. Furthermore, the FPGA design and implementation for depth sensing is also introduced. In this FPGA design, intensity image and depth image are captured by the dual-camera synchronous acquisition system as the input of neural network. Experiments on various depth map restoration shows our algorithms has better performance than existing LRMC, DE-CNN and DDTF algorithms on standard datasets and has a better depth map super-resolution, and our FPGA completed the test of the system to ensure that the data throughput of the USB 3.0 interface of the acquisition system is stable at 226 Mbps, and support dual-camera to work at full speed, that is, 54 fps@ (1280 x 960 + 328 x 248 x 3).
WOS:000653873600001
</snippet>
</document>

<document id="557">
<title>A Dual-Branch Deep Learning Architecture for Multisensor and Multitemporal Remote Sensing Semantic Segmentation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2023.3243396</url>
<snippet>Multisensor data analysis allows exploiting heterogeneous data regularly acquired by the many available remote sensing (RS) systems. Machine- and deep-learning methods use the information of heterogeneous sources to improve the results obtained by using single-source data. However, the state-of-the-art methods analyze either the multiscale information of multisensor multiresolution images or the time component of image time series. We propose a supervised deep-learning classification method that jointly performs a multiscale and multitemporal analysis of RS multitemporal images acquired by different sensors. The proposed method processes very-high-resolution (VHR) images using a residual network with a wide receptive field that handles geometrical details and multitemporal high-resolution (HR) image using a 3-D convolutional neural network that analyzes both the spatial and temporal information. The multiscale and multitemporal features are processed together in a decoder to retrieve a land-cover map. We tested the proposed method on two multisensor and multitemporal datasets. One is composed of VHR orthophotos and Sentinel-2 multitemporal images for pasture classification, and another is composed of VHR orthophotos and Sentinel-1 multitemporal images. Results proved the effectiveness of the proposed classification method.
WOS:000942401000004
</snippet>
</document>

<document id="558">
<title>Unsupervised Cluster Guided Object Detection in Aerial Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3122152</url>
<snippet>Object detection from high-resolution aerial images has received increasing attention during the last few years. It is a common practice to downsize images before feeding them into a network. In real life, there are lots of scenes where many objects gather together in certain areas, such as crossroads, parking lots, and playgrounds. The downsizing operation significantly limits the detection ability in these scenes. In this article, we proposed an unsupervised cluster guided detection framework (UCGNet) to address these issues by guiding the detector focus on the object-densely distributed area. In particular, a local location module is first applied to predict a binary map presenting how objects distribute in terms of the pixel of the map. Then, an unsupervised cluster method is used to produce dense regions. Each adjusted dense region is fed into the detector for object detection. Finally, a global merge module generates the final predict results. Experiments were conducted on two popular aerial image datasets including VisDrone2019 and UAVDT. In both datasets, our proposed method outperforms the existing baseline methods with achieving 32.8&#37; and 19.1&#37; mAP, respectively.
WOS:000719563200001
</snippet>
</document>

<document id="559">
<title>Multiscale Spatial-Spectral Feature Extraction Network for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3179446</url>
<snippet>Convolutional neural networks have garnered increasing interest for the supervised classification of hyperspectral imagery. However, images with a wide variety ofspatial land-cover sizes can hinder the feature-extraction ability of traditional convolutional networks. Consequently, many approaches intended to extract multiscale features have emerged; these techniques typically extract features in multiple parallel branches using convolutions of differing kernel sizes with concatenation or addition employed to fuse the features resulting from the various branches. In contrast, the present work explores a multiscale spatial-spectral feature-extraction network that operates in a more granular manner. Specifically, in the proposed network, a multibranch structure expands the convolutional receptive fields through the partitioning of input feature maps, applying hierarchical connections across the partitions, crosschannel feature fusion via pointwise convolution, and depthwise three-dimensional (3-D) convolutions for feature extraction. Experimental results reveal that the proposed multiscale spatial-spectral feature-fusion network outperforms other state-of-the-art networks at the supervised classification of hyperspectral imagery while being robust to limited training data.
WOS:000814673000002
</snippet>
</document>

<document id="560">
<title>TOWARDS UNCOVERING SOCIO-ECONOMIC INEQUALITIES USING VHR SATELLITE IMAGES AND DEEP LEARNING</title>
<url>http://dx.doi.org/10.1109/IGARSS39084.2020.9324399</url>
<snippet>In many cities of the Global South, informal and deprived neighborhoods, also commonly called slums, continue to proliferate, but their locations and dwellers' socio-economic status are often invisible in official statistics and maps. Very high resolution (VHR) satellite images coupled with deep learning allow us to efficiently map these areas and study their socio-economic and spatio-temporal variability to support interventions. This paper investigates a deep transfer learning approach based on convolutional neural networks (CNN) to identify the socio-economic variability of poor neighborhoods in Bangalore, India. Our deep network, pretrained on a slum classification data set, is tuned towards the prediction of a continuous-valued socio-economic index capturing multiple levels of deprivation. Experimental results show that the CNN-based regression model can explain the socio-economic variability with an R-2 of 0.75. The use of additional publicly available geographic information layers allow us to spatially extend the analysis beyond the surveyed deprived area data samples to uncover city-wide patterns of socio-economic inequalities.
WOS:000664335303181
</snippet>
</document>

<document id="561">
<title>Deep-Learning-Based Multispectral Satellite Image Segmentation for Water Body Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3098678</url>
<snippet>Automated water body detection from satellite imagery is a fundamental stage for urban hydrological studies. In recent years, various deep convolutional neural network (DCNN)-based methods have been proposed to segment remote sensing data collected by conventional RGB or multispectral imagery for such studies. However, how to effectively explore the wider spectrum bands of multispectral sensors to achieve significantly better performance compared to the use of only RGB bands has been left underexplored. In this article, we propose a novel DCNN model-multichannel water body detection network (MC-WBDN)-that incorporates three innovative components, i.e., a multichannel fusion module, an Enhanced Atrous Spatial Pyramid Pooling module, and Space-to-Depth/Depth-to-Space operations, to outperform state-of-the-art DCNN-based water body detection methods. Experimental results convincingly show that our MC-WBDN model achieves remarkable water body detection performance, is more robust to light and weather variations, and can better distinguish tiny water bodies compared to other DCNN models.
WOS:000681156300011
</snippet>
</document>

<document id="562">
<title>Generalizability in convolutional neural networks for various types of building scene recognition in High-Resolution imagery</title>
<url>http://dx.doi.org/10.1080/10106049.2020.1856196</url>
<snippet>Building recognition is a core task for urban image classification (mapping), especially in optical high-resolution imagery. Convolutional Neural Networks (CNNs) have recently achieved unprecedented performance in the automatic recognition of objects (e.g. buildings, roads, or trees) in high-resolution imagery. Although these results are promising, questions remain about generalizability. This is a great challenge, as there is a wide variability in the visual characteristics of the building image scene across different geographic locations. CNNs are overfitted with limited and low diversity samples and are tested on the same or nearby geographic locations. In this work, we propose two scenarios with regard to transfer learning CNN features for building scene classification. We also investigate the generalizability of CNNs for building recognition across different geographic locations. The results of the two scenarios show that the final model, generalizable in different geographic locations, unseen areas.
WOS:000612757700001
</snippet>
</document>

<document id="563">
<title>Detecting individual abandoned houses from google street view: A hierarchical deep learning approach</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.03.020</url>
<snippet>Abandoned houses (AH) are focal points in urban communities by threatening local security, destroying housing markets, and burdening government finance in the U.S. legacy cities. In particular, individual-level AH detection provides essential information for fine-resolution urban studies, government decision-makers, and private sector practitioners. However, three primary conventional data sources (field data, utility data, and remote sensing data) cannot suffice to collect such fine-resolution data in the large spatial area via a cost-effective approach. To this end, Google Street View (GSV) imagery, which emerges as the mainstream open-access data source with global coverage, provides an opportunity to address this issue. Subsequently, a follow-up challenge confronting the detection of AH arises from the fact that it lacks an effective method that can discern authentic visual features from the redundant noise in GSV images. In this study, we aim to develop an effective method to detect individual-level AH from GSV imagery. Specifically, we developed a new hierarchical deep learning method to leverage both global and local visual features of AH in the detection. The method can be further divided into three steps: (1) Scene-based classification that can extract global visual features of AH was implemented through fine-tuning a pre-trained deep convolutional neural network (CNN) model. (2) We developed a patch-based classification method that can extract specific local features of AH. In this method, patches were generated from GSV images based on auto-detected local features, followed by being labeled as three categories: building patches, vegetation patches, and others. Two deep CNN models were employed to identify deteriorated building facade patches and overgrown vegetation patches, respectively. (3) Individual-level AH were detected by integrating scene classification results and patch classification results in a decision-tree model. Experimental results showed that the F-score of AH was 0.84 in a well-prepared dataset collected from five different Rust Belt cities. The proposed hierarchical deep learning approach effectively improved the accuracy comparing with the traditional scene-based method. In addition, the proposed method was applied to generate an AH map in a new site in Detroit, MI. Our study demonstrated the feasibility of GSV imagery in AH detection and showed great potential to detect AH in a large spatial extent.
WOS:000644695700021
</snippet>
</document>

<document id="564">
<title>Using the Boruta algorithm and deep learning models for mapping land susceptibility to atmospheric dust emissions in Iran</title>
<url>http://dx.doi.org/10.1016/j.aeolia.2021.100682</url>
<snippet>Wind erosion have many negative effects on global terrestrial and aquatic ecosystems and these phenomena are controlled by several factors including climatic, meteorological, topographic, vegetation, surface and soil characteristics. This study applied, for the first time, the Boruta algorithm for identification of effective variables controlling wind erosion. The novelty of the study was increased further using application of two deep learning (DL) models comprising a simple recurrent neural network (RNN) and restricted boltzmann machine (RBM). Collectively, these tools were used to map land susceptibility to wind erosion in parts of Kerman province, southeastern Iran. Among 18 potential variables for controlling dust emissions via wind erosion, 4 and 14 were identified as non-important and important, respectively, by the Boruta algorithm, while three (precipitation, digital elevation model and soil organic carbon) were selected as the most important factors. An inventory map of the wind erosion confirmed using both a test dataset (30&#37;) and a training dataset (70&#37;) was used to construct predictive models of land susceptibility to wind erosion. Both DL predictive models exhibited highly satisfactory performance according to a Taylor diagram, but the simple RNN performed slightly better than RBM. Based on the simple RNN, 35.6&#37;, 5&#37;, 2.4&#37;, 22.7&#37; and 34.3&#37; of the total study area were characterized by very low, low, moderate, high and very high susceptibility, respectively. Convergent prediction of the same susceptibility classes by intersecting the maps generated by both models classified 17.4&#37;, 0.07&#37;, 0.06&#37;, 7.4&#37; and 34&#37; of the total study area as very low, low, moderate, high and very high susceptibility classes, respectively. We conclude that applying the Boruta algorithm and DL models as new methods in aeolian geomorphology, may provide accurate spatial maps of dust sources to help target mitigation of detrimental dust effects on climate, ecosystems and human health.
WOS:000647796000001
</snippet>
</document>

<document id="565">
<title>An Object-Based Approach for Mapping Crop Coverage Using Multiscale Weighted and Machine Learning Methods</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2983439</url>
<snippet>Accurate mapping of crop distribution on Earths surface aids in predicting grain production. Pattern classification along with remote sensing imagery can facilitate traditional manual field measurement techniques using machine learning. With the rapid increase in satellite sensor resolution, the object-based classification paradigm has increasingly been applied. However, scale parameter selection is always a difficult part of the object-based classification. Based on ensemble learning, this study proposes a classification method using the multiscale object-based weighted method which includes manual digitizing of crop distribution in the southern region of Jishan County, Shanxi Province, China, applying Gaofen-2 (GF-2) images. This method initially uses estimations of the scale parameter (ESP) tool to select "good" scales, defined here as "preferred" scales, after which feature subsets are screened by each preferred scale as the input of multiple classifiers and classifies. Finally, all classification results are then fused. Our research results indicate that: 1) Feature importance values are sorted differently at different preferred scales; 2) accuracy differences become clear when different preferred scales are combined with different classifiers, and determining the "best" single appropriate scale is generally difficult; 3) accuracy of the multiscale weighted classification method is higher compared to the single preferred scale approach. Furthermore, ensemble learning can be achieved using this method on multiple scales and on multiple classifiers. With this method, procedures that necessitate the selection of segmentation scales and the selection and optimization of classifiers can be skipped altogether.
WOS:000536175000001
</snippet>
</document>

<document id="566">
<title>Landslide Segmentation with Deep Learning: Evaluating Model Generalization in Rainfall-Induced Landslides in Brazil</title>
<url>http://dx.doi.org/10.3390/rs14092237</url>
<snippet>Automatic landslide mapping is crucial for a fast response in a disaster scenario and improving landslide susceptibility models. Recent studies highlighted the potential of deep learning methods for automatic landslide segmentation. However, only a few works discuss the generalization capacity of these models to segment landslides in areas that differ from the ones used to train the models. In this study, we evaluated three different locations to assess the generalization capacity of these models in areas with similar and different environmental aspects. The model training consisted of three distinct datasets created with RapidEye satellite images, Normalized Vegetation Index (NDVI), and a digital elevation model (DEM). Here, we show that larger patch sizes (128 x 128 and 256 x 256 pixels) favor the detection of landslides in areas similar to the training area, while models trained with smaller patch sizes (32 x 32 and 64 x 64 pixels) are better for landslide detection in areas with different environmental aspects. In addition, we found that the NDVI layer helped to balance the models results and that morphological post-processing operations are efficient for improving the segmentation precision results. Our research highlights the potential of deep learning models for segmenting landslides in different areas and is a starting point for more sophisticated investigations that evaluate model generalization in images from various sensors and resolutions.
WOS:000794859900001
</snippet>
</document>

<document id="567">
<title>Aerial Imagery Feature Engineering Using Bidirectional Generative Adversarial Networks: A Case Study of the Pilica River Region, Poland</title>
<url>http://dx.doi.org/10.3390/rs13020306</url>
<snippet>Generative adversarial networks (GANs) are a type of neural network that are characterized by their unique construction and training process. Utilizing the concept of the latent space and exploiting the results of a duel between different GAN components opens up interesting opportunities for computer vision (CV) activities, such as image inpainting, style transfer, or even generative art. GANs have great potential to support aerial and satellite image interpretation activities. Carefully crafting a GAN and applying it to a high-quality dataset can result in nontrivial feature enrichment. In this study, we have designed and tested an unsupervised procedure capable of engineering new features by shifting real orthophotos into the GAN's underlying latent space. Latent vectors are a low-dimensional representation of the orthophoto patches that hold information about the strength, occurrence, and interaction between spatial features discovered during the network training. Latent vectors were combined with geographical coordinates to bind them to their original location in the orthophoto. In consequence, it was possible to describe the whole research area as a set of latent vectors and perform further spatial analysis not on RGB images but on their lower-dimensional representation. To accomplish this goal, a modified version of the big bidirectional generative adversarial network (BigBiGAN) has been trained on a fine-tailored orthophoto imagery dataset covering the area of the Pilica River region in Poland. Trained models, precisely the generator and encoder, have been utilized during the processes of model quality assurance and feature engineering, respectively. Quality assurance was performed by measuring model reconstruction capabilities and by manually verifying artificial images produced by the generator. The feature engineering use case, on the other hand, has been presented in a real research scenario that involved splitting the orthophoto into a set of patches, encoding the patch set into the GAN latent space, grouping similar patches latent codes by utilizing hierarchical clustering, and producing a segmentation map of the orthophoto.
WOS:000611947900001
</snippet>
</document>

<document id="568">
<title>Dehazing for Multispectral Remote Sensing Images Based on a Convolutional Neural Network With the Residual Architecture</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2812726</url>
<snippet>Multispectral remote sensing images are often contaminated by haze, which causes low image quality. In this paper, a novel dehazing method based on a deep convolutional neural network (CNN) with the residual structure is proposed for multispectral remote sensing images. First, multiple CNN individuals with the residual structure are connected in parallel and each individual is used to learn a regression from the hazy image to the clear image. Then, the outputs of CNN individuals are fused with weight maps to produce the final dehazing result. In the designed network, the CNN individuals, mining multiscale haze features through multiscale convolutions, are trained using different levels of haze samples to achieve different dehazing abilities. In addition, the weight maps change with the haze distribution, and the fusion of the CNN individuals is adaptive. The designed network is end-to-end, and putting a hazy image into it, the clear scene can be restored. To train the network, a wavelength-dependent haze simulation method is proposed to generate labeled data, which can synthesize hazy multispectral images highly close to real conditions. Experimental results show that the proposed method can accurately remove the haze in each band of multispectral images under different scenes.
WOS:000431427400023
</snippet>
</document>

<document id="569">
<title>UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.06.008</url>
<snippet>Semantic segmentation of remotely sensed urban scene images is required in a wide range of practical applications, such as land cover mapping, urban change detection, environmental protection, and economic assessment. Driven by rapid developments in deep learning technologies, the convolutional neural network (CNN) has dominated semantic segmentation for many years. CNN adopts hierarchical feature representation, demonstrating strong capabilities for information extraction. However, the local property of the convolution layer limits the network from capturing the global context. Recently, as a hot topic in the domain of computer vision, Transformer has demonstrated its great potential in global information modelling, boosting many vision-related tasks such as image classification, object detection, and particularly semantic segmentation. In this paper, we propose a Transformer-based decoder and construct an UNet-like Transformer (UNetFormer) for real-time urban scene segmentation. For efficient segmentation, the UNetFormer selects the lightweight ResNet18 as the encoder and develops an efficient global-local attention mechanism to model both global and local information in the decoder. Extensive experiments reveal that our method not only runs faster but also produces higher accuracy compared with state-of-the-art lightweight models. Specifically, the proposed UNetFormer achieved 67.8&#37; and 52.4&#37; mIoU on the UAVid and LoveDA datasets, respectively, while the inference speed can achieve up to 322.4 FPS with a 512 x 512 input on a single NVIDIA GTX 3090 GPU. In further exploration, the proposed Transformer-based decoder combined with a Swin Transformer encoder also achieves the state-of-the-art result (91.3&#37; F1 and 84.1&#37; mIoU) on the Vaihingen dataset. The source code will be freely available at https://github. com/WangLibo1995/GeoSeg.
WOS:000822994200001
</snippet>
</document>

<document id="570">
<title>A deep learning framework for road marking extraction, classification and completion from mobile laser scanning point clouds</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.10.007</url>
<snippet>Road markings play a critical role in road traffic safety and are one of the most important elements for guiding autonomous vehicles (AVs). High-Definition (HD) maps with accurate road marking information are very useful for many applications ranging from road maintenance, improving navigation, and prediction of upcoming road situations within AVs. This paper presents a deep learning-based framework for road marking extraction, classification and completion from three-dimensional (3D) mobile laser scanning (MLS) point clouds. Compared with existing road marking extraction methods, which are mostly based on intensity thresholds, our method is less sensitive to data quality. We added the step of road marking completion to further optimize the results. At the extraction stage, a modified U-net model was used to segment road marking pixels to overcome the intensity variation, low contrast and other issues. At the classification stage, a hierarchical classification method by integrating multi-scale clustering with Convolutional Neural Networks (CNN) was developed to classify different types of road markings with considerable differences. At the completion stage, a method based on a Generative Adversarial Network (GAN) was developed to complete small-size road markings first, then followed by completing broken lane lines and adding missing markings using a context-based method. In addition, we built a point cloud road marking dataset to train the deep network model and evaluate our method. The dataset contains urban road and highway MLS data and underground parking lot data acquired by our own assembled backpacked laser scanning system. Our experimental results obtained using the point clouds of different scenes demonstrated that our method is very promising for road marking extraction, classification and completion.
WOS:000455690800013
</snippet>
</document>

<document id="571">
<title>Semantic Segmentation and Edge Detection-Approach to Road Detection in Very High Resolution Satellite Images</title>
<url>http://dx.doi.org/10.3390/rs14030613</url>
<snippet>Road detection technology plays an essential role in a variety of applications, such as urban planning, map updating, traffic monitoring and automatic vehicle navigation. Recently, there has been much development in detecting roads in high-resolution (HR) satellite images based on semantic segmentation. However, the objects being segmented in such images are of small size, and not all the information in the images is equally important when making a decision. This paper proposes a novel approach to road detection based on semantic segmentation and edge detection. Our approach aims to combine these two techniques to improve road detection, and it produces sharp-pixel segmentation maps, using the segmented masks to generate road edges. In addition, some well-known architectures, such as SegNet, used multi-scale features without refinement; thus, using attention blocks in the encoder to predict fine segmentation masks resulted in finer edges. A combination of weighted cross-entropy loss and the focal Tversky loss as the loss function is also used to deal with the highly imbalanced dataset. We conducted various experiments on two datasets describing real-world datasets covering the three largest regions in Saudi Arabia and Massachusetts. The results demonstrated that the proposed method of encoding HR feature maps effectively predicts sharp segmentation masks to facilitate accurate edge detection, even against a harsh and complicated background.
WOS:000760482900001
</snippet>
</document>

<document id="572">
<title>A domain specific knowledge extraction transformer method for multisource satellite-borne SAR detection</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2023.02.011</url>
<snippet>Multisource satellite-borne synthetic aperture radar (SAR) images have different probability distributions. Traditional supervised learning, consequently, cannot achieve good test performance on one novel satellite -borne SAR dataset while training a good model on another existing satellite-borne SAR dataset. In this article, a domain adaptation (DA) Transformer object detection method is proposed to solve the unlabeled multisource satellite-borne SAR image object detection problem. Unlike existing DA methods based on convolutional neural network (CNN) that focus more on multi-level local feature extraction, we choose to use Vision Transformer (ViT) Faster region CNN (FRCNN) as the baseline network to cope with the extraction of global features of SAR images. Then, two classification tokens are used to learn the mapping of different domains and fully extract domain-specific knowledge, generating two different feature spaces that rely on the original label and pseudo-label to train the source and target domains feature spaces, respectively. Besides, the pseudo-label of target domain is also refined and reconstructed by feature clustering in order to improve the accuracy of target domain knowledge. Finally, the original detection head of FRCNN is employed to detect the target domain SAR image objects. Extensive experiments on image datasets from multisource satellite-borne SAR such as Gaofen-3, TerraSAR-X, Sentinel-1, and RadarSAT-2 show that compared to the other state of the art (SOTA) methods, the proposed method can achieve the greatest object detection accuracy. Especially, taking the recently proposed Transformer-based method as an example, our method has more than 5&#37; improvement in accuracy and more than 16&#37; reduction in training time.
WOS:000949489600001
</snippet>
</document>

<document id="573">
<title>Forest mapping and monitoring in Africa using Sentinel-2 data and deep learning</title>
<url>http://dx.doi.org/10.1016/j.jag.2022.102840</url>
<snippet>We propose and investigate a method for creating large scale forest height maps at 10 m resolution from Sentinel 2 data using deep neural networks. In addition, we demonstrate how clear-cutting events can be detected in a time series of the resulting forest height maps. The network architecture is a convolutional neural network based on the U-Net architecture. The 13 Sentinel-2 spectral bands are resampled to 10 m spatial resolution and input to the U-Net, which outputs a map with per-pixel forest height estimates. The network is trained with ground truth data acquired from airborne lidar scanning surveys from three different geographical regions. They cover different types of forests: lowland tropical rainforest in the Democratic Republic of Congo, Miombo woodlands (dry forest) in Liwale, Tanzania, and submontane tropical rainforest in Amani, Tanzania. We demonstrate that the trained network generalizes to new geographical regions within the African continent with a mean average error of 4.6 m. This is on-par with a previously published methods ability to generalize to new geographical regions within the same country. Clear-cutting events are detected using a t-test. The null-hypothesis of the t-test is that the forest height has not changed after any given point in time in the forest height time-series.
WOS:000811611600002
</snippet>
</document>

<document id="574">
<title>CMR-CNN: Cross-Mixing Residual Network for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3213865</url>
<snippet>With the development of deep learning, various convolutional neural network (CNN)-based methods have been proposed for the hyperspectral image (HSI) classification. Although most of them achieve good classification performance, there are still more misclassifications in the prediction map with fewer training samples. In order to address this shortcoming, this article proposes to simultaneously use pixels' spatial information and spectral information for HSI classification. Briefly speaking, a new cross-mixing residual network denoted by CMR-CNN is developed, wherein one three-dimensional residual structure responsible for extracting the spectral characteristics, one two-dimensional residual structure responsible for extracting the spatial characteristics, and one assisted feature extraction (AFE) structure responsible for linking the first two structures are, respectively, designed. With respect to experiments performed on five different datasets Indian Pines, the University of Pavia, Salinas Scene, KSC, and Xuzhou in the case of different numbers of training samples show that, compared to some state-of-the-art methods, CMR-CNN can achieve higher overall accuracy (OA), average accuracy (AA), and Kappa values. Particularly, compared with the newly proposed HSI classification methods OCT-MCNN and CMR-CNN, respectively, improves OA, AA, and kappa by 4.13&#37;, 3.67&#37;, and 2.75&#37; on average.
WOS:000873830800006
</snippet>
</document>

<document id="575">
<title>A Novel Multitemporal Deep Fusion Network (MDFN) for Short-Term Multitemporal HR Images Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3119942</url>
<snippet>High-resolution (HR) satellite images, due to the technical constraints on spectral and spatial resolutions, usually contain only several broad spectral bands but with a very high spatial resolution. This provides rich spatial details of the objects on the Earth surface, while their spectral discrimination is relatively low. Recently, the increase of the satellite revisit times made it possible to acquire more frequent data coverage for finer classification. In this article, we proposed a novel multitemporal deep fusion network (MDFN) for short-term multitemporal HR images classification. Specifically, a two-branch structure of MDFN is designed, which includes a long short-term memory (LSTM) and a convolutional neural network (CNN). The LSTM branch is mainly used to learn the joint expression of different temporal-spectral features. For the CNN branch, the three-dimensional (3-D) convolution is firstly applied along the temporal and spectral dimensions to jointly learn the temporal-spatial and spectral-spatial information, respectively, and then the 2-D convolution is performed along the spatial dimension to further extract the spatial context information. Finally, features generated from the two different branches are fused to obtain the discriminative high-level semantic information for classification. Experimental results carried on two real multitemporal HR remote sensing datasets demonstrate that the proposed MDFN provides better classification performance over the state-of-the-art methods, and it also shows the potentiality to use short-term multitemporal HR images for more accurate land use/land cover mapping.
WOS:000714204000008
</snippet>
</document>

<document id="576">
<title>Attention-Based Tri-UNet for Remote Sensing Image Pan-Sharpening</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3068274</url>
<snippet>Pan-sharpening of remote sensing images is a significant method for integrating remote sensing information in the field of computer vision, where complementary and redundant information between multispectral (MS) images and panchromatic (PAN) images is used to generate high-resolution MS (HRMS) images. Inspired by the remarkable achievements of convolutional neural networks in a variety of computer-vision tasks, we incorporate domain-specific knowledge to design our attention-based triangle UNet (Tri-UNet) architecture to generate high-quality HRMS images. The attention-based Tri-UNet is mainly divided into the following three modules: 1) feature extraction; 2) feature fusion; and 3) image reconstruction. In the feature extraction step, the feature extraction module simultaneously extracts spectral and spatial information from the MS and PAN images. The feature maps are then fused in the feature fusion module, which makes the final feature image contain rich spectral and spatial information. Finally, the image reconstruction module generates a high-resolution MS image that uses the fused image as input. The attention mechanism is introduced into the image reconstruction module to make the network focus more on key information in the feature image. The experimental results demonstrate that the proposed method can generate high-quality HRMS images. A quantitative comparison and qualitative analysis of the experimental results indicate that our method is superior to the existing methods.
WOS:000640757900009
</snippet>
</document>

<document id="577">
<title>Accounting for Label Errors When Training a Convolutional Neural Network to Estimate Sea Ice Concentration Using Operational Ice Charts</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3141063</url>
<snippet>Convolutional neural networks (CNNs) are being increasingly investigated as a means to extract sea ice concentration from synthetic aperture radar (SAR) in an automated manner. This is often done using ice charts as training data. However, in these charts, an ice concentration label is given to a large region, which may not have a spatially uniform sea ice concentration distribution at the prediction scale of the CNN. This leads to representativity errors, which can be more pronounced at intermediate sea ice concentrations. In this study, we first investigate ways to perturb the ice chart labels to obtain improved predictions to account for the label uncertainty for intermediate ice concentrations. We then propose a method to augment the ice chart data by rescaling the information in the SAR imagery. The method is found to lead to improved accuracy in comparison to using the ice chart labels alone, with accuracy improving from 0.919 to 0.966. The sea ice concentration maps with the augmented labels also have much finer detail than the other approaches evaluated. These details are visually in agreement with expected sea ice concentration from the SAR data.
WOS:000753445400005
</snippet>
</document>

<document id="578">
<title>Applying CNN Classifier to Road Interchange Classification</title>
<url>http://dx.doi.org/</url>
<snippet>The identification and classification of interchange structures in OSM (Open Street Map) data can provide important information for the construction of multi-scale model, navigation and location services, congestion analysis, etc. The traditional method of interchange identification relies on the low-level characteristics of artificial design, and cannot distinguish the complex interchange structure with interferences effectively. In this paper, a new method based on convolutional neural network for identification of the interchange is proposed. The method combines vector data with raster image, and uses neural network to learn the fuzzy characteristics of the interchange, and classifies the complex interchange structure in OSM. Experiments show that this method has strong anti-interference, and has achieved good results in the classification of complex interchanges, and there is room for further improvement with the expansion of the sample base and the optimization of neural network model.
WOS:000459847200104
</snippet>
</document>

<document id="579">
<title>Deep Learning for Dense Labeling of Hydrographic Regions in Very High Resolution Imagery</title>
<url>http://dx.doi.org/10.1117/12.2533161</url>
<snippet>Automatic dense labeling of multispectral satellite images facilitates faster map update process. Water objects are essential elements of a geographic map. While modern dense labeling methods perform robust segmentation of such objects like roads, buildings, and vegetation, dense labeling of hydrographic regions remains a challenging problem. Water objects change their surface albedo, color, and reflection in different weather and different seasons. Moreover, rivers and lakes can change their boundaries after floods or d roughts. Robust documentation of such seasonal changes is an essential task in the field of analysis of satellite imagery. Due to the high variance in water object appearance, their segmentation is usually performed manually by a human operator. Recent advances in machine learning have made possible robust segmentation of static objects such as buildings and roads. To the best of our knowledge, there is little research in the modern literature regarding dense labeling of water regions. This paper is focused on the development of a deep-learning-based method for dense labeling of hydrographic in aerial and satellite imagery. We use the GeoGAN framework(1) and MobileNetV2(2) as the starting point for our research. The GeoGAN framework uses an aerial image as an input to generate pixel-level annotations of five object c lasses: building, low vegetation, high vegetation, road, and car. The GeoGAN framework leverages two deep learning approaches to ensure robust labeling: a generator with skip connections(3) and Generative Adversarial Networks.(4) A generator with skip connections performs image -&gt; label translation using feed-forward connections between convolutional and deconvolutional layers of the same depth. A GAN framework consists of two competing networks: a generator and a discriminator. The adversarial loss improves the quality of the resulting dense labeling. We made the following contributions to the GeoGAN framework: (1) new MobileNetV2-based generator, (2) adversarial loss function. We term the resulting framework as HydroGAN. We evaluate our HydroGAN model using a new HydroViews dataset focused on dense labeling of areas that are subject to severe flooding during the spring season. The evaluation results are encouraging and demonstrate that our HydroGAN model competes with the state-of-the-art models for dense labeling of aerial and satellite imagery. The evaluation demonstrates that our model can generalize from the training data to previously unseen samples. The developed HydroGAN model is capable of performing dense labeling of water objects in different seasons. We made our model publicly available*.
WOS:000526177000028
</snippet>
</document>

<document id="580">
<title>Multi-View Instance Matching with Learned Geometric Soft-Constraints</title>
<url>http://dx.doi.org/10.3390/ijgi9110687</url>
<snippet>We present a new approach for matching urban object instances across multiple ground-level images for the ultimate goal of city-scale mapping of objects with high positioning accuracy. What makes this task challenging is the strong change in view-point, different lighting conditions, high similarity of neighboring objects, and variability in scale. We propose to turn object instance matching into a learning task, where image-appearance and geometric relationships between views fruitfully interact. Our approach constructs a Siamese convolutional neural network that learns to match two views of the same object given many candidate image cut-outs. In addition to image features, we propose utilizing location information about the camera and the object to support image evidence via soft geometric constraints. Our method is compared to existing patch matching methods to prove its edge over state-of-the-art. This takes us one step closer to the ultimate goal of city-wide object mapping from street-level imagery to benefit city administration.
WOS:000593242300001
</snippet>
</document>

<document id="581">
<title>Semi-supervised learning for topographic map analysis over time: a study of bridge segmentation</title>
<url>http://dx.doi.org/10.1038/s41598-022-23364-w</url>
<snippet>Geographical research using historical maps has progressed considerably as the digitalization of topological maps across years provides valuable data and the advancement of Al machine learning models provides powerful analytic tools. Nevertheless, analysis of historical maps based on supervised learning can be limited by the laborious manual map annotations. In this work, we propose a semisupervised learning method that can transfer the annotation of maps across years and allow map comparison and anthropogenic studies across time. Our novel two-stage framework first performs style transfer of topographic map across years and versions, and then supervised learning can be applied on the synthesized maps with annotations. We investigate the proposed semi-supervised training with the style-transferred maps and annotations on four widely-used deep neural networks (DNN), namely U-Net, fully-convolutional network (FCN), DeepLabV3, and MobileNetV3. The best performing network of U-Net achieves F1(inst:0.1) = 0.725 and F-1inst:0.01 = 0.743 trained on styletransfer synthesized maps, which indicates that the proposed framework is capable of detecting target features (bridges) on historical maps without annotations. In a comprehensive comparison, the F1(inst:0.1) of U-Net trained on Contrastive Unpaired Translation (CUT) generated dataset (0.662 +/- 0.008) achieves 57.3 &#37;than the comparative score (0.089 +/- 0.065) of the least valid configuration (MobileNetV3 trained on CycIeGAN synthesized dataset). We also discuss the remaining challenges and future research directions.
WOS:000880437400006
</snippet>
</document>

<document id="582">
<title>Very High Resolution Remote Sensing Imagery Classification Using a Fusion of Random Forest and Deep Learning Technique-Subtropical Area for Example</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2953234</url>
<snippet>Recently, convolutional neural networks (CNNs) showed excellent performance in many tasks, such as computer vision and remote sensing semantic segmentation. Especially, the ability to learn high-representation features of CNN draws much attention. And random forest (RF) algorithm, on the other hand, is widely applied for variables selection, classification, and regression. Based on the previous fusion models that fused CNN with the other models, such as conditional random fields (CRFs), support vector machine (SVM), and RF, this article tested a method based on the fusion of an RF classifier and the CNN for a very high resolution remote sensing (VHRRS) based forests mapping. The study area is located in the south of China and the main purpose was to precisely distinguish Lei bamboo forests from the other subtropical forests. The main novelties of this article are as follows. First, a test was conducted to confirm if a fusion of CNN and RF make an improvement in the VHRRS information extraction. Second, based on RF, variables with high importance were selected. Then, a test was again conducted to confirm if the learning from the selected variables will further give better results.
WOS:000526639900010
</snippet>
</document>

<document id="583">
<title>Generating high-accuracy urban distribution map for short-term change monitoring based on convolutional neural network by utilizing SAR imagery</title>
<url>http://dx.doi.org/10.1117/12.2277901</url>
<snippet>In the developing countries, urban areas are expanding rapidly. With the rapid developments, a short term monitoring of urban changes is important. A constant observation and creation of urban distribution map of high accuracy and without noise pollution are the key issues for the short term monitoring. SAR satellites are highly suitable for day or night and regardless of atmospheric weather condition observations for this type of study. The current study highlights the methodology of generating high-accuracy urban distribution maps derived from the SAR satellite imagery based on Convolutional Neural Network (CNN), which showed the outstanding results for image classification. Several improvements on SAR polarization combinations and dataset construction were performed for increasing the accuracy. As an additional data, Digital Surface Model (DSM), which are useful to classify land cover, were added to improve the accuracy. From the obtained result, high-accuracy urban distribution map satisfying the quality for short-term monitoring was generated. For the evaluation, urban changes were extracted by taking the difference of urban distribution maps. The change analysis with time series of imageries revealed the locations of urban change areas for short-term. Comparisons with optical satellites were performed for validating the results. Finally, analysis of the urban changes combining X-band, L-band and C-band SAR satellites was attempted to increase the opportunity of acquiring satellite imageries. Further analysis will be conducted as future work of the present study.
WOS:000423869700002
</snippet>
</document>

<document id="584">
<title>A Remote-Sensing Image Pan-Sharpening Method Based on Multi-Scale Channel Attention Residual Network</title>
<url>http://dx.doi.org/10.1109/ACCESS.2020.2971502</url>
<snippet>Pan-sharpening is a significant task that aims to generate high spectral- and spatial- resolution remote-sensing image by fusing multi-spectral (MS) and panchromatic (PAN) image. The conventional approaches are insufficient to protect the fidelity both in spectral and spatial domains. Inspired by the robust capability and outstanding performance of convolutional neural networks (CNN) in natural image super-resolution tasks, CNN-based pan-sharpening methods are worthy of further exploration. In this paper, a novel pan-sharpening method is proposed by introducing a multi-scale channel attention residual network (MSCARN), which can represent features accurately and reconstruct a pan-sharpened image comprehensively. In MSCARN, the multi-scale feature extraction blocks comprehensively extract the coarse structures and high-frequency details. Moreover, the multi-residual architecture guarantees the consistency of feature learning procedure and accelerates convergence. Specifically, we introduce a channel attention mechanism to recalibrate the channel-wise features by considering interdependencies among channels adaptively. The extensive experiments are implemented on two real-datasets from GaoFen series satellites. And the results show that the proposed method performs better than the existing methods both in full-reference and no-reference metrics, meanwhile, the visual inspection displays in accordance with the quantitative metrics. Besides, in comparison with pan-sharpening by convolutional neural networks (PNN), the proposed method achieves faster convergence rate and lower loss.
WOS:000525466900053
</snippet>
</document>

<document id="585">
<title>Automatic recognition of holistic functional brain networks using iteratively optimized convolutional neural networks (IO-CNN) with weak label initialization</title>
<url>http://dx.doi.org/10.1016/j.media.2018.04.002</url>
<snippet>fMRI data decomposition techniques have advanced significantly from shallow models such as Independent Component Analysis (ICA) and Sparse Coding and Dictionary Learning (SCDL) to deep learning models such Deep Belief Networks (DBN) and Convolutional Autoencoder (DCAE). However, interpretations of those decomposed networks are still open questions due to the lack of functional brain atlases, no correspondence across decomposed or reconstructed networks across different subjects, and significant individual variabilities. Recent studies showed that deep learning, especially deep convolutional neural networks (CNN), has extraordinary ability of accommodating spatial object patterns, e.g., our recent works using 3D CNN for fMRI-derived network classifications achieved high accuracy with a remarkable tolerance for mistakenly labelled training brain networks. However, the training data preparation is one of the biggest obstacles in these supervised deep learning models for functional brain network map recognitions, since manual labelling requires tedious and time-consuming labours which will sometimes even introduce label mistakes. Especially for mapping functional networks in large scale datasets such as hundreds of thousands of brain networks used in this paper, the manual labelling method will become almost infeasible. In response, in this work, we tackled both the network recognition and training data labelling tasks by proposing a new iteratively optimized deep learning CNN (IO-CNN) framework with an automatic weak label initialization, which enables the functional brain networks recognition task to a fully automatic large-scale classification procedure. Our extensive experiments based on ABIDE-II 1099 brains fMRI data showed the great promise of our IO-CNN framework. (C) 2018 Elsevier B.V. All rights reserved.
WOS:000437390100008
</snippet>
</document>

<document id="586">
<title>3D autoencoder algorithm for lithological mapping using ZY-1 02D hyperspectral imagery: a case study of Liuyuan region</title>
<url>http://dx.doi.org/10.1117/1.JRS.15.042610</url>
<snippet>A hyperspectral image (HSI) contains hundreds of spectral bands, which provide detailed spectral information, thus offering an inherent advantage in classification. The successful launch of the Gaofen-5 and ZY-1 02D hyperspectral satellites has promoted the need for large-scale geological applications, such as mineral and lithological mapping (LM). In recent years, following the success of computer vision, deep learning methods have shown their advantage in solving the problem of hyperspectral classification. However, the combination of deep learning and HSI to solve the problem of geological mapping is insufficient. We propose a new 3D convolutional autoencoder for LM. A pixel-based and cube-based 3D convolutional neural network architecture is designed to extract spatial-spectral features. Traditional and machine learning methods are employed as competing methods, trained on two real hyperspectral datasets, and evaluated according to the overall accuracy, F1 score, and other metrics. Results indicate that the proposed method can provide convincing results for LM applications on the basis of the hyperspectral data provided by the ZY-1 02D satellite. Compared with traditional methods, the combination of deep learning and hyperspectral can provide more efficient and highly accurate results. The proposed method has better robustness than supervised learning methods and shows great promise under small sample conditions. As far as we know, this work is the first attempt to apply unsupervised spatial-spectral feature learning technology in LM applications, which is of great significance for large-scale applications. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 International License.
WOS:000811514200002
</snippet>
</document>

<document id="587">
<title>Thin Cloud Removal Fusing Full Spectral and Spatial Features for Sentinel-2 Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3211857</url>
<snippet>Multispectral remote sensing images are widely used for monitoring the globe. Although thin clouds can affect all optical bands, the influences of thin clouds differ with band wavelength. When processing multispectral bands at different resolutions, many methods only remove thin clouds in visible/near-infrared bands or rescale multiresolution bands to the same resolution and then process them together. The former cannot make full use of multispectral information, and in the latter, the rescaling process will introduce noise. In this article, a deep-learning-based thin cloud removal method that fuses full spectral and spatial features in original Sentinel-2 bands is proposed, named CR4S2. A multi-input and output architecture is designed for better fusing information in all bands and reconstructing the background at original resolutions. In addition, two parallel downsampling residual blocks are designed to transfer features extracted from different depths to the bottom of the network. Experiments were conducted on a new globally distributed Sentinel-2 thin cloud removal dataset called WHUS2-CRv. The results show that the best averaged peak signal-to-noise ratio, structural similarity index measurement, normalized root-mean-square error, and spectral angle mapper of the proposed method over 12 bands in all 20 testing images were 39.55, 0.9443, 0.0245, and 2.5676 degrees, respectively. Compared with baseline methods, the proposed CR4S2 method can better restore not only the spatial features but also spectral features. This indicates that the proposed method is very promising for removing thin clouds in multispectral remote sensing images at different resolutions.
WOS:000871047900003
</snippet>
</document>

<document id="588">
<title>Deploying machine learning to assist digital humanitarians: making image annotation in OpenStreetMap more efficient</title>
<url>http://dx.doi.org/10.1080/13658816.2020.1814303</url>
<snippet>Locating populations in rural areas of developing countries has attracted the attention of humanitarian mapping projects since it is important to plan actions that affect vulnerable areas. Recent efforts have tackled this problem as the detection of buildings in aerial images. However, the quality and the amount of rural building annotated data in open mapping services like OpenStreetMap (OSM) is not sufficient for training accurate models for such detection. Although these methods have the potential of aiding in the update of rural building information, they are not accurate enough to automatically update the rural building maps. In this paper, we explore a human-computer interaction approach and propose an interactive method to support and optimize the work of volunteers in OSM. The user is asked to verify/correct the annotation of selected tiles during several iterations and therefore improving the model with the new annotated data. The experimental results, with simulated and real user annotation corrections, show that the proposed method greatly reduces the amount of data that the volunteers of OSM need to verify/correct. The proposed methodology could benefit humanitarian mapping projects, not only by making more efficient the process of annotation but also by improving the engagement of volunteers.
WOS:000563370900001
</snippet>
</document>

<document id="589">
<title>Deep Learning Approaches Applied to Remote Sensing Datasets for Road Extraction: A State-Of-The-Art Review</title>
<url>http://dx.doi.org/10.3390/rs12091444</url>
<snippet>One of the most challenging research subjects in remote sensing is feature extraction, such as road features, from remote sensing images. Such an extraction influences multiple scenes, including map updating, traffic management, emergency tasks, road monitoring, and others. Therefore, a systematic review of deep learning techniques applied to common remote sensing benchmarks for road extraction is conducted in this study. The research is conducted based on four main types of deep learning methods, namely, the GANs model, deconvolutional networks, FCNs, and patch-based CNNs models. We also compare these various deep learning models applied to remote sensing datasets to show which method performs well in extracting road parts from high-resolution remote sensing images. Moreover, we describe future research directions and research gaps. Results indicate that the largest reported performance record is related to the deconvolutional nets applied to remote sensing images, and the F1 score metric of the generative adversarial network model, DenseNet method, and FCN-32 applied to UAV and Google Earth images are high: 96.08&#37;, 95.72&#37;, and 94.59&#37;, respectively.
WOS:000543394000091
</snippet>
</document>

<document id="590">
<title>Learning Slimming SAR Ship Object Detector Through Network Pruning and Knowledge Distillation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3041783</url>
<snippet>The deployment of deep convolutional neural networks (CNNs) in synthetic aperture radar (SAR) ship real-time detection is largely hindered by huge computational cost. In this article, we propose a novel learning scheme for training a lightweight ship detector called Tiny YOLO-Lite, which simultaneously 1) reduces the model storage size; 2) decreases the floating point operations (FLOPs) calculation; and 3) guarantees the high accuracy with faster speed. This is achieved by self-designed backbone structure and network pruning, which enforces channel-level sparsity in the backbone network and yields a compact model. In addition, knowledge distillation is also applied to make up for the performance decline caused by network pruning. Hereinto, we propose to let small student model mimic cumbersome teacher's output to achieve improved generalization. Rather than applying vanilla full feature imitation, we redefine the distilled knowledge as the inter-relationship between different levels of feature maps and then transfer it from the large network to a smaller one. On account that the detectors should focus more on the salient regions containing ships while background interference is overwhelming, a novel attention mechanism is designed and then attached to the distilled feature for enhanced representation. Finally, extensive experiments are conducted on SSDD, HRSID, and two large-scene SAR images to verify the effectiveness of the thinner SAR ship object detector in comparison of with other CNN-based algorithms. The detection results demonstrate that the proposed detector can achieve lighter architecture with 2.8-M model size, more efficient inference (&gt;$200 fps) with low computation cost, and more accurate prediction with knowledge transfer strategy.
WOS:000607413900041
</snippet>
</document>

<document id="591">
<title>RoadSeg-CD: A Network With Connectivity Array and Direction Map for Road Extraction From SAR Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3175594</url>
<snippet>Road extraction from synthetic aperture radar (SAR) images has attracted much attention in the field of remote sensing image processing. General road extraction algorithms, affected by shadows of buildings and trees, are prone to producing fragmented road segments. To improve the accuracy and completeness of road extraction, we propose a neural network-based algorithm, which takes the connectivity and direction features of roads into consideration, named RoadSeg-CD. It consists of two branches: one is the main branch for road segmentation; the other is the auxiliary branch for learning road directions. In the main branch, a connectivity array is designed to utilize local contextual information and construct a connectivity loss based on the predicted probabilities of neighboring pixels. In the auxiliary branch, we proposed a novel road direction map, which is used for learning the directions of roads. The two branches are connected by specific feature fusion process, and the output from the main branch is taken as the road extraction result. Experiments on real radar images are implemented to validate the effectiveness of our method. The experimental results demonstrate that our method can obtain more continuous and more complete roads than several state-of-the-art road extraction algorithms.
WOS:000803307300008
</snippet>
</document>

<document id="592">
<title>Exposing Fake Faces Through Deep Neural Networks Combining Content and Trace Feature Extractors</title>
<url>http://dx.doi.org/10.1109/ACCESS.2021.3110859</url>
<snippet>With the breakthrough of computer vision and deep learning, there has been a surge of realistic-looking fake face media manipulated by AI such as DeepFake or Face2Face that manipulate facial identities or expressions. The fake faces were mostly created for fun, but abuse has caused social unrest. For example, some celebrities have become victims of fake pornography made by DeepFake. There are also growing concerns about fake political speech videos created by Face2Face. To maintain individual privacy as well as social, political, and international security, it is imperative to develop models that detect fake faces in media. Previous research can be divided into general-purpose image forensics and face image forensics. While the former has been studied for several decades and focuses on extracting hand-crafted features of traces left in the image after manipulation, the latter is based on convolutional neural networks mainly inspired by object detection models specialized to extract images content features. This paper proposes a hybrid face forensics framework based on a convolutional neural network combining the two forensics approaches to enhance the manipulation detection performance. To validate the proposed framework, we used a public Face2Face dataset and a custom DeepFake dataset collected on our own. Experimental results using the two datasets showed that the proposed model is more accurate and robust at various video compression rates compared to the previous methods. Throughout class activation map visualization, the proposed framework provided information on which face parts are considered important and revealed the tempering traces invisible to naked eyes.
WOS:000696074000001
</snippet>
</document>

<document id="593">
<title>TransCloudSeg: Ground-Based Cloud Image Segmentation With Transformer</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3194316</url>
<snippet>Cloud image segmentation plays an important role in ground-based cloud observation. Recently, most existing methods for ground-based cloud image segmentation learn feature representations using the convolutional neural network (CNN), which results in the loss of global information because of the limited receptive field size of the filters in the CNN. In this article, we propose a novel deep model named TransCloudSeg, which makes full use of the advantages of the CNN and transformer to extract detailed information and global contextual information for ground-based cloud image segmentation. Specifically, TransCloudSeg hybridizes the CNN and transformer as the encoders to obtain different features. To recover and fuse the feature maps from the encoders, we design the CNN decoder and the transformer decoder for TransCloudSeg. After obtaining two sets of feature maps from two different decoders, we propose the heterogeneous fusion module to effectively fuse the heterogeneous feature maps by applying the self-attention mechanism. We conduct a series of experiments on Tianjin Normal University large-scale cloud detection database and Tianjin Normal University cloud detection database, and the results show that our method achieves a better performance than other state-of-the-art methods, thus proving the effectiveness of the proposed TransCloudSeg.
WOS:000838675100001
</snippet>
</document>

<document id="594">
<title>Detecting and mapping tree seedlings in UAV imagery using convolutional neural networks and field-verified data</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.08.005</url>
<snippet>Mapping of tree seedlings is useful for tasks ranging from monitoring natural succession and regeneration to effective silvicultural management. Development of methods that are both accurate and cost-effective is especially important considering the dramatic increase in tree planting that is required globally to mitigate the impacts of climate change. The combination of high-resolution imagery from unmanned aerial vehicles and object detection by convolutional neural networks (CNNs) is one promising approach. However, unbiased assessments of these models and methods to integrate them into geospatial workflows are lacking. In this study, we present a method for rapid, large-scale mapping of young conifer seedlings using CNNs applied to RGB orthomosaic imagery. Importantly, we provide an unbiased assessment of model performance by using two well-characterised trial sites together containing over 30,000 seedlings to assemble datasets with a high level of completeness. Our results showed CNN-based models trained on two sites detected seedlings with sensitivities of 99.5&#37; and 98.8&#37;. False positives due to tall weeds at one site and naturally regenerating seedlings of the same species led to slightly lower precision of 98.5&#37; and 96.7&#37;. A model trained on examples from both sites had 99.4&#37; sensitivity and precision of 97&#37;, showing applicability across sites. Additional testing showed that the CNN model was able to detect 68.7&#37; of obscured seedlings missed during the initial annotation of the imagery but present in the field data. Finally, we demonstrate the potential to use a form of weakly supervised training and a tile-based processing chain to enhance the accuracy and efficiency of CNNs applied to large, high-resolution orthomosaics.
WOS:000567932300012
</snippet>
</document>

<document id="595">
<title>A Subpixel Mapping Method for Urban Land Use by Reducing Shadow Effects</title>
<url>http://dx.doi.org/10.1109/JSTARS.2023.3243895</url>
<snippet>Urban land use classification is significant for urban development planning. Considering complex environments of urban surface features, traditional semantic segmentation methods are difficult to solve the problems of mixed pixels and limited spatial resolution of images. The subpixel mapping technology is an effective method to solve the above problems in urban land use classification. However, traditional subpixel mapping methods are sensitive to mountain shadow, high-rise building shadow and impermeable surface heterogeneity, resulting in false classification. Therefore, we propose a subpixel mapping method that can reduce the shadow effect. This method uses a multi-index feature fusion strategy to optimize the abundance of the shadow errors in the abundance image, and uses a super-resolution reconstruction neural network model to reconstruct the optimized abundance image for the subpixel mapping of urban land use. Experiments were conducted on sentinel-2 images obtained over Yuelu District of Changsha City, Hunan Province, China. The experimental results show that the method proposed in this article can effectively overcome the influence of building shadows and mountain shadows in urban land cover classification and is superior to traditional subpixel/pixel spatial attraction model, radial basis function, super-resolution subpixel mapping, and other methods in the effect and accuracy of urban land use subpixel mapping.
WOS:000942401000005
</snippet>
</document>

<document id="596">
<title>A CNN-Based System for Mobile Robot Navigation in Indoor Environments via Visual Localization with a Small Dataset</title>
<url>http://dx.doi.org/10.3390/wevj12030134</url>
<snippet>Deep learning has made great advances in the field of image processing, which allows automotive devices to be more widely used in humans' daily lives than ever before. Nowadays, the mobile robot navigation system is among the hottest topics that researchers are trying to develop by adopting deep learning methods. In this paper, we present a system that allows the mobile robot to localize and navigate autonomously in the accessible areas of an indoor environment. The proposed system exploits the Convolutional Neural Network (CNN) model's advantage to extract data feature maps for image classification and visual localization, which attempts to precisely determine the location region of the mobile robot focusing on the topological maps of the real environment. The system attempts to precisely determine the location region of the mobile robot by integrating the CNN model and topological map of the robot workspace. A dataset with small numbers of images is acquired from the MYNT EYE camera. Furthermore, we introduce a new loss function to tackle the bounded generalization capability of the CNN model in small datasets. The proposed loss function not only considers the probability of the input data when it is allocated to its true class but also considers the probability of allocating the input data to other classes rather than its actual class. We investigate the capability of the proposed system by evaluating the empirical studies based on provided datasets. The results illustrate that the proposed system outperforms other state-of-the-art techniques in terms of accuracy and generalization capability.
WOS:000937526300046
</snippet>
</document>

<document id="597">
<title>Use of Deep Learning to Examine the Association of the Built Environment With Prevalence of Neighborhood Adult Obesity</title>
<url>http://dx.doi.org/10.1001/jamanetworkopen.2018.1535</url>
<snippet>IMPORTANCE More than one-third of the adult population in the United States is obese. Obesity has been linked to factors such as genetics, diet, physical activity, and the environment. However, evidence indicating associations between the built environment and obesity has varied across studies and geographical contexts. OBJECTIVE To propose an approach for consistent measurement of the features of the built environment (ie, both natural and modified elements of the physical environment) and its association with obesity prevalence to allow for comparison across studies. DESIGN The cross-sectional study was conducted from February 14 through October 31, 2017. A convolutional neural network, a deep learning approach, was applied to approximately 150 000 high-resolution satellite images from Google Static Maps API (application programing interface) to extract features of the built environment in Los Angeles, California; Memphis, Tennessee; San Antonio, Texas; and Seattle (representing Seattle, Tacoma, and Bellevue), Washington. Data on adult obesity prevalence were obtained from the Centers for Disease Control and Preventions 500 Cities project. Regression models were used to quantify the association between the features and obesity prevalence across census tracts. MAIN OUTCOMES AND MEASURES Model-estimated obesity prevalence (obesity defined as body mass index &gt;= 30, calculated as weight in kilograms divided by height in meters squared) based on built environment information. RESULTS The study included 1695 census tracts in 6 cities. The age-adjusted obesity prevalence was 18.8&#37; (95&#37; Cl. 18.6&#37;48.9&#37;) for Bellevue. 22.4&#37; (95&#37; Cl. 22.3&#37;-22.5&#37;) for Seattle, 30.8&#37; (95&#37; Cl, 30.6&#37;-31.0&#37;) for Tacoma, 26.7&#37; (95&#37; Cl. 26.7&#37;-26.8&#37;) for Los Angeles, 36.3&#37; (95&#37; Cl. 36.2&#37;-36.5&#37;) for Memphis. and 32.9&#37; (95&#37; Cl, 32.8&#37;-32.9&#37;) for San Antonio. Features of the built environment explained 64.8&#37; (root mean square error [RMSE), 4.3) of the variation in obesity prevalence across all census tracts. Individually, the variation explained was 55.8&#37; (RMSE, 3.2) for Seattle (213 census tracts), 56.1&#37; (RMSE, 4.2) for Los Angeles (993 census tracts), 73.3&#37; (RMSE, 4.5) for Memphis (178 census tracts), and 61.5&#37; (RMSE, 3.5) for San Antonio (311 census tracts). CONCLUSIONS AND RELEVANCE This study illustrates that convolutional neural networks can be used to automate the extraction of features of the built environment from satellite images for studying health indicators. Understanding the association between specific features of the built environment and obesity prevalence can lead to structural changes that could encourage physical activity and decreases in obesity prevalence.
WOS:000452643200028
</snippet>
</document>

<document id="598">
<title>Predicting cancer outcomes from histology and genomics using convolutional networks</title>
<url>http://dx.doi.org/10.1073/pnas.1717139115</url>
<snippet>Cancer histology reflects underlying molecular processes and disease progression and contains rich phenotypic information that is predictive of patient outcomes. In this study, we show a computational approach for learning patient outcomes from digital pathology images using deep learning to combine the power of adaptive machine learning algorithms with traditional survival models. We illustrate how these survival convolutional neural networks (SCNNs) can integrate information from both histology images and genomic biomarkers into a single unified framework to predict time-to-event outcomes and show prediction accuracy that surpasses the current clinical paradigm for predicting the overall survival of patients diagnosed with glioma. We use statistical sampling techniques to address challenges in learning survival from histology images, including tumor heterogeneity and the need for large training cohorts. We also provide insights into the prediction mechanisms of SCNNs, using heat map visualization to show that SCNNs recognize important structures, like microvascular proliferation, that are related to prognosis and that are used by pathologists in grading. These results highlight the emerging role of deep learning in precision medicine and suggest an expanding utility for computational analysis of histology in the future practice of pathology.
WOS:000428382400012
</snippet>
</document>

<document id="599">
<title>IMAGE LABELING FOR LIDAR INTENSITY IMAGE USING K-NN OF FEATURE OBTAINED BY CONVOLUTIONAL NEURAL NETWORK</title>
<url>http://dx.doi.org/10.5194/isprsarchives-XLI-B3-931-2016</url>
<snippet>We propose an image labeling method for LIDAR intensity image obtained by Mobile Mapping System (MMS) using K-Nearest Neighbor (KNN) of feature obtained by Convolutional Neural Network (CNN). Image labeling assigns labels (e.g., road, cross-walk and road shoulder) to semantic regions in an image. Since CNN is effective for various image recognition tasks, we try to use the feature of CNN (Caffenet) pre-trained by ImageNet. We use 4,096-dimensional feature at fc7 layer in the Caffenet as the descriptor of a region because the feature at fc7 layer has effective information for object classification. We extract the feature by the Caffenet from regions cropped from images. Since the similarity between features reflects the similarity of contents of regions, we can select top K similar regions cropped from training samples with a test region. Since regions in training images have manually-annotated ground truth labels, we vote the labels attached to top K similar regions to the test region. The class label with the maximum vote is assigned to each pixel in the test image. In experiments, we use 36 LIDAR intensity images with ground truth labels. We divide 36 images into training (28 images) and test sets (8 images). We use class average accuracy and pixel-wise accuracy as evaluation measures. Our method was able to assign the same label as human beings in 97.8&#37; of the pixels in test LIDAR intensity images.
WOS:000392743800136
</snippet>
</document>

<document id="600">
<title>Comparative Analysis of Modeling Algorithms for Forest Aboveground Biomass Estimation in a Subtropical Region</title>
<url>http://dx.doi.org/10.3390/rs10040627</url>
<snippet>Remote sensing-based forest aboveground biomass (AGB) estimation has been extensively explored in the past three decades, but how to effectively combine different sensor data and modeling algorithms is still poorly understood. This research conducted a comparative analysis of different datasets (e.g., Landsat Thematic Mapper (TM), ALOS PALSAR L-band data, and their combinations) and modeling algorithms (e.g., artificial neural network (ANN), support vector regression (SVR), Random Forest (RF), k-nearest neighbor (kNN), and linear regression (LR)) for AGB estimation in a subtropical region under non-stratification and stratification of forest types. The results show the following: (1) Landsat TM imagery provides more accurate AGB estimates (root mean squared error (RMSE) values in 27.7-29.3 Mg/ha) than ALOS PALSAR (RMSE values in 30.3-33.7 Mg/ha). The combination of TM and PALSAR data has similar performance for ANN and SVR, worse performance for RF and KNN, and slightly improved performance for LR. (2) Overestimation for small AGB values and underestimation for large AGB values are major problems when using the optical (e.g., Landsat) or radar (e.g., ALOS PALSAR) data. (3) LR is still an important tool for AGB modeling, especially for the AGB range of 40-120 Mg/ha. Machine learning algorithms have limited effects on improving AGB estimation overall, but ANN can improve AGB modeling when AGB values are greater than 120 Mg/ha. (4) Forest type and AGB range are important factors that influence AGB modeling performance. (5) Stratification based on forest types improved AGB estimation, especially when AGB was greater than 160 Mg/ha, using the LR approach. This research provides new insight for remote sensing-based AGB modeling for the subtropical forest ecosystem through a comprehensive analysis of different source data, modeling algorithms, and forest types. It is critical to develop an optimal AGB modeling procedure, including the collection of a sufficient number of sample plots, extraction of suitable variables and modeling algorithms, and evaluation of the AGB estimates.
WOS:000435187500139
</snippet>
</document>

<document id="601">
<title>Remote Sensing Image Classification with a Graph-Based Pre-Trained Neighborhood Spatial Relationship</title>
<url>http://dx.doi.org/10.3390/s21165602</url>
<snippet>Previous knowledge of the possible spatial relationships between land cover types is one factor that makes remote sensing image classification "smarter". In recent years, knowledge graphs, which are based on a graph data structure, have been studied in the community of remote sensing for their ability to build extensible relationships between geographic entities. This paper implements a classification scheme considering the neighborhood relationship of land cover by extracting information from a graph. First, a graph representing the spatial relationships of land cover types was built based on an existing land cover map. Empirical probability distributions of the spatial relationships were then extracted using this graph. Second, an image was classified based on an object-based fuzzy classifier. Finally, the membership of objects and the attributes of their neighborhood objects were joined to decide the final classes. Two experiments were implemented. Overall accuracy of the two experiments increased by 5.2&#37; and 0.6&#37;, showing that this method has the ability to correct misclassified patches using the spatial relationship between geo-entities. However, two issues must be considered when applying spatial relationships to image classification. The first is the "siphonic effect" produced by neighborhood patches. Second, the use of global spatial relationships derived from a pre-trained graph loses local spatial relationship in-formation to some degree.
WOS:000689803200001
</snippet>
</document>

<document id="602">
<title>DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.01.011</url>
<snippet>Nowadays, modem Earth Observation systems continuously generate huge amounts of data. A notable example is represented by the Sentinel-2 mission, which provides images at high spatial resolution (up to 10 m) with high temporal revisit period (every 5 days), which can be organized in Satellite Image Time Series (SITS). While the use of SITS has been proved to be beneficial in the context of Land Use/Land Cover (LULC) map generation, unfortunately, most of machine learning approaches commonly leveraged in remote sensing field fail to take advantage of spatio-temporal dependencies present in such data. Recently, new generation deep learning methods allowed to significantly advance research in this field. These approaches have generally focused on a single type of neural network, i.e., Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), which model different but complementary information: spatial autocorrelation (CNNs) and temporal dependencies (RNNs). In this work, we propose the first deep learning architecture for the analysis of SITS data, namely DuPLO (DUal view Point deep Learning architecture for time series classificatiOn), that combines Convolutional and Recurrent neural networks to exploit their complementarity. Our hypothesis is that, since CNNs and RNNs capture different aspects of the data, a combination of both models would produce a more diverse and complete representation of the information for the underlying land cover classification task. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the Gard site in Mainland France and Reunion Island, a overseas department of France in the Indian Ocean), demonstrate the significance of our proposal.
WOS:000461535600008
</snippet>
</document>

<document id="603">
<title>Zanthoxylum bungeanum Maxim mapping with multi-temporal Sentinel-2 images: The importance of different features and consistency of results</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.02.003</url>
<snippet>Zanthoxylum bungeanum Maxim (ZBM) is an important woody species in large parts of Asia, which provides oils and medicinal materials. Timely and accurate mapping of its spatial distribution and planting area is of great significance to local economy and ecology. As a special tree species planted in the Grain for Green Program of China, Linxia Hui Autonomous Prefecture (Linxia) in Gansu Province of China has vigorously developed ZBM cultivation since the launch of the program. However, lacking the accurate ZBM planting information hinders the assessment of the benefits and losses of the program to local people. Therefore, this study investigated the potential of multi-temporal Sentinel-2 Multi-Spectral Instrument (MSI) to accurately map ZBM in the study area in 2019. We first investigated the classification accuracies of four alternative Random Forest (RF) classifications using either alone or in combination, spectral bands, vegetation indices (VIs), and topographical variables. The importance of the three categories of features was examined based on the mean decrease accuracy (MDA) metric. The classification results with the most important features were further assessed for their consistency by considering the voting rates of 800 trees based on testing samples. Results show that the sole use of the spectral bands (40 input features) already achieves a satisfactory classification accuracy of 95.43&#37;. Adding extra VIs and topographical variables further improves the results, but only to a small extent. However, certain VIs and topographic variables are far more effective in classification compared with the original spectral bands, especially the Red Edge Normalized Difference Vegetation Index (NDVI705) and Normalized Difference Yellow Index (NDYI). The classification accuracy achieves nearly 95&#37; when using only the top 15 most important features. The desirable periods for differencing of ZBM and other land cover types are fruit coloring and ripening periods. The final map shows that the ZBM planting in Linxia is mainly distributed along the Yellow River and around the Liujiaxia reservoir. The total mapped acreages of ZBM is 51,601 ha, covering 9.51&#37; of the study area. 99&#37; of ZBM tends to grow between 1500 and 2400 m altitude, and 67&#37; of ZBM are planted in areas with slopes between 5 and 25.. Voting rates show that the percentages of classification results with strong and good consistency are generally over 70&#37; for all land cover types, proving the derived land cover maps high credibility, including ZBM. Altogether, our results demonstrate the high potential of multi-temporal Sentinel-2 images in accurate mapping of ZBM, which can serve as a reference for other specialty crops or tree species.Y
WOS:000640987800006
</snippet>
</document>

<document id="604">
<title>A Wafer Map Yield Prediction Based on Machine Learning for Productivity Enhancement</title>
<url>http://dx.doi.org/10.1109/TSM.2019.2945482</url>
<snippet>Manufacturing productivity in the semiconductor industry is a key factor in determining the competitiveness of manufacturers. In order to enhance productivity, evaluating the productivity of wafer maps prior to production and optimizing the productivity of wafer maps is one of the most effective solutions. The productivity of a wafer map is evaluated in advance by considering various factors affecting wafer productivity such as: gross dies, shot counts, lithography throughputs, mask field occupancy (MFO), prices, etc. Manufacturing process information is not determined at the initial wafer map design stage. Predicting the yield of new wafer maps before fabrication is a difficult challenge due to lack of process information. However, a yield prediction model is required to precisely evaluate the productivity of new wafer maps, because the yield is directly related to the productivity and the design of wafer map affects the yield. In this paper, we propose a novel yield prediction model based on deep learning algorithms. Our approach exploits spatial relationships among positions of dies on a wafer and die-level yield variations collected from a wafer test without process parameters. By modeling these spatial features, the accuracy of yield prediction significantly increased. Furthermore, experimental results showed that the proposed yield model and approach helps to design wafer maps with up to 8.59 higher productivity.
WOS:000502719000007
</snippet>
</document>

<document id="605">
<title>Preliminary geological mapping with convolution neural network using statistical data augmentation on a 3D model</title>
<url>http://dx.doi.org/10.1016/j.cageo.2022.105187</url>
<snippet>Airborne magnetic data are commonly processed and interpreted by geologists to produce preliminary geological maps of unexplored areas. Machine learning can partly fulfill this task rapidly and objectively as convolutional neural networks applied to image segmentation showed promising results in comparable applications. As these algorithms require a large and high-quality dataset to be trained, we developed an innovative geostatistical data augmentation workflow that uses a 3D conceptual lithological and magnetic susceptibility model as input. The workflow uses soft-constrained Multi-Point Statistics to create equiprobable synthetic 3D geological models and Sequential Gaussian Simulation to populate these models with a geologi-cally meaningful magnetic susceptibility spatial distribution. Then, geophysical forward modeling is computed to get the airborne magnetic responses of the synthetic models, which are associated with their counterpart synthetic geological maps. We applied this workflow on a 3D model of the Malartic Mine area to obtain a large synthetic airborne magnetic and counterpart geological map dataset. Then, a Gated Shape Convolutional Network is trained on this dataset to perform a preliminary geological mapping. A semi-supervised approach using clustering on the feature maps from the trained network is also implemented. Testing the trained network on synthetic data and remote areas of similar geological context shows that the methodology is suitable for producing preliminary geological maps using airborne magnetic data. Notably, the clustering module shows a precise and adaptive segmentation of the magnetic anomalies into pertinent preliminary geological maps. The quality of the results empirically validates our data augmentation method. Our method allows producing geological maps by training a convolutional neural network using an area where a detailed geological and petrophysical 3D model exists and then permits applying the pre-trained model in new areas of the same geological context, where only airborne magnetic data is available.
WOS:000863316600004
</snippet>
</document>

<document id="606">
<title>VirtuaLot-A Case Study on Combining UAS Imagery and Terrestrial Video with Photogrammetry and Deep Learning to Track Vehicle Movement in Parking Lots</title>
<url>http://dx.doi.org/10.3390/rs14215451</url>
<snippet>This study investigates the feasibility of applying monoplotting to video data from a security camera and image data from an uncrewed aircraft system (UAS) survey to create a mapping product which overlays traffic flow in a university parking lot onto an aerial orthomosaic. The framework, titled VirtuaLot, employs a previously defined computer-vision pipeline which leverages Darknet for vehicle detection and tests the performance of various object tracking algorithms. Algorithmic object tracking is sensitive to occlusion, and monoplotting is applied in a novel way to efficiently extract occluding features from the video using a digital surface model (DSM) derived from the UAS survey. The security camera is also a low fidelity model not intended for photogrammetry with unstable interior parameters. As monoplotting relies on static camera parameters, this creates a challenging environment for testing its effectiveness. Preliminary results indicate that it is possible to manually monoplot between aerial and perspective views with high degrees of transition tilt, achieving coordinate transformations between viewpoints within one deviation of vehicle short and long axis measurements throughout 70.5&#37; and 99.6&#37; of the study area, respectively. Attempted automation of monoplotting on video was met with limited success, though this study offers insight as to why and directions for future work on the subject.
WOS:000883572500001
</snippet>
</document>

<document id="607">
<title>AGD-Linknet: A Road Semantic Segmentation Model for High Resolution Remote Sensing Images Integrating Attention Mechanism, Gated Decoding Block and Dilated Convolution</title>
<url>http://dx.doi.org/10.1109/ACCESS.2023.3253289</url>
<snippet>Road information is an important geographic information. Road information extracted from remote sensing images has been widely used in map, traffic, navigation and many other fields. However, the autonomous extraction of road information from high resolution remote sensing images has some problems such as incoherence, incompleteness and poor connectivity, therefore, a semantic segmentation model for roads in high resolution remote sensing images, called AGD-Linknet, is proposed, which integrates attention mechanisms, gated decoder block, and dilated convolution. This model mainly consists of three parts. Firstly, the stem block is used as the initial convolution layer of the model to reduce the information loss in the convolution stage; Secondly, the series-parallel combined dilated convolution and coordinate attention block into the center of the network, which enlarges the receptive field of the network and improves the feature extraction ability of spatial domain and channel domain information; Finally, in the decoder part, gated convolution is introduced to improve the extraction of road edge. Compared with U-Net, Linknet and D-Linknet on the DeepGlobe dataset, the proposed AGD-Linknet has improved the pixel accuracy, mean intersection over union and F1-Score index of road recognition by 1.41&#37;-11.52&#37;, 0.0077-0.1473, 0.0057-0.1292, and has certain effectiveness and feasibility in many scenarios in rural areas, urban, and suburbs. And can be apply to the tasks of road recognition and extraction in high-resolution remote sensing.
WOS:000947586100001
</snippet>
</document>

<document id="608">
<title>A Mountain Summit Recognition Method Based on Improved Faster R-CNN</title>
<url>http://dx.doi.org/10.1155/2021/8235108</url>
<snippet>Mountain summits are vital topographic feature points, which are essential for understanding landform processes and their impacts on the environment and ecosystem. Traditional summit detection methods operate on handcrafted features extracted from digital elevation model (DEM) data and apply parametric detection algorithms to locate mountain summits. However, these methods may no longer be effective to achieve desirable recognition results in small summits and suffer from the objective criterion lacking problem. Thus, to address these problems, we propose an improved Faster region-convolutional neural network (R-CNN) to accurately detect the mountain summits from DEM data. Based on Faster R-CNN, the improved network adopts a residual convolution block to replace the traditional part and adds a feature pyramid network (FPN) to fuse the features with adjacent layers to better address the mountain summit detection task. The residual convolution is employed to capture the deep correlation between visual and physical morphological features. The FPN is utilized to integrate the location and semantic information in the extracted feature maps to effectively represent the mountain summit area. The experimental results demonstrate that the proposed network could achieve the highest recall and precision without manually designed summit features and accurately identify small summits.
WOS:000687467200002
</snippet>
</document>

<document id="609">
<title>Evaluation of the Impact of Image Spatial Resolution in Designing a Context-based Fully Convolution Neural Networks for Flood Mapping</title>
<url>http://dx.doi.org/</url>
<snippet>In this paper, our main aim is to investigate the context-based pixel-wise classification using a fully convolutional neural networks model for flood extent mapping from multispectral remote sensing images. Our approach helps to overcome the limitation of the conventional classification methods with low generalisation ability that used per-pixel spectral information for pixel-wise classification. In this study, a comparative analysis with conventional pixel-wise SVM classifier shows that our proposed model has higher generalisation ability for flooded area detection. By using remote sensing images with different spatial resolutions we also aim to investigate the relationship between image-sensor resolution and neighbourhood window size for context-based classification. Instead of fine-tuning a pre-established deep neural network model, we developed a preliminary base model with two convolutional layers. The model was tested on images with two different spatial resolutions of 3 meters (PlanetScope image) and 30 meters (Landsat-5 Thematic Mapper). During training phase we determined the structure of the convolutional layer as well as the appropriate size of the contextual neighbourhood for those two data types. Preliminary results showed that with increasing the scale of spatial resolutions the required neighbourhood size for training samples also increases. We tested different neighbourhood sized training samples to train the model and the analysis of the performance of those models showed that a 11 x 11 neighbourhood window for PlanetScope data and a 3 x 3 neighbourhood window for Landsat data were found to be the optimum size for classification. Insights from this work may be used to design efficient classifiers in scenarios where data with different resolutions are available.
WOS:000529063300047
</snippet>
</document>

<document id="610">
<title>Edge detection with feature re-extraction deep convolutional neural network</title>
<url>http://dx.doi.org/10.1016/j.jvcir.2018.10.017</url>
<snippet>In this paper, we propose an edge detector based on feature re-extraction (FRE) of a deep convolutional neural network to effectively utilize features extracted from each stage, and design a new loss function. The proposed detector is mainly composed of three modules: backbone, side-output, and feature fusion. The backbone module provides preliminary feature extraction; the side-output module makes network architecture more robustly map features from different stages of the backbone network to edge-pixel space by applying residual learning, and the feature fusion module generates the edge map. Generalization ability on the same distribution is verified using the BSDS500 dataset, achieving optimal dataset scale (ODS) F-score = 0.804. Cross-distribution generalization ability is verified on the NYUDv2 dataset, achieving ODS F-score = 0.701. In addition, we find that freezing backbone network can significantly speed up training process, without much overall accuracy loss (ODS F-score of 0.791 after 5.4k iterations). (C) 2018 Elsevier Inc. All rights reserved.
WOS:000452694400011
</snippet>
</document>

<document id="611">
<title>DeepWindows: Windows Instance Segmentation through an Improved Mask R-CNN Using Spatial Attention and Relation Modules</title>
<url>http://dx.doi.org/10.3390/ijgi11030162</url>
<snippet>Windows, as key components of building facades, have received increasing attention in facade parsing. Convolutional neural networks have shown promising results in window extraction. Most existing methods segment a facade into semantic categories and subsequently employ regularization based on the structure of manmade architectures. These methods merely concern the optimization of individual windows, without considering the spatial areas or relationships of windows. This paper presents a novel windows instance segmentation method based on Mask R-CNN architecture. The method features a spatial attention region proposal network and a relation module-enhanced head network. First, an attention module is introduced in the region proposal network to generate a spatial attention map, then the attention map is multiplied with the objectness scores of the classification branch. Second, for the head network, relation modules are added to model the spatial relationships between proposals. Appearance and geometric features are combined for instance recognition. Furthermore, we constructed a new window instance segmentation dataset with 1200 annotated images. With our dataset, the average precisions of our method on detection and segmentation increased from 53.1&#37; and 53.7&#37; to 56.4&#37; and 56.7&#37; compared with Mask R-CNN. A comparison with state-of-the-art methods also proves the predominance of our proposed method.
WOS:000775355900001
</snippet>
</document>

<document id="612">
<title>Extensibility of U-Net Neural Network Model for Hydrographic Feature Extraction and Implications for Hydrologic Modeling</title>
<url>http://dx.doi.org/10.3390/rs13122368</url>
<snippet>Accurate maps of regional surface water features are integral for advancing ecologic, atmospheric and land development studies. The only comprehensive surface water feature map of Alaska is the National Hydrography Dataset (NHD). NHD features are often digitized representations of historic topographic map blue lines and may be outdated. Here we test deep learning methods to automatically extract surface water features from airborne interferometric synthetic aperture radar (IfSAR) data to update and validate Alaska hydrographic databases. U-net artificial neural networks (ANN) and high-performance computing (HPC) are used for supervised hydrographic feature extraction within a study area comprised of 50 contiguous watersheds in Alaska. Surface water features derived from elevation through automated flow-routing and manual editing are used as training data. Model extensibility is tested with a series of 16 U-net models trained with increasing percentages of the study area, from about 3 to 35 percent. Hydrography is predicted by each of the models for all watersheds not used in training. Input raster layers are derived from digital terrain models, digital surface models, and intensity images from the IfSAR data. Results indicate about 15 percent of the study area is required to optimally train the ANN to extract hydrography when F1-scores for tested watersheds average between 66 and 68. Little benefit is gained by training beyond 15 percent of the study area. Fully connected hydrographic networks are generated for the U-net predictions using a novel approach that constrains a D-8 flow-routing approach to follow U-net predictions. This work demonstrates the ability of deep learning to derive surface water feature maps from complex terrain over a broad area.
WOS:000666342600001
</snippet>
</document>

<document id="613">
<title>Vegetation Detection Using Deep Learning and Conventional Methods</title>
<url>http://dx.doi.org/10.3390/rs12152502</url>
<snippet>Land cover classification with the focus on chlorophyll-rich vegetation detection plays an important role in urban growth monitoring and planning, autonomous navigation, drone mapping, biodiversity conservation, etc. Conventional approaches usually apply the normalized difference vegetation index (NDVI) for vegetation detection. In this paper, we investigate the performance of deep learning and conventional methods for vegetation detection. Two deep learning methods, DeepLabV3+ and our customized convolutional neural network (CNN) were evaluated with respect to their detection performance when training and testing datasets originated from different geographical sites with different image resolutions. A novel object-based vegetation detection approach, which utilizes NDVI, computer vision, and machine learning (ML) techniques, is also proposed. The vegetation detection methods were applied to high-resolution airborne color images which consist of RGB and near-infrared (NIR) bands. RGB color images alone were also used with the two deep learning methods to examine their detection performances without the NIR band. The detection performances of the deep learning methods with respect to the object-based detection approach are discussed and sample images from the datasets are used for demonstrations.
WOS:000559577000001
</snippet>
</document>

<document id="614">
<title>Four Decades of Estuarine Wetland Changes in the Yellow River Delta Based on Landsat Observations Between 1973 and 2013</title>
<url>http://dx.doi.org/10.3390/w10070933</url>
<snippet>Yellow River Delta wetlands are essential for the migration of endangered birds and breeding. The wetlands, however, have been severely damaged during recent decades, partly due to the lack of wetland ecosystem protection by authorities. To have a better historical understanding of the spatio-temporal dynamics of the wetlands, this study aims to map and characterize patterns of the loss and degradation of wetlands in the Yellow River Delta using a time series of remotely sensed images (at nine points in time) based on object-based image analysis and knowledge transfer learning technology. Spatio-temporal analysis was conducted to document the long-term changes taking place in different wetlands over the four decades. The results showed that the Yellow River Delta wetlands have experienced significant changes between 1973 and 2013. The total area of wetlands has been reduced by 683.12 km(2) during the overall period and the trend of loss continues. However, the rates and trends of change for the different types of wetlands were not the same. The natural wetlands showed a statistically significant decrease in area during the overall period (36.04 km(2) year(-1)). Meanwhile, the artificial wetlands had the opposite trend and showed a statistically significant increase in area during the past four decades (18.96 km(2) year(-1)). According to the change characteristics revealed by the time series wetland classification maps, the evolution process of the Yellow River Delta wetlands could be divided into three stages: (1) From 1973-1984, basically stable, but with little increase; (2) from 1984-1995, rapid loss; and (3) from 1995-2013, slow loss. The area of the wetlands reached a low point around 1995, and then with a little improvement, the regional wetlands entered a slow loss stage. It is believed that interference by human activities (e.g., urban construction, cropland creation, and oil exploitation) was the main reason for wetland degradation in the Yellow River Delta over the past four decades. Climate change also has long-term impacts on regional wetlands. In addition, due to the special geographical environment, the hydrological and sediment conditions and the location of the Yellow River mouth also have a significant influence on the evolution process of the wetlands.
WOS:000442579700116
</snippet>
</document>

<document id="615">
<title>Compact Global-Local Convolutional Network With Multifeature Fusion and Learning for Scene Classification in Synthetic Aperture Radar Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3096941</url>
<snippet>Feature learning of convolutional neural networks (CNNs) has gained considerable attention and achieved good performance on synthetic aperture radar (SAR) image scene classification. However, the performance of the existing convolutional feature learning methods is limited for generating the distinguishable feature representations because such techniques inherently suffer from shortcomings, i.e., they do not consider the local feature distribution of deep orderless feature statistics and deep orderless multifeature learning style. To alleviate these drawbacks, we propose a compact global-local convolutional network with multifeature fusion and learning (CGML) for SAR image scene classification, which contains double branches of convolutional feature learning net (C-net) and local feature distribution learning net (L-net). L-net employs the localized and parameterized affine subspace coding layer for local feature distribution learning and captures the feature statistics of each cluster center via detailed local feature division. The standard convolutional feature map is utilized for the convolutional feature learning in C-net. Subsequently, the compact multifeature fusion and learning strategy captures the compact global second-order orderless feature representation and allows the double branches to interact with each other via the tensor sketch algorithm. Especially, the feature learning strategy of L-net is defined in affine subspace which fully characterizes the feature distribution inside each cluster space. Finally, we concatenate the outputs of the multifeature fusion and learning network, then pool and feed them into softmax loss. Based on extensive evaluations on TerraSAR-X1 and TerraSAR-X2 image scene classification datasets, CGML can yield superior performances when compared with those of several state-of-the-art networks.
WOS:000679532200010
</snippet>
</document>

<document id="616">
<title>Snow detection in alpine regions with Convolutional Neural Networks: discriminating snow from cold clouds and water body</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2112391</url>
<snippet>Accurately monitoring the variation of snow cover from remote sensing is vital since it assists in various fields including prediction of floods, control of runoff values, and the ice regime of rivers. Spectral indices methods are traditional ways to realize snow segmentation, including the most common one - the Normalized Difference Snow Index (NDSI), which utilizes the combination of green and short-wave infrared (SWIR) bands. In addition, spectral indices methods heavily depend on the optimal threshold to determine the accuracy, making it time-consuming to find optimal values for different places. Convolutional neural networks ensemble model with DeepLabV3+ was employed as sub-models for snow segmentation using (Sentinel-2), which aims to distinguish clouds and water body from snow. The imagery dataset generated in this article contains sites in global alpine regions such as Tibetan Plateau in China, the Alps in Switzerland, Alaska in the United States, Southern Patagonian Icefield in Chile, Tsylos Provincial Park, Tatsamenie Peak, and Dalton Peak in Canada. To overcome the limitation of DeepLabV3+, which only accepts three channels as input features, and the need to use six features: green, red, blue, near-infraRed, SWIR, and NDSI, 20 three-channel DeepLabV3+ sub-models, were constructed with different combinations of three features and then ensembled together. The proposed ensemble model showed superior performance than benchmark spectral indices method, with mIoU values ranging from 0.8075 to 0.9538 in different test sites. The results of this project contribute to the development of automated snow segmentation tools to assist earth observation applications.
WOS:000842843000001
</snippet>
</document>

<document id="617">
<title>Pan-Sharpening Based on Convolutional Neural Network by Using the Loss Function With No-Reference</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3038057</url>
<snippet>In order to preserve the spatial and spectral information of the original panchromatic and multispectral images, this article designs a loss function suitable for pan-sharpening and a four-layer convolutional neural network that could adequately extract spectral and spatial features from original source images. The major advantage of this study is that the designed loss function does not need the reference fused image, and then the proposed pan-sharpening method does not need to make the simulation data for training. This is the big difference from most existing pan-sharpening methods. Moreover, the loss function takes into account the characteristics of remote sensing images, including the spatial and spectral evaluation indicators. We also add the feature enhancement layer in convolutional neural network, thus, the proposed four-layer network contains feature extraction, feature enhancement, linear mapping and reconstruction. In order to evaluate the effectiveness and universality of the proposed fusion model, we selected thousands of remote sensing images that include different sensors, different times and different land-cover types to make the training dataset. By evaluating the performance on the WorldView-2, Pleiades and Gaofen-1 experimental data, the results show that the proposed method achieves optimal performance in terms of both the subjective visual effect and the object assessment. Furthermore, the codes will be available at https://github.com/Zhangxi-Xiong/pan-sharpening.
WOS:000607413900028
</snippet>
</document>

<document id="618">
<title>A Radio Environment Map Updating Mechanism Based on an Attention Mechanism and Siamese Neural Networks</title>
<url>http://dx.doi.org/10.3390/s22186797</url>
<snippet>A radio environment map (REM) is an effective spectrum management tool. With the increase in the number of mobile devices, the wireless environment changes more and more frequently, bringing new challenges to REM updates. Traditional update methods usually rely on the amount of data collected for updating without paying attention to whether the wireless environment has changed enough. In particular, a waste of computational resources results from the frequently updated REM when the wireless environment does not change much. When the wireless environment changes a lot, the REM is not updated promptly, resulting in a decrease in REM accuracy. To overcome the above problems, this work combines the Siamese neural network and an attention mechanism in computer vision and proposes an update mechanism based on the amount of wireless environmental change starting from image data. The method compares the newly collected crowdsourced data with the constructed REM in terms of similarity. It uses similarity to measure the necessity of the REM to be updated. The algorithm in this paper can achieve a controlled update by setting a similarity threshold with good controllability. In addition, the effectiveness of the algorithm in detecting changes of the wireless environment has been demonstrated by combing simulation data.
WOS:000858671600001
</snippet>
</document>

<document id="619">
<title>Deep regression for LiDAR-based localization in dense urban areas</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.12.013</url>
<snippet>LiDAR-based localization in a city-scale map is a fundamental question in autonomous driving research. As a reasonable localization scheme, the localization can be performed by global retrieval (that suggests potential candidates from the database) followed by geometric registration (that obtains an accurate relative pose). In this work, we develop a novel end-to-end, deep multi-task network that simultaneously performs global retrieval and geometric registration for LiDAR-based localization. Both retrieval and registration are formulated and solved as regression problems, and they can be deployed independently during inference time. We also design two mechanisms to enhance our multi-task regression networks performance: residual connections for point clouds and a new loss function with learnable parameters. To alleviate the common phenomenon of vanishing gradients in neural networks, we employ residual connections to support constructing a deeper network effectively. At the same time, to solve the problem of huge differences in scale and units between different tasks, we propose a loss function that can automatically balance multi-tasks. Experiments on two public benchmarks validate the state-of-the-art performance of our algorithm in large-scale LiDAR-based localization.
WOS:000610187300016
</snippet>
</document>

<document id="620">
<title>Multi-Resolution Feature Fusion for Image Classification of Building Damages with Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3390/rs10101636</url>
<snippet>Remote sensing images have long been preferred to perform building damage assessments. The recently proposed methods to extract damaged regions from remote sensing imagery rely on convolutional neural networks (CNN). The common approach is to train a CNN independently considering each of the different resolution levels (satellite, aerial, and terrestrial) in a binary classification approach. In this regard, an ever-growing amount of multi-resolution imagery are being collected, but the current approaches use one single resolution as their input. The use of up/down-sampled images for training has been reported as beneficial for the image classification accuracy both in the computer vision and remote sensing domains. However, it is still unclear if such multi-resolution information can also be captured from images with different spatial resolutions such as imagery of the satellite and airborne (from both manned and unmanned platforms) resolutions. In this paper, three multi-resolution CNN feature fusion approaches are proposed and tested against two baseline (mono-resolution) methods to perform the image classification of building damages. Overall, the results show better accuracy and localization capabilities when fusing multi-resolution feature maps, specifically when these feature maps are merged and consider feature information from the intermediate layers of each of the resolution level networks. Nonetheless, these multi-resolution feature fusion approaches behaved differently considering each level of resolution. In the satellite and aerial (unmanned) cases, the improvements in the accuracy reached 2&#37; while the accuracy improvements for the airborne (manned) case was marginal. The results were further confirmed by testing the approach for geographical transferability, in which the improvements between the baseline and multi-resolution experiments were overall maintained.
WOS:000448555800136
</snippet>
</document>

<document id="621">
<title>Winter wheat mapping using a random forest classifier combined with multi-temporal and multi-sensor data</title>
<url>http://dx.doi.org/10.1080/17538947.2017.1356388</url>
<snippet>Wheat is a major staple food crop in China. Accurate and cost-effective wheat mapping is exceedingly critical for food production management, food security warnings, and food trade policy-making in China. To reduce confusion between wheat and non-wheat crops for accurate growth stage wheat mapping, we present a novel approach that combines a random forest (RF) classifier with multi-sensor and multi-temporal image data. This study aims to (1) determine whether an RF combined with multi-sensor and multi-temporal imagery can achieve accurate winter wheat mapping, (2) to find out whether the proposed approach can provide improved performance over the traditional classifiers, and (3) examine the feasibility of deriving reliable estimates of winter wheat-growing areas from medium-resolution remotely sensed data. Winter wheat mapping experiments were conducted in Boxing County. The experimental results suggest that the proposed method can achieve good performance, with an overall accuracy of 92.9&#37; and a kappa coefficient () of 0.858. The winter wheat acreage was estimated at 33,895.71ha with a relative error of only 9.3&#37;. The effectiveness and feasibility of the proposed approach has been evaluated through comparison with other image classification methods. We conclude that the proposed approach can provide accurate delineation of winter wheat areas.
WOS:000435179200002
</snippet>
</document>

<document id="622">
<title>Topological Semantic Mapping and Localization in Urban Road Scenarios</title>
<url>http://dx.doi.org/10.1007/s10846-017-0744-x</url>
<snippet>Autonomous vehicle self-localization must be robust to environment changes, such as dynamic objects, variable illumination, and atmospheric conditions. Topological maps provide a concise representation of the world by only keeping information about relevant places, being robust to environment changes. On the other hand, semantic maps correspond to a high level representation of the environment that includes labels associated with relevant objects and places. Hence, the use of a topological map based on semantic information represents a robust and efficient solution for large-scale outdoor scenes for autonomous vehicles and Advanced Driver Assistance Systems (ADAS). In this work, a novel topological semantic mapping and localization methodology for large-scale outdoor scenarios for autonomous driving and ADAS applications is presented. The methodology uses: (i) a deep neural network for obtaining semantic observations of the environment, (ii) a Topological Semantic Map (TSM) for storing selected semantic observations, and (iii) a topological localization algorithm which uses a Particle Filter for obtaining the vehicles pose in the TSM. The proposed methodology was tested on a real driving scenario, where a True Estimate Rate of the vehicles pose of 96.9&#37; and a Mean Position Accuracy of 7.7[m] were obtained. These results are much better than the ones obtained by other two methods used for comparative purposes. Experiments also show that the method is able to obtain the pose of the vehicle when its initial pose is unknown.
WOS:000443859600003
</snippet>
</document>

<document id="623">
<title>Flash-Flood Potential Mapping Using Deep Learning, Alternating Decision Trees and Data Provided by Remote Sensing Sensors</title>
<url>http://dx.doi.org/10.3390/s21010280</url>
<snippet>There is an evident increase in the importance that remote sensing sensors play in the monitoring and evaluation of natural hazards susceptibility and risk. The present study aims to assess the flash-flood potential values, in a small catchment from Romania, using information provided remote sensing sensors and Geographic Informational Systems (GIS) databases which were involved as input data into a number of four ensemble models. In a first phase, with the help of high-resolution satellite images from the Google Earth application, 481 points affected by torrential processes were acquired, another 481 points being randomly positioned in areas without torrential processes. Seventy percent of the dataset was kept as training data, while the other 30&#37; was assigned to validating sample. Further, in order to train the machine learning models, information regarding the 10 flash-flood predictors was extracted in the training sample locations. Finally, the following four ensembles were used to calculate the Flash-Flood Potential Index across the Basca Chiojdului river basin: Deep Learning Neural Network-Frequency Ratio (DLNN-FR), Deep Learning Neural Network-Weights of Evidence (DLNN-WOE), Alternating Decision Trees-Frequency Ratio (ADT-FR) and Alternating Decision Trees-Weights of Evidence (ADT-WOE). The models performances were assessed using several statistical metrics. Thus, in terms of Sensitivity, the highest value of 0.985 was achieved by the DLNN-FR model, meanwhile the lowest one (0.866) was assigned to ADT-FR ensemble. Moreover, the specificity analysis shows that the highest value (0.991) was attributed to DLNN-WOE algorithm, while the lowest value (0.892) was achieved by ADT-FR. During the training procedure, the models achieved overall accuracies between 0.878 (ADT-FR) and 0.985 (DLNN-WOE). K-index shows again that the most performant model was DLNN-WOE (0.97). The Flash-Flood Potential Index (FFPI) values revealed that the surfaces with high and very high flash-flood susceptibility cover between 46.57&#37; (DLNN-FR) and 59.38&#37; (ADT-FR) of the study zone. The use of the Receiver Operating Characteristic (ROC) curve for results validation highlights the fact that FFPIDLNN-WOE is characterized by the most precise results with an Area Under Curve of 0.96.
WOS:000606198700001
</snippet>
</document>

<document id="624">
<title>A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for Reconstructing High-Resolution Urban DEMs</title>
<url>http://dx.doi.org/10.3390/w12051369</url>
<snippet>The scarcity of high-resolution urban digital elevation model (DEM) datasets, particularly in certain developing countries, has posed a challenge for many water-related applications such as flood risk management. A solution to address this is to develop effective approaches to reconstruct high-resolution DEMs from their low-resolution equivalents that are more widely available. However, the current high-resolution DEM reconstruction approaches mainly focus on natural topography. Few attempts have been made for urban topography, which is typically an integration of complex artificial and natural features. This study proposed a novel multi-scale mapping approach based on convolutional neural network (CNN) to deal with the complex features of urban topography and to reconstruct high-resolution urban DEMs. The proposed multi-scale CNN model was firstly trained using urban DEMs that contained topographic features at different resolutions, and then used to reconstruct the urban DEM at a specified (high) resolution from a low-resolution equivalent. A two-level accuracy assessment approach was also designed to evaluate the performance of the proposed urban DEM reconstruction method, in terms of numerical accuracy and morphological accuracy. The proposed DEM reconstruction approach was applied to a 121 km(2) urbanized area in London, United Kingdom. Compared with other commonly used methods, the current CNN-based approach produced superior results, providing a cost-effective innovative method to acquire high-resolution DEMs in other data-scarce regions.
WOS:000555915200148
</snippet>
</document>

<document id="625">
<title>GeoNat v1.0: A dataset for natural feature mapping with artificial intelligence and supervised learning</title>
<url>http://dx.doi.org/10.1111/tgis.12633</url>
<snippet>Machine learning allows "the machine" to deduce the complex and sometimes unrecognized rules governing spatial systems, particularly topographic mapping, by exposing it to the end product. Often, the obstacle to this approach is the acquisition of many good and labeled training examples of the desired result. Such is the case with most types of natural features. To address such limitations, this research introduces GeoNat v1.0, a natural feature dataset, used to support artificial intelligence-based mapping and automated detection of natural features under a supervised learning paradigm. The dataset was created by randomly selecting points from the U.S. Geological Surveys Geographic Names Information System and includes approximately 200 examples each of 10 classes of natural features. Resulting data were tested in an object-detection problem using a region-based convolutional neural network. The object-detection tests resulted in a 62&#37; mean average precision as baseline results. Major challenges in developing training data in the geospatial domain, such as scale and geographical representativeness, are addressed in this article. We hope that the resulting dataset will be useful for a variety of applications and shed light on training data collection and labeling in the geospatial artificial intelligence domain.
WOS:000530906800001
</snippet>
</document>

<document id="626">
<title>A convolutional neural network approach for counting and geolocating citrus-trees in UAV multispectral imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.12.010</url>
<snippet>Visual inspection has been a common practice to determine the number of plants in orchards, which is a labor-intensive and time-consuming task. Deep learning algorithms have demonstrated great potential for counting plants on unmanned aerial vehicle (UAV)-borne sensor imagery. This paper presents a convolutional neural network (CNN) approach to address the challenge of estimating the number of citrus trees in highly dense orchards from UAV multispectral images. The method estimates a dense map with the confidence that a plant occurs in each pixel. A flight was conducted over an orchard of Valencia-orange trees planted in linear fashion, using a multispectral camera with four bands in green, red, red-edge and near-infrared. The approach was assessed considering the individual bands and their combinations. A total of 37,353 trees were adopted in point feature to evaluate the method. A variation of a (0.5; 1.0 and 1.5) was used to generate different ground truth confidence maps. Different stages (T) were also used to refine the confidence map predicted. To evaluate the robustness of our method, we compared it with two state-of-the-art object detection CNN methods (Faster R-CNN and RetinaNet). The results show better performance with the combination of green, red and near-infrared bands, achieving a Mean Absolute Error (MAE), Mean Square Error (MSE), R-2 and Normalized Root-MeanSquared Error (NRMSE) of 2.28, 9.82, 0.96 and 0.05, respectively. This band combination, when adopting sigma = 1 and a stage (T = 8), resulted in an R-2, MAE, Precision, Recall and F1 of 0.97, 2.05, 0.95, 0.96 and 0.95, respectively. Our method outperforms significantly object detection methods for counting and geolocation. It was concluded that our CNN approach developed to estimate the number and geolocation of citrus trees in highdensity orchards is satisfactory and is an effective strategy to replace the traditional visual inspection method to determine the number of plants in orchards trees.
WOS:000510525500007
</snippet>
</document>

<document id="627">
<title>Deep learning framework for geological symbol detection on geological maps</title>
<url>http://dx.doi.org/10.1016/j.cageo.2021.104943</url>
<snippet>Dynamic legend generation for geological maps aims to detect and identify geological map symbols within the current viewshed and generate a corresponding real-time legend to help users quickly obtain the name and meaning of symbols. Detection and recognition entail high complexity and uncertainty because of the diversity of symbol types and the randomness of symbol distribution, and thus the generation of dynamic legends for geological maps is challenging. A new framework based on deep learning is proposed in this study, combining the deep convolutional neural network (CNN) and graph convolutional network (GCN) to realize the extraction and recognition of geological map symbols. Within the framework, a CNN-based model called single symbol detection network (SSDN) is developed to detect and identify single geological map symbols, and a novel GCN combined with L2 distance attention (DAGCN) is proposed to deal with the difficulty of extracting compound symbols caused by the randomness of symbol distribution. This work systematically solves the problem of geological symbol detection with the aid of object detection technology based on deep learning, providing foundation for the dynamic legend generation. Experiments show that the framework of the proposed method is effective, and a new benchmark is established for geological symbol detection on geological maps. All of our data and code are publicly available.
WOS:000702878000003
</snippet>
</document>

<document id="628">
<title>Deep convolutional neural network-based pixel-wise landslide inventory mapping</title>
<url>http://dx.doi.org/10.1007/s10346-020-01557-6</url>
<snippet>This paper reports a feasible alternative to compile a landslide inventory map (LIM) from remote sensing datasets using the application of an artificial intelligence-driven methodology. A deep convolutional neural network model, called LanDCNN, was developed to generate segmentation maps of landslides, and its performance was compared with the benchmark model, named U-Net, and other conventional object-based methods. The landslides that occurred in Lantau Island, Hong Kong, were taken as the case study, in which the pre- and post-landslide aerial images, and a rasterized digital terrain model (DTM) were used. The assessment reveals that LanDCNN trained with bitemporal images and DTM yields the smoothest and most semantically meaningfully LIM, compared to other methods. This LIM is the most balanced segmentation results, represented by the highest F-1 measure among all analyzed cases. With the encoding capability of LanDCNN, the application of DTM as the input renders better LIM production, especially when the landslide signatures are relatively subtle. With the computational setup used in this study, LanDCNN requires similar to 3 min to map landslides from the datasets of approximately 25 km(2) in area and with a resolution of 0.5 m. In short, the proposed landslide mapping framework, featured LanDCNN, is scalable to handle the vast amount of remote sensing data from different types of measurements within a short processing period.
WOS:000584932700001
</snippet>
</document>

<document id="629">
<title>A Comparative Study of Convolutional Neural Networks and Conventional Machine Learning Models for Lithological Mapping Using Remote Sensing Data</title>
<url>http://dx.doi.org/10.3390/rs14040819</url>
<snippet>Lithological mapping is a critical aspect of geological mapping that can be useful in studying the mineralization potential of a region and has implications for mineral prospectivity mapping. This is a challenging task if performed manually, particularly in highly remote areas that require a large number of participants and resources. The combination of machine learning (ML) methods and remote sensing data can provide a quick, low-cost, and accurate approach for mapping lithological units. This study used deep learning via convolutional neural networks and conventional ML methods involving support vector machines and multilayer perceptron to map lithological units of a mineral-rich area in the southeast of Iran. Moreover, we used and compared the efficiency of three different types of multispectral remote-sensing data, including Landsat 8 operational land imager (OLI), advanced spaceborne thermal emission and reflection radiometer (ASTER), and Sentinel-2. The results show that CNNs and conventional ML methods effectively use the respective remote-sensing data in generating an accurate lithological map of the study area. However, the combination of CNNs and ASTER data provides the best performance and the highest accuracy and adaptability with field observations and laboratory analysis results so that almost all the test data are predicted correctly. The framework proposed in this study can be helpful for exploration geologists to create accurate lithological maps in other regions by using various remote-sensing data at a low cost.
WOS:000765117300001
</snippet>
</document>

<document id="630">
<title>Using CarcassonNet to automatically detect and trace hollow roads in LiDAR data from the Netherlands</title>
<url>http://dx.doi.org/10.1016/j.culher.2020.10.009</url>
<snippet>The systematic mapping of hollow roads, traces of (post)medieval sunken cart tracks ways, can provide information on past human movement and historical route networks. However, the sheer amount of traces and of available high-quality data necessitates the use of computational methods for the automatic detection of these archaeological objects. Therefore, a novel approach, named CarcassonNet, has been developed that uses a combination of a Deep Learning convolutional neural network and image processing algorithms to detect and trace hollow roads in LiDAR data from the Netherlands. CarcassonNet has been specifically developed for the archaeological domain, focusing on being computationally light and suited for reconstructing partially preserved and intersected hollow roads. Instead of using the whole roads as input for the convolutional neural network, in CarcassonNet individual sections are used. This makes it much more cost-effective to create a sufficient training dataset, and makes the classification task (performed by the neural network) relatively simple, with better detection results. The output of CarcassonNet consists of two types of geospatial vectors that offer the opportunity to efficiently study the roads themselves and their precise location in the landscape (polygons), and the course of the roads and the resulting route network (lines). An experimental evaluation shows that CarcassonNet is able to effectively detect hollow roads, with a MCC score of 0.47. Furthermore, it is shown that using the Digital Terrain Model, instead of visualized LiDAR data (hillshade) improves the performance of the convolutional neural network. The results of this research offer opportunities to reconstruct vanished and abandoned (post)medieval routes and answer questions about human-landscape interactions. (C) 2020 LAuteur(s). Publie par Elsevier Masson SAS. Cet article est publie en Open Access sous licence CC BY.
WOS:000620783300002
</snippet>
</document>

<document id="631">
<title>Deep Learning for Enrichment of Vector Spatial Databases: Application to Highway Interchange</title>
<url>http://dx.doi.org/10.1145/3382080</url>
<snippet>Spatial analysis and pattern recognition with vector spatial data is particularly useful to enrich raw data. In road networks, for instance, there are many patterns and structures that are implicit with only road line features, among which highway interchange appeared very complex to recognize with vector-based techniques. The goal is to find the roads that belong to an interchange, such as the slip roads and the highway roads connected to the slip roads. To go further than state-of-the-art vector-based techniques, this article proposes to use raster-based deep learning techniques to recognize highway interchanges. The contribution of this work is to study how to optimally convert vector data into small images suitable for state-of-the-art deep learning models. Image classification with a convolutional neural network (i.e., is there an interchange in this image or not?) and image segmentation with a u-net (i.e., find the pixels that cover the interchange) are experimented and give better results than existing vector-based techniques in this specific use case (99.5&#37; against 74&#37;).
WOS:000583755200008
</snippet>
</document>

<document id="632">
<title>Automatic extraction of road intersection points from USGS historical map series using deep convolutional neural networks</title>
<url>http://dx.doi.org/10.1080/13658816.2019.1696968</url>
<snippet>Road intersection data have been used across a range of geospatial analyses. However, many datasets dating from before the advent of GIS are only available as historical printed maps. To be analyzed by GIS software, they need to be scanned and transformed into a usable (vector-based) format. Because the number of scanned historical maps is voluminous, automated methods of digitization and transformation are needed. Frequently, these processes are based on computer vision algorithms. However, the key challenges to this are (1) the low conversion accuracy for low quality and visually complex maps, and (2) the selection of optimal parameters. In this paper, we used a region-based deep convolutional neural network-based framework (RCNN) for object detection, in order to automatically identify road intersections in historical maps of several cities in the United States of America. We found that the RCNN approach is more accurate than traditional computer vision algorithms for double-line cartographic representation of the roads, though its accuracy does not surpass all traditional methods used for single-line symbols. The results suggest that the number of errors in the outputs is sensitive to complexity and blurriness of the maps, and to the number of distinct red-green-blue (RGB) combinations within them.
WOS:000499070400001
</snippet>
</document>

<document id="633">
<title>URBAN-i: From urban scenes to mapping slums, transport modes, and pedestrians in cities using deep learning and computer vision</title>
<url>http://dx.doi.org/10.1177/2399808319846517</url>
<snippet>In recent years, deep learning and computer vision have been applied to solve complex problems across many domains. In urban studies, these technologies have been instrumental in the development of smart cities and autonomous vehicles. However, a knowledge gap is present when it comes to informal urban regions in less developed countries. How can deep learning and artificial intelligence untangle the complexities of informality to advance urban modelling? In this paper, we introduce a framework for multipurpose realistic-dynamic urban modelling using deep convolutional neural networks. The purpose of the framework is twofold: (1) to sense and detect informality and slums in urban scenes from aerial and street-level images and (2) to detect pedestrian and transport modes. The model has been trained on images of urban scenes in cities across the globe. The framework shows strong validation performance in the identification of planned and unplanned regions, despite broad variations in the classified images. The algorithms of the URBAN-i model are coded in Python and the trained models can be applied to images of any urban setting, including informal settlements and slum regions.
WOS:000612136400006
</snippet>
</document>

<document id="634">
<title>New trends on digitisation of complex engineering drawings</title>
<url>http://dx.doi.org/10.1007/s00521-018-3583-1</url>
<snippet>Engineering drawings are commonly used across different industries such as oil and gas, mechanical engineering and others. Digitising these drawings is becoming increasingly important. This is mainly due to the legacy of drawings and documents that may provide rich source of information for industries. Analysing these drawings often requires applying a set of digital image processing methods to detect and classify symbols and other components. Despite the recent significant advances in image processing, and in particular in deep neural networks, automatic analysis and processing of these engineering drawings is still far from being complete. This paper presents a general framework for complex engineering drawing digitisation. A thorough and critical review of relevant literature, methods and algorithms in machine learning and machine vision is presented. Real-life industrial scenario on how to contextualise the digitised information from specific type of these drawings, namely piping and instrumentation diagrams, is discussed in details. A discussion of how new trends on machine vision such as deep learning could be applied to this domain is presented with conclusions and suggestions for future research directions.
WOS:000470746700003
</snippet>
</document>

<document id="635">
<title>A new approach for mobile robot localization based on an online IoT system</title>
<url>http://dx.doi.org/10.1016/j.future.2019.05.074</url>
<snippet>In Mobile Robotics, localization is a primordial task, as it makes possible the navigation of the robot, thus enabling it to carry out its activities. From the emergence of the Internet of Things (IoT), there was a different approach of interacting objects with each other, as well as between objects and humans. Based on the context presented, this article evidences the utilization of IoT for the development of a system aimed for localization mobile robots employing Convolutional Neural Networks (CNN) in the process of feature extraction of the images, according to the concept of Transfer Learning. The mechanism uses the topological mapping method to orient themselves in the exploration environment considered. The effectiveness of the approach is demonstrated by parameters such as Accuracy, F1-score and time of processing. The IoT system confers the centralization of processing, reducing costs and allowing reuse of the robots idle computing power. Combined with this benefit, CNN still achieves 100&#37; Accuracy and F1-Score, proving to be an effective technique for the required activity. With this, the proposed approach demonstrates to be efficient for the use in the task of locating mobile robots. (C) 2019 Elsevier B.V. All rights reserved.
WOS:000503827500063
</snippet>
</document>

<document id="636">
<title>Online Ground Multitarget Geolocation Based on 3-D Map Construction Using a UAV Platform</title>
<url>http://dx.doi.org/10.1109/TGRS.2022.3168266</url>
<snippet>Geolocating multiple targets of interest on the ground from an aerial platform is an important activity in many applications, such as visual surveillance. However, due to the limited measurement accuracy of commonly used airborne sensors (including altimeters, accelerometer, gyroscopes, and so on) and the small size, complex motion, and a large number of ground targets in aerial images, most of the current unmanned aerial vehicle (UAV)-based ground target geolocation algorithms have difficulty in obtaining accurate geographic location coordinates online, especially at middle and high altitudes. To solve these problems, in this article, a novel online ground multitarget geolocation framework using a UAV platform is proposed, which minimizes the introduction of sensor error sources and uses only monocular aerial image sequences and global positioning system (GPS) data to perform parallel processing of target detection and rapid 3-D sparse geographic map construction and target geographic location estimations, thereby improving the accuracy and speed of ground multitarget online geolocation. In this framework, a detection algorithm based on deep learning is first adopted to improve the accuracy and robustness of small target detection in aerial images by constructing an aerial image dataset. Then, we propose a novel target geolocation algorithm based on 3-D map construction, which combines continuous images and GPS data collected online using a UAV platform to generate a 3-D geographic map and accurately estimates the GPS location of the target center pixel through projection and triangulation of the map points on the image. Finally, we design a data transmission architecture that selects multiple processes to perform image acquisition, target detection, and target geolocation tasks in parallel and utilizes database communication between the processes to achieve accurate online geolocation of ground targets in aerial images, regardless of whether the target is in a static or moving state. To evaluate the effectiveness of the proposed framework, we build an online ground multitarget geolocation system using a quad-rotor UAV and carry out a large number of experiments in simulations and real environments. Qualitative and quantitative experimental results proved that the framework can accurately locate ground targets in various complex environments, such as parks, highways, schools, cities, different flight altitudes (50-2000 m), and different attitude angles, and the average positioning error is approximately 1 m at 2000 m for cities with rich 3-D structures.
WOS:000788976800003
</snippet>
</document>

<document id="637">
<title>A novel multi-discriminator deep network for image segmentation</title>
<url>http://dx.doi.org/10.1007/s10489-021-02427-x</url>
<snippet>Several studies have shown the excellent performance of deep learning in image segmentation. Usually, this benefits from a large amount of annotated data. Medical image segmentation is challenging, however, since there is always a scarcity of annotated data. This study constructs a novel deep network for medical image segmentation, referred to as asymmetric U-Net generative adversarial networks with multi-discriminators (AU-MultiGAN). Specifically, the asymmetric U-Net is designed to produce multiple segmentation maps simultaneously and use the dual-dilated blocks in the feature extraction stage only. Further, the multi-discriminator module is embedded into the asymmetric U-Net structure, which can capture the available information of samples sufficiently and thereby promote the information transmission of features. A hybrid loss by the combination of segmentation and discriminator losses is developed, and an adaptive method of selecting the scale factors is devised for this new loss. More importantly, the convergence of the proposed model is proved mathematically. The proposed AU-MultiGAN approach is implemented on some standard medical image benchmarks. Experimental results show that the proposed architecture can be successfully applied to medical image segmentation, and obtain superior performance in comparison with the state-of-the-art baselines.
WOS:000651027400004
</snippet>
</document>

<document id="638">
<title>Flood severity mapping from Volunteered Geographic Information by interpreting water level from images containing people: A case study of Hurricane Harvey</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.09.011</url>
<snippet>With increasing urbanization, in recent years there has been a growing interest and need in monitoring and analyzing urban flood events. Social media, as a new data source, can provide real-time information for flood monitoring. The social media posts with locations are often referred to as Volunteered Geographic Information (VGI), which can reveal the spatial pattern of such events. Since more images are shared on social media than ever before, recent research focused on the extraction of flood-related posts by analyzing images in addition to texts. Apart from merely classifying posts as flood relevant or not, more detailed information, e.g. the flood severity, can also be extracted based on image interpretation. However, it has been less tackled and has not yet been applied for flood severity mapping. In this paper, we propose a novel three-step process to extract and map flood severity information. First, flood relevant images are retrieved with the help of pre-trained convolutional neural networks as feature extractors. Second, the images containing people are further classified into four severity levels by observing the relationship between body parts and their partial inundation, i.e. images are classified according to the water level with respect to different body parts, namely ankle, knee, hip, and chest. Lastly, locations of the Tweets are used for generating a map of estimated flood extent and severity. This process was applied to an image dataset collected during Hurricane Harvey in 2017, as a proof of concept. The results show that VGI can be used as a supplement to remote sensing observations for flood extent mapping and is beneficial, especially for urban areas, where the infrastructure is often occluding water. Based on the extracted water level information, an integrated overview of flood severity can be provided for the early stages of emergency response.
WOS:000584231200024
</snippet>
</document>

<document id="639">
<title>Analysis of Airglow Image Classification Based on Feature Map Visualization</title>
<url>http://dx.doi.org/10.3390/app13063671</url>
<snippet>All-sky airglow imagers (ASAIs) are used in the Meridian Project to observe the airglow in the middle and upper atmosphere to study the atmospheric perturbation. However, the ripples of airglow caused by the perturbation are only visible in the airglow images taken on a clear night. It is a problem to effectively select images suitable for scientific analysis from the enormous amount of airglow images captured under various environments due to the low efficiency and subjectivity of traditional manual classification. We trained a classification model based on convolutional neural network to distinguish between airglow images from clear nights and unclear nights. The data base contains 1688 images selected from the airglow images captured at Xinglong station (40.4 degrees N, 30.5 degrees E). The entire training process was tracked by feature maps which visualized every resulting classification model. The classification models with the clearest feature maps were saved for future use. We cropped the central part of the airglow images to avoid disturbance from the artificial lights at the edge of the vision field according to the feature maps of our first training. The accuracy of the saved model is 99&#37;. The feature maps of five categories also indicate the reliability of the classification model.
WOS:000957417400001
</snippet>
</document>

<document id="640">
<title>Aerial imagery for roof segmentation: A large-scale dataset towards automatic mapping of buildings (Withdrawn Publication)</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.11.011</url>
<snippet>As an important branch of deep learning, convolutional neural network has largely improved the performance of building detection. For further accelerating the development of building detection toward automatic mapping, a benchmark dataset bears significance in fair comparisons. However, several problems still remain in the current public datasets that address this task. First, although building detection is generally considered equivalent to extracting roof outlines, most datasets directly provide building footprints as ground truths for testing and evaluation; the challenges of these benchmarks are more complicated than roof segmentation, as relief displacement leads to varying degrees of misalignment between roof outlines and footprints. On the other hand, an image dataset should feature a large quantity and high spatial resolution to effectively train a high-performance deep learning model for accurate mapping of buildings. Unfortunately, the remote sensing community still lacks proper benchmark datasets that can simultaneously satisfy these requirements. In this paper, we present a new large-scale benchmark dataset termed Aerial Imagery for Roof Segmentation (AIRS). This dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. We implement several state-of-the-art deep learning methods of semantic segmentation for performance evaluation and analysis of the proposed dataset. The results can serve as the baseline for future work.
WOS:000455690800003
</snippet>
</document>

<document id="641">
<title>EnvSLAM: Combining SLAM Systems and Neural Networks to Improve the Environment Fusion in AR Applications</title>
<url>http://dx.doi.org/10.3390/ijgi10110772</url>
<snippet>Augmented Reality (AR) has increasingly benefited from the use of Simultaneous Localization and Mapping (SLAM) systems. This technology has enabled developers to create AR markerless applications, but lack semantic understanding of their environment. The inclusion of this information would empower AR applications to better react to the surroundings more realistically. To gain semantic knowledge, in recent years, focus has shifted toward fusing SLAM systems with neural networks, giving birth to the field of Semantic SLAM. Building on existing research, this paper aimed to create a SLAM system that generates a 3D map using ORB-SLAM2 and enriches it with semantic knowledge originated from the Fast-SCNN network. The key novelty of our approach is a new method for improving the predictions of neural networks, employed to balance the loss of accuracy introduced by efficient real-time models. Exploiting sensor information provided by a smartphone, GPS coordinates are utilized to query the OpenStreetMap database. The returned information is used to understand which classes are currently absent in the environment, so that they can be removed from the networks prediction with the goal of improving its accuracy. We achieved 87.40&#37; Pixel Accuracy with Fast-SCNN on our custom version of COCO-Stuff and showed an improvement by involving GPS data for our self-made smartphone dataset resulting in 90.24&#37; Pixel Accuracy. Having in mind the use on smartphones, the implementation aimed to find a trade-off between accuracy and efficiency, making the system achieve an unprecedented speed. To this end, the system was carefully designed and a strong focus on lightweight neural networks is also fundamental. This enabled the creation of an above real-time Semantic SLAM system that we called EnvSLAM (Environment SLAM). Our extensive evaluation reveals the efficiency of the system features and the operability in above real-time (48.1 frames per second with an input image resolution of 640 x 360 pixels). Moreover, the GPS integration indicates an effective improvement of the networks prediction accuracy.
WOS:000724808900001
</snippet>
</document>

<document id="642">
<title>Comparing geomorphological maps made manually and by deep learning</title>
<url>http://dx.doi.org/10.1002/esp.5305</url>
<snippet>Geomorphological maps provide information on the relief, genesis and shape of the earths surface and are widely used in sustainable spatial developments. The quality of geomorphological maps is however rarely assessed or reported, which limits their applicability. Moreover, older geomorphological maps often do not meet current quality requirements and require updating. This updating is time-consuming and because of its qualitative nature difficult to reproduce, but can be supported by novel computational methods. In this paper, we address these issues by (1) quantifying the uncertainty associated with manual geomorphological mapping, (2) exploring the use of convolutional neural networks (CNNs) for semi-automated geomorphological mapping and (3) testing the sensitivity of CNNs to uncertainties in manually created evaluation data. We selected a test area in the Dutch push-moraine district with a pronounced relief and a high variety of landforms. For this test area we developed five manually created geomorphological maps and 27 automatically created landform maps using CNNs. The resulting manual maps are similar on a regional level. We could identify the causes of disagreement between the maps on a local level, which often related to differences in mapping experience, choices in delineation and different interpretations of the legend. Coordination of mapping efforts and field validation are necessary to create accurate and precise maps. CNNs perform well in identifying landforms and geomorphological units, but fail at correct delineation. The human geomorphologist remains necessary to correct the delineation and classification of the computed maps. The uncertainty in the manually created data that are used to train and evaluate CNNs have a large effect on the model performance and evaluation. This also advocates for coordinated mapping efforts to ensure the quality of manually created training and test data. Further model development and data processing are required before CNNs can act as standalone mapping techniques.
WOS:000748426200001
</snippet>
</document>

<document id="643">
<title>Joint Classification of Hyperspectral and Multispectral Images for Mapping Coastal Wetlands</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3040305</url>
<snippet>It is significant for restoration and protection of natural resources and ecological services in coastal wetlands to map different land cover types with satellite remote sensing data. Considering difficulties of wetland species classification, hyperspectral images (HSIs) with high spectral resolution and multispectral images (MSI) with high spatial resolution are considered to achieve complementary advantages of multisource data. An effective approach, named as multistream convolutional neural network, is proposed to achieve fine classification of coastal wetlands. First, regression processing is adopted to make chaotically scattered coastal wetland data more compact and different. Second, through appropriate feature extraction and feature fusion strategies, high-level information of multisource data in regression domain is fused to distinguish different land cover. Experiments on GF-5 HSIs and Sentinel-2 MSIs are carried out in order to validate the classification performance of the proposed approach in two coastal wetlands of research value in China, i.e., Yellow River Estuary and Yancheng coastal wetland. Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art methods in the field, especially when the number of sample size is extremely small.
WOS:000607413900030
</snippet>
</document>

<document id="644">
<title>A Multi-Channel and Multi-Spatial Attention Convolutional Neural Network for Prostate Cancer ISUP Grading</title>
<url>http://dx.doi.org/10.3390/app11104321</url>
<snippet>Prostate cancer (PCa) is one of the most prevalent cancers worldwide. As the demand for prostate biopsies increases, a worldwide shortage and an uneven geographical distribution of proficient pathologists place a strain on the efficacy of pathological diagnosis. Deep learning (DL) is able to automatically extract features from whole-slide images of prostate biopsies annotated by skilled pathologists and to classify the severity of PCa. A whole-slide image of biopsies has many irrelevant features that weaken the performance of DL models. To enable DL models to focus more on cancerous tissues, we propose a Multi-Channel and Multi-Spatial (MCMS) Attention module that can be easily plugged into any backbone CNN to enhance feature extraction. Specifically, MCMS learns a channel attention vector to assign weights to channels in the feature map by pooling from multiple attention branches with different reduction ratios; similarly, it also learns a spatial attention matrix to focus on more relevant areas of the image, by pooling from multiple convolutional layers with different kernel sizes. The model is verified on the most extensive multi-center PCa dataset that consists of 11,000 H&amp;E-stained histopathology whole-slide images. Experimental results demonstrate that an MCMS-assisted CNN can effectively boost prediction performance in accuracy (ACC) and quadratic weighted kappa (QWK), compared with prior studies. The proposed model and results can serve as a credible benchmark for future research in automated PCa grading.
WOS:000662615200001
</snippet>
</document>

<document id="645">
<title>Building-A-Nets: Robust Building Extraction From High-Resolution Remote Sensing Images With Adversarial Networks</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2865187</url>
<snippet>With the proliferation of high-resolution remote sensing sensor and platforms, vast amounts of aerial image data are becoming easily accessed. High-resolution aerial images provide sufficient structural and texture information for image recognition while also raise new challenges for existing segmentation methods. In recent years, deep neural networks have gained much attention in remote sensing field and achieved remarkable performance for high-resolution remote sensing images segmentation. However, there still exists spatial inconsistency problems caused by independently pixelwise classification while ignoring high-order regularities. In this paper, we developed a novel deep adversarial network, named Building-A-Nets, that jointly trains a deep convolutional neural network (generator) and an adversarial discriminator network for the robust segmentation of building rooftops in remote sensing images. More specifically, the generator produces pixelwise image classification map using a fully convolutional DenseNet model, whereas the discriminator tends to enforce forms of high-order structural features learned from ground-truth label map. The generator and discriminator compete with each other in an adversarial learning process until the equivalence point is reached to produce the optimal segmentation map of building objects. Meanwhile, a soft weight coefficient is adopted to balance the operation of the pixelwise classification and high-order structural feature learning. Experimental results show that our Building-A-Net can successfully detect and rectify spatial inconsistency on aerial images while archiving superior performances compared to other state-of-the-art building extraction methods.
WOS:000447950200025
</snippet>
</document>

<document id="646">
<title>A graph convolutional neural network for classification of building patterns using spatial vector data</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.02.010</url>
<snippet>Machine learning methods, specifically, convolutional neural networks (CNNs), have emerged as an integral part of scientific research in many disciplines. However, these powerful methods often fail to perform pattern analysis and knowledge mining with spatial vector data because in most cases, such data are not underlying grid-like or array structures but can only be modeled as graph structures. The present study introduces a novel graph convolution by converting it from the vertex domain into a point-wise product in the Fourier domain using the graph Fourier transform and convolution theorem. In addition, the graph convolutional neural network (GCNN) architecture is proposed to analyze graph-structured spatial vector data. The focus of this study is the classical task of building pattern classification, which remains limited by the use of design rules and manually extracted features for specific patterns. The spatial vector data representing grouped buildings are modeled as graphs, and indices for the characteristics of individual buildings are investigated to collect the input variables. The pattern features of these graphs are directly extracted by training labeled data. Experiments confirmed that the GCNN produces satisfactory results in terms of identifying regular and irregular patterns, and thus achieves a significant improvement over existing methods. In summary, the GCNN has considerable potential for the analysis of graph structured spatial vector data as well as scope for further improvement.
WOS:000464088400018
</snippet>
</document>

<document id="647">
<title>Classification of land use/land cover using artificial intelligence (ANN-RF)</title>
<url>http://dx.doi.org/10.3389/frai.2022.964279</url>
<snippet>Because deep learning has various downsides, such as complexity, expense, and the need to wait longer for results, this creates a significant incentive and impetus to invent and adopt the notion of developing machine learning because it is simple. This study intended to increase the accuracy of machine-learning approaches for land use/land cover classification using Sentinel-2A, and Landsat-8 satellites. This study aimed to implement a proposed method, neural-based with object-based, to produce a model addressed by artificial neural networks (limited parameters) with random forest (hyperparameter) called ANN_RF. This study used multispectral satellite images (Sentinel-2A and Landsat-8) and a normalized digital elevation model as input datasets for the Sanaa city map of 2016. The results showed that the accuracy of the proposed model (ANN_RF) is better than the ANN classifier with the Sentinel-2A and Landsat-8 satellites individually, which may contribute to the development of machine learning through newer researchers and specialists; it also conventionally developed traditional artificial neural networks with seven to ten layers but with access to 1,000s and millions of simulated neurons without resorting to deep learning techniques (ANN_RF).
WOS:000922229900001
</snippet>
</document>

<document id="648">
<title>OpenStreetMap: Challenges and Opportunities in Machine Learning and Remote Sensing</title>
<url>http://dx.doi.org/10.1109/MGRS.2020.2994107</url>
<snippet>OpenStreetMap (OSM) is a community-based, freely available, editable map service created as an alternative to authoritative sources. Given that it is edited mainly by volunteers with different mapping skills, the completeness and quality of its annotations are heterogeneous across different geographical locations. Despite that, OSM has been widely used in several applications in geosciences, Earth observation, and environmental sciences. In this article, we review recent methods based on machine learning to improve and use OSM data. Such methods aim to either 1) improve the coverage and quality of OSM layers, typically by using geographic information systems (GISs) and remote sensing technologies, or 2) use the existing OSM layers to train models based on image data to serve applications such as navigation and land use classification. We believe that OSM (as well as other sources of open land maps) can change the way we interpret remote sensing data and that the synergy with machine learning can scale participatory mapmaking and its quality to the level needed for global and up-to-date land mapping. A preliminary version of this manuscript was presented in [120].
WOS:000641764700010
</snippet>
</document>

<document id="649">
<title>Injecting spectral indices to transferable convolutional neural network under imbalanced and noisy labels for Landsat image classification</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2036833</url>
<snippet>Stable and continuous remote sensing land-cover mapping is important for agriculture, ecosystems, and land management. Convolutional neural networks (CNNs) are promising methods for achieving this goal. However, the large number of high-quality training samples required to train a CNN is difficult to acquire. In practice, imbalanced and noisy labels originating from existing land-cover maps can be used as alternatives. Experiments have shown that the inconsistency in the training samples has a significant impact on the performance of the CNN. To overcome this drawback, a method is proposed to inject highly consistent information into the network, to learn general and transferable features to alleviate the impact of imperfect training samples. Spectral indices are important features that can provide consistent information. These indices can be fused with CNN feature maps which utilize information entropy to choose the most appropriate CNN layer, to compensate for the inconsistency caused by the imbalanced, noisy labels. The proposed transferable CNN, tested with imbalanced and noisy labels for inter-regional Landsat time-series, not only is superior in terms of accuracy for land-cover mapping but also demonstrates excellent transferability between regions in both time series and cross-regional Landsat image classification.
WOS:000759163100001
</snippet>
</document>

<document id="650">
<title>Spatial-Temporal Feature Fusion Neural Network for EEG-Based Emotion Recognition</title>
<url>http://dx.doi.org/10.1109/TIM.2022.3165280</url>
<snippet>The temporal and spatial information of electroencephalogram (EEG) are essential for the emotion recognition model to learn the discriminative features. Hence, we propose a novel hybrid spatial-temporal feature fusion neural network (STFFNN) to extract the discriminative features and integrate complementary information. The generated power topographic maps, which capture dependencies among the electrodes, are fed to convolutional neural network (CNN) for spatial feature learning. Furthermore, instance normalizations (INs) and batch normalizations (BNs) within the CNN are appropriately combined to alleviate the individual difference and preserve the domain-invariant information. Meanwhile, a feedforward network is adopted for temporal feature learning. Due to the high dimensionality of EEG features, we propose a grid-search-based configurational optimization method to robustly reduce the dimensionality. Finally, inspired by the multimodal fusion strategies that leverage the complementarity of data to obtain more robust predictions, we utilize a bidirectional long short-term memory (Bi-LSTM) network for temporal and spatial feature fusion. To validate the effectiveness of the proposed method, the tenfold cross-validation experiments and subject-dependent experiments are both conducted on the DEAP database. The experimental results demonstrate that the proposed method achieves outstanding performance in emotion recognition with arousal and valence level.
WOS:000784206300010
</snippet>
</document>

<document id="651">
<title>Spectral and Temporal Feature Learning With Two-Stream Neural Networks for Mental Workload Assessment</title>
<url>http://dx.doi.org/10.1109/TNSRE.2019.2913400</url>
<snippet>Peoples mental workload profoundly affects their work efficiency and health. Mental workload assessment can be used to effectively avoid serious accidents caused by excessive mental workload. Both electroencephalogram (EEG) spectral features and its temporal features have proven to be useful in addressing this problem. The fusion of the two types of features can provide rich distinguishing information for improving mental workload assessment. Benefiting from the progress of deep learning, this study proposes the two-stream neural networks (TSNN) for fusing the two types of EEG features. Compared with hand-crafted features, the TSNN can learn and fuse EEG features from the spectral and temporal dimensions automatically without prior knowledge. The TSNN includes a spectral stream and a temporal stream. Each stream consists of a convolutional neural network (CNN) and a temporal convolutional network (TCN) to learn spectral or temporal features from EEG topographic maps. To fuse the learned spectral and temporal information, we concatenate the output of the two streams prior to the fully connected layer. EEG data were collected from 17 subjects who performed n-back tasks with easy, medium, and hard difficulty levels, leading to a three-class mental workload classification. The results show that the TSNN achieves an average accuracy of 91.9&#37;, which is a significant improvement over baseline classifiers based on hand-crafted features. The TSNN also outperforms state-of-the-art deep learning methods developed for EEG classification. The results indicate that the proposed structure is promising for fusing spectral and temporal features for mental workload assessment. In addition, it provides a high-precision approach for potential applications during cognitive activities.
WOS:000471121000005
</snippet>
</document>

<document id="652">
<title>Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.02.006</url>
<snippet>Unprecedented urbanization in particular in countries of the global south result in informal urban development processes, especially in mega cities. With an estimated 1 billion slum dwellers globally, the United Nations have made the fight against poverty the number one sustainable development goal. To provide better infrastructure and thus a better life to slum dwellers, detailed information on the spatial location and size of slums is of crucial importance. In the past, remote sensing has proven to be an extremely valuable and effective tool for mapping slums. The nature of used mapping approaches by machine learning, however, made it necessary to invest a lot of effort in training the models. Recent advances in deep learning allow for transferring trained fully convolutional networks (FCN) from one data set to another. Thus, in our study we aim at analyzing transfer learning capabilities of FCNs to slum mapping in various satellite images. A model trained on very high resolution optical satellite imagery from QuickBird is transferred to Sentinel-2 and TerraSAR-X data. While free-of-charge Sentinel 2 data is widely available, its comparably lower resolution makes slum mapping a challenging task. TerraSAR-X data on the other hand, has a higher resolution and is considered a powerful data source for infra-urban structure analysis. Due to the different image characteristics of SAR compared to optical data, however, transferring the model could not improve the performance of semantic segmentation but we observe very high accuracies for mapped slums in the optical data: QuickBird image obtains 86-88&#37; (positive prediction value and sensitivity) and a significant increase for Sentinel-2 applying transfer learning can be observed (from 38 to 55&#37; and from 79 to 85&#37; for PPV and sensitivity, respectively). Using transfer learning proofs extremely valuable in retrieving information on small-scaled urban structures such as slum patches even in satellite images of decametric resolution.
WOS:000464088400005
</snippet>
</document>

<document id="653">
<title>ColorMapGAN: Unsupervised Domain Adaptation for Semantic Segmentation Using Color Mapping Generative Adversarial Networks</title>
<url>http://dx.doi.org/10.1109/TGRS.2020.2980417</url>
<snippet>Due to the various reasons, such as atmospheric effects and differences in acquisition, it is often the case that there exists a large difference between the spectral bands of satellite images collected from different geographic locations. The large shift between the spectral distributions of training and test data causes the current state-of-the-art supervised learning approaches to output unsatisfactory maps. We present a novel semantic segmentation framework that is robust to such a shift. The key component of the proposed framework is color mapping generative adversarial networks (ColorMapGANs) that can generate fake training images that are semantically exactly the same as training images, but whose spectral distribution is similar to the distribution of the test images. We then use the fake images and the ground truth for the training images to fine-tune the already trained classifier. Contrary to the existing generative adversarial networks (GANs), the generator in ColorMapGAN does not have any convolutional or pooling layers. It learns to transform the colors of the training data to the colors of the test data by performing only one elementwise matrix multiplication and one matrix-addition operation. Due to the architecturally simple but powerful design of ColorMapGAN, the proposed framework outperforms the existing approaches with a large margin in terms of both accuracy and computational complexity.
WOS:000573923100031
</snippet>
</document>

<document id="654">
<title>Assessing the Impact of Neighborhood Size on Temporal Convolutional Networks for Modeling Land Cover Change</title>
<url>http://dx.doi.org/10.3390/rs14194957</url>
<snippet>Land cover change (LCC) studies are increasingly using deep learning (DL) modeling techniques. Past studies have leveraged temporal or spatiotemporal sequences of historical LC data to forecast changes with DL models. However, these studies do not adequately assess the association between neighborhood size and DL model capability to forecast LCCs, where neighborhood size refers to the spatial extent captured by each data sample. The objectives of this research study were to: (1) evaluate the effect of neighborhood size on the capacity of DL models to forecast LCCs, specifically Temporal Convolutional Networks (TCN) and Convolutional Neural Networks (CNN-TCN), and (2) assess the effect of auxiliary spatial variables on model capacity to forecast LCCs. First, each model type and neighborhood setting configuration was assessed using data derived from multitemporal MODIS LC for the Regional District of Bulkley-Nechako, Canada, comparing subareas exhibiting different amounts of LCCs with trends obtained for the full region. Next, outcomes were compared with three other study regions. The modeling results were evaluated with three-map comparison measures, where the real-world LC for the next timestep, the real-world LC for the previous timestep, and the forecasted LC for the next year were used to calculate correctly transitioned areas. Across all regions explored, it was observed that increasing neighborhood sizes improved the DL models capabilities to forecast short-term LCCs. CNN-TCN models forecasted the most correct LCCs for several regions while reducing error due to quantity when provided additional spatial variables. This study contributes to the systematic exploration of neighborhood sizes on selected spatiotemporal DL techniques for geographic applications.
WOS:000867212300001
</snippet>
</document>

<document id="655">
<title>CEGFNet: Common Extraction and Gate Fusion Network for Scene Parsing of Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3109626</url>
<snippet>Scene parsing of high spatial resolution (HSR) remote sensing images has achieved notable progress in recent years by the adoption of convolutional neural networks. However, for scene parsing of multimodal remote sensing images, effectively integrating complementary information remains challenging. For instance, the decrease in feature map resolution through a neural network causes loss of spatial information, likely leading to blurred object boundaries and misclassification of small objects. In addition, object scales on a remote sensing image vary substantially, undermining the parsing performance. To solve these problems, we propose an end-to-end common extraction and gate fusion network (CEGFNet) to capture both high-level semantic features and low-level spatial details for scene parsing of remote sensing images. Specifically, we introduce a gate fusion module to extract complementary features from spectral data and digital surface model data. A gate mechanism removes redundant features in the data stream and extracts complementary features that improve multimodal feature fusion. In addition, a global context module and a multilayer aggregation decoder handle scale variations between objects and the loss of spatial details due to downsampling, respectively. The proposed CEGFNet was quantitatively evaluated on benchmark scene parsing datasets containing HSR remote sensing images, and it achieved state-of-the-art performance.
WOS:000732773500001
</snippet>
</document>

<document id="656">
<title>Superpixel-Based Shallow Convolutional Neural Network (SSCNN) for Scanned Topographic Map Segmentation</title>
<url>http://dx.doi.org/10.3390/rs12203421</url>
<snippet>Motivated by applications in topographic map information extraction, our goal was to discover a practical method for scanned topographic map (STM) segmentation. We present an advanced guided watershed transform (AGWT) to generate superpixels on STM. AGWT utilizes the information from both linear and area elements to modify detected boundary maps and sequentially achieve superpixels based on the watershed transform. With achieving an average of 0.06 on under-segmentation error, 0.96 on boundary recall, and 0.95 on boundary precision, it has been proven to have strong ability in boundary adherence, with fewer over-segmentation issues. Based on AGWT, a benchmark for STM segmentation based on superpixels and a shallow convolutional neural network (SCNN), termed SSCNN, is proposed. There are several notable ideas behind the proposed approach. Superpixels are employed to overcome the false color and color aliasing problems that exist in STMs. The unification method of random selection facilitates sufficient training data with little manual labeling while keeping the potential color information of each geographic element. Moreover, with the small number of parameters, SCNN can accurately and efficiently classify those unified pixel sequences. The experiments show that SSCNN achieves an overall F1 score of 0.73 on our STM testing dataset. They also show the quality of the segmentation results and the short run time of this approach, which makes it applicable to full-size maps.
WOS:000585673300001
</snippet>
</document>

<document id="657">
<title>An attention-based deep learning model for citywide traffic flow forecasting</title>
<url>http://dx.doi.org/10.1080/17538947.2022.2028912</url>
<snippet>Prompt and accurate traffic flow forecasting is a key foundation of urban traffic management. However, the flows in different areas and feature channels (inflow/outflow) may correspond to different degrees of importance in forecasting flows. Many forecasting models inadequately consider this heterogeneity, resulting in decreased predictive accuracy. To overcome this problem, an attention-based hybrid spatiotemporal residual model assisted by spatial and channel information is proposed in this study. By assigning different weights (attention levels) to different regions, the spatial attention module selects relatively important locations from all inputs in the modeling process. Similarly, the channel attention module selects relatively important channels from the multichannel feature map in the modeling process by assigning different weights. The proposed model provides effective selection and attention results for key areas and channels, respectively, during the forecasting process, thereby decreasing the computational overhead and increasing the accuracy. In the case involving Beijing, the proposed model exhibits a 3.7&#37; lower prediction error, and its runtime is 60.9&#37; less the model without attention, indicating that the spatial and channel attention modules are instrumental in increasing the forecasting efficiency. Moreover, in the case involving Shanghai, the proposed model outperforms other models in terms of generalizability and practicality.
WOS:000751785500001
</snippet>
</document>

<document id="658">
<title>GRAINet: mapping grain size distributions in river beds from UAV images with convolutional neural networks</title>
<url>http://dx.doi.org/10.5194/hess-25-2567-2021</url>
<snippet>Grain size analysis is the key to understand the sediment dynamics of river systems. We propose GRAINet, a data-driven approach to analyze grain size distributions of entire gravel bars based on georeferenced UAV images. A convolutional neural network is trained to regress grain size distributions as well as the characteristic mean diameter from raw images. GRAINet allows for the holistic analysis of entire gravel bars, resulting in (i) high-resolution estimates and maps of the spatial grain size distribution at large scale and (ii) robust grading curves for entire gravel bars. To collect an extensive training dataset of 1491 samples, we introduce digital line sampling as a new annotation strategy. Our evaluation on 25 gravel bars along six different rivers in Switzerland yields high accuracy: the resulting maps of mean diameters have a mean absolute error (MAE) of 1.1 cm, with no bias. Robust grading curves for entire gravel bars can be extracted if representative training data are available. At the gravel bar level the MAE of the predicted mean diameter is even reduced to 0.3 cm, for bars with mean diameters ranging from 1.3 to 29.3 cm. Extensive experiments were carried out to study the quality of the digital line samples, the generalization capability of GRAINet to new locations, the model performance with respect to human labeling noise, the limitations of the current model, and the potential of GRAINet to analyze images with low resolutions.
WOS:000654339500001
</snippet>
</document>

<document id="659">
<title>HMSM-Net: Hierarchical multi-scale matching network for disparity estimation of high-resolution satellite stereo images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.04.020</url>
<snippet>Disparity estimation of satellite stereo images is an essential and challenging task in photogrammetry and remote sensing. Recent researches have greatly promoted the development of disparity estimation algorithms by using CNN (Convolutional Neural Networks) based deep learning techniques. However, it is still difficult to handle intractable regions that are mainly caused by occlusions, disparity discontinuities, texture-less areas, and re-petitive patterns. Besides, the lack of training datasets for satellite stereo images remains another major issue that blocks the usage of CNN techniques due to the difficulty of obtaining ground-truth disparities. In this paper, we propose an end-to-end disparity learning model, termed hierarchical multi-scale matching network (HMSM-Net), for the disparity estimation of high-resolution satellite stereo images. First, multi-scale cost volumes are con-structed by using pyramidal features that capture spatial information of multiple levels, which learn corre-spondences at multiple scales and enable HMSM-Net to be more robust in intractable regions. Second, stereo matching is executed in a hierarchical coarse-to-fine manner by applying supervision to each scale, which allows a lower scale to act as prior knowledge and guides a higher scale to attain finer matching results. Third, a refinement module that incorporates the intensity and gradient information of the input left image is designed to regress a detailed full-resolution disparity map for local structure preservation. For network training and testing, a dense stereo matching dataset is created and published by using GaoFen-7 satellite stereo images. Finally, the proposed network is evaluated on the Urban Semantic 3D and GaoFen-7 datasets. Experimental results demonstrate that HMSM-Net achieves superior accuracy compared with state-of-the-art methods, and the improvement on intractable regions is noteworthy. Additionally, results and comparisons of different methods on the GaoFen-7 dataset show that it can severs as a challenging benchmark for performance assessment of methods applied to disparity estimation of satellite stereo images. The source codes and evaluation dataset are made publicly available at https://github.com/Sheng029/HMSM-Net.
WOS:000799999700002
</snippet>
</document>

<document id="660">
<title>PSTNet: Progressive Sampling Transformer Network for Remote Sensing Image Change Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3204191</url>
<snippet>Remote sensing change detection (CD) is to use multitemporal remote sensing data to extract change information by using a variety of image processing and pattern recognition methods, and quantitatively analyze and determine the characteristics and processes of surface changes. In recent research on CD, how to more accurately segment objects and how to extract and effectively link spatiotemporal information are important parts. To achieve this, we propose a progressive sampling (PS) transformer network for remote sensing image CD, which continuously extracts and optimizes feature information in an iterative manner, so that pixels can establish better connections in the spatial domain to model the context. Our intuition is that, through this iterative sampling method, the parts of interest in the image can be gradually extracted. This allows subsequent processing to be more focused on useful areas, which in turn reduces interference from uninteresting parts, and the information after PS will be generalize into several tokens containing rich semantic information. Using the excellent modeling ability of the transformer, the optimized tokens are mapped back to the original image features to achieve the purpose of segmenting accurate difference images. We conducted extensive experiments on three CD datasets, LEVIR-CD, DSIFN-CD, and WHU-CD, and achieved evaluation scores of 90.73/84.11, 80.10/68.93, and 91.67/85.15 on F1-score and IoU metrics, respectively. Notably, the convolutional neural network (CNN) backbone of our network uses only a simplified ResNet model, without using structurally complex frameworks, such as FPN and Unet, but our model uses PS module and transformer to achieve better performance than the recent advanced CD models.
WOS:000866530700004
</snippet>
</document>

<document id="661">
<title>Longitudinal Speech Biomarkers for Automated Alzheimer's Detection</title>
<url>http://dx.doi.org/10.3389/fcomp.2021.624694</url>
<snippet>We introduce a novel audio processing architecture, the Open Voice Brain Model (OVBM), improving detection accuracy for Alzheimers (AD) longitudinal discrimination from spontaneous speech. We also outline the OVBM design methodology leading us to such architecture, which in general can incorporate multimodal biomarkers and target simultaneously several diseases and other AI tasks. Key in our methodology is the use of multiple biomarkers complementing each other, and when two of them uniquely identify different subjects in a target disease we say they are orthogonal. We illustrate the OBVM design methodology by introducing sixteen biomarkers, three of which are orthogonal, demonstrating simultaneous above state-of-the-art discrimination for two apparently unrelated diseases such as AD and COVID-19. Depending on the context, throughout the paper we use OVBM indistinctly to refer to the specific architecture or to the broader design methodology. Inspired by research conducted at the MIT Center for Brain Minds and Machines (CBMM), OVBM combines biomarker implementations of the four modules of intelligence: The brain OS chunks and overlaps audio samples and aggregates biomarker features from the sensory stream and cognitive core creating a multi-modal graph neural network of symbolic compositional models for the target task. In this paper we apply the OVBM design methodology to the automated diagnostic of Alzheimers Dementia (AD) patients, achieving above state-of-the-art accuracy of 93.8&#37; using only raw audio, while extracting a personalized subject saliency map designed to longitudinally track relative disease progression using multiple biomarkers, 16 in the reported AD task. The ultimate aim is to help medical practice by detecting onset and treatment impact so that intervention options can be longitudinally tested. Using the OBVM design methodology, we introduce a novel lung and respiratory tract biomarker created using 200,000+ cough samples to pre-train a model discriminating cough cultural origin. Transfer Learning is subsequently used to incorporate features from this model into various other biomarker-based OVBM architectures. This biomarker yields consistent improvements in AD detection in all the starting OBVM biomarker architecture combinations we tried. This cough dataset sets a new benchmark as the largest audio health dataset with 30,000+ subjects participating in April 2020, demonstrating for the first time cough cultural bias.
WOS:000705962900001
</snippet>
</document>

<document id="662">
<title>Optimizing Image Size of Convolutional Neural Networks for Producing Remote Sensing-based Thematic Map</title>
<url>http://dx.doi.org/10.7780/kjrs.2018.34.4.8</url>
<snippet>This study aims to develop a methodology of convolutional neural networks (CNNs) to produce thematic maps from remote sensing data. Optimizing the image size for CNNs was studied, since the size of the image affects to accuracy, working as hyper-parameter. The selected study area is Mt. Ung, located in Dangjin-si, Chungcheongnam-do, South Korea, consisting of both coniferous forest and deciduous forest. Spatial structure analysis and the classification of forest type using CNNs was carried in the study area at a diverse range of scales. As a result of the spatial structure analysis, it was found that the local variance (LV) was high, in the range of 7.65 m to 18.87 m, meaning that the size of objects in the image is likely to be with in this range. As a result of the classification, the image measuring 15.81 m, belonging to the range with highest LV values, had the highest classification accuracy of 85.09&#37;. Also, there was a positive correlation between LV and the accuracy in the range under 15.81 m, which was judged to be the optimal image size. Therefore, the trial and error selection of the optimum image size could be minimized by choosing the result of the spatial structure analysis as the starting point. This study estimated the optimal image size for CNNs using spatial structure analysis and found that this can be used to promote the application of deep-learning in remote sensing.
WOS:000447067400008
</snippet>
</document>

<document id="663">
<title>MSFTrans: a multi-task frequency-spatial learning transformer for building extraction from high spatial resolution remote sensing images</title>
<url>http://dx.doi.org/10.1080/15481603.2022.2143678</url>
<snippet>Building extraction is significant in urban planning, economic evaluation, and driverless technology development. However, automatic building extraction from high spatial resolution remote sensing images has been a challenging task due to the various building shapes and colors, imaging conditions, and complex background objects. Current methods in building extraction are generally based on deep convolution networks, and they mostly use an encoder-decoder architecture, wherein detailed building features and small buildings are easily omitted in continuous convolution operations. Moreover, buildings with blurred boundaries are only completely extracted with difficulty. To meet these challenges, we propose a multi-task architecture of frequency-spatial learning Transformer to extract buildings from high spatial resolution remote sensing images. Different from current architecture, we designed a frequency-spatial learning module in the framework of multi-task to synthesize the multi-scale spatial features and frequency decomposition features of high-resolution image. Spiking convolution is proposed in this study to enhance the frequency features of buildings by mimicking the neural transmission in human brains. In this way, multi-scale building features can be better preserved and distinguished from background objects. Moreover, a masked-attention Transformer is adopted to improve multi-scale building mask prediction accuracy by synthesizing successive pixel-wise up-sampled feature maps. We also propose a strategy to evaluate the practical transferability of the proposed method by mimicking practical application cases through training and evaluating images with different spatial resolutions from different study areas and datasets. Experiments using five public building datasets (WHU-Building Satellite Dataset I, WHU-Building Satellite Dataset II, Massachusetts Buildings Dataset, Inria Aerial Image Dataset, xBD Building Dataset) demonstrate the strong potential applicability of our proposed method for practical application cases. Our method outperforms five recently proposed state-of-the-art semantic segmentation methods with 36.60&#37; accuracy improvement on extracted buildings and approximately 53.55&#37; recall progress in extracting small building instances. The implementation code will be released after the paper is published.
WOS:000884571900001
</snippet>
</document>

<document id="664">
<title>Multispectral Pansharpening Based on Multisequence Convolutional Recurrent Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3218367</url>
<snippet>Multispectral (MS) pansharpening is defined as the fusion of spatial information in panchromatic (PAN) image and spectral information in MS image. In this work, we propose an MS pansharpening based on multisequence convolutional recurrent neural network (MCRNN). The proposed MCRNN contains two subnetworks (shallow feature extraction subnetwork and deep feature fusion subnetwork). In the shallow feature extraction subnetwork, PAN and MS images are superimposed in the spectral dimension as multisequence data. A convolutional neural network based on residual learning is then used to obtain the feature maps from multisequence data. In the deep feature fusion subnetwork, since MS and PAN images are highly correlated, a convolutional recurrent neural network belonging to recurrent neural network is used to model adjacent and across-band relationships between these feature maps to capture the local and global correlations of the features in different bands. The global average pooling is then performed on the output results to yield the pansharpening result. Several datasets are tested at reduced and full-resolution experiments, the experimental results show that the performance of the proposed MCRNN is superior to the traditional pansharpening methods.
WOS:000882001000007
</snippet>
</document>

<document id="665">
<title>A Multi-Kernel Mode Using a Local Binary Pattern and Random Patch Convolution for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3076198</url>
<snippet>With the development of deep learning technology, more and more scholars have applied it to hyperspectral image (HSI) classification to improve classification accuracy. However, these deep-learning methods not only take a lot of time in the pre-training phase, but also have relatively limited classification performance when there are fewer labeled samples. In order to improve classification performance while reducing costs, this article proposes a multikernel method based on a local binary pattern and random patches (LBPRP-MK), which integrates a local binary pattern (LBP) and deep learning into a multiple-kernel framework. First, we use LBP and hierarchical convolutional neural networks to extract local textural features and multilayer convolutional features, respectively. The convolution kernel for the convolution operation is obtained from the original image using a random strategy without training. Then, we input local textural features, multilayer convolutional features, and spectral features obtained from the original image into the radial basis function to obtain three kernel functions. Finally, the three kernel functions are merged into a multikernel function according to their optimal weights under the composite kernel strategy. This multikernel function is used as the input for the support vector machine to obtain the classification result map. Experiments show that compared with other HSI classification methods, the proposed method achieves better classification performance on three HSI datasets.
WOS:000652054400004
</snippet>
</document>

<document id="666">
<title>Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery</title>
<url>http://dx.doi.org/10.1145/3308561.3353798</url>
<snippet>Recent work has applied machine learning methods to automatically find and/or assess pedestrian infrastructure in online map imagery (e.g., satellite photos, streetscape panoramas). While promising, these methods have been limited by two interrelated issues: small training sets and the choice of machine learning model. In this paper, aided by the recently released Project Sidewalk dataset of 300,000+ image-based sidewalk accessibility labels, we present the first examination of deep learning to automatically assess sidewalks in Google Street View (GSV) panoramas. Specifically, we investigate two application areas: automatically validating crowdsourced labels and automatically labeling sidewalk accessibility issues. For both tasks, we introduce and use a residual neural network (ResNet) modified to support both image and non-image (contextual) features (e.g., geography). We present an analysis of performance, the effect of our non-image features and training set size, and cross-city generalizability. Our results significantly improve on prior automated methods and, in some cases, meet or exceed human labeling performance.
WOS:000522450600018
</snippet>
</document>

<document id="667">
<title>Imaging Reality and Abstraction an Exploration of Natural and Symbolic Patterns</title>
<url>http://dx.doi.org/10.5220/0010295704150422</url>
<snippet>Understanding visual symbols is a strictly human skill, as opposed to comprehending natural scenes-which is an essential survival skill, common to many species. As an illustration of the natural vs. symbolic dichotomy, selective features are computed for differentiating a satellite photograph from a map of the same geographical region. Images of physical scenes /objects are currently captured in all parts of the electromagnetic spectrum. Symbols, whether produced by man or machine, are almost always imaged in the visible range. Although natural and symbolic images differ in many ways, there is no universal set of differentiating characteristics. With respect to the traditional branches of pattern recognition, it is tempting to suggest that statistical, neural network and genetic/evolutionary pattern recognition methods are eminently suitable for images of scenes and simple symbols, whereas structural and syntactic approaches are best for more complex, composite graphical symbols.
WOS:000668577400044
</snippet>
</document>

<document id="668">
<title>An efficient saliency prediction model for Unmanned Aerial Vehicle video</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.10.008</url>
<snippet>Visual saliency prediction plays an important role in Unmanned Aerial Vehicle (UAV) video analysis tasks. In this paper, an efficient saliency prediction model for UAV video is proposed based on spatial-temporal features, prior information and the relationship of frames. It can achieve high efficiency by designing a simplified network model. Since UAV videos usually cover a wide range of scenes containing various background disturbances, a cascading architecture module is proposed for feature extraction from coarse to fine, in which a saliency related feature sub-network is utilized to obtain useful clues from each frame, then a new convolution block is designed to capture spatial-temporal features. This structure can achieve advanced performance and high speed based on a 2D CNN framework. Moreover, a multi-stream prior module is proposed to model the bias phenomenon in viewing behavior for UAV video scenes. It can automatically learn prior information based on the video context, and can also combine other priors. Finally, based on the spatial-temporal features and learned priors, a temporal weighted average module is proposed to model the inter-frame relationship and generate the final saliency map, which can make the generated saliency maps look smoother in the temporal dimension. The proposed method is compared with 17 state-of-the-art models on two public UAV video saliency prediction datasets. The experimental results demonstrate that our model outperforms other competitors. Source code is available at: https://github.com/zhangkao/IIP_UAVSal_Saliency.
WOS:000882413500002
</snippet>
</document>

<document id="669">
<title>Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.02.017</url>
<snippet>Cloud detection is an important preprocessing step for the precise application of optical satellite imagery. In this paper, we propose a deep learning based cloud detection method named multi-scale convolutional feature fusion (MSCFF) for remote sensing images of different sensors. In the network architecture of MSCFF, the symmetric encoder-decoder module, which provides both local and global context by densifying feature maps with trainable convolutional filter banks, is utilized to extract multi-scale and high-level spatial features. The feature maps of multiple scales are then up-sampled and concatenated, and a novel multi-scale feature fusion module is designed to fuse the features of different scales for the output. The two output feature maps of the network are cloud and cloud shadow maps, which are in turn fed to binary classifiers outside the model to obtain the final cloud and cloud shadow mask. The MSCFF method was validated on hundreds of globally distributed optical satellite images, with spatial resolutions ranging from 0.5 to 50 m, including Landsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and collected high-resolution images exported from Google Earth. The experimental results show that MSCFF achieves a higher accuracy than the traditional rule-based cloud detection methods and the state-of-the-art deep learning models, especially in bright surface covered areas. The effectiveness of MSCFF means that it has great promise for the practical application of cloud detection for multiple types of medium and high-resolution remote sensing images. Our established global high-resolution cloud detection validation dataset has been made available online (http://sendimage.whu.edu.cn/en/mscff/).
WOS:000464088400014
</snippet>
</document>

<document id="670">
<title>Single-Tree Detection in High-Resolution Remote-Sensing Images Based on a Cascade Neural Network</title>
<url>http://dx.doi.org/10.3390/ijgi7090367</url>
<snippet>Traditional single-tree detection methods usually need to set different thresholds and parameters manually according to different forest conditions. As a solution to the complicated detection process for non-professionals, this paper presents a single-tree detection method for high-resolution remote-sensing images based on a cascade neural network. In this method, we firstly calibrated the tree and non-tree samples in high-resolution remote-sensing images to train a classifier with the backpropagation (BP) neural network. Then, we analyzed the differences in the first-order statistic features, such as energy, entropy, mean, skewness, and kurtosis of the tree and non-tree samples. Finally, we used these features to correct the BP neural network model and build a cascade neural network classifier to detect a single tree. To verify the validity and practicability of the proposed method, six forestlands including two areas of oil palm in Thailand, and four areas of small seedlings, red maples, or longan trees in China were selected as test areas. The results from different methods, such as the region-growing method, template-matching method, BP neural network, and proposed cascade-neural-network method were compared considering these test areas. The experimental results show that the single-tree detection method based on the cascade neural network exhibited the highest root mean square of the matching rate (RMS_R-mat = 90&#37;) and matching score (RMS_M = 68) in all the considered test areas.
WOS:000445767900035
</snippet>
</document>

<document id="671">
<title>MSLWENet: A Novel Deep Learning Network for Lake Water Body Extraction of Google Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/rs12244140</url>
<snippet>Lake water body extraction from remote sensing images is a key technique for spatial geographic analysis. It plays an important role in the prevention of natural disasters, resource utilization, and water quality monitoring. Inspired by the recent years of research in computer vision on fully convolutional neural networks (FCN), an end-to-end trainable model named the multi-scale lake water extraction network (MSLWENet) is proposed. We use ResNet-101 with depthwise separable convolution as an encoder to obtain the high-level feature information of the input image and design a multi-scale densely connected module to expand the receptive field of feature points by different dilation rates without increasing the computation. In the decoder, the residual convolution is used to abstract the features and fuse the features at different levels, which can obtain the final lake water body extraction map. Through visual interpretation of the experimental results and the calculation of the evaluation indicators, we can see that our model extracts the water bodies of small lakes well and solves the problem of large intra-class variance and small inter-class variance in the lakes water bodies. The overall accuracy of our model is up to 98.53&#37; based on the evaluation indicators. Experimental results demonstrate that the MSLWENet, which benefits from the convolutional neural network, is an excellent lake water body extraction network.
WOS:000603299500001
</snippet>
</document>

<document id="672">
<title>SPATIAL MORPHING KERNEL REGRESSION FOR FEATURE INTERPOLATION</title>
<url>http://dx.doi.org/</url>
<snippet>In recent years, geotagged social media has become popular as a novel source for geographic knowledge discovery. Ground-level images and videos provide a different perspective than overhead imagery and can be applied to a range of applications such as land use mapping, activity detection, pollution mapping, etc. The sparse and uneven distribution of this data presents a problem, however, for generating dense maps. We therefore investigate the problem of spatially interpolating the high-dimensional features extracted from sparse social media to enable dense labeling using standard classifiers. Further, we show how prior knowledge about region boundaries can be used to improve the interpolation through spatial morphing kernel regression. We show that an interpolate-then-classify framework can produce dense maps from sparse observations but that care must be taken in choosing the interpolation method. We also show that the spatial morphing kernel improves the results.
WOS:000455181502061
</snippet>
</document>

<document id="673">
<title>MASNET: IMPROVE PERFORMANCE OF SIAMESE NETWORKS WITH MUTUALATTENTION FOR REMOTE SENSING CHANGE DETECTION TASKS</title>
<url>http://dx.doi.org/10.5194/isprs-annals-V-3-2022-681-2022</url>
<snippet>Siamese networks are widely used for remote sensing change detection tasks. A vanilla siamese network has two identical feature extraction branches which share weights, these two branches work independently and the feature maps are not fused until about to be sent to a decoder head. However we find that it is critical to exchange information between two feature extraction branches at early stage for change detection task. In this work we present Mutual-Attention Siamese Network (MASNet), a general siamese network with mutual-attention plug-in, so to exchange information between the two feature extraction branches. We show that our modification improve the performance of siamese networks on multi change detection datasets, and it works for both convolutional neural network and visual transformer.
WOS:000855203200092
</snippet>
</document>

<document id="674">
<title>Multimodal deep learning from satellite and street-level imagery for measuring income, overcrowding, and environmental deprivation in urban areas</title>
<url>http://dx.doi.org/10.1016/j.rse.2021.112339</url>
<snippet>Data collected at large scale and low cost (e.g. satellite and street level imagery) have the potential to substantially improve resolution, spatial coverage, and temporal frequency of measurement of urban inequalities. Multiple types of data from different sources are often available for a given geographic area. Yet, most studies utilize a single type of input data when making measurements due to methodological difficulties in their joint use. We propose two deep learning-based methods for jointly utilizing satellite and street level imagery for measuring urban inequalities. We use London as a case study for three selected outputs, each measured in decile classes: income, overcrowding, and environmental deprivation. We compare the performances of our proposed multimodal models to corresponding unimodal ones using mean absolute error (MAE). First, satellite tiles are appended to street level imagery to enhance predictions at locations where street images are available leading to improvements in accuracy by 20, 10, and 9&#37; in units of decile classes for income, overcrowding, and living environment. The second approach, novel to the best of our knowledge, uses a U-Net architecture to make predictions for all grid cells in a city at high spatial resolution (e.g. for 3 m ? 3 m pixels in London in our experiments). It can utilize city wide availability of satellite images as well as more sparse information from street level images where they are available leading to improvements in accuracy by 6, 10, and 11&#37;. We also show examples of prediction maps from both approaches to visually highlight performance differences.
WOS:000632439100002
</snippet>
</document>

<document id="675">
<title>TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.07.007</url>
<snippet>Fine-grained semantic segmentation results are typically difficult to obtain for subdecimeter aerial imagery segmentation as a result of complex remote sensing content and optical conditions. Recently, convolutional neural networks (CNNs) have shown outstanding performance on this task. Although many deep neural network structures and techniques have been applied to improve accuracy, few have attended to improving the differentiation of easily confused classes. In this paper, we propose TreeUNet, a tool that uses an adaptive network to increase the classification rate at the pixel level. Specifically, based on a deep semantic model infrastructure, a Tree-CNN block in which each node represents a ResNeXt unit is constructed adaptively in accordance with the confusion matrix and the proposed TreeCutting algorithm. By transmitting feature maps through concatenating connections, the Tree-CNN block fuses multiscale features and learns best weights for the model. In experiments on the ISPRS two-dimensional Vaihingen and Potsdam semantic labelling datasets, the results obtained by TreeUNet are competitive among published state-of-the-art methods. Detailed comparison and analysis show that the improvement brought by the adaptive Tree-CNN block is significant.
WOS:000487765800001
</snippet>
</document>

<document id="676">
<title>Multilevel Feature Fusion-Based CNN for Local Climate Zone Classification From Sentinel-2 Images: Benchmark Results on the So2Sat LCZ42 Dataset</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2995711</url>
<snippet>As a unique classification scheme for urban forms and functions, the local climate zone (LCZ) system provides essential general information for any studies related to urban environments, especially on a large scale. Remote sensing data-based classification approaches are the key to large-scale mapping and monitoring of LCZs. The potential of deep learning-based approaches is not yet fully explored, even though advanced convolutional neural networks (CNNs) continue to push the frontiers for various computer vision tasks. One reason is that published studies are based on different datasets, usually at a regional scale, which makes it impossible to fairly and consistently compare the potential of different CNNs for real-world scenarios. This article is based on the big So2Sat LCZ42 benchmark dataset dedicated to LCZ classification. Using this dataset, we studied a range of CNNs of varying sizes. In addition, we proposed a CNN to classify LCZs from Sentinel-2 images, Sen2LCZ-Net. Using this base network, we propose fusing multilevel features using the extended Sen2LCZ-Net-MF. With this proposed simple network architecture, and the highly competitive benchmark dataset, we obtain results that are better than those obtained by the state-of-the-art CNNs, while requiring less computation with fewer layers and parameters. Large-scale LCZ classification examples of completely unseen areas are presented, demonstrating the potential of our proposed Sen2LCZ-Net-MF as well as the So2Sat LCZ42 dataset. We also intensively investigated the influence of network depth and width, and the effectiveness of the design choices made for Sen2LCZ-Net-MF. This article will provide important baselines for future CNN-based algorithm developments for both LCZ classification and other urban land cover land use classification. Code and pretrained models are available at https://github.com/ChunpingQiu/benchmark-on-So2SatLCZ42-dataset-a-simple-tour.
WOS:000544047400019
</snippet>
</document>

<document id="677">
<title>Mapping Rice Paddies in Complex Landscapes with Convolutional Neural Networks and Phenological Metrics</title>
<url>http://dx.doi.org/10.1080/15481603.2019.1658960</url>
<snippet>Rice mapping with remote sensing imagery provides an alternative means for estimating crop-yield and performing land management due to the large geographical coverage and low cost of remotely sensed data. Rice mapping in Southern China, however, is very difficult as rice paddies are patchy and fragmented, reflecting the undulating and varied topography. In addition, abandoned lands widely exist in Southern China due to rapid urbanization. Abandoned lands are easily confused with paddy fields, thereby degrading the classification accuracy of rice paddies in such complex landscape regions. To address this problem, the present study proposes an innovative method for rice mapping through combining a convolutional neural network (CNN) model and a decision tree (DT) method with phenological metrics. First, a pre-trained LeNet-5 Model using the UC Merced Dataset was developed to classify the cropland class from other land cover types, i.e. built-up, rivers, forests. Then, paddy rice field was separated from abandoned land in the cropland class using a DT model with phenological metrics derived from the time-series data of the normalized difference vegetation index (NDVI). The accuracy of the proposed classification methods was compared with three other classification techniques, namely, back propagation neural network (BPNN), original CNN, pre-trained CNN applied to HJ-1 A/B charge-coupled device (CCD) images of Zhuzhou City, Hunan Province, China. Results suggest that the proposed method achieved an overall accuracy of 93.56&#37;, much higher than those of other methods. This indicates that the proposed method can efficiently accommodate the challenges of rice mapping in regions with complex landscapes.
WOS:000484769200001
</snippet>
</document>

<document id="678">
<title>Detecting Asymptomatic Infections of Rice Bacterial Leaf Blight Using Hyperspectral Imaging and 3-Dimensional Convolutional Neural Network With Spectral Dilated Convolution</title>
<url>http://dx.doi.org/10.3389/fpls.2022.963170</url>
<snippet>Rice is one of the most important food crops for human beings. Its total production ranks third in the grain crop output. Bacterial Leaf Blight (BLB), as one of the three major diseases of rice, occurs every year, posing a huge threat to rice production and safety. There is an asymptomatic period between the infection and the onset periods, and BLB will spread rapidly and widely under suitable conditions. Therefore, accurate detection of early asymptomatic BLB is very necessary. The purpose of this study was to test the feasibility of detecting early asymptomatic infection of the rice BLB disease based on hyperspectral imaging and Spectral Dilated Convolution 3-Dimensional Convolutional Neural Network (SDC-3DCNN). First, hyperspectral images were obtained from rice leaves infected with the BLB disease at the tillering stage. The spectrum was smoothed by the Savitzky-Golay (SG) method, and the wavelength between 450 and 950 nm was intercepted for analysis. Then Principal Component Analysis (PCA) and Random Forest (RF) were used to extract the feature information from the original spectra as inputs. The overall performance of the SDC-3DCNN model with different numbers of input features and different spectral dilated ratios was evaluated. Lastly, the saliency map visualization was used to explain the sensitivity of individual wavelengths. The results showed that the performance of the SDC-3DCNN model reached an accuracy of 95.4427&#37; when the number of inputs is 50 characteristic wavelengths (extracted by RF) and the dilated ratio is set at 5. The saliency-sensitive wavelengths were identified in the range from 530 to 570 nm, which overlaps with the important wavelengths extracted by RF. According to our findings, combining hyperspectral imaging and deep learning can be a reliable approach for identifying early asymptomatic infection of the rice BLB disease, providing sufficient support for early warning and rice disease prevention.
WOS:000885364200001
</snippet>
</document>

<document id="679">
<title>Exploring Google Street View with deep learning for crop type mapping</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.11.022</url>
<snippet>Ground reference data are an essential prerequisite for supervised crop mapping. The lack of a low-cost and efficient ground referencing method results in pervasively limited reference data and hinders crop classification. In this study, we apply a convolutional neural network (CNN) model to explore the efficacy of automatic ground truthing via Google Street View (GSV) images in two distinct farming regions: Illinois and the Central Valley in California. We demonstrate the feasibility and reliability of our new ground referencing technique by performing pixel-based crop mapping at the state level using the cloud-based Google Earth Engine platform. The mapping results are evaluated using the United States Department of Agriculture (USDA) crop data layer (CDL) products. From similar to 130,000 GSV images, the CNN model identified similar to 9,400 target crop images. These images are well classified into crop types, including alfalfa, almond, corn, cotton, grape, rice, soybean, and pistachio. The overall GSV image classification accuracy is 92&#37; for the Central Valley and 97&#37; for Illinois. Subsequently, we shifted the image geographical coordinates 2-3 times in a certain direction to produce 31,829 crop reference points: 17,358 in Illinois, and 14,471 in the Central Valley. Evaluation of the mapping results with CDL products revealed satisfactory coherence. GSV-derived mapping results capture the general pattern of crop type distributions for 2011-2019. The overall agreement between CDL products and our mapping results is indicated by R-2 values of 0.44-0.99 for the Central Valley and 0.81-0.98 for Illinois. To show the applicational value of the proposed method in other countries, we further mapped rice paddy (2014-2018) in South Korea which yielded fairly well outcomes (R-2 = 0.91). These results indicate that GSV images used with a deep learning model offer an efficient and cost-effective alternative method for ground referencing, in many regions of the world.
WOS:000604406500019
</snippet>
</document>

<document id="680">
<title>A Spatiotemporal Recurrent Neural Network for Prediction of Atmospheric PM2.5: A Case Study of Beijing</title>
<url>http://dx.doi.org/10.1109/TCSS.2021.3056410</url>
<snippet>With rapid industrial development, air pollution problems, especially in urban and metropolitan centers, have become a serious societal problem and require our immediate attention and comprehensive solutions to protect human and animal health and the environment. Because bad air quality brings prominent effects on our daily life, how to forecast future air quality accurately and tenuously has emerged as a priority for guaranteeing the quality of human life in many urban areas worldwide. Existing models usually neglect the influence of wind and do not consider both distance and similarity to select the most related stations, which can provide significant information in prediction. Therefore, we propose a Geographic Self-Organizing Map (GeoSOM) spatiotemporal gated recurrent unit (GRU) model, which clusters all the monitor stations into several clusters by geographical coordinates and time-series features. For each cluster, we build a GRU model and weighted different models with the Gaussian vector weights to predict the target sequence. The experimental results on real air quality data in Beijing validate the superiority of the proposed method over a number of state-of-the-art ones in metrics, such as R-2, mean relative error (MRE), and mean absolute error (MAE). The MAE, MRE, and R-2 are 16.1, 0.79, and 035 at the Gucheng station and 19.53, 0.82, and 036 at the Dongsi station.
WOS:000655822700005
</snippet>
</document>

<document id="681">
<title>Tilt Correction Toward Building Detection of Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3083481</url>
<snippet>Building detection is a crucial task in the field of remote sensing, which can facilitate urban construction planning, disaster survey, and emergency landing. However, for large-size remote sensing images, the great majority of existing works have ignored the image tilt problem. This problem can result in partitioning buildings into separately oblique parts when the large-size images are partitioned. This is not beneficial to preserve semantic completeness of the building objects. Motivated by the above fact, we first propose a framework for detecting objects in a large-size image, particularly for building detection. The framework mainly consists of two phases. In the first phase, we particularly propose a tilt correction (TC) algorithm, which contains three steps: texture mapping, tilt angle assessment, and image rotation. In the second phase, building detection is performed with object detectors, especially deep-neural-network-based methods. Last but not least, the detection results will be inversely mapped to the original large-size image. Furthermore, a challenging dataset named Aerial Image Building Detection is contributed for the public research. To evaluate the TC method, we also define an evaluation metric to compute the cost of building partition. The experimental results demonstrate the effects of the proposed method for building detection.
WOS:000663535500017
</snippet>
</document>

<document id="682">
<title>IM2ELEVATION: Building Height Estimation from Single-View Aerial Imagery</title>
<url>http://dx.doi.org/10.3390/rs12172719</url>
<snippet>Estimation of the Digital Surface Model (DSM) and building heights from single-view aerial imagery is a challenging inherently ill-posed problem that we address in this paper by resorting to machine learning. We propose an end-to-end trainable convolutional-deconvolutional deep neural network architecture that enables learning mapping from a single aerial imagery to a DSM for analysis of urban scenes. We perform multisensor fusion of aerial optical and aerial light detection and ranging (Lidar) data to prepare the training data for our pipeline. The dataset quality is key to successful estimation performance. Typically, a substantial amount of misregistration artifacts are present due to georeferencing/projection errors, sensor calibration inaccuracies, and scene changes between acquisitions. To overcome these issues, we propose a registration procedure to improve Lidar and optical data alignment that relies on Mutual Information, followed by Hough transform-based validation step to adjust misregistered image patches. We validate our building height estimation model on a high-resolution dataset captured over central Dublin, Ireland: Lidar point cloud of 2015 and optical aerial images from 2017. These data allow us to validate the proposed registration procedure and perform 3D model reconstruction from single-view aerial imagery. We also report state-of-the-art performance of our proposed architecture on several popular DSM estimation datasets.
WOS:000570425500001
</snippet>
</document>

<document id="683">
<title>Estimating fractional snow cover from passive microwave brightness temperature data using MODIS snow cover product over North America</title>
<url>http://dx.doi.org/10.5194/tc-15-835-2021</url>
<snippet>The dynamic characteristics of seasonal snow cover are critical for hydrology management, the climate system, and the ecosystem functions. Optical satellite remote sensing has proven to be an effective tool for monitoring global and regional variations in snow cover. However, accurately capturing the characteristics of snow dynamics at a finer spatiotemporal resolution continues to be problematic as observations from optical satellite sensors are greatly impacted by clouds and solar illumination. Traditional methods of mapping snow cover from passive microwave data only provide binary information at a spatial resolution of 25 km. This innovative study applies the random forest regression technique to enhanced-resolution passive microwave brightness temperature data (6.25 km) to estimate fractional snow cover over North America in winter months (January and February). Many influential factors, including land cover, topography, and location information, were incorporated into the retrieval models. Moderate Resolution Imaging Spectro-radiometer (MODIS) snow cover products between 2008 and 2017 were used to create the reference fractional snow cover data as the "true" observations in this study. Although over-estimating and underestimating around two extreme values of fractional snow cover, the proposed retrieval algorithm outperformed the other three approaches (linear regression, artificial neural networks, and multivariate adaptive regression splines) using independent test data for all land cover classes with higher accuracy and no out-of-range estimated values. The method enabled the evaluation of the estimated fractional snow cover using independent datasets, in which the root mean square error of evaluation results ranged from 0.189 to 0.221. The snow cover detection capability of the proposed algorithm was validated using meteorological station observations with more than 310 000 records. We found that binary snow cover obtained from the estimated fractional snow cover was in good agreement with ground measurements (kappa: 0.67). There was significant improvement in the accuracy of snow cover identification using our algorithm; the overall accuracy increased by 18&#37; (from 0.71 to 0.84), and the omission error was reduced by 71&#37; (from 0.48 to 0.14) when the threshold of fractional snow cover was 0.3. The experimental results show that passive microwave brightness temperature data may potentially be used to estimate fractional snow cover directly in that this retrieval strategy offers a competitive advantage in snow cover detection.
WOS:000621382900001
</snippet>
</document>

<document id="684">
<title>A Novel Quality-Guided Two-Dimensional InSAR Phase Unwrapping Method via GAUNet</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3099485</url>
<snippet>Phase unwrapping (PU) has always been a critical and challenging step in interferometric synthetic aperture radar (InSAR) data processing. Inspired by existing research, i.e., the PGNet, we propose a novel quality-guided 2-D InSAR PU method via deep learning, and regard PU as a two-stage process. In the first stage, the ambiguity gradient is estimated using the proposed global attention U-Net (GAUNet) architecture, which combines the classic U-Net structure and global attention mechanism. Then, in the second stage, the classical PU framework (e.g., the L1- or L2-norm) is applied as a post-processing operation to retrieve the absolute phase. Since class imbalance is a key factor affecting the estimation of ambiguity gradient, different strategies based on four commonly used quality maps are adopted to deal with the problem. The quality map is not only input as additional information for the guidance of the training process, but also participates in the construction of loss function. As a result, GAUNet can pay more attention to the nonzero ambiguity gradients. By using the number of residues as the evaluation metric, we can choose the optimum strategy for the restoration of the absolute phase. In addition to the simulated interferograms, the proposed method is tested both on a real topographic interferogram exhibiting rugged topography and phase aliasing and a differential interferogram measuring the deformation from MW 6.9 Hawaii earthquake, all yield state-of-art performance when comparing with the widely used traditional 2-D PU methods.
WOS:000690441600001
</snippet>
</document>

<document id="685">
<title>COASTAL HABITAT MAPPING WITH UAV MULTI-SENSOR DATA: AN EXPERIMENT AMONG DCNN-BASED APPROACHES</title>
<url>http://dx.doi.org/10.5194/isprs-annals-V-3-2022-439-2022</url>
<snippet>With recent abundant availability of high resolution multi-sensor UAV data and rapid development of deep learning models, efficient automatic mapping using deep neural network is becoming a common approach. However, with the ever-expanding inventories of both data and deep neural network models, it can be confusing to know how to choose. Most models expect input as conventional RGB format, but that can be extended to incorporate multi-sensor data. In this study, we re-implement and modify three deep neural network models of various complexities, namely UNET, DeepLabv3+ and Dense Dilated Convolutions Merging Network to use both RGB and near infrared (NIR) data from a multi-sensor UAV dataset over a Norwegian coastal area. The dataset has been carefully annotated by marine experts for coastal habitats. We find that the NIR channel increases UNET performance significantly but has mixed effects on DeepLabv3+ and DDCM. The latter two are capable of achieving best performance with RGB-only. The class-wise evaluation shows that the NIR channel greatly increases the performance in UNET for green, red algae, vegetation and rock. However, the purpose of the study is not to merely compare the models or to achieve the best performance, but to gain more insights on the compatibility between various models and data types. And as there is an ongoing effort in acquiring and annotating more data, we aim to include them in the coming year.
WOS:000855203200060
</snippet>
</document>

<document id="686">
<title>A comparison of the integrated fuzzy object-based deep learning approach and three machine learning techniques for land use/cover change monitoring and environmental impacts assessment</title>
<url>http://dx.doi.org/10.1080/15481603.2021.2000350</url>
<snippet>Recent improvements in the spatial, temporal, and spectral resolution of satellite images necessitate (semi-)automated classification and information extraction approaches. Therefore, we developed an integrated fuzzy object-based image analysis and deep learning (FOBIA-DL) approach for monitoring the land use/cover (LULC) and respective changes and compared it to three machine learning (ML) algorithms, namely the support vector machine (SVM), random forest (RF), and classification and regression tree (CART). We investigated LULC impacts on drought by analyzing Landsat satellite images from 1990 to 2020 for the Urmia Lake area in northern Iran. In the FOBIA-DL approach, following the initial segmentation steps, object features were identified for each LULC class. We then derived their respective attributes using fuzzy membership functions and deep convolutional neural networks (DCNNs), a deep learning method. The Fuzzy Synthetic Evaluation and Dempster-Shafer Theory (FSE-DST) also applied to validate and carryout the spatial uncertainties. Our results indicate that the FOBIA-DL, with an accuracy of 90.1&#37; to 96.4&#37; and a spatial certainty of 0.93 to 0.97, outperformed the other approaches, closely followed by the SVM. Our results also showed that the integration of Fuzzy-OBIA and DCNNs could improve the strength and robustness of the OBIAs decision rules, while the FSE-DST approach notably improved the spatial accuracy of the object-based classification maps. While object-based image analysis (OBIA) is already considered a paradigm shift in GIScience, the integration of OBIA with fuzzy and deep learning creates more flexibility and robust OBIA decision rules for image analysis and classification. This research integrated popular data-driven approaches and developed a novel methodology for image classification and spatial accuracy assessment. From the environmental perspective, the results of this research support lake restoration initiatives by decision-makers and authorities in applications such as drought mitigation, land use management and precision agriculture programs.
WOS:000724069000001
</snippet>
</document>

<document id="687">
<title>Forest Conservation with Deep Learning: A Deeper Understanding of Human Geography around the Betampona Nature Reserve, Madagascar</title>
<url>http://dx.doi.org/10.3390/rs13173495</url>
<snippet>Documenting the impacts of climate change and human activities on tropical rainforests is imperative for protecting tropical biodiversity and for better implementation of REDD+ and UN Sustainable Development Goals. Recent advances in very high-resolution satellite sensor systems (i.e., WorldView-3), computing power, and machine learning (ML) have provided improved mapping of fine-scale changes in the tropics. However, approaches so far focused on feature extraction or the extensive tuning of ML parameters, hindering the potential of ML in forest conservation mapping by not using textural information, which is found to be powerful for many applications. Additionally, the contribution of shortwave infrared (SWIR) bands in forest cover mapping is unknown. The objectives were to develop end-to-end mapping of the tropical forest using fully convolution neural networks (FCNNs) with WorldView-3 (WV-3) imagery and to evaluate human impact on the environment using the Betampona Nature Reserve (BNR) in Madagascar as the test site. FCNN (U-Net) using spatial/textural information was implemented and compared with feature-fed pixel-based methods including Support Vector Machine (SVM), Random Forest (RF), and Deep Neural Network (DNN). Results show that the FCNN model outperformed other models with an accuracy of 90.9&#37;, while SVM, RF, and DNN provided accuracies of 88.6&#37;, 84.8&#37;, and 86.6&#37;, respectively. When SWIR bands were excluded from the input data, FCNN provided superior performance over other methods with a 1.87&#37; decrease in accuracy, while the accuracies of other models-SVM, RF, and DNN-decreased by 5.42&#37;, 3.18&#37;, and 8.55&#37;, respectively. Spatial-temporal analysis showed a 0.7&#37; increase in Evergreen Forest within the BNR and a 32&#37; increase in tree cover within residential areas likely due to forest regeneration and conservation efforts. Other effects of conservation efforts are also discussed.
WOS:000694540300001
</snippet>
</document>

<document id="688">
<title>Use of Mamdani Fuzzy Algorithm for Multi-Hazard Susceptibility Assessment in a Developing Urban Settlement (Mamak, Ankara, Turkey)</title>
<url>http://dx.doi.org/10.3390/ijgi9020114</url>
<snippet>Urban areas may be affected by multiple hazards, and integrated hazard susceptibility maps are needed for suitable site selection and planning. Furthermore, geological-geotechnical parameters, construction costs, and the spatial distribution of existing infrastructure should be taken into account for this purpose. Up-to-date land-use and land-cover (LULC) maps, as well as natural hazard susceptibility maps, can be frequently obtained from high-resolution satellite sensors. In this study, an integrated hazard susceptibility assessment was performed for a developing urban settlement (Mamak District of Ankara City, Turkey) considering landslide and flood potential. The flood susceptibility map of Ankara City was produced in a previous study using modified analytical hierarchical process (M-AHP) approach. The landslide susceptibility map was produced using the logistic regression technique in this study. Sentinel-2 images were employed for generating LULC data with the random forest classification method. Topographical derivatives obtained from a high-resolution digital elevation model and lithological parameters were employed for the production of landslide susceptibility maps. For the integrated hazard susceptibility assessment, the Mamdani fuzzy algorithm was considered, and the results are discussed in the present study. The results demonstrate that multi-hazard susceptibility assessment maps for urban planning can be obtained by combining a set of expert-based and ensemble learning methods.
WOS:000522449700054
</snippet>
</document>

<document id="689">
<title>A hybrid representation of the environment to improve autonomous navigation of mobile robots in agriculture</title>
<url>http://dx.doi.org/10.1007/s11119-020-09773-9</url>
<snippet>This paper considers the problem of autonomous navigation in agricultural fields. It proposes a localization and mapping framework based on semantic place classification and key location estimation, which together build a hybrid topological map. This map benefits from generic partitioning of the field, which contains a finite set of well-differentiated workspaces and, through a semantic analysis, it is possible to estimate in a probabilistic way the position (state) of a mobile system in the field. Moreover, this map integrates both metric (key locations) and semantic features (working areas). One of its advantages is that a full and precise map prior to navigation is not necessary. The identification of the key locations and working areas is carried out by a perception system based on 2D LIDAR and RGB cameras. Fusing these data with odometry allows the robot to be located in the topological map. The approach is assessed through off-line data recorded in real conditions in diverse fields during different seasons. It exploits a real-time object detector based on a convolutional neural network called you only look once, version 3, which has been trained to classify a considerable number of crops, including market-garden crops such as broccoli and cabbage, and to identify grapevine trunks. The results show the interest in the approach, which allows (i) obtaining a simple and easy-to-update map, (ii) avoiding the use of artificial landmarks, and thus (iii) improving the autonomy of agricultural robots.
WOS:000604197700005
</snippet>
</document>

<document id="690">
<title>Hyperspectral Image Classification Based on 3-D Multihead Self-Attention Spectral-Spatial Feature Fusion Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3226758</url>
<snippet>Convolutional neural networks are a popular method in hyperspectral image classification. However, the accuracy of the models is closely related to the number and spatial size of training samples. Which relieve the performance decline by the number and spatial size of training samples, we designed a 3-D multihead self-attention spectral-spatial feature fusion network (3DMHSA-SSFFN) that contains step-by-step feature extracted blocks (SBSFE) and 3-D multihead-self-attention-module (3DMHSA). The proposed step-by-step feature extracted blocks relieved the declining-accuracy phenomenon for the limited number of training samples. Multiscale convolution kernels extract more spatial-spectral features in the step-by-step feature-extracted blocks. In hyperspectral image classification, the 3DMHSA module enhances the stability of classification by correlating disparate features. Experimental results show that 3DMHSA-SSFFN possesses a better classification performance than other advanced models through the limited number of balance and imbalance training data in three data.
WOS:000912413700009
</snippet>
</document>

<document id="691">
<title>A CNN approach to simultaneously count plants and detect plantation-rows from UAV imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.01.024</url>
<snippet>Accurately mapping croplands is an important prerequisite for precision farming since it assists in field management, yield-prediction, and environmental management. Crops are sensitive to planting patterns and some have a limited capacity to compensate for gaps within a row. Optical imaging with sensors mounted on Unmanned Aerial Vehicles (UAV) is a cost-effective option for capturing images covering croplands nowadays. However, visual inspection of such images can be a challenging and biased task, specifically for detecting plants and rows on a one-step basis. Thus, developing an architecture capable of simultaneously extracting plant individually and plantation-rows from UAV-images is yet an important demand to support the management of agricultural systems. In this paper, we propose a novel deep learning method based on a Convolutional Neural Network (CNN) that simultaneously detects and geolocates plantation-rows while counting its plants considering highly-dense plantation configurations. The experimental setup was evaluated in (a) a cornfield (Zea mays L.) with different growth stages (i.e. recently planted and mature plants) and in a (b) Citrus orchard (Citrus Sinensis Pera). Both datasets characterize different plant density scenarios, in different locations, with different types of crops, and from different sensors and dates. This scheme was used to prove the robustness of the proposed approach, allowing a broader discussion of the method. A two-branch architecture was implemented in our CNN method, where the information obtained within the plantation-row is updated into the plant detection branch and retro-feed to the row branch; which are then refined by a Multi-Stage Refinement method. In the corn plantation datasets (with both growth phases - young and mature), our approach returned a mean absolute error (MAE) of 6.224 plants per image patch, a mean relative error (MRE) of 0.1038, precision and recall values of 0.856, and 0.905, respectively, and an F-measure equal to 0.876. These results were superior to the results from other deep networks (HRNet, Faster R-CNN, and RetinaNet) evaluated with the same task and dataset. For the plantation-row detection, our approach returned precision, recall, and F-measure scores of 0.913, 0.941, and 0.925, respectively. To test the robustness of our model with a different type of agriculture, we performed the same task in the citrus orchard dataset. It returned an MAE equal to 1.409 citrus-trees per patch, MRE of 0.0615, precision of 0.922, recall of 0.911, and F-measure of 0.965. For the citrus plantation-row detection, our approach resulted in precision, recall, and F-measure scores equal to 0.965, 0.970, and 0.964, respectively. The proposed method achieved state-of-the-art performance for counting and geolocating plants and plant-rows in UAV images from different types of crops. The method proposed here may be applied to future decision-making models and could contribute to the sustainable management of agricultural systems.
WOS:000640987800001
</snippet>
</document>

<document id="692">
<title>First Dataset of Wind Turbine Data Created at National Level With Deep Learning Techniques From Aerial Orthophotographs With a Spatial Resolution of 0.5 M/Pixel</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3101934</url>
<snippet>Deep learning applied to feature extraction and mapping from high-resolution images is demonstrating the potential of this branch of data-intensive Artificial Intelligence to improve terrain mapping processes. The documented experiences have been applied on a small scale and there is a great expectation about its applicability on a country scale. For example, when extracting wind turbines using semantic segmentation models from a region of 28 km x 19 km containing unseen data, we obtained a commission rate of 1.4&#37; and an omission rate of 0.38&#37;. In this article, we present a methodology generated on the basis of two iterations. In these iterations, processing and postprocessing time, energy consumption, and finally results have been optimized to map wind turbines for the first time throughout the Spanish peninsular territory. In addition to adding a binary classification neural network prior to the semantic segmentation that extracts the turbines, a third multiclass recognition network has been used to classify the turbines by their power capacity complementing the features extracted with attributes. The proposed methodology can be adapted in the vectorization phase and applied to other types of features with linear or polygon representation to achieve a large-scale efficient extraction of geospatial elements using automated procedures.
WOS:000692210900001
</snippet>
</document>

<document id="693">
<title>Mapping Potential Plant Species Richness over Large Areas with Deep Learning, MODIS, and Species Distribution Models</title>
<url>http://dx.doi.org/10.3390/rs13132490</url>
<snippet>The spatial patterns of species richness can be used as indicators for conservation and restoration, but data problems, including the lack of species surveys and geographical data gaps, are obstacles to mapping species richness across large areas. Lack of species data can be overcome with remote sensing because it covers extended geographic areas and generates recurring data. We developed a Deep Learning (DL) framework using Moderate Resolution Imaging Spectroradiometer (MODIS) products and modeled potential species richness by stacking species distribution models (S-SDMs) to ask, "What are the spatial patterns of potential plant species richness across the Korean Peninsula, including inaccessible North Korea, where survey data are limited?" First, we estimated plant species richness in South Korea by combining the probability-based SDM results of 1574 species and used independent plant surveys to validate our potential species richness maps. Next, DL-based species richness models were fitted to the species richness results in South Korea, and a time-series of the normalized difference vegetation index (NDVI) and leaf area index (LAI) from MODIS. The individually developed models from South Korea were statistically tested using datasets that were not used in model training and obtained high accuracy outcomes (0.98, Pearson correlation). Finally, the proposed models were combined to estimate the richness patterns across the Korean Peninsula at a higher spatial resolution than the species survey data. From the statistical feature importance tests overall, growing season NDVI-related features were more important than LAI features for quantifying biodiversity from remote sensing time-series data.
WOS:000671056500001
</snippet>
</document>

<document id="694">
<title>GIS-based spatial prediction of tropical forest fire danger using a new hybrid machine learning method</title>
<url>http://dx.doi.org/10.1016/j.ecoinf.2018.08.008</url>
<snippet>Forest fire danger map at regional scale is considered of utmost importance for local authority to efficiently allocate its resources to fire prevention measures and establish appropriate land use plans. This study aims at introduce a new machine learning method, named as DFP-MnBpAnn, based on Artificial Neural Network (Ann) with a novel hybrid training algorithm of Differential Flower Pollination (DFP) and mini-match backpropagation (MnBp) for spatial modeling of forest fire danger. Tropical forest of the Lam Dong province (Vietnam) was used as case study. To achieve this task, a Geographical Information System (GIS) database of the forest fire for the study area was established. Accordingly, DFP, as a metaheuristic method, is used to optimize the weights and structure of Ann to fit the GIS database at hand. Whereas, MnBp is employed periodically during the DFP-based optimization process, in which MnBp acts as a local search aiming to accelerate both the quality of the found solutions and the convergence rate. Experimental outcomes demonstrate that the proposed DFP-MnBpAnn model is superior to other benchmark methods with satisfactory prediction accuracy (Classification Accuracy Rate = 88.43&#37;). This fact confirms that DFP-MnBpAnn is a promising alternative for the problem of large-scale forest fire danger mapping.
WOS:000453641900011
</snippet>
</document>

<document id="695">
<title>Landslide Susceptibility Mapping Using Feature Fusion-Based CPCNN-ML in Lantau Island, Hong Kong</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3066378</url>
<snippet>Landslide susceptibility mapping (LSM) is an effective way to predict spatial probability of landslide occurrence. Existing convolutional neural network (CNN)-based methods apply self-built CNN with simple structure, which failed to reach CNNs full potential on high-level feature extraction, meanwhile ignored the use of numerical predisposing factors. For the purpose of exploring feature fusion based CNN models with greater reliability in LSM, this study proposes an ensemble model based on channel-expanded pre-trained CNN and traditional machine learning model (CPCNN-ML). In CPCNN-ML, pre-trained CNN with mature structure is modified to excavate high-level features of multichannel predisposing factor layers. LSM result is generated by traditional machine learning (ML) model based on hybrid feature of high-level features and numerical predisposing factors. Lantau Island, Hong Kong is selected as study area; temporal landslide inventory is used for model training and evaluation. Experimental results show that CPCNN-ML has ability to predict landslide occurrence with high reliability, especially the CPCNN-ML based on random forest. Contrast experiments with self-built CNN and traditional ML models further embody the superiority of CPCNN-ML. It is worth noting that coastal regions are newly identified landslide-prone regions compared with previous research. This finding is of great reference value for Hong Kong authorities to formulate appropriate disaster prevention and mitigation policies.
WOS:000640757900003
</snippet>
</document>

<document id="696">
<title>PMENet: phase map enhancement for Fourier transform profilometry using deep learning</title>
<url>http://dx.doi.org/10.1088/1361-6501/abf805</url>
<snippet>Fringe projection profilometry (FPP) is a three-dimensional (3D) shape measurement method that involves projecting fringe patterns onto the object. A phase map retrieved from these fringe images is used for reconstructing the 3D surface of the object. Fourier transform and phase-shifting are two of the widely used fringe analysis techniques for performing 3D shape measurement using FPP. Fourier transform profilometry (FTP) has the advantage of performing high-speed measurement due to its single-shot nature. However, the reconstructed 3D surface has artifacts and high noise, specifically on the edges. On the other hand, phase-shifting profilometry (PSP) has the advantage of higher accuracy (relatively less noise level) but compromises on measurement speed. In this research, we propose a deep learning method to enhance the quality of the FTP phase maps using an efficient deep learning model called phase map enhancement net (PMENet). PMENet takes an FTP phase map as input and predicts a high-quality phase map in a supervised manner by using the phase maps obtained from 18-step PSP as ground truth. The training dataset was generated using a virtual FPP system. Validations were conducted on both the simulated data (generated by virtual FPP system) and the real-world data. The experimental results demonstrate that the trained neural network model can successfully improve the quality of the 3D geometric reconstruction with FTP and reduced the mean and root-mean-square error by 66&#37; and 43&#37;, respectively.
WOS:000662670900001
</snippet>
</document>

<document id="697">
<title>Incorporating Open Source Data for Bayesian Classification of Urban Land Use From VHR Stereo Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2017.2737702</url>
<snippet>This study investigates the incorporation of open source data into a Bayesian classification of urban land use from very high resolution (VHR) stereo satellite images. The adopted classification framework starts from urban land cover classification, proceeds to building-type characterization, and results in urban land use. For urban land cover classification, a preliminary classification distinguishes trees, grass, and shadow objects using a random forest at a fine segmentation level. Fuzzy decision trees derived from hierarchical Bayesian models separate buildings from other man-made objects at a coarse segmentation level, where an open street map provides prior building information. A Bayesian network classifier combining commonly used land use indicators and spatial arrangement is used for the urban land use classification. The experiments were conducted on GeoEye stereo images over Oklahoma City, USA. Experimental results showed that the urban land use classification using VHR stereo images performed better than that using a monoscopic VHR image, and the integration of open source data improved the final urban land use classification. Our results also show a way of transferring the adopted urban land use classification framework, developed for a specific urban area in China, to other urban areas. The study concludes that incorporating open source data by Bayesian analysis improves urban land use classification. Moreover, a pretrained convolutional neural network fine tuned on the UC Merced land use dataset offers a useful tool to extract additional information for urban land use classification.
WOS:000415719000022
</snippet>
</document>

<document id="698">
<title>Feature-Based Constraint Deep CNN Method for Mapping Rainfall-Induced Landslides in Remote Regions With Mountainous Terrain: An Application to Brazil</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3161383</url>
<snippet>Landslides have caused tremendous damage to human lives and property safety. However, the complex environment of mountain landslides and the vegetation coverage around landslides make it difficult to identify landslides quickly and efficiently using high-resolution images. To address this challenge, this article presents a feature-based constraint deep U-Net (FCDU-Net) method to detect rainfall-induced mountainous landslides. Usually, the vegetation in the landslide area is severely damaged, and the vegetation coverage can indirectly reflect the spatial extent of the landslide. Meanwhile, the texture features of high-resolution images can characterize the surface environment of landslide hazards to a certain extent. We first introduce auxiliary features of normalized difference vegetation index and gray-level co-occurrence matrix into the proposed method to further improve the detection performance. Then, to minimize the information redundancy of these features and the image, we combine Relief-F and Deep U-Net to screen the optimal features to effectively identify accurate and detailed landslide boundaries. Compared with traditional semantic segmentation methods, the FCDU-Net method can capture fine-grained details in high-resolution images and produce more accurate segmentation results. We conducted experiments by applying the proposed method and other most popular semantic segmentation methods to a high-resolution RapidEye image in Rio de Janeiro, Brazil. The results demonstrate that the FCDU-Net method can achieve better landslide detection results than the other semantic segmentation methods, and the evaluation measures of Precision, F1 score, and mean Intersection-over-Union are as high as 88.87&#37;, 81.17&#37;, and 83.19&#37;, respectively. Furthermore, we quantitatively analyze the effect of the convolution input window size on the performance of FCDU-Net in detecting landslides. We believe that FCDU-Net can serve as a reliable tool for fast and accurate regional landslide hazard surveys.
WOS:000779605200003
</snippet>
</document>

<document id="699">
<title>Mapping Industrial Poultry Operations at Scale With Deep Learning and Aerial Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3191544</url>
<snippet>Concentrated animal feeding operations (CAFOs) pose serious risks to air, water, and public health, but have proven to be challenging to regulate. The U.S. Government Accountability Office notes that a basic challenge is the lack of comprehensive location information on CAFOs. We use the U.S. Department of Agricultures National Agricultural Imagery Program 1 m/pixel aerial imagery to detect poultry CAFOs across the continental USA. We train convolutional neural network models to identify individual poultry barns and apply the best-performing model to over 42 TB of imagery to create the first national open-source dataset of poultry CAFOs We validate the model predictions against held-out validation set on poultry CAFO facility locations from ten hand-labeled counties in California and demonstrate that this approach has significant potential to fill gaps in environmental monitoring.
WOS:000853871300010
</snippet>
</document>

<document id="700">
<title>Fine-Grained Land Use Classification at the City Scale Using Ground-Level Images</title>
<url>http://dx.doi.org/10.1109/TMM.2019.2891999</url>
<snippet>Multimedia researchers have exploited large collections of community-contributed geo-referenced images to better understand a particular image, such as its subject matter or where it was taken, as well as to better understand a geographic location, such as the most visited tourist spots in a city or what the local cuisine is like. The goal of this paper is to better understand location. In particular, we use geo-referenced image collections to better understand what occurs in different parts of a city at fine spatial and activity class scales. This problem is known as land use mapping in the geographical sciences. We propose a novel framework to perform fine-grained land use mapping at the city scale using ground-level images. Mapping land use is considerably more difficult than mapping land cover and is generally not possible using overhead imagery as it requires close-up views and seeing inside buildings. We postulate that the growing collections of geo-referenced, ground-level images suggest an alternate approach to this geographic knowledge discovery problem. We develop a general framework that uses Flickr images to map 45 different land-use classes for the city of San Francisco, CA, USA. Individual images are classified using a novel convolutional neural network containing two streams: one for recognizing objects and another for recognizing scenes. This network is trained in an end-to-end manner directly on the labeled training images. We propose several novel strategies to overcome the noisiness of our user-generated data including search-based training set augmentation and online adaptive training. We derive a ground truth map of San Francisco in order to evaluate our method. We demonstrate the effectiveness of our approach through geovisualization and quantitative analysis. Our framework achieves over 29&#37; recall at the individual land parcel level that represents a strong baseline for the challenging 45-way land use classification problem, especially given the noisiness of the image data.
WOS:000473183700017
</snippet>
</document>

<document id="701">
<title>Deep learning based regression for optically inactive inland water quality parameter estimation using airborne hyperspectral imagery*</title>
<url>http://dx.doi.org/10.1016/j.envpol.2021.117534</url>
<snippet>Airborne hyperspectral remote sensing has the characteristics of high spatial and spectral resolutions, and provides an opportunity for accurate and efficient inland water qauality monitoring. Many studies have focused on evaluating and quantifying the concentrations of the optically active water quality parameters, for parameters such as chlorophyll-a (Chla), cyanobacteria, and colored dissolved organic matter (CDOM). For the optically inactive parameters, such as the permanganate index (CODMn), total nitrogen (TN), total phosphorus (TP), ammoniacal nitrogen (NH3-N), and heavy metals, it is difficult to estimate the concentrations directly, and the traditional indirect estimation models cannot meet the accuracy requirements, especially in heavily polluted inland waters. In this study, 60 water samples were collected at a depth of 50 cm from the Guanhe River in China, at the same time as the airborne data acquisition. We also developed and investigated two deep learning based regression models-a pixel-based deep neural network regression (pixel_DNNR) model and a patch-based deep neural network regression (patch_DNNR) model-to estimate seven optically inactive water quality parameters. Compared with the partial least squares regression (PLSR) and support vector regression (SVR) models, the deep learning based regression models can obtain a superior accuracy, especially the patch_DNNR model, which obtained a superior prediction accuracy for all parameters, with the prediction dataset coefficient of determination (Rp2) and the residual prediction deviation (RPD) values being greater than 0.6 and 1.6, respectively. In addition, thematic maps of the water quality classification results and water parameter concentrations were generated and the overall water quality and pollution sources were analyzed in the study area. The experimental results demonstrate that the deep learning based regression models show a good performance in the feature extraction and image understanding of high-dimensional data, and they provide us with a new approach for optically inactive inland water quality parameter estimation.
WOS:000686592000007
</snippet>
</document>

<document id="702">
<title>A new attention-based CNN approach for crop mapping using time series Sentinel-2 images</title>
<url>http://dx.doi.org/10.1016/j.compag.2021.106090</url>
<snippet>Accurate crop mapping is of great importance for agricultural applications, and deep learning methods have been applied on multi-temporal remotely sensed images to classify crops. However, due to the geographic heterogeneity, the spectral profiles of the same crop can vary spatially, and thus using the spectral features alone can limit the model performance in mapping crops in large scales. Moreover, it is a challenge for traditional deep learning models to accurately capture the important information from a large number of features. To address these issues, in this study, we developed a novel attention-based convolutional neural network (CNN) approach (Geo-CBAM-CNN) for crop classification using time series Sentinel-2 images. Specifically, geographic information of crops was first integrated into an advanced attention module, Convolutional Block Attention Module (CBAM) to form a Geo-CBAM module which can help mitigate the impacts of geographic heterogeneity and restrain unnecessary information. Then, the developed Geo-CBAM module was embedded into a CNN model to boost the model?s attention both spectrally and spatially. The proposed Geo-CBAM-CNN model was validated on four main crops over six counties with different geographic environments in the U.S. Also, it was compared to three other state-of-the-art machine learning approaches, including CBAM-CNN, CNN and Random Forest (RF). The results showed that the proposed model achieved the best performance, reaching 97.82&#37; overall accuracy, 96.82&#37; Kappa coefficient and 96.96&#37; Macro-average F1 score. Moreover, the developed Geo-CBAM-CNN model showed strong spatial adaptability, indicating its superior performance in large scale applications. Furthermore, by visualizing the structure of the Geo-CBAM-CNN, we found that the model automatically allocated different weights to the features, and generally, the red-edge features in the middle of the year obtained more attention.
WOS:000641351000001
</snippet>
</document>

<document id="703">
<title>Transferring multiscale map styles using generative adversarial networks</title>
<url>http://dx.doi.org/10.1080/23729333.2019.1615729</url>
<snippet>The advancement of the Artificial Intelligence (AI) technologies makes it possible to learn stylistic design criteria from existing maps or other visual art and transfer these styles to make new digital maps. In this paper, we propose a novel framework using AI for map style transfer applicable across multiple map scales. Specifically, we identify and transfer the stylistic elements from a target group of visual examples, including Google Maps, OpenStreetMap, and artistic paintings, to unstylized GIS vector data through two generative adversarial network (GAN) models. We then train a binary classifier based on a deep convolutional neural network to evaluate whether the transfer styled map images preserve the original map design characteristics. Our experiment results show that GANs have great potential for multiscale map style transferring, but many challenges remain requiring future research.
WOS:000668116600002
</snippet>
</document>

<document id="704">
<title>Discriminative Context-Aware Network for Target Extraction in Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3138187</url>
<snippet>Extracting objects of interest from remote sensing imagery is an essential part in various practical applications. The objects that people pay attention to in the remote sensing scene mainly include buildings, roads, vehicles, etc. In this article, extracting the aforementioned objects are collectively referred to as the target extraction task. Arising from object scale variation, appearance similarity between adjacent patches, diversity of imaging orientation, and complexity of background, it is difficult to extract complete objects from cluttered backgrounds. Deep neural network has made great achievement in dense prediction for target extraction. However, most of the previous works are still faced with a formidable challenge in discriminative context feature representation to extract targets of various categories and correctly classify pixels around the boundary. In this article, we propose a target extraction neural network, named discriminative context-aware network, to focus on discriminative high-level context features and preserve spatial information. First, a discriminative context-aware feature module is designed to generate the feature maps in the top layer, which not only captures the rich image context information but also aggregates the contrasted local information at multiple scales. Second, a refine decoder module is adopted to preserve spatial information from low-level layers and enhance the feature representation, leading to precise segmentation results. We conducted extensive experiments on building and road extraction benchmarks, including WHU building dataset and Massachusetts road dataset, together with a self-constructed dataset for vehicle extraction in SAR images. Our method achieves state-of-the-art results with fewer parameters and faster inference.
WOS:000742179500002
</snippet>
</document>

<document id="705">
<title>MU-Net: A MULTISCALE UNSUPERVISED NETWORK FOR REMOTE SENSING IMAGE REGISTRATION</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLIII-B3-2022-537-2022</url>
<snippet>Registration for multi-sensor or multi-modal image pairs with a large degree of distortions is a fundamental task for many remote sensing applications. To achieve accurate and low-cost remote sensing image registration, we propose a multiscale unsupervised network (MU-Net). Without costly ground truth labels, MU-Net directly learns the end-to-end mapping from the image pairs to their transformation parameters. MU-Net performs a coarse-to-fine registration pipeline by stacking several deep neural network models on multiple scales, which prevents the backpropagation being falling into a local extremum and resists significant image distortions. In addition, a novel loss function paradigm is designed based on structural similarity, which makes MU-Net suitable for various types of multi-modal images. MU-Net is compared with traditional feature-based and area-based methods, as well as supervised and other unsupervised learning methods on the Optical-Optical, Optical-Infrared, Optical-SAR and Optical-Map datasets. Exp erimental results show that MU-Net achieves more robust and accurate registration performance between these image pairs with geometric and radiometric distortions. We share the datasets and the code implemented by Pytorch at https://github.com/y eyuanxin110/MU-Net.
WOS:000855647800075
</snippet>
</document>

<document id="706">
<title>Robust Topological Navigation via Convolutional Neural Network Feature and Sharpness Measure</title>
<url>http://dx.doi.org/10.1109/ACCESS.2017.2757765</url>
<snippet>Visual navigation for mobile robots has emerged in recent years. Among the various methods, topological navigation using visual information provides a scalable map representation for large-scale mapping and navigation. A topological map is essentially a graph with keyframes as its nodes and adjacency relations as its edges. Previous topological mapping uses local feature descriptors, such as scale-invariant feature transform or Speeded-Up Robust Features, to select keyframes in mapping, localization, and estimate relative pose. In practice, local features are not robust for severe motion blur or large illumination change. In this paper, we improve topological mapping to make it more efficient and robust. First, we use a convolutional neural network (CNN) feature as the holistic image representation. The CNN feature can be used to effectively retrieve keyframes that have similar appearance from a topological map, and it is robust to motion blur and illumination change. Thus, it improves the performance for place recognition and robot relocalization. Second, we use sharpness measure to select high-quality keyframes and avoid selecting blurry ones. Third, an efficient and robust non-rigid matching method, vector field consensus, is used for efficient geometric verification and to retrieve the most similar keyframe. The qualitative and quantitative experimental results demonstrate that our method is satisfactory.
WOS:000413942100007
</snippet>
</document>

<document id="707">
<title>Residual Multi-Attention Classification Network for A Forest Dominated Tropical Landscape Using High-Resolution Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.3390/ijgi10010022</url>
<snippet>Tropical forests are of vital importance for maintaining biodiversity, regulating climate and material cycles while facing deforestation, agricultural reclamation, and managing various pressures. Remote sensing (RS) can support effective monitoring and mapping approaches for tropical forests, and to facilitate this we propose a deep neural network with an encoder-decoder architecture here to classify tropical forests and their environment. To deal with the complexity of tropical landscapes, this method utilizes a multi-scale convolution neural network (CNN) to expand the receptive field and extract multi-scale features. The model refines the features with several attention modules and fuses them through an upsampling module. A two-stage training strategy is proposed to alleviate misclassifications caused by sample imbalances. A joint loss function based on cross-entropy loss and the generalized Dice loss is applied in the first stage, and the second stage used the focal loss to fine-tune the weights. As a case study, we use Hainan tropical reserves to test the performance of this model. Compared with four state-of-the-art (SOTA) semantic segmentation networks, our network achieves the best performance with two Hainan datasets (mean intersection over union (MIoU) percentages of 85.78&#37; and 82.85&#37;). We also apply the new model to classify a public true color dataset which has 17 semantic classes and obtain results with an 83.75&#37; MIoU. This further demonstrates the applicability and potential of this model in complex classification tasks.
WOS:000610272800001
</snippet>
</document>

<document id="708">
<title>Unsupervised spectral-spatial processing of drone imagery for identification of pine seedlings</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.11.013</url>
<snippet>Reliable and accurate monitoring of replanted forest areas is a vital component of the plantation cycle if sustainable management practices are to be maintained following resource extraction. Unmanned aerial vehicles (UAVs) can rapidly survey newly planted forest areas using high resolution imagery. Automated image processing techniques can cost-effectively examine large quantities of data. Correctly combined, such technologies can efficiently create accurate maps of seedling locations, which offers the prospect of management practices for forestry and ecology operations that involve less fieldwork. This paper presents a technique based on unsupervised machine learning for detecting healthy young pinus caribaea (Caribbean pine) and pinus radiata (Monterey pine) seedlings. The locations of individual trees requiring replacement are also determined based on the spatial distribution of the identified healthy trees. The approach was tested on data from 30 sites covering over 700 ha, with seedlings ranging from 9 months to 3 years old (mean tree heights 30-2 m). Test sites contained a range of geomorphologies and levels of weed infestation. The results indicate detection precision and specificity in excess of 90&#37; and recall approaching 99&#37; for dense point cloud resolution of 4-6 cm. As network pre-training is unnecessary, there is no need for high resolution imagery, which allows greater operational coverage per unit time. The algorithm may also be used on multi-or hyperspectral data; and may be useful if employed in conjunction with other supervised learning techniques to reduce the need for manual labelling of training data sets.
WOS:000782581400001
</snippet>
</document>

<document id="709">
<title>Unsupervised Deep Learning for Landslide Detection from Multispectral Sentinel-2 Imagery</title>
<url>http://dx.doi.org/10.3390/rs13224698</url>
<snippet>This paper proposes a new approach based on an unsupervised deep learning (DL) model for landslide detection. Recently, supervised DL models using convolutional neural networks (CNN) have been widely studied for landslide detection. Even though these models provide robust performance and reliable results, they depend highly on a large labeled dataset for their training step. As an alternative, in this paper, we developed an unsupervised learning model by employing a convolutional auto-encoder (CAE) to deal with the problem of limited labeled data for training. The CAE was used to learn and extract the abstract and high-level features without using training data. To assess the performance of the proposed approach, we used Sentinel-2 imagery and a digital elevation model (DEM) to map landslides in three different case studies in India, China, and Taiwan. Using minimum noise fraction (MNF) transformation, we reduced the multispectral dimension to three features containing more than 80&#37; of scene information. Next, these features were stacked with slope data and NDVI as inputs to the CAE model. The Huber reconstruction loss was used to evaluate the inputs. We achieved reconstruction losses ranging from 0.10 to 0.147 for the MNF features, slope, and NDVI stack for all three study areas. The mini-batch K-means clustering method was used to cluster the features into two to five classes. To evaluate the impact of deep features on landslide detection, we first clustered a stack of MNF features, slope, and NDVI, then the same ones plus with the deep features. For all cases, clustering based on deep features provided the highest precision, recall, F1-score, and mean intersection over the union in landslide detection.
WOS:000771958500022
</snippet>
</document>

<document id="710">
<title>Multimodal Bilinear Fusion Network With Second-Order Attention-Based Channel Selection for Land Cover Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2975252</url>
<snippet>As two different tools for earth observation, the optical and synthetic aperture radar (SAR) images can provide complementary information of the same land types for better land cover classification. However, because of the different imaging mechanisms of optical and SAR images, how to efficiently exploit the complementary information becomes an interesting and challenging problem. In this article, we propose a novel multimodal bilinear fusion network (MBFNet), which is used to fuse the optical and SAR features for land cover classification. The MBFNet consists of three components: the feature extractor, the second-order attention-based channel selection module (SACSM), and the bilinear fusion module. First, in order to avoid the network parameters tempting to ingratiate dominant modality, the pseudo-siamese convolutional neural network (CNN) is taken as the feature extractor to extract deep semantic feature maps of optical and SAR images, respectively. Then, the SACSM is embedded into each stream, and the fine channel-attention maps with second-order statistics are obtained by bilinear integrating the global average-pooling and global max-pooling information. The SACSM can not only automatically highlight the important channels of feature maps to improve the representation power of networks, but also uses the channel selection mechanism to reconfigure compact feature maps with better discrimination. Finally, the bilinear pooling is used as the feature-level fusion method, which establishes the second-order association between two compact feature maps of the optical and SAR streams to obtain the low-dimension bilinear fusion features for land cover classification. Experimental results on three broad coregistered optical and SAR datasets demonstrate that our method achieves more effective land cover classification performance than the state-of-the-art methods.
WOS:000522209200008
</snippet>
</document>

<document id="711">
<title>Frequency-Dependent Depth Map Enhancement via Iterative Depth-Guided Affine Transformation and Intensity-Guided Refinement</title>
<url>http://dx.doi.org/10.1109/TMM.2020.2987706</url>
<snippet>Recently, deep convolutional neural network sho-ws significant improvement for intensity-guided depth map enhancement. The most networks focus on either increasing depth or easing features propagation via residual learning and dense connection. However, it has not been explicitly considered yet to mitigate the artifacts caused by the differences of the distributions between the depth map and the corresponding color image, e.g., edge misalignment. In this paper, a novel depth-guided affine transformation is used to filter out the unrelated intensity features, which is further used to refine the depth features. Since the quality of initial depth features is low, the depth-guided intensity features filtering and the intensity-guided depth features refinement are iteratively performed, which progressively promotes effects of such tasks. To make full use of the iterations, all the refined depth features are dense connected followed by a 1 x 1 convolution layer. In addition, to improve the performance in the case of large upsampling factors (e.g., 16x), the depth features are enhanced from coarse to fine. In each frequency-dependent refinement of the depth features, the above iterative subnetwork as well as the residual learning are introduced. The proposed method is tested for the noise-free and noisy cases which compares against 16 state-of-the-art methods. Our experimental results show the improved performances based on the qualitative and quantitative evaluations.
WOS:000619321200003
</snippet>
</document>

<document id="712">
<title>A Deep Learning-Based Framework for Automated Extraction of Building Footprint Polygons from Very High-Resolution Aerial Imagery</title>
<url>http://dx.doi.org/10.3390/rs13183630</url>
<snippet>Accurate building footprint polygons provide essential data for a wide range of urban applications. While deep learning models have been proposed to extract pixel-based building areas from remote sensing imagery, the direct vectorization of pixel-based building maps often leads to building footprint polygons with irregular shapes that are inconsistent with real building boundaries, making it difficult to use them in geospatial analysis. In this study, we proposed a novel deep learning-based framework for automated extraction of building footprint polygons (DLEBFP) from very high-resolution aerial imagery by combining deep learning models for different tasks. Our approach uses the U-Net, Cascade R-CNN, and Cascade CNN deep learning models to obtain building segmentation maps, building bounding boxes, and building corners, respectively, from very high-resolution remote sensing images. We used Delaunay triangulation to construct building footprint polygons based on the detected building corners with the constraints of building bounding boxes and building segmentation maps. Experiments on the Wuhan University building dataset and ISPRS Vaihingen dataset indicate that DLEBFP can perform well in extracting high-quality building footprint polygons. Compared with the other semantic segmentation models and the vector map generalization method, DLEBFP is able to achieve comparable mapping accuracies with semantic segmentation models on a pixel basis and generate building footprint polygons with concise edges and vertices with regular shapes that are close to the reference data. The promising performance indicates that our method has the potential to extract accurate building footprint polygons from remote sensing images for applications in geospatial analysis.
WOS:000701723300001
</snippet>
</document>

<document id="713">
<title>Domain-Adapted Convolutional Networks for Satellite Image Classification: A Large-Scale Interactive Learning Workflow</title>
<url>http://dx.doi.org/10.1109/JSTARS.2018.2795753</url>
<snippet>Satellite imagery often exhibits large spatial extent areas that encompass object classes with considerable variability. This often limits large-scale model generalization with machine learning algorithms. Notably, acquisition conditions, including dates, sensor position, lighting condition, and sensor types, often translate into class distribution shifts introducing complex nonlinear factors and hamper the potential impact of machine learning classifiers. This paper investigates the challenge of exploiting satellite images using convolutional neural networks (CNN) for settlement classification where the class distribution shifts are significant. We present a large-scale human settlement mapping workflow based-off multiple modules to adapt a pretrained CNN to address the negative impact of distribution shift on classification performance. To extend a locally trained classifier onto large spatial extents areas we introduce several submodules: First, a human-in-the-loop element for relabeling of misclassified target domain samples to generate representative examples for model adaptation; second, an efficient hashing module to minimize redundancy and noisy samples from themass-selected examples; and third, a novel relevance ranking module tominimize the dominance of source example on the target domain. The workflow presents a novel and practical approach to achieve large-scale domain adaptation with binary classifiers that are based-off CNN features. Experimental evaluations are conducted on areas of interest that encompass various image characteristics, including multisensors, multitemporal, and multiangular conditions. Domain adaptation is assessed on source-target pairs through the transfer loss and transfer ratio metrics to illustrate the utility of the workflow.
WOS:000427425000025
</snippet>
</document>

<document id="714">
<title>A deep learning framework for remote sensing image registration</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2017.12.012</url>
<snippet>We propose an effective deep neural network aiming at remote sensing image registration problem. Unlike conventional methods doing feature extraction and feature matching separately, we pair patches from sensed and reference images, and then learn the mapping directly between these patch-pairs and their matching labels for later registration. This end-to-end architecture allows us to optimize the whole processing (learning mapping function) through information feedback when training the network, which is lacking in conventional methods. In addition, to alleviate the small data issue of remote sensing images for training, our proposal introduces a self-learning by learning the mapping function using images and their transformed copies. Moreover, we apply a transfer learning to reduce the huge computation cost in the training stage. It does not only speed up our framework, but also get extra performance gains. The comprehensive experiments conducted on seven sets of remote sensing images, acquired by Radarsat, SPOT and Landsat, show that our proposal improves the registration accuracy up to 2.4-53.7&#37;. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000449125400010
</snippet>
</document>

<document id="715">
<title>Building instance classification using street view images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.02.006</url>
<snippet>Land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades. Such classification is usually a patch-wise or pixel-wise labeling over the whole image. But for many applications, such as urban population density mapping or urban utility planning, a classification map based on individual buildings is much more informative. However, such semantic classification still poses some fundamental challenges, for example, how to retrieve fine boundaries of individual buildings. In this paper, we proposed a general framework for classifying the functionality of individual buildings. The proposed method is based on Convolutional Neural Networks (CNNs) which classify facade structures from street view images, such as Google Street View, in addition to remote sensing images which usually only show roof structures. Geographic information was utilized to mask out individual buildings, and to associate the corresponding street view images. We created a benchmark dataset which was used for training and evaluating CNNs. In addition, the method was applied to generate building classification maps on both region and city scales of several cities in Canada and the US. (C) 2018 The Author(s). Published by Elsevier B.V. on behalf of International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).
WOS:000449125400004
</snippet>
</document>

<document id="716">
<title>Multiscale Dual-Branch Residual Spectral-Spatial Network With Attention for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3188732</url>
<snippet>The development of remote sensing images in recent years has made it possible to identify materials in inaccessible environments and study natural materials on a large scale. But hyperspectral images (HSIs) are a rich source of information with their unique features in various applications. However, several problems reduce the accuracy of HSI classification; for example, the extracted features are not effective, noise, the correlation of bands, and most importantly, the limited labeled samples. To improve accuracy in the case of limited training samples, we propose a multiscale dual-branch residual spectral-spatial network with attention to the HSI classification model named MDBRSSN in this article. First, due to the correlation and redundancy between HSI bands, a principal component analysis operation is applied to preprocess the raw HSI data. Then, in MDBRSSN, a dual-branch structure is designed to extract the useful spectral-spatial features of HSI. The advanced feature, multiscale abstract information extracted by the convolution neural network, is applied to image processing, which can improve complex hyperspectral data classification accuracy. In addition, the attention mechanisms applied separately to each branch enable MDBRSSN to optimize and refine the extracted feature maps. Such an MDBRSSN framework can learn and fuse deeper hierarchical spectral-spatial features with fewer training samples. The purpose of designing the MDBRSSN model is to have high classification accuracy compared to state-of-the-art methods when the training samples are limited, which is proved by the results of the experiments in this article on four datasets. In Salinas, Pavia University, Indian Pines, and Houston 2013, the proposed model obtained 99.64&#37;, 98.93&#37;, 98.17&#37;, and 96.57&#37; overall accuracy using only 1&#37;, 1&#37;, 5&#37;, and 5&#37; of labeled data for training, respectively, which are much better compared to the state-of-the-art methods.
WOS:000828222400001
</snippet>
</document>

<document id="717">
<title>A Scale Sequence Object-based Convolutional Neural Network (SS-OCNN) for crop classification from fine spatial resolution remotely sensed imagery</title>
<url>http://dx.doi.org/10.1080/17538947.2021.1950853</url>
<snippet>The highly dynamic nature of agro-ecosystems in space and time usually leads to high intra-class variance and low inter-class separability in the fine spatial resolution (FSR) remotely sensed imagery. This makes traditional classifiers essentially relying on spectral information for crop mapping from FSR imagery an extremely challenging task. To mine effectively the rich spectral and spatial information in FSR imagery, this paper proposed a Scale Sequence Object-based Convolutional Neural Network (SS-OCNN) that classifies images at the object level by taking segmented objects (crop parcels) as basic units of analysis, thus, ensuring that the boundaries between crop parcels are delineated precisely. These segmented objects were subsequently classified using a CNN model integrated with an automatically generated scale sequence of input patch sizes. This scale sequence can fuse effectively the features learned at different scales by transforming progressively the information extracted at small scales to larger scales. The effectiveness of the SS-OCNN was investigated using two heterogeneous agricultural areas with FSR SAR and optical imagery, respectively. Experimental results revealed that the SS-OCNN consistently achieved the most accurate classification results. The SS-OCNN, thus, provides a new paradigm for crop classification over heterogeneous areas using FSR imagery, and has a wide application prospect.
WOS:000670807600001
</snippet>
</document>

<document id="718">
<title>Landslide Susceptibility Mapping with Deep Learning Algorithms</title>
<url>http://dx.doi.org/10.3390/su14031734</url>
<snippet>Among natural hazards, landslides are devastating in China. However, little is known regarding potential landslide-prone areas in Maoxian County. The goal of this study was to apply four deep learning algorithms, the convolutional neural network (CNN), deep neural network (DNN), long short-term memory (LSTM) networks, and recurrent neural network (RNN) in evaluating the possibility of landslides throughout Maoxian County, Sichuan, China. A total of 1290 landslide records was developed using historical records, field observations, and remote sensing techniques. The landslide susceptibility maps showed that most susceptible areas were along the Minjiang River and in some parts of the southeastern portion of the study area. Slope, rainfall, and distance to faults were the most influential factors affecting landslide occurrence. Results revealed that proportion of landslide susceptible areas in Maoxian County was as follows: identified landslides (13.65-23.71&#37;) and non-landslides (76.29-86.35&#37;). The resultant maps were tested against known landslide locations using the area under the curve (AUC). This study indicated that the DNN algorithm performed better than LSTM, CNN, and RNN in identifying landslides in Maoxian County, with AUC values (for prediction accuracy) of 87.30&#37;, 86.50&#37;, 85.60&#37;, and 82.90&#37;, respectively. The results of this study are useful for future landslide risk reduction along with devising sustainable land use planning in the study area.
WOS:000759976600001
</snippet>
</document>

<document id="719">
<title>An Automated Snow Mapper Powered by Machine Learning</title>
<url>http://dx.doi.org/10.3390/rs13234826</url>
<snippet>Snow preserves fresh water and impacts regional climate and the environment. Enabled by modern satellite Earth observations, fast and accurate automated snow mapping is now possible. In this study, we developed the Automated Snow Mapper Powered by Machine Learning (AutoSMILE), which is the first machine learning-based open-source system for snow mapping. It is built in a Python environment based on object-based analysis. AutoSMILE was first applied in a mountainous area of 1002 km(2) in Bome County, eastern Tibetan Plateau. A multispectral image from Sentinel-2B, a digital elevation model, and machine learning algorithms such as random forest and convolutional neural network, were utilized. Taking only 5&#37; of the study area as the training zone, AutoSMILE yielded an extraordinarily satisfactory result over the rest of the study area: the producers accuracy, users accuracy, intersection over union and overall accuracy reached 99.42&#37;, 98.78&#37;, 98.21&#37; and 98.76&#37;, respectively, at object level, corresponding to 98.84&#37;, 98.35&#37;, 97.23&#37; and 98.07&#37;, respectively, at pixel level. The model trained in Bome County was subsequently used to map snow at the Qimantag Mountain region in the northern Tibetan Plateau, and a high overall accuracy of 97.22&#37; was achieved. AutoSMILE outperformed threshold-based methods at both sites and exhibited superior performance especially in handling complex land covers. The outstanding performance and robustness of AutoSMILE in the case studies suggest that AutoSMILE is a fast and reliable tool for large-scale high-accuracy snow mapping and monitoring.
WOS:000735166300001
</snippet>
</document>

<document id="720">
<title>Automated crater detection with human level performance</title>
<url>http://dx.doi.org/10.1016/j.cageo.2020.104645</url>
<snippet>Crater cataloging is an important yet time-consuming part of geological mapping. We present an automated Crater Detection Algorithm (CDA) that is competitive with expert-human researchers and hundreds of times faster. The CDA uses multiple neural networks to process digital terrain model and thermal infra-red imagery to identify and locate craters across the surface of Mars. We use additional post-processing filters to refine and remove potential false crater detections, improving our precision and recall by 10&#37; compared to Lee (2019). We now find 80&#37; of known craters above 3km in diameter, and identify 7,000 potentially new craters (13&#37; of the identified craters). The median differences between our catalog and other independent catalogs is 2&#37;-4&#37; in location and diameter, in-line with other inter-catalog comparisons. The CDA has been used to process global terrain maps and infra-red imagery for Mars, and the software and generated global catalog are available at https://doi.org/10.5683/SP2/CFUNII.
WOS:000609495800005
</snippet>
</document>

<document id="721">
<title>Mapping fine-scale urban housing prices by fusing remotely sensed imagery and social media data</title>
<url>http://dx.doi.org/10.1111/tgis.12330</url>
<snippet>The accurate mapping of urban housing prices at a fine scale is essential to policymaking and urban studies, such as adjusting economic factors and determining reasonable levels of residential subsidies. Previous studies focus mainly on housing price analysis at a macro scale, without fine-scale study due to a lack of available data and effective models. By integrating a convolutional neural network for united mining (UMCNN) and random forest (RF), this study proposes an effective deep-learning-based framework for fusing multi-source geo-spatial data, including high spatial resolution (HSR) remotely sensed imagery and several types of social media data, and maps urban housing prices at a very fine scale. With the collected housing price data from Chinas biggest online real estate market, we produced the spatial distribution of housing prices at a spatial resolution of 5 m in Shenzhen, China. By comparing with eight other multi-source data mining techniques, the UMCNN obtained the highest housing price simulation accuracy (Pearson R = 0.922, OA = 85.82&#37;). The results also demonstrated a complex spatial heterogeneity inside Shenzhens housing price distribution. In future studies, we will work continuously on housing price policymaking and residential issues by including additional sources of spatial data.
WOS:000430399600010
</snippet>
</document>

<document id="722">
<title>A deep learning ensemble model for wildfire susceptibility mapping</title>
<url>http://dx.doi.org/10.1016/j.ecoinf.2021.101397</url>
<snippet>Devastating wildfires have increased in frequency and intensity over the last few years, worsened by climate change and prolonged droughts. Wildfire susceptibility mapping with machine learning has been proven useful for fire-prevention plans, turning into an indispensable tool in wildfire prevention. However, applications of deep learning models in wildfire susceptibility prediction to date are scarce. This study proposes a new Ensemble model based on two deep learning networks previously presented in literature that achieved remarkable results for forest fire susceptibility and other environmental risks. We compare our model with each of its sub-models, two more deep learning networks, and other machine learning benchmark, namely, XGBoost and SVM. Furthermore, we analyze the effects that different sample patch sizes have on the predictive performance of the algorithms. As case study we selected the fire occurrences in two regions in Chile, from 2013 to 2019. Satellite imagery data for fifteen fire influencing factors in the study area were retrieved to build a dataset to extract the samples to train the models. These factors include elevation, aspect, surface roughness, slope, minimum and maximum temperature, wind speed, precipitation, actual evapotranspiration, climatic water deficit, NDVI, land cover type, distance to rivers, distance to roads and distance to urban areas. During training, the best sample patch size was found to be 25 x 25 pixels. As a result, the highest area under the curve (AUC) was 0.953 achieved by the Ensemble model, followed by CNN-1 with AUC = 0.902. The Ensemble model also achieved the best accuracy, sensitivity, specificity, negative predictive value and F1 score. Finally, the predicted susceptibility maps suggest that static variables can be considered as predisposing factors, while dynamic variables affect the intensity of the predicted probabilities, with an important role of the anthropogenic variables. These resulting maps may be useful to prioritize wildfire surveillance and monitoring in extensive high risk areas.
WOS:000703766700001
</snippet>
</document>

<document id="723">
<title>Triple-Attention-Based Parallel Network for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.3390/rs13020324</url>
<snippet>Convolutional neural networks have been highly successful in hyperspectral image classification owing to their unique feature expression ability. However, the traditional data partitioning strategy in tandem with patch-wise classification may lead to information leakage and result in overoptimistic experimental insights. In this paper, we propose a novel data partitioning scheme and a triple-attention parallel network (TAP-Net) to enhance the performance of HSI classification without information leakage. The dataset partitioning strategy is simple yet effective to avoid overfitting, and allows fair comparison of various algorithms, particularly in the case of limited annotated data. In contrast to classical encoder-decoder models, the proposed TAP-Net utilizes parallel subnetworks with the same spatial resolution and repeatedly reuses high-level feature maps of preceding subnetworks to refine the segmentation map. In addition, a channel-spectral-spatial-attention module is proposed to optimize the information transmission between different subnetworks. Experiments were conducted on three benchmark hyperspectral datasets, and the results demonstrate that the proposed method outperforms state-of-the-art methods with the overall accuracy of 90.31&#37;, 91.64&#37;, and 81.35&#37; and the average accuracy of 93.18&#37;, 87.45&#37;, and 78.85&#37; over Salinas Valley, Pavia University and Indian Pines dataset, respectively. It illustrates that the proposed TAP-Net is able to effectively exploit the spatial-spectral information to ensure high performance.
WOS:000611551600001
</snippet>
</document>

<document id="724">
<title>A global coral reef probability map generated using convolutional neural networks</title>
<url>http://dx.doi.org/10.1007/s00338-020-02005-6</url>
<snippet>Coral reef research and management efforts can be improved when supported by reef maps providing local-scale details across global extents. However, such maps are difficult to generate due to the broad geographic range of coral reefs, the complexities of relating satellite imagery to geomorphic or ecological realities, and other challenges. However, reef extent maps are one of the most commonly used and most valuable data products from the perspective of reef scientists and managers. Here, we used convolutional neural networks to generate a globally consistent coral reef probability map-a probabilistic estimate of the geospatial extent of reef ecosystems-to facilitate scientific, conservation, and management efforts. We combined a global mosaic of high spatial resolution Planet Dove satellite imagery with regional Millennium Coral Reef Mapping Project reef extents to build training, validation, and application datasets. These datasets trained our reef extent prediction model, a neural network with a dense-unet architecture followed by a random forest classifier, which was used to produce a global coral reef probability map. Based on this probability map, we generated a global coral reef extent map from a 60&#37; threshold of reef probability (reef: probability &gt;= 60&#37;, non-reef: probability &lt; 60&#37;). Our findings provide a proof-of-concept method for global reef extent estimates using a consistent and readily updateable methodology that leverages modern deep learning approaches to support downstream users. These maps are openly-available through the Allen Coral Atlas.
WOS:000572603800001
</snippet>
</document>

<document id="725">
<title>Light-YOLOv4: An Edge-Device Oriented Target Detection Method for Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3120009</url>
<snippet>Most deep-learning-based target detection methods have high computational complexity and memory consumption, and they are difficult to deploy on edge devices with limited computing resources and memory. To tackle this problem, this article proposes to learn a lightweight detector named Light-YOLOv4, and it is obtained from YOLOv4 through model compression. To this end, first, we perform sparsity training by applying L1 regularization to the channel scaling factors, so the less important channels and layers can be found. Second, channel pruning and layer pruning are enforced on the network to prune the less important parts, which could significantly reduce networks width and depth. Third, the pruned model is retrained with a knowledge distillation method to improve the detection accuracy. Fourth, the model is quantized from FP32 to FP16, and it could further accelerate the model with almost no loss of detection accuracy. Finally, to evaluate Light-YOLOv4s performance on edge devices, Light-YOLOv4 is deployed on NVIDIA Jetson TX2. Experiments on the SAR ship detection dataset (SSDD) demonstrate that the model size, parameter size, and FLOPs of Light-YOLOv4 have been reduced by 98.63&#37;, 98.66&#37;, and 91.30&#37; compared with YOLOv4, and the detection speed has been increased to 4.2x. While the detection accuracy of Light-YOLOv4 is only slightly reduced, for example, the mAP has only reduced by 0.013. Besides, experiments on the Gaofen Airplane dataset also prove the feasibility of Light-YOLOv4. Moreover, compared with other state-of-the-art methods, such as SSD and FPN, Light-YOLOv4 is more suitable for edge devices.
WOS:000714714100003
</snippet>
</document>

<document id="726">
<title>Geospatial data mining for digital raster mapping</title>
<url>http://dx.doi.org/10.1080/15481603.2018.1517445</url>
<snippet>We performed an in-depth literature survey to identify the most popular data mining approaches that have been applied for raster mapping of ecological parameters through the use of Geographic Information Systems (GIS) and remotely sensed data. Popular data mining approaches included decision trees or "data mining" trees which consist of regression and classification trees, random forests, neural networks, and support vector machines. The advantages of each data mining approach as well as approaches to avoid overfitting are subsequently discussed. We also provide suggestions and examples for the mapping of problematic variables or classes, future or historical projections, and avoidance of model bias. Finally, we address the separate issues of parallel processing, error mapping, and incorporation of "no data" values into modeling processes. Given the improved availability of digital spatial products and remote sensing products, data mining approaches combined with parallel processing potentials should greatly improve the quality and extent of ecological datasets.
WOS:000457866600005
</snippet>
</document>

<document id="727">
<title>Progress Guidance Representation for Robust Interactive Extraction of Buildings from Remotely Sensed Images</title>
<url>http://dx.doi.org/10.3390/rs13245111</url>
<snippet>Accurate building extraction from remotely sensed images is essential for topographic mapping, cadastral surveying and many other applications. Fully automatic segmentation methods still remain a great challenge due to the poor generalization ability and the inaccurate segmentation results. In this work, we are committed to robust click-based interactive building extraction in remote sensing imagery. We argue that stability is vital to an interactive segmentation system, and we observe that the distance of the newly added click to the boundaries of the previous segmentation mask contains progress guidance information of the interactive segmentation process. To promote the robustness of the interactive segmentation, we exploit this information with the previous segmentation mask, positive and negative clicks to form a progress guidance map, and feed it to a convolutional neural network (CNN) with the original RGB image, we name the network as PGR-Net. In addition, an adaptive zoom-in strategy and an iterative training scheme are proposed to further promote the stability of PGR-Net. Compared with the latest methods FCA and f-BRS, the proposed PGR-Net basically requires 1-2 fewer clicks to achieve the same segmentation results. Comprehensive experiments have demonstrated that the PGR-Net outperforms related state-of-the-art methods on five natural image datasets and three building datasets of remote sensing images.
WOS:000737301100001
</snippet>
</document>

<document id="728">
<title>INITIAL EVALUATION OF THE POTENTIAL OF SMARTPHONE STEREO-VISION IN MUSEUM VISITS</title>
<url>http://dx.doi.org/10.5194/isprs-archives-XLII-2-W11-837-2019</url>
<snippet>The recent introduction of new technologies such as augmented reality, machine learning and the worldwide spread of mobile devices provided with imaging, navigation sensors and high computational power can be exploited in order to drammatically change the museum visit experience. Differently from the traditional use of museum docents or audio guides, the introduction of digital technologies already proved to be useful in order to improve the interest of the visitor thanks to the increased interaction and involvement, reached also by means of visual effects and animations. Actually, the availability of 3D representations, augmented reality and navigation abilities directly on the visitors device can lead to a personalized visit, enabling the visitor to have an experience tailored on his/her needs. In this framework, this paper aims at investigating the potentialities of smartphone stereo-vision to improve the geometric information about the artworks available on the visitors device. More specifically, in this work smartphone stereo-vision will used as a 3D model generation tool in a 3D artwork recognition system based on a neural network classifier.
WOS:000568508700115
</snippet>
</document>

<document id="729">
<title>Remote-Sensing Image Usability Assessment Based on ResNet by Combining Edge and Texture Maps</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2914715</url>
<snippet>Authentic remote-sensing images suffer non-uniform complex distortions during acquisition, transmission, and storage. Clouds, light, and exposure also affect local quality. This paper constructs a usability-based subjective remote-sensing image dataset and gives a definition of usability for images with nonuniform distortion, where the image usability is determined by the weighted quality of images blocks. It is difficult to extract the handcraft features from remote-sensing images with complex mixture distortion. Recently, convolutional neural network (CNN) has been introduced into blind quality assessment for images with uniform distortion, which includes feature learning and regression in one processing. In this paper, we first describe and systematically analyze the usability of remote-sensing images in detail. Then, we propose a remote-sensing image usability assessment (RSIUA) method based on a residual network by combining edge and texture maps. The score of remote-sensing image usability was obtainedwith the weighted averaging of the quality scores of all image blocks, and the weight of each image block was determined by its quality score. We compared the proposed method with three traditional image quality assessment methods, one CNN-based method for images with simulated distortion, and one scale-invariant feature transform-based RSIUA method. The linear correlation coefficient, Spearmans rank ordered correlation coefficient, and root-mean-squared error of experiments demonstrate that our method outperforms all five competitors. The experiments also reveal that the edge and texture maps can improve the performance.
WOS:000476807300018
</snippet>
</document>

<document id="730">
<title>Building Footprint Extraction From Unmanned Aerial Vehicle Images Via PRU-Net: Application to Change Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3052495</url>
<snippet>As the manual detection of building footprint is inefficient and labor-intensive, this study proposed a method of building footprint extraction and change detection based on deep convolutional neural networks. The study modified the existing U-Net model to develop the "PRU-Net" model. PRU-Net incorporates pyramid scene parsing (PSP) to allow multiscale scene parsing, a residual block (RB) in ResNet for feature extraction, and focal loss to address sample imbalance. Within the proposed method, building footprint extraction is conducted as follows: 1) unmanned aerial vehicle images are cropped, denoised, and semantically marked, and datasets are created (including training/validation and prediction datasets); 2) the training/validation and prediction datasets are input into the full convolutional neural network PRU-Net for model training/validation and prediction. Compared with the U-Net, PSP+U-Net (PU-Net), and U-Net++ models, PRU-Net offers improved footprint extraction of buildings with a range of sizes and shapes. The large-scale experimental results demonstrated the effectiveness of the PSP module for multiscale scene analysis and the RB module for feature extraction. After demonstrating the improvements in building extraction offered by PRU-Net, the building footprint results were further processed to generate a building change map.
WOS:000617374800007
</snippet>
</document>

<document id="731">
<title>FPS-Net: A convolutional fusion network for large-scale LiDAR point cloud segmentation</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.04.011</url>
<snippet>Scene understanding based on LiDAR point cloud is an essential task for autonomous cars to drive safely, which often employs spherical projection to map 3D point cloud into multi-channel 2D images for semantic segmentation. Most existing methods simply stack different point attributes/modalities (e.g. coordinates, intensity, depth, etc.) as image channels to increase information capacity, but ignore distinct characteristics of point attributes in different image channels. We design FPS-Net, a convolutional fusion network that exploits the uniqueness and discrepancy among the projected image channels for optimal point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead of simply stacking multiple channel images as a single input, we group them into different modalities to first learn modality-specific features separately and then map the learnt features into a common high-dimensional feature space for pixel-level fusion and learning. Specifically, we design a residual dense block with multiple receptive fields as a building block in encoder which preserves detailed information in each modality and learns hierarchical modality-specific and fused features effectively. In the FPS-Net decoder, we use a recurrent convolution block likewise to hierarchically decode fused features into output space for pixel-level classification. Extensive experiments conducted on two widely adopted point cloud datasets show that FPS-Net achieves superior semantic segmentation as compared with state-of-the-art projection-based methods. Specifically, FPS-Net outperforms the state-of-the-art in both accuracy (4.9&#37; higher than RangeNet++ and 2.8&#37; higher than PolarNet in mIoU) and computation speed (15.0 FPS faster than Squeeze-SegV3) for SemanticKITTI benchmark. For KITTI benchmark, FPS-Net achieves significant accuracy improvement (12.6&#37; higher than RangeNet++ in mIoU) with comparable computation speed. In addition, the proposed modality fusion idea is compatible with typical projection-based methods and can be incorporated into them with consistent performance improvement.
WOS:000655474600018
</snippet>
</document>

<document id="732">
<title>Accuracy Assessment in Convolutional Neural Network-Based Deep Learning Remote Sensing Studies-Part 1: Literature Review</title>
<url>http://dx.doi.org/10.3390/rs13132450</url>
<snippet>Convolutional neural network (CNN)-based deep learning (DL) is a powerful, recently developed image classification approach. With origins in the computer vision and image processing communities, the accuracy assessment methods developed for CNN-based DL use a wide range of metrics that may be unfamiliar to the remote sensing (RS) community. To explore the differences between traditional RS and DL RS methods, we surveyed a random selection of 100 papers from the RS DL literature. The results show that RS DL studies have largely abandoned traditional RS accuracy assessment terminology, though some of the accuracy measures typically used in DL papers, most notably precision and recall, have direct equivalents in traditional RS terminology. Some of the DL accuracy terms have multiple names, or are equivalent to another measure. In our sample, DL studies only rarely reported a complete confusion matrix, and when they did so, it was even more rare that the confusion matrix estimated population properties. On the other hand, some DL studies are increasingly paying attention to the role of class prevalence in designing accuracy assessment approaches. DL studies that evaluate the decision boundary threshold over a range of values tend to use the precision-recall (P-R) curve, the associated area under the curve (AUC) measures of average precision (AP) and mean average precision (mAP), rather than the traditional receiver operating characteristic (ROC) curve and its AUC. DL studies are also notable for testing the generalization of their models on entirely new datasets, including data from new areas, new acquisition times, or even new sensors.
WOS:000671055800001
</snippet>
</document>

<document id="733">
<title>Automated lithological mapping by integrating spectral enhancement techniques and machine learning algorithms using AVIRIS-NG hyperspectral data in Gold-bearing granite-greenstone rocks in Hutti, India</title>
<url>http://dx.doi.org/10.1016/j.jag.2019.102006</url>
<snippet>In this study, we proposed an automated lithological mapping approach by using spectral enhancement techniques and Machine Learning Algorithms (MLAs) using Airborne Visible Infrared Imaging Spectroradiometer-Next Generation (AVIRIS-NG) hyperspectral data in the greenstone belt of the Hutti area, India. We integrated spectral enhancement techniques such as Principal Component Analysis (PCA) and Independent Component Analysis (ICA) transformation and different MLAs for an accurate mapping of rock types. A conjugate utilization of conventional geological map and spectral enhancement products derived from ASTER data were used for the preparation of a high-resolution reference lithology map. Feature selection and extraction methods were applied on the AVIRIS-NG data to derive different input dataset such as (a) all spectral bands, (b) shortwave infrared bands, 
WOS:000509787800002
</snippet>
</document>

<document id="734">
<title>Flood depth mapping in street photos with image processing and deep neural networks</title>
<url>http://dx.doi.org/10.1016/j.compenvurbsys.2021.101628</url>
<snippet>Many parts of the world experience severe episodes of flooding every year. In addition to the high cost of mitigation and damage to property, floods make roads impassable and hamper community evacuation, movement of goods and services, and rescue missions. Knowing the depth of floodwater is critical to the success of response and recovery operations that follow. However, flood mapping especially in urban areas using traditional methods such as remote sensing and digital elevation models (DEMs) yields large errors due to reshaped surface topography and microtopographic variations combined with vegetation bias. This paper presents a deep neural network approach to detect submerged stop signs in photos taken from flooded roads and intersections, coupled with Canny edge detection and probabilistic Hough transform to calculate pole length and estimate floodwater depth. Additionally, a tilt correction technique is implemented to address the problem of sideways tilt in visual analysis of submerged stop signs. An in-house dataset, named BluPix 2020.1 consisting of paired web-mined photos of submerged stop signs across 10 FEMA regions (for U.S. locations) and Canada is used to evaluate the models. Overall, pole length is estimated with an RMSE of 17.43 and 8.61 in. in pre- and post-flood photos, respectively, leading to a mean absolute error of 12.63 in. in floodwater depth estimation. Findings of this research are sought to equip jurisdictions, local governments, and citizens in flood-prone regions with a simple, reliable, and scalable solution that can provide (near-) real time estimation of floodwater depth in their surroundings.
WOS:000661278600004
</snippet>
</document>

<document id="735">
<title>Retro-Remote Sensing: Generating Images From Ancient Texts</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2895693</url>
<snippet>The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area.
WOS:000463025100015
</snippet>
</document>

<document id="736">
<title>Advances in multi- and hyperspectral remote sensing of mangrove species: A synthesis and study case on airborne and multisource spaceborne imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.12.003</url>
<snippet>This study summarizes the advances in mangrove species mapping based on multispectral and hyperspectral imagery achieved over the last decade. The influence of species diversity and sensor specifications on the per-formances of various classification approaches are discussed. Based on the limitations of previous approaches, we propose a novel framework to map mangrove species at medium, high, and very-high spatial resolution using multispectral and hyperspectral images. The framework relies on a multitask convolutional neural network to achieve accurate classification at pixel and object (i.e. individual tree crown) level. It was successfully applied to the most comprehensive dataset of optical imagery ever collected over a mangrove forest, achieving accurate mapping of species on airborne (OA = 95 &#37;, Kappa = 0.93) and spaceborne imagery (OA and Kappa up to 97 &#37; and 0.95, respectively), including the newly-operating hyperspectral DESIS and PRISMA instruments. This study brings new insights into the role of spatial and spectral resolutions in mangrove species classification, particu-larly the importance of short-wave infrared bands. Multispectral imagery performs well at very-high to high resolutions (up to 10-20 m) but suffers from the lack of spectral information at medium resolution (30 m). Hyperspectral imagery provided the best results at sub-metric resolution (0.5 m) and satisfactory results at 30-m resolution (OA &gt;= 85), and could benefit from implementing spectral unmixing in the latter case. Given the increasing attention accorded to mangroves, remote sensing will undoubtedly become an essential tool to monitor the diversity of these endangered ecosystems in the future. In that respect, efforts should concentrate on evaluating the capabilities of new and upcoming instruments and multisource data combination to improve mangrove species mapping and, more broadly, to meet conservation goals.
WOS:000913199400003
</snippet>
</document>

<document id="737">
<title>Neural Network-Based Urban Change Monitoring with Deep-Temporal Multispectral and SAR Remote Sensing Data</title>
<url>http://dx.doi.org/10.3390/rs13153000</url>
<snippet>Remote-sensing-driven urban change detection has been studied in many ways for decades for a wide field of applications, such as understanding socio-economic impacts, identifying new settlements, or analyzing trends of urban sprawl. Such kinds of analyses are usually carried out manually by selecting high-quality samples that binds them to small-scale scenarios, either temporarily limited or with low spatial or temporal resolution. We propose a fully automated method that uses a large amount of available remote sensing observations for a selected period without the need to manually select samples. This enables continuous urban monitoring in a fully automated process. Furthermore, we combine multispectral optical and synthetic aperture radar (SAR) data from two eras as two mission pairs with synthetic labeling to train a neural network for detecting urban changes and activities. As pairs, we consider European Remote Sensing (ERS-1/2) and Landsat 5 Thematic Mapper (TM) for 1991-2011 and Sentinel 1 and 2 for 2017-2021. For every era, we use three different urban sites-Limassol, Rotterdam, and Liege-with at least 500 km(2) each, and deep observation time series with hundreds and up to over a thousand of samples. These sites were selected to represent different challenges in training a common neural network due to atmospheric effects, different geographies, and observation coverage. We train one model for each of the two eras using synthetic but noisy labels, which are created automatically by combining state-of-the-art methods, without the availability of existing ground truth data. To combine the benefit of both remote sensing types, the network models are ensembles of optical- and SAR-specialized sub-networks. We study the sensitivity of urban and impervious changes and the contribution of optical and SAR data to the overall solution. Our implementation and trained models are available publicly to enable others to utilize fully automated continuous urban monitoring.
WOS:000682301100001
</snippet>
</document>

<document id="738">
<title>Keratoconus Screening Based on Deep Learning Approach of Corneal Topography</title>
<url>http://dx.doi.org/10.1167/tvst.9.2.53</url>
<snippet>Purpose: To develop and compare deep learning (DL) algorithms to detect keratoconus on the basis of corneal topography and validate with visualization methods. Methods: We retrospectively collected corneal topographies of the study group with clinically manifested keratoconus and the control group with regular astigmatism. All images were divided into training and test datasets. We adopted three convolutional neural network (CNN) models for learning. The test dataset was applied to analyze the performance of the three models. In addition, for better discrimination and understanding, we displayed the pixel-wise discriminative features and class-discriminative heat map of diopter images for visualization. Results: Overall, 170 keratoconus, 28 subclinical keratoconus and 156 normal topographic pictures were collected. The convergence of accuracy and loss for the training and test datasets after training revealed no overfitting in all three CNN models. The sensitivity and specificity of all CNN models were over 0.90, and the area under the receiver operating characteristic curve reached 0.995 in the ResNet152 model. The pixel-wise discriminative features and the heat map of the prediction layer in the VGG16 model both revealed it focused on the largest gradient difference of topographic maps, which was corresponding to the diagnostic clues of ophthalmologists. The subclinical keratoconus was positively predicted with our model and also correlated with topographic indexes. Conclusions: The DL models had fair accuracy for keratoconus screening based on corneal topographic images. The visualization mentioned in the current study revealed that the model focused on the appropriate region for diagnosis and rendered clinical explainability of deep learning more acceptable. Translational Relevance: These high accuracy CNN models can aid ophthalmologists in keratoconus screening with color-coded corneal topography maps.
WOS:000599489500039
</snippet>
</document>

<document id="739">
<title>PolyWorld: Polygonal Building Extraction with Graph Neural Networks in Satellite Images</title>
<url>http://dx.doi.org/10.1109/CVPR52688.2022.00189</url>
<snippet>While most state-of-the-art instance segmentation methods produce binary segmentation masks, geographic and cartographic applications typically require precise vector polygons of extracted objects instead of rasterized output. This paper introduces PolyWorld, a neural network that directly extracts building vertices from an image and connects them correctly to create precise polygons. The model predicts the connection strength between each pair of vertices using a graph neural network and estimates the assignments by solving a differentiable optimal transport problem. Moreover, the vertex positions are optimized by minimizing a combined segmentation and polygonal angle difference loss. PolyWorld significantly outperforms the state of the art in building polygonization and achieves not only notable quantitative results, but also produces visually pleasing building polygons. Code and trained weights are publicly available at https://github.com/zorzi-s/PolyWorldPretrainedNetwork.
WOS:000867754202010
</snippet>
</document>

<document id="740">
<title>Mapping Cropland Extent in Pakistan Using Machine Learning Algorithms on Google Earth Engine Cloud Computing Framework</title>
<url>http://dx.doi.org/10.3390/ijgi12020081</url>
<snippet>An actual cropland extent product with a high spatial resolution with a precision of up to 60 m is believed to be particularly significant in tackling numerous water security concerns and world food challenges. To advance the development of niche, advanced cropland goods such as crop variety techniques, crop intensities, crop water production, and crop irrigation, it is necessary to examine how cropland products typically span narrow or expansive farmlands. Some of the existing challenges are processing by constructing precision-high resolution cropland-wide items of training and testing data on diverse geographical locations and safe frontiers, computing capacity, and managing vast volumes of geographical data. This analysis includes eight separate Sentinel-2 multi-spectral instruments data from 2018 to 2019 (Short-wave Infrared Imagery (SWIR 2), SWIR 1, Cirrus, the near infrared, red, green, blue, and aerosols) have been used. Pixel-based classification algorithms have been employed, and their precision is measured and scrutinized in this study. The computations and analyses have been conducted on the cloud-based Google Earth Engine computing network. Training and testing data were obtained from the Google Earth Engine map console at a high spatial 10 m resolution for this analysis. The basis of research information for testing the computer algorithms consists of 855 training samples, culminating in a manufacturing field of 200 individual validation samples measuring product accuracy. The Pakistan cropland extent map produced in this study using four state-of-the-art machine learning (ML) approaches, Random Forest, SVM, Naive Bayes &amp; CART shows an overall validation accuracy of 82&#37;, 89&#37; manufacturer accuracy, and 77&#37; customer accuracy. Among these four machine learning algorithms, the CART algorithm overperformed the other three, with an impressive classification accuracy of 93&#37;. Pakistans average cropland areas were calculated to be 370,200 m(2), and the croplands scale of goods indicated that sub-national croplands could be measured. The research offers a conceptual change in the development of cropland maps utilizing a remote sensing multi-date.
WOS:000939323500001
</snippet>
</document>

<document id="741">
<title>Detection of Pictorial Map Objects with Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.1080/00087041.2020.1738112</url>
<snippet>In this work, realistically drawn objects are identified on digital maps by convolutional neural networks. For the first two experiments, 6200 images were retrieved from Pinterest. While alternating image input options, two binary classifiers based on Xception and InceptionResNetV2 were trained to separate maps and pictorial maps. Results showed that the accuracy is 95-97&#37; to distinguish maps from other images, whereas maps with pictorial objects are correctly classified at rates of 87-92&#37;. For a third experiment, bounding boxes of 3200 sailing ships were annotated in historic maps from different digital libraries. Faster R-CNN and RetinaNet were compared to determine the box coordinates, while adjusting anchor scales and examining configurations for small objects. A resulting average precision of 32&#37; was obtained for Faster R-CNN and of 36&#37; for RetinaNet. Research outcomes are relevant for trawling map images on the Internet and for enhancing the advanced search of digital map catalogues.
WOS:000570645000001
</snippet>
</document>

<document id="742">
<title>Morphological Convolutional Neural Networks for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3088228</url>
<snippet>Convolutional neural networks (CNNs) have become quite popular for solving many different tasks in remote sensing data processing. The convolution is a linear operation, which extracts features from the input data. However, nonlinear operations are able to better characterize the internal relationships and hidden patterns within complex remote sensing data, such as hyperspectral images (HSIs). Morphological operations are powerful nonlinear transformations for feature extraction that preserve the essential characteristics of the image, such as borders, shape, and structural information. In this article, a new end-to-end morphological deep learning framework (called MorphConvHyperNet) is introduced. The proposed approach efficiently models nonlinear information during the training process of HSI classification. Specifically, our method includes spectral and spatial morphological blocks to extract relevant features from the HSI input data. These morphological blocks consist of two basic 2-D morphological operators (erosion and dilation) in the respective layers, followed by a weighted combination of the feature maps. Both layers can successfully encode the nonlinear information related to shape and size, playing an important role in classification performance. Our experimental results, obtained on five widely used HSIs, reveal that our newly proposed MorphConvHyperNet offers comparable (and even superior) performance when compared to traditional 2-D and 3-D CNNs for HSI classification.
WOS:000694698900015
</snippet>
</document>

<document id="743">
<title>Land parcel-based digital soil mapping of soil nutrient properties in an alluvial-diluvia plain agricultural area in China</title>
<url>http://dx.doi.org/10.1016/j.geoderma.2019.01.018</url>
<snippet>The ability to accurately and precisely perform soil nutrient mapping over large areas is essential in the decision-making processes for precision agriculture. However, existing grid-based or non-grid-based digital soil mapping (DSM) can lead to the problem of mixed units of input information, which causes the mapping results to be unsuitable for direct use in guiding the implementation of precision agriculture. Instead, the goal of this study was to achieve DSM based on land parcels, which are the basic units of agricultural management and have practical geographical significance for precision mapping in agricultural areas. This study established a convolutional neural network-based automatic extraction model to extract land parcels from high resolution remote sensing images. Thirty environmental covariates were chosen and calibrated at land parcels to establish the relationships between soils and landscapes. Four prediction algorithms, namely, ordinary kriging, cokriging, random forest and artificial neural network, were combined with the land-parcel-based DSM framework to develop and evaluate their effectiveness in predicting four topsoil nutrient properties in an alluvial-diluvia plain agricultural region located in Ningxia province, China. The results of comparisons show that, overall, the land-parcel-based RF model achieved the best prediction accuracy; its relative improvement (RMSE&#37;) values over the competing models were 1.27, 4.23, 3.19 and 9.01 for soil organic matter, soil total nitrogen, available phosphorus and available potassium, respectively. In addition, land-parcel-based mapping can improve algorithmic efficiency by approximately 4 times by effectively reducing the mapping units for complex agricultural areas compared with the grid-based mapping results when using the same algorithm, and it also achieves a better performance at the detail level. Overall, the land-parcel-based DSM approach achieved good results in plain agricultural areas, but the model still needs improvement for land-parcel-based DSM in mountainous and hilly agricultural areas, and a challenge remains in selecting the most appropriate environmental covariates.
WOS:000457949200023
</snippet>
</document>

<document id="744">
<title>Automated Bale Mapping Using Machine Learning and Photogrammetry</title>
<url>http://dx.doi.org/10.3390/rs13224675</url>
<snippet>An automatic method of obtaining geographic coordinates of bales using monovision un-crewed aerial vehicle imagery was developed utilizing a data set of 300 images with a 20-megapixel resolution containing a total of 783 labeled bales of corn stover and soybean stubble. The relative performance of image processing with Otsus segmentation, you only look once version three (YOLOv3), and region-based convolutional neural networks was assessed. As a result, the best option in terms of accuracy and speed was determined to be YOLOv3, with 80&#37; precision, 99&#37; recall, 89&#37; F1 score, 97&#37; mean average precision, and a 0.38 s inference time. Next, the impact of using lower-cost cameras was evaluated by reducing image quality to one megapixel. The lower-resolution images resulted in decreased performance, with 79&#37; precision, 97&#37; recall, 88&#37; F1 score, 96&#37; mean average precision, and 0.40 s inference time. Finally, the output of the YOLOv3 trained model, density-based spatial clustering, photogrammetry, and map projection were utilized to predict the geocoordinates of the bales with a root mean squared error of 2.41 m.
WOS:000723933600001
</snippet>
</document>

<document id="745">
<title>High-Resolution Aerial Images Semantic Segmentation Using Deep Fully Convolutional Network With Channel Attention Mechanism</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2930724</url>
<snippet>Semantic segmentation is one of the fundamental tasks in understanding high-resolution aerial images. Recently, convolutional neural network (CNN) and fully convolutional network (FCN) have achieved excellent performance in general images' semantic segmentation tasks and have been introduced to the field of aerial images. In this paper, we propose a novel deep FCN with channel attention mechanism (CAM-DFCN) for high-resolution aerial images' semantic segmentation. The CAM-DFCN architecture follows the mode of encoder-decoder. In the encoder, two identical deep residual networks are both divided into multiple levels and acted on spectral images and auxiliary data, respectively. Then, the feature map concatenation is carried out at each level. In the decoder, the channel attentionmechanism (CAM) is introduced to automatically weigh the channels of featuremaps to perform feature selection. On the one hand, the CAM follows the concatenated feature maps at each level to select more discriminative features for classification. On the other hand, the CAM is used to further weigh the semantic information and spatial location information in the adjacent-level concatenated feature maps for more accurate predictions. We evaluate the proposed CAM-DFCN by using two benchmarks (the Potsdam set and the Vaihingen set) provided by the International Society for Photogrammetry and Remote Sensing. Experimental results show that the proposed method has considerable improvement.
WOS:000489785800029
</snippet>
</document>

<document id="746">
<title>One-Dimensional Convolution Neural Networks for Object-Based Feature Selection</title>
<url>http://dx.doi.org/10.1117/12.2325640</url>
<snippet>Recently, the new Geographic object-based image analysis (GEOBIA) was proposed as an alternative classification approach to pixel based ones. In GEOBIA, image segments can be depicted with various attributes such as spectral, texture, shape, deep features and context, and hence final classification can produce better land cover/use map. The presence of such a large number of features poses significant challenges to standard machine learning methods and has rendered many existing classification techniques impractical. In this work, we are interested to feature selection techniques, which are employed to reduce the dimensionality of the data while keeping the most of its expressive power. Inspired by recent works in remote sensing using Convolutional Neural Networks (CNNs), especially for hyperspectral band selection, a feature selection approach based on One-Dimensional Convolutional Neural Networks (1-D CNN) is proposed in this study. All object-based features are used to train the 1-D CNN to obtain well trained model. After testing different feature combinations, we use the well trained model to obtain their test classification accuracies, and finally we select the subset of features with the highest precision. In our experiments, we evaluate our feature selection approach on 30-cm resolution colour infrared (CIR) aerial orthoimagery. A multi-resolution segmentation is performed to segment the images into regions, which are characterized later using various spectral, textural and spatial attributes to form the final object-based feature dataset. The obtained experimental results show that the proposed method can achieve satisfactory results when compared with traditional feature selection approaches.
WOS:000455305000049
</snippet>
</document>

<document id="747">
<title>Mapping forest tree species in high resolution UAV-based RGB-imagery by means of convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.10.015</url>
<snippet>The use of unmanned aerial vehicles (UAVs) in vegetation remote sensing allows a time-flexible and cost-effective acquisition of very high-resolution imagery. Still, current methods for the mapping of forest tree species do not exploit the respective, rich spatial information. Here, we assessed the potential of convolutional neural networks (CNNs) and very high-resolution RGB imagery from UAVs for the mapping of tree species in temperate forests. We used multicopter UAVs to obtain very high-resolution (&lt;2 cm) RGB imagery over 51 ha of temperate forests in the Southern Black Forest region, and the Hainich National Park in Germany. To fully harness the end-to-end learning capabilities of CNNs, we used a semantic segmentation approach (U-net) that concurrently segments and classifies tree species from imagery. With a diverse dataset in terms of study areas, site conditions, illumination properties, and phenology, we accurately mapped nine tree species, three genus-level classes, deadwood, and forest floor (mean F1-score 0.73). A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy (mean F1-score of 0.26 at 32 cm resolution). Our results highlight the key role that UAVs can play in the mapping of forest tree species, given that air- and spaceborne remote sensing currently does not provide comparable spatial resolutions. The end-to-end learning capability of CNNs makes extensive preprocessing partly obsolete. The use of large and diverse datasets facilitate a high degree of generalization of the CNN, thus fostering transferability. The synergy of high-resolution UAV imagery and CNN provide a fast and flexible yet accurate means of mapping forest tree species.
WOS:000592242600015
</snippet>
</document>

<document id="748">
<title>Satellite Image for Cloud and Snow Recognition Based on Lightweight Feature Map Attention Network</title>
<url>http://dx.doi.org/10.3390/ijgi11070390</url>
<snippet>Cloud and snow recognition technology is of great significance in the field of meteorology, and is also widely used in remote sensing mapping, aerospace, and other fields. Based on the traditional method of manually labeling cloud-snow areas, a method of labeling cloud and snow areas using deep learning technology has been gradually developed to improve the accuracy and efficiency of recognition. In this paper, from the perspective of designing an efficient and lightweight network model, a cloud snow recognition model based on a lightweight feature map attention network (Lw-fmaNet) is proposed to ensure the performance and accuracy of the cloud snow recognition model. The model is improved based on the ResNet18 network with the premise of reducing the network parameters and improving the training efficiency. The main structure of the model includes a shallow feature extraction module, an intrinsic feature mapping module, and a lightweight adaptive attention mechanism. Overall, in the experiments conducted in this paper, the accuracy of the proposed cloud and snow recognition model reaches 95.02&#37;, with a Kappa index of 93.34&#37;. The proposed method achieves an average precision rate of 94.87&#37;, an average recall rate of 94.79&#37;, and an average F1-Score of 94.82&#37; for four sample recognition classification tasks: no snow and no clouds, thin cloud, thick cloud, and snow cover. Meanwhile, our proposed network has only 5.617M parameters and takes only 2.276 s. Compared with multiple convolutional neural networks and lightweight networks commonly used for cloud and snow recognition, our proposed lightweight feature map attention network has a better performance when it comes to performing cloud and snow recognition tasks.
WOS:000833788000001
</snippet>
</document>

<document id="749">
<title>Land-surface parameters for spatial predictive mapping and modeling</title>
<url>http://dx.doi.org/10.1016/j.earscirev.2022.103944</url>
<snippet>Land-surface parameters derived from digital land surface models (DLSMs) (for example, slope, surface curvature, topographic position, topographic roughness, aspect, heat load index, and topographic moisture index) can serve as key predictor variables in a wide variety of mapping and modeling tasks relating to geomorphic processes, landform delineation, ecological and habitat characterization, and geohazard, soil, wetland, and general thematic mapping and modeling. However, selecting features from the large number of potential derivatives that may be predictive for a specific feature or process can be complicated, and existing literature may offer contradictory or incomplete guidance. The availability of multiple data sources and the need to define moving window shapes, sizes, and cell weightings further complicate selecting and optimizing the feature space. This review focuses on the calculation and use of DLSM parameters for empirical spatial predictive modeling applications, which rely on training data and explanatory variables to make predictions of landscape features and processes over a defined geographic extent. The target audience for this review is researchers and analysts undertaking predictive modeling tasks that make use of the most widely used terrain variables. To outline best practices and highlight future research needs, we review a range of land-surface parameters relating to steepness, local relief, rugosity, slope orientation, solar insolation, and moisture and characterize their relationship to geomorphic processes. We then discuss important considerations when selecting such parameters for predictive mapping and modeling tasks to assist analysts in answering two critical questions: What landscape conditions or processes does a given measure characterize? How might a particular metric relate to the phenomenon or features being mapped, modeled, or studied? We recommend the use of landscape-and problem specific pilot studies to answer, to the extent possible, these questions for potential features of interest in a mapping or modeling task. We describe existing techniques to reduce the size of the feature space using feature selection and feature reduction methods, assess the importance or contribution of specific metrics, and parameterize moving windows or characterize the landscape at varying scales using alternative methods while highlighting strengths, drawbacks, and knowledge gaps for specific techniques. Recent developments, such as explainable machine learning and convolutional neural network (CNN)-based deep learning, may guide and/or minimize the need for feature space engineering and ease the use of DLSMs in predictive modeling tasks.
WOS:000779065800001
</snippet>
</document>

<document id="750">
<title>Can linguistic features extracted from geo-referenced tweets help building function classification in remote sensing?</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.04.006</url>
<snippet>The fusion of two or more different data sources is a widely accepted technique in remote sensing while becoming increasingly important due to the availability of big Earth Observation satellite data. As a complementary source of geo-information to satellite data, massive text messages from social media form a temporally quasi-seamless, spatially multi-perspective stream, but with unknown and diverse quality. Despite the uncontrolled quality: can linguistic features extracted from geo-referenced tweets support remote sensing tasks? This work presents a straightforward decision fusion framework for very high-resolution remote sensing images and Twitter text messages. We apply our proposed fusion framework to a land-use classification task - the building function classification task - in which we classify building functions like commercial or residential based on linguistic features derived from tweets and remote sensing images. Using building tags from OpenStreetMap (OSM), we labeled tweets and very high-resolution (VHR) images from Google Maps. We collected English tweets from San Francisco, New York City, Los Angeles, and Washington D.C. and trained a stacked bi-directional LSTM neural network with these tweets. For the aerial images, we predicted building functions with state-of-the-art Convolutional Neural Network (CNN) architectures fine-tuned from ImageNet on the given task. After predicting each modality separately, we combined the prediction probabilities of both models building-wise at a decision level. We show that the proposed fusion framework can improve the classification results of the building type classification task. To the best of our knowledge, we are the first to use semantic contents of Twitter messages and fusing them with remote sensing images to classify building functions at a single building level.
WOS:000797181100002
</snippet>
</document>

<document id="751">
<title>TCIANet: Transformer-Based Context Information Aggregation Network for Remote Sensing Image Change Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2023.3241157</url>
<snippet>Change detection based on remote sensing data is an important method to detect the earth surface changes. With the development of deep learning, convolutional neural networks have excelled in the field of change detection. However, the existing neural network models are susceptible to external factors in the change detection process, leading to pseudo change and missed detection in the detection results. In order to better achieve the change detection effect and improve the ability to discriminate pseudo change, this article proposes a new method, namely, transformer-based context information aggregation network for remote sensing image change detection. First, we use a filter-based visual tokenizer to segment each temporal feature map into multiple visual semantic tokens. Second, the addition of the progressive sampling vision transformer not only effectively excludes the interference of irrelevant changes, but also uses the transformer encoder to obtain compact spatiotemporal context information in the token set. Then, the tokens containing rich semantic information are fed into the pixel space, and the transformer decoder is used to acquire pixel-level features. In addition, we use the feature fusion module to fuse low-level semantic feature information to complete the extraction of coarse contour information of the changed region. Then, the semantic relationships between object regions and contours are captured by the contour-graph reasoning module to obtain feature maps with complete edge information. Finally, the prediction model is used to discriminate the change of feature information and generate the final change map. Numerous experimental results show that our method has more obvious advantages in visual effect and quantitative evaluation than other methods.
WOS:000940195200003
</snippet>
</document>

<document id="752">
<title>Binary patterns encoded convolutional neural networks for texture recognition and remote sensing scene classification</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2018.01.023</url>
<snippet>Designing discriminative powerful texture features robust to realistic imaging conditions is a challenging computer vision problem with many applications, including material recognition and analysis of satellite or aerial imagery. In the past, most texture description approaches were based on dense orderless statistical distribution of local features. However, most recent approaches to texture recognition and remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The de facto practice when learning these CNN models is to use RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show that Local Binary Patterns (LBP) encoded CNN models, codenamed TEX-Nets, trained using mapped coded images with explicit LBP based texture information provide complementary information to the standard RGB deep models. Additionally, two deep architectures, namely early and late fusion, are investigated to combine the texture and color information. To the best of our knowledge, we are the first to investigate Binary Patterns encoded CNNs and different deep network fusion architectures for texture recognition and remote sensing scene classification. We perform comprehensive experiments on four texture recognition datasets and four remote sensing scene classification benchmarks: UC-Merced with 21 scene categories, WHU-RS19 with 19 scene classes, RSSCN7 with 7 categories and the recently introduced large scale aerial image dataset (AID) with 30 aerial scene types. We demonstrate that TEX-Nets provide complementary information to standard RGB deep model of the same network architecture. Our late fusion TEX-Net architecture always improves the overall performance compared to the standard RGB network on both recognition problems. Furthermore, our final combination leads to consistent improvement over the state-of-the-art for remote sensing scene classification. (C) 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000428824300007
</snippet>
</document>

<document id="753">
<title>Concise and Effective Network for 3D Human Modeling From Orthogonal Silhouettes</title>
<url>http://dx.doi.org/10.1115/1.4054001</url>
<snippet>In this article, we revisit the problem of 3D human modeling from two orthogonal silhouettes of individuals (i.e., front and side views). Different from our previous work (Wang et al. (2003, "Virtual Human Modeling From Photographs for Garment Industry," Comput. Aided Des., 35, pp. 577-589).), a supervised learning approach based on the convolutional neural network (CNN) is investigated to solve the problem by establishing a mapping function that can effectively extract features from two silhouettes and fuse them into coefficients in the shape space of human bodies. A new CNN structure is proposed in our work to extract not only the discriminative features of front and side views but also their mixed features for the mapping function. 3D human models with high accuracy are synthesized from coefficients generated by the mapping function. Existing CNN approaches for 3D human modeling usually learn a large number of parameters (from 8.5 M to 355.4 M) from two binary images. Differently, we investigate a new network architecture and conduct the samples on silhouettes as the input. As a consequence, more accurate models can be generated by our network with only 2.4 M coefficients. The training of our network is conducted on samples obtained by augmenting a publicly accessible dataset. Learning transfer by using datasets with a smaller number of scanned models is applied to our network to enable the function of generating results with gender-oriented (or geographical) patterns.
WOS:000851558400009
</snippet>
</document>

<document id="754">
<title>A Center Location Algorithm for Tropical Cyclone in Satellite Infrared Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2995158</url>
<snippet>The precise location of the tropical cyclone (TC) center is critical for intensity estimation and trajectory prediction. Due to the variability of TC morphology and structure, there are still some challenges in locating its center automatically. The ability of the deep convolutional network to capture multilevel structural features of the images is exploited. Furthermore, a two-step scheme for locating the TC center is proposed, which contains the object detection for TCs with deep learning and the comprehensive decision for TC centers. In the object detection, considering the statistical scale distribution of TCs, the global and local features extracted by the network are combined to form the fusion feature maps through the upsampling and concatenation. The changes in the TC scale are accommodated by two different scale outputs. A high detection rate and a low false alarm rate are obtained with the object detection, which provides an initial position for the TC center. Within the scope of the TCs, the final position of the center is obtained through segmentation, edge detection, circle fitting, and comprehensive decision. The experimental results show that the average latitude and longitude error of the proposed method is about 0.237 degrees. For the TC in the initial phase or dissipation stage, the location results are usually superior to the results of the comparison algorithms.
WOS:000542949600003
</snippet>
</document>

<document id="755">
<title>Detection and Geo-localization of Small Traffic Signs Based on Images and Laser Data</title>
<url>http://dx.doi.org/10.3788/CJL202047.0910002</url>
<snippet>Information on the spatial location of road facilities such as traffic signs is one of the basic element of urban three-dimensional (3D) modeling, and it is also an essential part of road facility maintenance and management. To this end, an automatic extraction scheme for small traffic signs based on mobile measurement data is proposed herein. Based on the improved convolutional neural network SlimNet model, small cross-marks on panoramic images are detected, and a 3D target geolocation based on depth maps is proposed. A distance assessment method based on the center point is used to extract the diagonal of the target. Measured data of the three types of small traffic signs are selected to verify and analyze the proposed method. Experiment results show that the average accuracy of the SlimNet model is 4.2 percentage higher than that of the classic VGG16 (Visual Geometry Group 16) model. Using the proposed geographic positioning and vectorization scheme, the recall rate and accuracy rate of the three types of targets in the experimental area reached over 86&#37;, proving the effective feasibility of the overall scheme. Furthermore, the proposed method provides a new idea for an accurate 3D spatial geolocalization of urban multi-class targets.
WOS:000577073400036
</snippet>
</document>

<document id="756">
<title>Research on text detection on building surfaces in smart cities based on deep learning</title>
<url>http://dx.doi.org/10.1007/s00500-022-07391-3</url>
<snippet>In recent years, with the construction and development of smart cities, text recognition in building images can not only achieve geolocation but also provide guiding significance for GIS mapping and automatic updating. Since buildings have different orientations, angles and shapes, it is difficult to recognize textual features in images. With the wide application of convolutional neural networks and recurrent neural networks in image processing, this paper proposes a BFPN-RCNN algorithm for detecting and recognizing curved text in architectural images. A comparison with other image detection algorithms on different datasets proves that the algorithm can effectively identify curved text at different angles in natural scene images.
WOS:000840063500004
</snippet>
</document>

<document id="757">
<title>Deep Learning in Forest Structural Parameter Estimation Using Airborne LiDAR Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3046053</url>
<snippet>Accurately estimating and mapping forest structural parameters are essential for monitoring forest resources and understanding ecological processes. The novel deep learning algorithm has the potential to be a promising approach to improve the estimation accuracy while combining with advanced remote sensing technology. Airborne light detection and ranging (LiDAR) has the preferable capability to characterize 3-D canopy structure and estimate forest structural parameters. In this study, we developed a deep learning-based algorithm (Deep-RBN) that combined the fully connected network (FCN) deep learning algorithm with the optimized radial basis neural network (RBN) algorithm for forest structural parameter estimation using airborne LiDAR data. The multiple iterations were used to constantly update the internal weights to achieve the optimized accuracy of model fitting, and the optimized RBN algorithm was developed for the limited training sets. We assessed the efficiency and capability of the Deep-RBN in the estimation of forest structural parameters in a subtropical planted forest of southern China, by comparing the traditional FCN algorithm and multiple linear regression. We found that Deep-RBN had the strongest capability in estimates of forest structural parameters (R-2 = 0.67-0.86, rRMSE = 6.95&#37;-20.34&#37;). The sensitivity analysis of the key hyperparameters of Deep-RBN algorithm showed that the learning rate is one of the most important parameters that influence the performance of predictive models, and while its value equal is to 0.001, the predictive models had the highest accuracy (mean DBH: RMSE = 1.01, mean height: RMSE = 1.45, volume: RMSE = 26.49, stem density: RMSE = 121.06). With the increase of training samples added in Deep-RBN model, the predictive models performed better; however, no significant improvements of accuracy were observed while the number of training set is larger than 80. This study demonstrates the benefits of jointly using the Deep-RBN algorithm and airborne LiDAR data to improve the accuracy of forest structural parameter estimation and mapping, which provides a promising methodology for sustainable forest resources monitoring.
WOS:000607810600013
</snippet>
</document>

<document id="758">
<title>Automatic mapping of national surface water with OpenStreetMap and Sentinel-2 MSI data using deep learning</title>
<url>http://dx.doi.org/10.1016/j.jag.2021.102571</url>
<snippet>Large-scale mapping activities can benefit from the vastly increasing availability of earth observation (EO) data, especially when combined with volunteered geographical information (VGI) using machine learning (ML). Highresolution maps of inland surface water bodies are important for water supply and natural disaster mitigation as well as for monitoring, managing, and preserving landscapes and ecosystems. In this paper, we propose an automatic surface water mapping workflow by training a deep residual neural network (ResNet) based on OpenStreetMap (OSM) data and Sentinel-2 multispectral data, where the Simple Non-Iterative Clustering (SNIC) superpixel algorithm was employed for generating object-based training samples. As a case study, we produced an open surface water layer for Germany using a national ResNet model at a 10 m spatial resolution, which was then harmonized with OSM data for final surface water products. Moreover, we evaluated the mapping accuracy of our open water products via conducting expert validation campaigns, and comparing to existing water products, namely the WasserBLIcK and Global Surface Water Layer (GSWL). Using 4,600 validation samples in Germany, the proposed model (ResNet+SNIC) achieved an overall accuracy of 86.32&#37; and competitive detection rates over the WasserBLIcK (87.47&#37;) and GSWL (98.61&#37;). This study provides comprehensive insights into how to best explore the synergy of VGI and ML of EO data in a large-scale surface water mapping task.
WOS:000721846200004
</snippet>
</document>

<document id="759">
<title>SYNTHETIC-APERTURE RADAR IMAGE BASED POSITIONING IN GPS-DENIED ENVIRONMENTS USING DEEP COSINE SIMILARITY NEURAL NETWORKS</title>
<url>http://dx.doi.org/10.3934/ipi.2021013</url>
<snippet>Navigating unmanned aerial vehicles in precarious environments is of great importance. It is necessary to rely on alternative information processing techniques to attain spatial information that is required for navigation in such settings. This paper introduces a novel deep learning-based approach for navigating that exclusively relies on synthetic aperture radar (SAR) images. The proposed method utilizes deep neural networks (DNNs) for image matching, retrieval, and registration. To this end, we introduce Deep Cosine Similarity Neural Networks (DCSNNs) for mapping SAR images to a global descriptive feature vector. We also introduce a fine-tuning algorithm for DC-SNNs, and DCSNNs are used to generate a database of feature vectors for SAR images that span a geographic area of interest, which, in turn, are compared against a feature vector of an inquiry image. Images similar to the inquiry are retrieved from the database by using a scalable distance measure between the feature vector outputs of DCSNN. Methods for reranking the retrieved SAR images that are used to update position coordinates of an inquiry SAR image by estimating from the best retrieved SAR image are also introduced. Numerical experiments comparing with baselines on the Polarimetric SAR (PolSAR) images are presented.
WOS:000648571700007
</snippet>
</document>

<document id="760">
<title>Earthquake Crack Detection From Aerial Images Using a Deformable Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/TGRS.2022.3183157</url>
<snippet>Detecting the terrain surface cracks caused by earthquakes, which are termed coseismic ruptures, has important significance for discovering concealed faults, monitoring their movements, and forecasting possible follow-on earthquakes. On May 22, 2021, Maduo County in Qinghai province, China, suffered an earthquake with a magnitude of 7.4, which created densely distributed cracks. In this study, we designed an automatic crack detection framework based on remote sensing technology. With the use of low-altitude unmanned aerial vehicles (UAVs), we obtained very high-resolution aerial images of the area affected by the earthquake, which were further processed by photogrammetric software to produce digital orthophoto maps (DOMs). We then designed a novel terrain surface crack detection neural network, which differs from the previous methods that focus on detecting cracks in man-made object surfaces such as flat roads. We investigated the spatial property of the sinuous linear cracks and handled this by introducing adaptive deformable convolutions with a context-channel-space boosted mechanism. The feature extraction stage, feature optimization stage, and upsampling stage were embedded with the deformable convolutions to form a compact and powerful crack detector, named Crack-CADNet [the Context-chAnnel-space boosted Deformable convolutional neural network (CNN) for crack detection]. The postprocessing included filtering out the nontectonic cracks, aided by annotations from experts, and grouping and vectorizing the generated binary segmentation map as crack polygons, which were evaluated at the instance level. In addition to the first in-depth investigation of detecting earthquake cracks with aerial remote sensing and a deep learning-based process, the crack detection network we propose outperformed the recent CNN-based methods designed for general semantic segmentation and crack detection. Source code and the Maduo earthquake crack dataset will be available at http://gpcv.whu.edu.cn/data/.
WOS:000815664200006
</snippet>
</document>

<document id="761">
<title>High-resolution paleovalley classification from airborne electromagnetic imaging and deep neural network training using digital elevation model data</title>
<url>http://dx.doi.org/10.5194/hess-23-2561-2019</url>
<snippet>Paleovalleys are buried ancient river valleys that often form productive aquifers, especially in the semiarid and arid areas of Australia. Delineating their extent and hydrostratigraphy is however a challenging task in groundwater system characterization. This study developed a methodology based on the deep learning super-resolution convolutional neural network (SRCNN) approach, to convert electrical conductivity (EC) estimates from an airborne electromagnetic (AEM) survey in South Australia to a high-resolution binary paleovalley map. The SRCNN was trained and tested with a synthetic training dataset, where valleys were generated from readily available digital elevation model (DEM) data from the AEM survey area. Electrical conductivities typical of valley sediments were generated by Archies law, and subsequently blurred by down-sampling and bicubic interpolation to represent noise from the AEM survey, inversion and interpolation. After a model training step, the SRCNN successfully removed such noise, and reclassified the low-resolution, converted unimodal but skewed EC values into a high-resolution paleovalley index following a bimodal distribution. The latter allows us to distinguish valley from non-valley pixels. Furthermore, a realistic spatial connectivity structure of the paleovalley was predicted when compared with borehole lithology logs and a valley bottom flatness indicator. Overall the methodology permitted us to better constrain the three-dimensional paleovalley geometry from AEM images that are becoming more widely available for groundwater prospecting.
WOS:000471598700001
</snippet>
</document>

<document id="762">
<title>A Twice Optimizing Net With Matrix Decomposition for Hyperspectral and Multispectral Image Fusion</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3009250</url>
<snippet>Fusing a low-resolution hyperspectral (LRHS) image and a high-resolution multispectral (HRMS) image to generate a high-resolution hyperspectral (HRHS) image has grown a significant and attractive application in remote sensing fields. Recently, the popularization of deep learning has injected more possibilities into the fusion work. However, there still exists a difficulty that is how to make the best of the acquired LRHS and HRMS images. In this article, we present a twice optimizing net with matrix decomposition to fulfill the fusion task, which can be roughly divided into three stages: pre-optimization, deep prior learning, post-optimization. Specifically, we first transform this fusion problem into a spectral optimization problem and a spatial optimization problem with the help of matrix decomposition. These two optimization problems can be handled sequentially by solving a linear equation, respectively, and then we can obtain the initial HRHS image by multiplying the two solutions. Next, we establish the mapping between the initial image and the reference image through an end-to-end deep residual network based on local and nonlocal connectivity. In order to get better performance, we have customized a loss function specifically for the fusion task as well. Finally, we return the predicted result again to the optimization procedure to get the final fusion image. After the evaluation on three simulated datasets and one real dataset, it illustrates that the proposed method outperforms many state-of-the-art ones.
WOS:000554889400005
</snippet>
</document>

<document id="763">
<title>Memory, attention and prediction: a deep learning architecture for car-following</title>
<url>http://dx.doi.org/10.1080/21680566.2019.1650674</url>
<snippet>Car-following (CF) models are an appealing research area because they fundamentally describe the longitudinal interactions of vehicles and contribute substantially to an understanding of traffic flow. In this study, by combining numerous deep neural network structures to mimic the memory, attention and prediction (MAP) mechanisms of real drivers, a deep learning-based car following model named MAP is built. The proposed MAP learned CF behaviour from real-world datasets. Experiments on predicting future driving behaviour show that the MAP model with memory, attention and prediction mechanisms outperforms models without those mechanisms. Several analysis are conducted to understand the MAP model, and quantitatively provides some explanations of how MAP achieves the CF behaviour. A simulation is also presented and explicitly analysed. The results show that the proposed approach can produce space-time diagrams similar to real traffic. The analysis also shows that the supervised learning models generate the most likely, rather than the best, CF behaviour.
WOS:000479956900001
</snippet>
</document>

<document id="764">
<title>ANN*: A Heuristic Search Algorithm Based on Artificial Neural Networks</title>
<url>http://dx.doi.org/10.1145/3028842.3028893</url>
<snippet>Heuristic search algorithm has been widely used in the fields of geographic information system, intelligent robot path planning, etc. This paper proposes a heuristic search algorithm based on artificial neural network, which is named ANN*. The architecture of the ANN* algorithm is similar to the A* algorithm, but it contains a novel heuristic function of the new design. This novel heuristic function utilizes the regression network, which is a kind of classical artificial neural network. In the ANN* algorithm, the artificial neural network is used to learn the features of massive map data and predict the path search difficulty of the map. In order to guarantee the diversity of the training data, this paper designs a random map generation framework, which is able to generate random grid maps according to the length, width and density. The experiments show that the ANN* algorithm can accomplish the task of path planning in the grid map with high quality. This paper provides a new scheme of exploring the applications of artificial neural network for heuristic search algorithm.
WOS:000469113800051
</snippet>
</document>

<document id="765">
<title>Automated detection of rock glaciers using deep learning and object-based image analysis</title>
<url>http://dx.doi.org/10.1016/j.rse.2020.112033</url>
<snippet>Rock glaciers are an important component of the cryosphere and are one of the most visible manifestations of permafrost. While the significance of rock glacier contribution to streamflow remains uncertain, the contribution is likely to be important for certain parts of the world. High-resolution remote sensing data has permitted the creation of rock glacier inventories for large regions. However, due to the spectral similarity between rock glaciers and the surrounding material, the creation of such inventories is typically conducted based on manual interpretation, which is both time consuming and subjective. Here, we present a novel method that combines deep learning (convolutional neural networks or CNNs) and object-based image analysis (OBIA) into one workflow based on freely available Sentinel-2 optical imagery (10 m spatial resolution), Sentinel-1 interferometric coherence data, and a digital elevation model (DEM). CNNs identify recurring patterns and textures and produce a prediction raster, or heatmap where each pixel indicates the probability that it belongs to a certain class (i.e. rock glacier) or not. By using OBIA we can segment the datasets and classify objects based on their heatmap value as well as morphological and spatial characteristics. We analysed two distinct catchments, the La Laguna catchment in the Chilean semi-arid Andes and the Poiqu catchment in the central Himalaya. In total, our method mapped 108 of the 120 rock glaciers across both catchments with a mean overestimation of 28&#37;. Individual rock glacier polygons howevercontained false positives that are texturally similar, such as debris-flows, avalanche deposits, or fluvial material causing the users accuracy to be moderate (63.9-68.9&#37;) even if the producers accuracy was higher (75.0-75.4&#37;). We repeated our method on very-high-resolution Pleiades satellite imagery and a corresponding DEM (at 2 m resolution) for a subset of the Poiqu catchment to ascertain what difference image resolution makes. We found that working at a higher spatial resolution has little influence on the producers accuracy (an increase of 1.0&#37;), however the rock glaciers delineated were mapped with a greater users accuracy (increase by 9.1&#37; to 72.0&#37;). By running all the processing within an object-based environment it was possible to both generate the deep learning heatmap and perform post-processing through image segmentation and object reshaping. Given the difficulties in differentiating rock glaciers using image spectra, deep learning combined with OBIA offers a promising method for automating the process of mapping rock glaciers over regional scales and lead to a reduction in the workload required in creating inventories.
WOS:000585306100006
</snippet>
</document>

<document id="766">
<title>A Text Correction and Recognition for Intelligent Railway Drawing Detection</title>
<url>http://dx.doi.org/10.1109/ICIEA51954.2021.9516318</url>
<snippet>In current train control system, there are many drawings that need to be identified manually. This approach leads to many problems, such as misidentification and low efficiency. It is difficult to recognize these drawings automatically because the text on them often has a series of changes, like rotation, tilt, font changes and close to lines To solve this problem, we divide the task into two parts: text recognition and graphical symbol recognition. This article focuses on text recognition. We use aerial detection technology to classify and detect graphical symbol and text, followed by choosing BIL-STM to conducting sequence modeling and using convolutional recurrent neural network (CRNN) iterative training to focus on single word rotation, including tilting, noise-addition, and blurring post-processing. So that the training model can cope with complex scenario and improve the recognition rate of text on drawings. Finally, RESNET is applied to CRNN feature extraction network, and CRNN outputs the recognition and detection results in sequence according to the detection sequence, achieving entry-level detection, and the text recognition rate reaches 98.36&#37;
WOS:000709847700257
</snippet>
</document>

<document id="767">
<title>The "Paris-End" of Town? Deriving Urban Typologies Using Three Imagery Types</title>
<url>http://dx.doi.org/10.3390/urbansci4020027</url>
<snippet>Urban typologies allow areas to be categorised according to form and the social, demographic, and political uses of the areas. The use of these typologies and finding similarities and dissimilarities between cities enables better targeted interventions for improved health, transport, and environmental outcomes in urban areas. A better understanding of local contexts can also assist in applying lessons learned from other cities. Constructing urban typologies at a global scale through traditional methods, such as functional or network analysis, requires the collection of data across multiple political districts, which can be inconsistent and then require a level of subjective classification. To overcome these limitations, we use neural networks to analyse millions of images of urban form (consisting of street view, satellite imagery, and street maps) to find shared characteristics between the largest 1692 cities in the world. The comparison city of Paris is used as an exemplar and we perform a case study using two Australian cities, Melbourne and Sydney, to determine if a "Paris-end" of town exists or can be found in these cities using these three big data imagery sets. The results show specific advantages and disadvantages of each type of imagery in constructing urban typologies. Neural networks trained with map imagery will be highly influenced by the structural mix of roads, public transport, and green and blue space. Satellite imagery captures a combination of both urban form and decorative and natural details. The use of street view imagery emphasises the features of a human-scaled visual geography of streetscapes. However, for both satellite and street view imagery to be highly effective, a reduction in scale and more aggressive pre-processing might be required in order to reduce detail and create greater abstraction in the imagery.
WOS:000620955400013
</snippet>
</document>

<document id="768">
<title>Automatic vectorization of point symbols on archive maps using deep convolutional neural network</title>
<url>http://dx.doi.org/10.5194/ica-proc-4-109-2021</url>
<snippet>Archive topographical maps are a key source of geographical information from past ages, which can be valuable for several science fields. Since manual digitization is usually slow and takes much human resource, automatic methods are preferred, such as deep learning algorithms. Although automatic vectorization is a common problem, there have been few approaches regarding point symbols. In this paper, a point symbol vectorization method is proposed, which was tested on Third Military Survey map sheets using a Mask Regional Convolutional Neural Network (MRCNN). The MRCNN implementation uses the ResNet101 network improved with the Feature Pyramid Network architecture and is developed in a Google Colab environment. The pretrained network was trained on four point symbol categories simultaneously. Results show 90&#37; accuracy, while 94&#37; of symbols detected for some categories on the complete test sheet.
WOS:000855572500107
</snippet>
</document>

<document id="769">
<title>Large-scale point cloud contour extraction via 3D guided multi-conditional generative adversarial network</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.04.003</url>
<snippet>As one of the most important features for human perception, contours are widely used in many graphics and mapping applications. However, for large outdoor scale point clouds, contour extraction is considerably challenging due to the huge, unstructured and irregular point space, thus leading to massive failure for existing approaches. In this paper, to generate contours consistent with human perception for outdoor scenes, we propose, for the first time, 3D guided multi-conditional GAN (3D-GMcGAN), a deep neural network based contour extraction network for large scale point clouds. Specifically, two ideas are proposed to enable the network to learn the distributions of labeled samples. First, a parametric space based framework is proposed via a novel similarity measurement of two parametric models. Such a framework significantly compresses the huge point data space, thus making it much easier for the network to "remember" target distribution. Second, to prevent network loss in the huge solution space, a guided learning framework is designed to assist finding the target contour distribution via an initial guidance. To evaluate the effectiveness of the pro-posed network, we open-sourced the first, to our knowledge, dataset for large scale point cloud with contour annotation information. Experimental results demonstrate that 3D-GMcGAN efficiently generates contours for the data with more than ten million points (about several minutes), while avoiding ad hoc stages or parameters. Also, the proposed framework produces minimal outliers and pseudo-contours, as suggested by comparisons with the state-of-the-art approaches.
WOS:000535696600008
</snippet>
</document>

<document id="770">
<title>Characterizing Tourism Destination Image Using Photos' Visual Content</title>
<url>http://dx.doi.org/10.3390/ijgi9120730</url>
<snippet>"A picture is worth a thousand words". Analysis of the visual content of tourist photos is an effective way to explore the image of tourist destinations. With the development of computer deep learning and big data mining technology, identifying the content of massive numbers of tourist photos by convolutional neural network (CNN) approaches breaks through the limitations of manual approaches of identifying photos visual information, e.g., small sample size, complex identification process, and results deviation. In this study, 531,629 travel photos of Jiangxi were identified as 365 scenes through deep learning technology. Through the latent Dirichlet allocation (LDA) model, five major tourism topics are found and visualized by map. Then, we explored the spatial and temporal distribution characteristics of different tourism scenes based on hot spot analysis technology and the seasonal evaluation index. Our research shows that the visual content mining on travel photos makes it possible to understand the tourism destination image and to reveal the temporal and spatial heterogeneity of the image, thereby providing an important reference for tourism marketing.
WOS:000602136500001
</snippet>
</document>

<document id="771">
<title>Post-analysis of OSM-GAN Spatial Change Detection</title>
<url>http://dx.doi.org/10.1007/978-3-031-06245-2_3</url>
<snippet>Keeping crowdsourced maps up-to-date is important for a wide range of location-based applications (route planning, urban planning, navigation, tourism, etc.). We propose a novel map updating mechanism that combines the latest freely available remote sensing data with the current state of online vector map data to train a Deep Learning (DL) neural network. It uses a Generative Adversarial Network (GAN) to perform image-to-image translation, followed by segmentation and raster-vector comparison processes to identify changes to map features (e.g. buildings, roads, etc.) when compared to existing map data. This paper evaluates various GAN models trained with sixteen different datasets designed for use by our change detection/map updating procedure. Each GAN model is evaluated quantitatively and qualitatively to select the most accurate DL model for use in future spatial change detection applications.
WOS:000877296000003
</snippet>
</document>

<document id="772">
<title>Comparison between convolutional neural networks and random forest for local climate zone classification in mega urban areas using Landsat images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.09.009</url>
<snippet>The Local Climate Zone (LCZ) scheme is a classification system providing a standardization framework to present the characteristics of urban forms and functions, especially for urban heat island (UHI) research. Landsat-based 100 m resolution LCZ maps have been classified by the World Urban Database and Portal Tool (WUDAPT) method using a random forest (RF) machine learning classifier. Some studies have proposed modified RF and convolutional neural network (CNN) approaches. This study aims to compare CNN with an RF classifier for LCZ mapping in great detail. We designed five schemes (three RF-based schemes (S1-S3) and two CNN-based ones (S4-S5)), which consist of various combinations of input features from bitemporal Landsat 8 data over four global mega cities: Rome, Hong Kong, Madrid, and Chicago. Among the five schemes, the CNN-based one with the incorporation of a larger neighborhood information showed the best classification performance. When compared to the WUDAPT workflow, the overall accuracies for entire land cover classes (OA) and for urban LCZ types (i.e., LCZ1-10; OA(urb)) increased by about 6-8&#37; and 10-13&#37;, respectively, for the four cities. The transferability of LCZ models for the four cities were evaluated, showing that CNN consistently resulted in higher accuracy (increased by about 7-18&#37; and 18-29&#37; for OA and OA(urb), respectively) than RF. This study revealed that the CNN classifier classified particularly well for the specific LCZ classes in which buildings were mixed with trees or buildings or plants were sparsely distributed. The research findings can provide a basis for guidance of future LCZ classification using deep learning.
WOS:000491613300011
</snippet>
</document>

<document id="773">
<title>Place Classification and Semantic Mapping for MAV Applications</title>
<url>http://dx.doi.org/10.33012/2019.16730</url>
<snippet>Micro air vehicles (MAVs) are promising mobile platforms for exploring tasks. They are able to quickly reach the place of interest and to obtain important and crucial information on the ongoing situation. Beyond metrical representation, semantic information of the observed environment enhances the ability of situational awareness. Thus, the aim of this paper is the semantic mapping. It discusses how to gain place labels and assign them to metrical space. Herby, fine-tuned Convolution Neural Networks are used for place classification. After that, the gained semantics are assigned to the metrical space by using a grid map update strategy. In this context, different metrics for weighting of the observed grid elements are considered. The overall approach is evaluated by experimental data within a typical office environment. The resulting semantic map is easy to interpret.
WOS:000542920400067
</snippet>
</document>

<document id="774">
<title>Prediction of the Solar Resource through Differences</title>
<url>http://dx.doi.org/</url>
<snippet>Experience shows that solar resource prediction is a difficult task. The available solar irradiance where a photovoltaic plant is located or is planned to be installed depends mainly on the cloud incidence at the site. This incidence of clouds depends on the climate system of the region, which is well known to be a non-linear, chaotic, and extremely complex, for which there is no exact mathematical model. In fact, the chaos level has been determined for various time series of wind and solar irradiance, and it turns out that the chaos level of the solar time series is greater than that of the wind series. This indicates that the complexity of solar irradiance prediction is considerable. In previous works of solar irradiance prediction, using Artificial Neural Networks, it has been observed that the trained models fail to predict irradiance spikes in conditions of intermittent cloudiness. By conducting a study in this area, we have found that, for a given date, there exist a model to determine the ideal solar irradiance in any geographical location of the planet. These models, so-called clear sky models, have been taken as a reference to predict not the solar irradiance, but the amount of irradiance occluded by the clouds. That is, the difference between ideal irradiance and that measured by the weather station. The proposed model is called SolarDiff, which predicts this difference using Artificial Neural Networks. This article empirically demonstrates that the SolarDiff model exhibits better behavior than models based on direct data. The performance, as in most forecast models, is measured by quantifying the forecast error. In this case the symmetric MAPE error is used.
WOS:000651431100014
</snippet>
</document>

<document id="775">
<title>DFFAN: Dual Function Feature Aggregation Network for Semantic Segmentation of Land Cover</title>
<url>http://dx.doi.org/10.3390/ijgi10030125</url>
<snippet>Analyzing land cover using remote sensing images has broad prospects, the precise segmentation of land cover is the key to the application of this technology. Nowadays, the Convolution Neural Network (CNN) is widely used in many image semantic segmentation tasks. However, existing CNN models often exhibit poor generalization ability and low segmentation accuracy when dealing with land cover segmentation tasks. To solve this problem, this paper proposes Dual Function Feature Aggregation Network (DFFAN). This method combines image context information, gathers image spatial information, and extracts and fuses features. DFFAN uses residual neural networks as backbone to obtain different dimensional feature information of remote sensing images through multiple downsamplings. This work designs Affinity Matrix Module (AMM) to obtain the context of each feature map and proposes Boundary Feature Fusion Module (BFF) to fuse the context information and spatial information of an image to determine the location distribution of each images category. Compared with existing methods, the proposed method is significantly improved in accuracy. Its mean intersection over union (MIoU) on the LandCover dataset reaches 84.81&#37;.
WOS:000633709200001
</snippet>
</document>

<document id="776">
<title>An end-to-end shape modeling framework for vectorized building outline generation from aerial images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.10.008</url>
<snippet>The identification and annotation of buildings has long been a tedious and expensive part of high-precision vector map production. The deep learning techniques such as fully convolution network (FCN) have largely promoted the accuracy of automatic building segmentation from remote sensing images. However, compared with the deep-learning-based building segmentation methods that greatly benefit from data-driven feature learning, the building boundary vector representation generation techniques mainly rely on handcrafted features and high human intervention. These techniques continue to employ manual design and ignore the opportunity of using the rich feature information that can be learned from training data to directly generate vectorized boundary descriptions. Aiming to address this problem, we introduce PolygonCNN, a learnable end-to-end vector shape modeling framework for generating building outlines from aerial images. The framework first performs an FCN-like segmentation to extract initial building contours. Then, by encoding the vertices of the building polygons along with the pooled image features extracted from segmentation step, a modified PointNet is proposed to learn shape priors and predict a polygon vertex deformation to generate refined building vector results. Additionally, we propose 1) a simplify-and-densify sampling strategy to generate homogeneously sampled polygon with well-kept geometric signals for shape prior learning; and 2) a novel loss function for estimating shape similarity between building polygons with vastly different vertex numbers. The experiments on over 10,000 building samples verify that PolygonCNN can generate building vectors with higher vertex-based F1-score than the state-of-the-art method, and simultaneously well maintains the building segmentation accuracy achieved by the FCN-like model.
WOS:000592242600009
</snippet>
</document>

<document id="777">
<title>A Configurable Event-Driven Convolutional Node with Rate Saturation Mechanism for Modular ConvNet Systems Implementation</title>
<url>http://dx.doi.org/10.3389/fnins.2018.00063</url>
<snippet>Convolutional Neural Networks (ConvNets) are a particular type of neural network often used for many applications like image recognition, video analysis or natural language processing. They are inspired by the human brain, following a specific organization of the connectivity pattern between layers of neurons known as receptive field. These networks have been traditionally implemented in software, but they are becoming more computationally expensive as they scale up, having limitations for real-time processing of high-speed stimuli. On the other hand, hardware implementations show difficulties to be used for different applications, due to their reduced flexibility. In this paper, we propose a fully configurable event-driven convolutional node with rate saturation mechanism that can be used to implement arbitrary ConvNets on FPGAs. This node includes a convolutional processing unit and a routing element which allows to build large 2D arrays where any multilayer structure can be implemented. The rate saturation mechanism emulates the refractory behavior in biological neurons, guaranteeing a minimum separation in time between consecutive events. A 4-layer ConvNet with 22 convolutional nodes trained for poker card symbol recognition has been implemented in a Spartan6 FPGA. This network has been tested with a stimulus where 40 poker cards were observed by a Dynamic Vision Sensor (DVS) in 1 s time. Different slow-down factors were applied to characterize the behavior of the system for high speed processing. For slow stimulus play-back, a 96&#37; recognition rate is obtained with a power consumption of 0.85mW. At maximum play-back speed, a traffic control mechanism downsamples the input stimulus, obtaining a recognition rate above 63&#37; when less than 20&#37; of the input events are processed, demonstrating the robustness of the network.
WOS:000425525600001
</snippet>
</document>

<document id="778">
<title>JOINT HEIGHT ESTIMATION AND SEMANTIC LABELING OF MONOCULAR AERIAL IMAGES WITH CNNS</title>
<url>http://dx.doi.org/</url>
<snippet>We aim to jointly estimate height and semantically label monocular aerial images. These two tasks are traditionally addressed separately in remote sensing, despite their strong correlation. Therefore, a model learning both height and classes jointly seems advantageous and so, we propose a multitask Convolutional Neural Network (CNN) architecture with two losses: one performing semantic labeling, and another predicting normalized Digital Surface Model (nDSM) from the pixel values. Since the nDSM/height information is used only in the second loss, there is no need to have a nDSM map at test time, and the model can estimate height automatically on new images. We test our proposed method on a set of sub-decimeter resolution images and show that our model equals the performances of two separate models, but at the cost of a single one.
WOS:000426954605042
</snippet>
</document>

<document id="779">
<title>GEOGRAPHICAL TRANSFERABILITY OF LULC IMAGE-BASED SEGMENTATION MODELS USING TRAINING DATA AUTOMATICALLY GENERATED FROM OPENSTREETMAP - CASE STUDY IN PORTUGAL</title>
<url>http://dx.doi.org/10.5194/isprs-annals-V-3-2022-25-2022</url>
<snippet>Synoptic remote sensing systems have been broadly used within supervised classification methods to map land use and land cover (LULC). Such methods rely on high quality sets of training data that are able to characterize the target classes. Often, training data is manually generated, either by field campaigns and/or by photointerpretation of ancillary remote sensing imagery. Several authors already proposed methodologies to attenuate such labour-intensive task of generating training data. One of the preferred datasets that are used as input training data is OpenStreetMap (OSM), which aims at creating a publicly available vector map of the world with the input of volunteers. However, OSM data is spatially heterogenous (e.g., capital cities and highly populated areas often have high degrees of completion while unpopulated regions often have a lower degree of completion), where there are still large areas without OSM coverage. In this paper we present a set of experiments that aim at assessing the geographical transferability of satellite imagebased segmentation models trained with OSM derived data. To this end, we chose two locations with different OSM coverage and disparate landscape (metropolitan region vs natural park region, in different landscape units), and assess how these models behave when trained in a region and applied in the other. The results show that the mapping of some classes is improved when considering a model trained in a different location.
WOS:000855203200005
</snippet>
</document>

<document id="780">
<title>CNN-Based Spectral Super-Resolution of Panchromatic Night-Time Light Imagery: City-Size-Associated Neighborhood Effects</title>
<url>http://dx.doi.org/10.3390/s21227662</url>
<snippet>Data on artificial night-time light (NTL), emitted from the areas, and captured by satellites, are available at a global scale in panchromatic format. In the meantime, data on spectral properties of NTL give more information for further analysis. Such data, however, are available locally or on a commercial basis only. In our recent work, we examined several machine learning techniques, such as linear regression, kernel regression, random forest, and elastic map models, to convert the panchromatic NTL images into colored ones. We compared red, green, and blue light levels for eight geographical areas all over the world with panchromatic light intensities and characteristics of built-up extent from spatially corresponding pixels and their nearest neighbors. In the meantime, information from more distant neighboring pixels might improve the predictive power of models. In the present study, we explore this neighborhood effect using convolutional neural networks (CNN). The main outcome of our analysis is that the neighborhood effect goes in line with the geographical extent of metropolitan areas under analysis: For smaller areas, optimal input image size is smaller than for bigger ones. At that, for relatively large cities, the optimal input image size tends to differ for different colors, being on average higher for red and lower for blue lights. Compared to other machine learning techniques, CNN models emerged comparable in terms of Pearsons correlation but showed performed better in terms of WMSE, especially for testing datasets.
WOS:000726726900001
</snippet>
</document>

<document id="781">
<title>Classification of Very-High-Spatial-Resolution Aerial Images Based on Multiscale Features with Limited Semantic Information</title>
<url>http://dx.doi.org/10.3390/rs13030364</url>
<snippet>Recently, deep learning has become the most innovative trend for a variety of high-spatial-resolution remote sensing imaging applications. However, large-scale land cover classification via traditional convolutional neural networks (CNNs) with sliding windows is computationally expensive and produces coarse results. Additionally, although such supervised learning approaches have performed well, collecting and annotating datasets for every task are extremely laborious, especially for those fully supervised cases where the pixel-level ground-truth labels are dense. In this work, we propose a new object-oriented deep learning framework that leverages residual networks with different depths to learn adjacent feature representations by embedding a multibranch architecture in the deep learning pipeline. The idea is to exploit limited training data at different neighboring scales to make a tradeoff between weak semantics and strong feature representations for operational land cover mapping tasks. We draw from established geographic object-based image analysis (GEOBIA) as an auxiliary module to reduce the computational burden of spatial reasoning and optimize the classification boundaries. We evaluated the proposed approach on two subdecimeter-resolution datasets involving both urban and rural landscapes. It presented better classification accuracy (88.9&#37;) compared to traditional object-based deep learning methods and achieves an excellent inference time (11.3 s/ha).
WOS:000615474300001
</snippet>
</document>

<document id="782">
<title>Methods of Mobile Robot Visual Navigation and Environment Mapping</title>
<url>http://dx.doi.org/10.3103/S8756699019020109</url>
<snippet>State-of-the-art methods of visual navigation for mobile robots are considered. A hierarchical representation structure of the environment corresponding to the hierarchical organization of the mobile robot control system is proposed. State-of-the-art approaches to constructing map models are presented. Their development will bring the navigation system closer to that formed by the human intellect and combines vision and a semantic view of the world within cognitive maps.
WOS:000471658400010
</snippet>
</document>

<document id="783">
<title>Hyperspectral and LiDAR Data Fusion Using Extinction Profiles and Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2016.2634863</url>
<snippet>This paper proposes a novel framework for the fusion of hyperspectral and light detection and ranging-derived rasterized data using extinction profiles (EPs) and deep learning. In order to extract spatial and elevation information from both the sources, EPs that include different attributes (e.g., height, area, volume, diagonal of the bounding box, and standard deviation) are taken into account. Then, the derived features are fused via either feature stacking or graph-based feature fusion. Finally, the fused features are fed to a deep learning-based classifier (convolutional neural network with logistic regression) to ultimately produce the classification map. The proposed approach is applied to two datasets acquired in Houston, TX, USA, and Trento, Italy. Results indicate that the proposed approach can achieve accurate classification results compared to other approaches. It should be noted that, in this paper, the concept of deep learning has been used for the first time to fuse LiDAR and hyperspectral features, which provides new opportunities for further research.
WOS:000406419400055
</snippet>
</document>

<document id="784">
<title>Weakly Supervised Spatial Deep Learning for Earth Image Segmentation Based on Imperfect Polyline Labels</title>
<url>http://dx.doi.org/10.1145/3480970</url>
<snippet>In recent years, deep learning has achieved tremendous success in image segmentation for computer vision applications. The performance of these models heavily relies on the availability of large-scale high-quality training labels (e.g., PASCAL VOC 2012). Unfortunately, such large-scale high-quality training data are often unavailable in many real-world spatial or spatiotemporal problems in earth science and remote sensing (e.g., mapping the nationwide river streams for water resource management). Although extensive efforts have been made to reduce the reliance on labeled data (e.g., semi-supervised or unsupervised learning, few-shot learning), the complex nature of geographic data such as spatial heterogeneity still requires sufficient training labels when transferring a pre-trained model from one region to another. On the other hand, it is often much easier to collect lower-quality training labels with imperfect alignment with earth imagery pixels (e.g., through interpreting coarse imagery by non-expert volunteers). However, directly training a deep neural network on imperfect labels with geometric annotation errors could significantly impact model performance. Existing research that overcomes imperfect training labels either focuses on errors in label class semantics or characterizes label location errors at the pixel level. These methods do not fully incorporate the geometric properties of label location errors in the vector representation. To fill the gap, this article proposes a weakly supervised learning framework to simultaneously update deep learning model parameters and infer hidden true vector label locations. Specifically, we model label location errors in the vector representation to partially reserve geometric properties (e.g., spatial contiguity within line segments). Evaluations on real-world datasets in the National Hydrography Dataset (NHD) refinement application illustrate that the proposed framework outperforms baseline methods in classification accuracy.
WOS:000784457600006
</snippet>
</document>

<document id="785">
<title>A Framework for Land Use Scenes Classification Based on Landscape Photos</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3028158</url>
<snippet>Space-earth integrated stereoscopic mapping promotes the progress of earth observation technologies. The method which combined remote sensing images with zenith perspectives and ground-level landscape photos with slanted viewing angles improves the efficiency and accuracy of land surveys. Recently, numerous efforts have been devoted to combining deep learning and remote sensing images for the classification of land use scenes. However, improvement of classification accuracy has been limited because of the lack of sectional representation. Landscape photos can describe the cross-sections in detail. For this reason, this study constructed a land-use semantic photo dataset (LSPD) and proposed a land-use classification framework for photos (LUCFP) based on Inception-v4. LSPD was constructed through semantic planning, scene segmentation, supervised iteration transfer learning, and augmentation of photos. LSPD has 1.4 million photos collected from seven geographic regions of China, and covers 13 land-use categories and 44 semantic categories. LUCFP adapts scene segmentation based on depth of field, multisemantic block labeling, and weighting of semantic joint spatial ranges to determine the land use category. To validate LUCFP, nine semantic samples (9x3x2000 photos) were chosen from LSPD, obtaining an overall accuracy of 97.64&#37;. The best photo cropping method was masking, which crops the boundary of the scene labeled by the photo, leading to an accuracy of 90.32&#37;. The optimal pixel size that balances speed and accuracy is 675x675, with speed reaching 30 photos per second with an average accuracy of 93.80&#37;. LUCFP has been successfully applied to the automatic verification of land surveys in China.
WOS:000579341600009
</snippet>
</document>

<document id="786">
<title>Handwritten Formula Symbol Recognition Based on Multi-Feature Convolutional Neural Network</title>
<url>http://dx.doi.org/10.3788/LOP56.072001</url>
<snippet>A model framework called DenseNet-SE is proposed based on a multi-featured dense convolutional neural network. Compared with the conventional methods, the DenseNet-SE adopts the data-driven approach and the manual extraction of features is not necessary. It contains the dense residual blocks so that the deep features can be acquired. In the jump-joining way, the fine-grained features arc obtained from the shallow layers to assist the deep features. The fused features can help the network structure obtain more global information and better represent the categories of formula symbols. The standard mathematical formula symbol library provided by the competition organization on recognition of online handwritten mathematical expression (CROHME) is used to verify the proposed algorithm, results show that the recognition rates of CROHME2011 and CROHME2016 arc 93.38&#37; and 92.93&#37;, respectively, higher than those of the existing algorithms.
WOS:000549842700029
</snippet>
</document>

<document id="787">
<title>Distilling Before Refine: Spatio-Temporal Transfer Learning for Mapping Irrigated Areas Using Sentinel-1 Time Series</title>
<url>http://dx.doi.org/10.1109/LGRS.2019.2960625</url>
<snippet>This letter proposes a deep learning model to deal with the spatial transfer challenge for the mapping of irrigated areas through the analysis of Sentinel-1 data. First, a convolutional neural network (CNN) model called "Teacher Model" is trained on a source geographical area characterized by a huge volume of samples. Then, this model is transferred from the source area to the target area characterized by a limited number of samples. The transfer learning framework is based on a distill and refine strategy, in which the teacher model is first distilled into a student model and, successively, refined by data samples coming from the target geographical area. The proposed strategy is compared with different approaches including a random forest (RF) classifier trained on the target data set and a CNN trained on the source data set and directly applied on the target area as well as several CNN classifiers trained on the target data set. The evaluation of the performed transfer strategy shows that the "distill and refine" framework obtains the best performance compared with other competing approaches. The obtained findings represent a first step toward the understanding of the spatial transferability of deep learning models in the Earth observation domain.
WOS:000583714200014
</snippet>
</document>

<document id="788">
<title>A generalized multi-task learning approach to stereo DSM filtering in urban areas</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.03.005</url>
<snippet>City models and height maps of urban areas serve as a valuable data source for numerous applications, such as disaster management or city planning. While this information is not globally available, it can be substituted by digital surface models (DSMs), automatically produced from inexpensive satellite imagery. However, stereo DSMs often suffer from noise and blur. Furthermore, they are heavily distorted by vegetation, which is of lesser relevance for most applications. Such basic models can be filtered by convolutional neural networks (CNNs), trained on labels derived from digital elevation models (DEMs) and 3D city models, in order to obtain a refined DSM. We propose a modular multi-task learning concept that consolidates existing approaches into a generalized framework. Our encoder-decoder models with shared encoders and multiple task-specific decoders leverage roof type classification as a secondary task and multiple objectives including a conditional adversarial term. The contributing single-objective losses are automatically weighted in the final multi-task loss function based on learned uncertainty estimates. We evaluated the performance of specific instances of this family of network architectures. Our method consistently outperforms the state of the art on common data, both quantitatively and qualitatively, and generalizes well to a new dataset of an independent study area.
WOS:000551268300017
</snippet>
</document>

<document id="789">
<title>Mapping and Monitoring of Land Cover/Land Use (LCLU) Changes in the Crozon Peninsula (Brittany, France) from 2007 to 2018 by Machine Learning Algorithms (Support Vector Machine, Random Forest, and Convolutional Neural Network) and by Post-classification Comparison (PCC)</title>
<url>http://dx.doi.org/10.3390/rs13193899</url>
<snippet>Land cover/land use (LCLU) is currently a very important topic, especially for coastal areas that connect the land and the coast and tend to change frequently. LCLU plays a crucial role in land and territory planning and management tasks. This study aims to complement information on the types and rates of LCLU multiannual changes with the distributions, rates, and consequences of these changes in the Crozon Peninsula, a highly fragmented coastal area. To evaluate the multiannual change detection (CD) capabilities using high-resolution (HR) satellite imagery, we implemented three remote sensing algorithms: a support vector machine (SVM), a random forest (RF) combined with geographic object-based image analysis techniques (GEOBIA), and a convolutional neural network (CNN), with SPOT 5 and Sentinel 2 data from 2007 and 2018. Accurate and timely CD is the most important aspect of this process. Although all algorithms were indicated as efficient in our study, with accuracy indices between 70&#37; and 90&#37;, the CNN had significantly higher accuracy than the SVM and RF, up to 90&#37;. The inclusion of the CNN significantly improved the classification performance (5-10&#37; increase in the overall accuracy) compared with the SVM and RF classifiers applied in our study. The CNN eliminated some of the confusion that characterizes a coastal area. Through the study of CD results by post-classification comparison (PCC), multiple changes in LCLU could be observed between 2007 and 2018: both the cultivated and non-vegetated areas increased, accompanied by high deforestation, which could be explained by the high rate of urbanization in the peninsula.
WOS:000756783900001
</snippet>
</document>

<document id="790">
<title>Machine Learning Classification Ensemble of Multitemporal Sentinel-2 Images: The Case of a Mixed Mediterranean Ecosystem</title>
<url>http://dx.doi.org/10.3390/rs12122005</url>
<snippet>Land cover type classification still remains an active research topic while new sensors and methods become available. Applications such as environmental monitoring, natural resource management, and change detection require more accurate, detailed, and constantly updated land-cover type mapping. These needs are fulfilled by newer sensors with high spatial and spectral resolution along with modern data processing algorithms. Sentinel-2 sensor provides data with high spatial, spectral, and temporal resolution for the in classification of highly fragmented landscape. This study applies six traditional data classifiers and nine ensemble methods on multitemporal Sentinel-2 image datasets for identifying land cover types in the heterogeneous Mediterranean landscape of Lesvos Island, Greece. Support vector machine, random forest, artificial neural network, decision tree, linear discriminant analysis, and k-nearest neighbor classifiers are applied and compared with nine ensemble classifiers on the basis of different voting methods. kappa statistic, F1-score, and Matthews correlation coefficient metrics were used in the assembly of the voting methods. Support vector machine outperformed the base classifiers with kappa of 0.91. Support vector machine also outperformed the ensemble classifiers in an unseen dataset. Five voting methods performed better than the rest of the classifiers. A diversity study based on four different metrics revealed that an ensemble can be avoided if a base classifier shows an identifiable superiority. Therefore, ensemble approaches should include a careful selection of base-classifiers based on a diversity analysis.
WOS:000550337500001
</snippet>
</document>

<document id="791">
<title>Deep learning model for seabed sediment classification based on fuzzy ranking feature optimization</title>
<url>http://dx.doi.org/10.1016/j.margeo.2020.106390</url>
<snippet>Accurate acquisition of information on seabed sediment distributions plays an important role in the construction of basic marine geographic databases. Although a multibeam echo-sounding system (MBES) can satisfy large-scale seafloor mapping with high precision and high resolution, the development of a consistent, stable, repeatable and validated seabed sediment classification method based on swath acoustic data is still in its infancy. To achieve accurate prediction and mapping of geographic seabed sediment information, this paper developed a deep learning model based on feature optimization. First, faced with high-dimensional features extracted from multibeam bathymetry and backscatter intensity measurement data, a fuzzy ranking (FR) feature optimization method was proposed. By combining the physical properties of actual sediment samples, the multidimensional features derived from terrain and intensity data are ranked and optimally selected according to the mean square error to eliminate redundant and irrelevant features. Second, the deep belief network (DBN) deep learning method was used to build a supervised seabed sediment classification model. The optimized features and actual sediment samples participate in model training, which further enhances the prediction ability of acoustic data to seabed sediments. Finally, to evaluate the performance of the DBN model, this experiment used large-scale multibeam survey data and ground-truth data (acquired by grabbers, core samplers, dredges, etc.) in the southern Irish Sea to achieve accurate prediction of 10 sediment types (slightly gravelly muddy sand, slightly gravelly sand, gravelly mud, gravelly muddy sand, gravelly sand, muddy sand, muddy sandy gravel, sand, sandy gravel and sandy mud). The experiment results show that by using the optimal feature combination based on FR, the overall classification accuracy and Kappa coefficient reached 86.20&#37; and 0.834, respectively, which are significantly improved compared to the evaluation metrics of other feature selection methods. In addition, compared with the current five typical supervised classification methods (i.e., the random forests, BP neural network, support vector machine, maximum likelihood and decision trees methods), the proposed DBN classification model achieves a better performance, highlighting its application potential in seabed sediment detection and mapping.
WOS:000608140700003
</snippet>
</document>

<document id="792">
<title>Spatial Resolution Enhancement for Large-Scale Land Cover Mapping via Weakly Supervised Deep Learning</title>
<url>http://dx.doi.org/10.14358/PERS.87.6.405</url>
<snippet>Multispectral satellite imagery is the primary data source for monitoring land cover change and characterizing land cover globally. However, the consistency of land cover monitoring is limited by the spatial and temporal resolutions of the acquired satellite images. The public availability of daily highresolution images is still scarce. This paper aims to fill this gap by proposing a novel spatiotemporal fusion method to enhance daily low spatial resolution land cover mapping using a weakly supervised deep convolutional neural network. We merge Sentinel images and moderate resolution imaging spectroradiometer (MODIS)-derived thematic land cover maps under the application background of massive remote sensing data and the large spatial resolution gaps between MODIS data and Sentinel images. The neural network training was conducted on the public data set SEN12MS, while the validation and testing used ground truth data from the 2020 IEEE Geoscience and Remote Sensing Society data fusion contest. The proposed data fusion method shows that the synthesized land cover map has significantly higher spatial resolution than the corresponding MODIS-derived land cover map. The ensemble approach can be implemented for generating highresolution time series of satellite images by fusing fine images from Sentinel-1 and -2 and daily coarse images from MODIS.
WOS:000698483300007
</snippet>
</document>

<document id="793">
<title>Scene Classification of Remotely Sensed Images via Densely Connected Convolutional Neural Networks and an Ensemble Classifier</title>
<url>http://dx.doi.org/10.14358/PERS.87.3.295</url>
<snippet>Deep learning techniques, especially convolutional neural networks, have boosted performance in analyzing and understanding remotely sensed images to a great extent. However, existing scene-classification methods generally neglect local and spatial information that is vital to scene classification of remotely sensed images. In this study, a method of scene classification for remotely sensed images based on pretrained densely connected convolutional neural networks combined with an ensemble classifier is proposed to tackle the under utilization of local and spatial information for image classification. Specifically, we first exploit the pretrained DenseNet and fine-tuned it to release its potential in remote-sensing image feature representation. Second, a spatial-pyramid structure and an improved Fisher-vector coding strategy are leveraged to further strengthen representation capability and the robustness of the feature map captured from convolutional layers. Then we integrate an ensemble classifier in our network architecture considering that lower attention to feature descriptors. Extensive experiments are conducted, and the proposed method achieves superior performance on UC Merced, AID, and NWPU-RESISC45 data sets.
WOS:000630246500008
</snippet>
</document>

<document id="794">
<title>Long Short-Term Memory Neural Network for Ionospheric Total Electron Content Forecasting Over China</title>
<url>http://dx.doi.org/10.1029/2020SW002706</url>
<snippet>An increasing number of terrestrial- and space-based radio-communication systems are influenced by the ionospheric space weather, making the ionospheric state increasingly important to forecast. In this study, a novel extended encoder-decoder long short-term memory extended (ED-LSTME) neural network, which can predict ionospheric total electron content (TEC) is proposed. Useful inherent features were automatically extracted from the historical TEC by LSTM layers, and the performance of the proposed model was enhanced by considering solar flux and geomagnetic activity data. The proposed ED-LSTME model was validated using 15-min TEC values from GPS measurements over one solar cycle (from January 2006 to July 2018) collected at 15 GPS stations in China. Different assessment experiments were conducted in different geographical locations and seasons as well as under varying geomagnetic activities, to comprehensively evaluate the models performance. These comparative experiments were conducted using an ED-LSTM, a traditional LSTM, a deep neural network, autoregressive integrated moving average, and the 2016 International Reference Ionosphere models. The results indicated that the ED-LSTME model is superior to the other statistical models, with R-2 and root mean square error values of 0.89 and 12.09 TECU, respectively. In addition, TEC was adequately predicted under different ionospheric conditions, and satisfactory results were obtained even under geomagnetically disturbed conditions. These results suggest that the prediction performance could be significantly improved by utilizing auxiliary data. These observations confirm that the proposed model outperforms several state-of-the-art models in making predictions at different times and under diverse conditions.
WOS:000644602700010
</snippet>
</document>

<document id="795">
<title>Revising Cadastral Data on Land Boundaries Using Deep Learning in Image-Based Mapping</title>
<url>http://dx.doi.org/10.3390/ijgi11050298</url>
<snippet>One of the main concerns of land administration in developed countries is to keep the cadastral system up to date. The goal of this research was to develop an approach to detect visible land boundaries and revise existing cadastral data using deep learning. The convolutional neural network (CNN), based on a modified architecture, was trained using the Berkeley segmentation data set 500 (BSDS500) available online. This dataset is known for edge and boundary detection. The model was tested in two rural areas in Slovenia. The results were evaluated using recall, precision, and the F1 score-as a more appropriate method for unbalanced classes. In terms of detection quality, balanced recall and precision resulted in F1 scores of 0.60 and 0.54 for Ponova vas and Odranci, respectively. With lower recall (completeness), the model was able to predict the boundaries with a precision (correctness) of 0.71 and 0.61. When the cadastral data were revised, the low values were interpreted to mean that the lower the recall, the greater the need to update the existing cadastral data. In the case of Ponova vas, the recall value was less than 0.1, which means that the boundaries did not overlap. In Odranci, 21&#37; of the predicted and cadastral boundaries overlapped. Since the direction of the lines was not a problem, the low recall value (0.21) was mainly due to overly fragmented plots. Overall, the automatic methods are faster (once the model is trained) but less accurate than the manual methods. For a rapid revision of existing cadastral boundaries, an automatic approach is certainly desirable for many national mapping and cadastral agencies, especially in developed countries.
WOS:000802726800001
</snippet>
</document>

<document id="796">
<title>Transition Is a Process: Pair-to-Video Change Detection Networks for Very High Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/TIP.2022.3226418</url>
<snippet>As an important yet challenging task in Earth observation, change detection (CD) is undergoing a technological revolution, given the broadening application of deep learning. Nevertheless, existing deep learning-based CD methods still suffer from two salient issues: 1) incomplete temporal modeling, and 2) space-time coupling. In view of these issues, we propose a more explicit and sophisticated modeling of time and accordingly establish a pair-to-video change detection (P2V-CD) framework. First, a pseudo transition video that carries rich temporal information is constructed from the input image pair, interpreting CD as a problem of video understanding. Then, two decoupled encoders are utilized to spatially and temporally recognize the type of transition, and the encoders are laterally connected for mutual promotion. Furthermore, the deep supervision technique is applied to accelerate the model training. We illustrate experimentally that the P2V-CD method compares favorably to other state-of-the-art CD approaches in terms of both the visual effect and the evaluation metrics, with a moderate model size and relatively lower computational overhead. Extensive feature map visualization experiments demonstrate how our method works beyond making contrasts between bi-temporal images. Source code is available at https://github.com/Bobholamovic/CDLab.
WOS:000902111900005
</snippet>
</document>

<document id="797">
<title>LCS-EnsemNet: A Semisupervised Deep Neural Network for SAR Image Change Detection With Dual Feature Extraction and Label-Consistent Self-Ensemble</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3122461</url>
<snippet>Change detection (CD) in synthetic aperture radar (SAR) images faces two challenging problems limiting the detection performance: inherent speckle noise in SAR data causes the overlapping nature of changed and unchanged classes and, thus, affects the image understanding for inferring category of each image pixel; and adequate labeled samples are quite laborious and time-consuming to collect, which is the major limitation for supervised methods. In this article, we develop a novel deep learning-based semisupervised method to address these challenges. The method first incorporates a pixel-wise log-ratio difference image (DI) and its saliency map to produce a spatially enhanced (SE) DI using a reweighting scheme based on the fact that changed pixels exhibit higher saliency than unchanged pixels. As a result, prominent changed regions are highlighted, and the class separability is significantly increased. We construct pixel-wise and context-wise features based on the log-ratio DI and SE DI, which respectively provide image detail cue and spatial context cue, as dual input features to jointly characterize the change information at each pixel position. Second, we propose a label-consistent self-ensemble network (LCS-EnsemNet), which can take advantage of the unlabeled samples to learn discriminative high-level features for the precise identification of changed pixels. By enforcing a label consistency between dual features and a label consistency across multiple classifiers, the label-consistent self-ensemble strategy enables the proposed network to selectively transform unlabeled samples into pseudo-labeled samples in an unsupervised manner and ensures that the selected pseudo-labels are reliably and stably predicted. Finally, the cross-entropy loss is calculated with the limited labeled data and selected pseudo-labeled samples to optimize the LCS-EnsemNet in a supervised way. The proposed method is evaluated on three low/medium-resolution SAR datasets and one high-resolution SAR dataset, and experimental results have demonstrated its efficiency and effectiveness.
WOS:000725801600008
</snippet>
</document>

<document id="798">
<title>Remote Sensing Data Classification Using A Hybrid Pre-Trained VGG16 CNN-SVM Classifier</title>
<url>http://dx.doi.org/10.1109/ElConRus51938.2021.9396706</url>
<snippet>In recent years, deep learning techniques have been improved to classify geographical information by assigning remote sensing images pixels. CNN models can fix the feature learning techniques in the field of visualization systems. This paper proposed a hybrid pre-trained VGG16 -convolutional neural networks (CNNs) - SVM classifier models. VGG16 conducts the features extraction from the input remote sensing data, and SVM classifier solves the classification output based on the CNN output feature maps. Our proposed model can play its neural network layers with a novel feature extraction strategy to achieve good classification accuracy over high-resolution remote sensing data. Classification experience is performed on the two remote sensing public datasets (UC Merced Land and RSSCN7), using high computational performance support that achieved reliable classification results within the shortest time.
WOS:000669709802044
</snippet>
</document>

<document id="799">
<title>The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence</title>
<url>http://dx.doi.org/10.3389/fncom.2020.00063</url>
<snippet>Recent advances in artificial intelligence (AI) and neuroscience are impressive. In AI, this includes the development of computer programs that can beat a grandmaster at GO or outperform human radiologists at cancer detection. A great deal of these technological developments are directly related to progress in artificial neural networks-initially inspired by our knowledge about how the brain carries out computation. In parallel, neuroscience has also experienced significant advances in understanding the brain. For example, in the field of spatial navigation, knowledge about the mechanisms and brain regions involved in neural computations of cognitive maps-an internal representation of space-recently received the Nobel Prize in medicine. Much of the recent progress in neuroscience has partly been due to the development of technology used to record from very large populations of neurons in multiple regions of the brain with exquisite temporal and spatial resolution in behaving animals. With the advent of the vast quantities of data that these techniques allow us to collect there has been an increased interest in the intersection between AI and neuroscience, many of these intersections involve using AI as a novel tool to explore and analyze these large data sets. However, given the common initial motivation point-to understand the brain-these disciplines could be more strongly linked. Currently much of this potential synergy is not being realized. We propose that spatial navigation is an excellent area in which these two disciplines can converge to help advance what we know about the brain. In this review, we first summarize progress in the neuroscience of spatial navigation and reinforcement learning. We then turn our attention to discuss how spatial navigation has been modeled using descriptive, mechanistic, and normative approaches and the use of AI in such models. Next, we discuss how AI can advance neuroscience, how neuroscience can advance AI, and the limitations of these approaches. We finally conclude by highlighting promising lines of research in which spatial navigation can be the point of intersection between neuroscience and AI and how this can contribute to the advancement of the understanding of intelligent behavior.
WOS:000561203400001
</snippet>
</document>

<document id="800">
<title>Exploring the vertical dimension of street view image based on deep learning: a case study on lowest floor elevation estimation</title>
<url>http://dx.doi.org/10.1080/13658816.2021.1981334</url>
<snippet>Street view imagery such as Google Street View is widely used in peoples daily lives. Many studies have been conducted to detect and map objects such as traffic signs and sidewalks for urban built-up environment analysis. While mapping objects in the horizontal dimension is common in those studies, automatic vertical measuring in large areas is underexploited. Vertical information from street view imagery can benefit a variety of studies. One notable application is estimating the lowest floor elevation, which is critical for building flood vulnerability assessment and insurance premium calculation. In this article, we explored the vertical measurement in street view imagery using the principle of tacheometric surveying. In the case study of lowest floor elevation estimation using Google Street View images, we trained a neural network (YOLO-v5) for door detection and used the fixed height of doors to measure doors elevation. The results suggest that the average error of estimated elevation is 0.218 m. The depthmaps of Google Street View were utilized to traverse the elevation from the roadway surface to target objects. The proposed pipeline provides a novel approach for automatic elevation estimation from street view imagery and is expected to benefit future terrain-related studies for large areas.
WOS:000704232700001
</snippet>
</document>

<document id="801">
<title>Multi-channel recurrent attention network for building extraction from high resolution remote sensing images</title>
<url>http://dx.doi.org/10.1088/1361-6501/ac4d5f</url>
<snippet>Building extraction from high-resolution remote sensing images is of great importance for urban planning, disaster assessment, and geography mapping. In recent years, convolutional neural networks have made outstanding achievements in improving the precision of building extraction. However, most existing approaches have some problems, such as insufficient detailed feature extraction and ignorance of the relationship between different features. In this study, we propose a novel multi-channel recurrent attention network (MCANet) for building extraction. Firstly, the multi-scale channel attention mechanism is used to expand the convolution kernel receptive field, making the model can extract rich building region feature information. Secondly, we use the spatial pyramid recurrent block to establish long-range dependencies over space, channel, and layer of different convolutions. Finally, the multi-channel feature fusion block is used to fuse the multi-scale channel features information, and improve the building extraction precision. Experimental results show that the proposed MCANet achieves better results (recall, precision, intersection-over-union, and F-1_score on the Inria Aerial Image Labeling Dataset are 89.82&#37;, 94.38&#37;, 87.42&#37;, and 88.25&#37;, respectively), and outperforms the other state-of-the-art approaches.
WOS:000754060200001
</snippet>
</document>

<document id="802">
<title>Geo-Object-Based Soil Organic Matter Mapping Using Machine Learning Algorithms With Multi-Source Geo-Spatial Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2902375</url>
<snippet>Soil is a complicated historical natural continuum that presents gradual changes in its properties and geographic area. Conventional soil survey and cartography methods on a macroscopic scale based on grids with a coarse resolution are inadequate for the rapid development of precision agriculture. The demand for soil mapping content and accuracy has increased as more convenient methods of acquiring multi-source geo-spatial data have been developed, and such data are commonly employed to extract basic mapping units and environmental variables in related algorithms. We employ geo-objects as basic units of soil property mapping, which are extracted from high-resolution remote sensing images using a convolutional neural network based learning algorithm. Multi-source geo-spatial data are transferred into each geo-object as environmental variables, and the relationships between soil properties and environmental variables are mined using powerful tree-based machine learning algorithms, including regressions with random forests and XGBoost. A data set that includes soil sample points and multi-source geo-spatial data is used to evaluate the effectiveness of the proposed method. The experimental results demonstrate that the method allows for better soil organic matter mapping than state-of-the-art interpolation-based and linear-regression-based methods. The proposed procedure has potential to be a general method for mapping other soil properties. Its advantages are embodied in the modeling of relatively miscellaneous data with implicitly associated non-linear relationships between soil properties and environmental variables. The spatial scale and accuracy of the finer maps capture more detailed characteristics of the soil properties and are applicable to the micro-domain fields required for refined soil mapping with small variations.
WOS:000464756600004
</snippet>
</document>

<document id="803">
<title>Urban Change Detection from Aerial Images Using Convolutional Neural Networks and Transfer Learning</title>
<url>http://dx.doi.org/10.3390/ijgi11040246</url>
<snippet>Urban change detection is an important part of sustainable urban planning, regional development, and socio-economic analysis, especially in regions with limited access to economic and demographic statistical data. The goal of this research is to create a strategy that enables the extraction of indicators from large-scale orthoimages of different resolution with practically acceptable accuracy after a short training process. Remote sensing data can be used to detect changes in number of buildings, forest areas, and other landscape objects. In this paper, aerial images of a digital raster orthophoto map at scale 1:10,000 of the Republic of Lithuania (ORT10LT) of three periods (2009-2010, 2012-2013, 2015-2017) were analyzed. Because of the developing technologies, the quality of the images differs significantly and should be taken into account while preparing the dataset for training the semantic segmentation model DeepLabv3 with a ResNet50 backbone. In the data preparation step, normalization techniques were used to ensure stability of image quality and contrast. Focal loss for the training metric was selected to deal with the misbalanced dataset. The suggested model training process is based on the transfer learning technique and combines using a model with weights pretrained in ImageNet with learning on coarse and fine-tuning datasets. The coarse dataset consists of images with classes generated automatically from Open Street Map (OSM) data and the fine-tuning dataset was created by manually reviewing the images to ensure that the objects in images match the labels. To highlight the benefits of transfer learning, six different models were trained by combining different steps of the suggested model training process. It is demonstrated that using pretrained weights results in improved performance of the model and the best performance was demonstrated by the model which includes all three steps of the training process (pretrained weights, training on coarse and fine-tuning datasets). Finally, the results obtained with the created machine learning model enable the implementation of different approaches to detect, analyze, and interpret urban changes for policymakers and investors on different levels on a local map, grid, or municipality level.
WOS:000785442400001
</snippet>
</document>

<document id="804">
<title>Coupling Dual Graph Convolution Network and Residual Network for Local Climate Zone Mapping</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3132394</url>
<snippet>Local climate zone (LCZ) has become a new standard classification scheme in urban landscapes and showed great potential in urban climate research. Traditional classifiers and ordinary neural networks only consider the spectral or local spatial features of the pixel, ignoring the effect of nonlocal information on the LCZ classification. The graph convolutional network (GCN) has been used to exploit the relationship between adjacent and global land covers owing to the ability to conduct flexible convolution over graphs. In this work, we integrated a convolutional neural network and two GCNs into an end-to-end hybrid framework and generated LCZs directly from the original images. Local-, regional-, and global-level features were extracted and grouped complementarily to foster better performance. Experiments were conducted in six cities around the world to verify the effectiveness of our method. Results showed that the average classification accuracy of the six cities reached 0.956 and performed better than any other comparable model. Ablation experiments also demonstrated the mutual promotion of the different modules. Finally, the small sample experiment provided a practical reference for the LCZ classification in the absence of samples in future.
WOS:000748370600003
</snippet>
</document>

<document id="805">
<title>Convolutional Neural Networks for Automated Built Infrastructure Detection in the Arctic Using Sub-Meter Spatial Resolution Satellite Imagery</title>
<url>http://dx.doi.org/10.3390/rs14112719</url>
<snippet>Rapid global warming is catalyzing widespread permafrost degradation in the Arctic, leading to destructive land-surface subsidence that destabilizes and deforms the ground. Consequently, human-built infrastructure constructed upon permafrost is currently at major risk of structural failure. Risk assessment frameworks that attempt to study this issue assume that precise information on the location and extent of infrastructure is known. However, complete, high-quality, uniform geospatial datasets of built infrastructure that are readily available for such scientific studies are lacking. While imagery-enabled mapping can fill this knowledge gap, the small size of individual structures and vast geographical extent of the Arctic necessitate large volumes of very high spatial resolution remote sensing imagery. Transforming this big imagery data into science-ready information demands highly automated image analysis pipelines driven by advanced computer vision algorithms. Despite this, previous fine resolution studies have been limited to manual digitization of features on locally confined scales. Therefore, this exploratory study serves as the first investigation into fully automated analysis of sub-meter spatial resolution satellite imagery for automated detection of Arctic built infrastructure. We tasked the U-Net, a deep learning-based semantic segmentation model, with classifying different infrastructure types (residential, commercial, public, and industrial buildings, as well as roads) from commercial satellite imagery of Utqiagvik and Prudhoe Bay, Alaska. We also conducted a systematic experiment to understand how image augmentation can impact model performance when labeled training data is limited. When optimal augmentation methods were applied, the U-Net achieved an average F1 score of 0.83. Overall, our experimental findings show that the U-Net-based workflow is a promising method for automated Arctic built infrastructure detection that, combined with existing optimized workflows, such as MAPLE, could be expanded to map a multitude of infrastructure types spanning the pan-Arctic.
WOS:000808975100001
</snippet>
</document>

<document id="806">
<title>Application and Evaluation of Deep Neural Networks for Airborne Hyperspectral Remote Sensing Mineral Mapping: A Case Study of the Baiyanghe Uranium Deposit in Northwestern Xinjiang, China</title>
<url>http://dx.doi.org/10.3390/rs14205122</url>
<snippet>Deep learning is a popular topic in machine learning and artificial intelligence research and has achieved remarkable results in various fields. In geological remote sensing, mineral mapping is an appealing application of hyperspectral remote sensing for geological surveyors. Whether deep learning can improve the mineral identification ability in hyperspectral remote sensing images, especially for the discrimination of spectrally similar and intimately mixed minerals, needs to be evaluated. In this study, shortwave airborne spectrographic imager (SASI) hyperspectral images of the Baiyanghe uranium deposit in Northwestern Xinjiang, China, were used as experimental data. Three deep neural network (DNN) models were designed: a fully connected neural network (FCNN), a one-dimensional convolutional neural network (1D CNN), and a one-dimensional and two-dimensional convolutional neural network (1D and 2D CNN). A sample dataset containing five minerals was constructed for model training and validation, which was divided into training, validation and test sets at a ratio of 6:2:2. The final test accuracies of the FCNN, 1D CNN, and 1D and 2D CNN were 91.24&#37;, 93.67&#37; and 94.77&#37;, respectively. The three DNNs were used for mineral identification and mapping of SASI hyperspectral images of the Baiyanghe uranium mining area. The mapping results were compared with the mapping results of the support vector machine (SVM) and the mixture-tuned matched filtering (MTMF) method. Combined with the ground spectral data obtained by the spectrometer, spectral verification and interpretation were carried out on sections that the two kinds of methods identified differently. The verification results show that the mapping results of the 1D and 2D CNN were more accurate than those of the other methods. More importantly, for minerals with similar spectral characteristics, such as short-wavelength white mica and medium-wavelength white mica, the 1D and 2D CNN model had a more accurate discrimination effect than the other DNN models, indicating that the introduction of spatial information can improve the mineral identification ability in hyperspectral remote sensing images. In general, CNNs have good application prospects in geological mapping of hyperspectral remote sensing images and are worthy of further development in future work.
WOS:000875070100001
</snippet>
</document>

<document id="807">
<title>A Behavioral Approach to Visual Navigation with Graph Localization Networks</title>
<url>http://dx.doi.org/</url>
<snippet>Inspired by research in psychology, we introduce a behavioral approach for visual navigation using topological maps. Our goal is to enable a robot to navigate from one location to another, relying only on its visual observations and the topological map of the environment. To this end, we propose using graph neural networks for localizing the agent in the map, and decompose the action space into primitive behaviors implemented as convolutional or recurrent neural networks. Using the Gibson simulator and the Stanford 2D-3D-S dataset, we verify that our approach outperforms relevant baselines and is able to navigate in both seen and unseen indoor environments.
WOS:000570976800010
</snippet>
</document>

<document id="808">
<title>Deep Graph Convolutional Networks for Accurate Automatic Road Network Selection</title>
<url>http://dx.doi.org/10.3390/ijgi10110768</url>
<snippet>The selection of road networks is very important for cartographic generalization. Traditional artificial intelligence methods have improved selection efficiency but cannot fully extract the spatial features of road networks. However, current selection methods, which are based on the theory of graphs or strokes, have low automaticity and are highly subjective. Graph convolutional networks (GCNs) combine graph theory with neural networks; thus, they can not only extract spatial information but also realize automatic selection. Therefore, in this study, we adopted GCNs for automatic road network selection and transformed the process into one of node classification. In addition, to solve the problem of gradient vanishing in GCNs, we compared and analyzed the results of various GCNs (GraphSAGE and graph attention networks [GAT]) by selecting small-scale road networks under different deep architectures (JK-Nets, ResNet, and DenseNet). Our results indicate that GAT provides better selection of road networks than other models. Additionally, the three abovementioned deep architectures can effectively improve the selection effect of models; JK-Nets demonstrated more improvement with higher accuracy (88.12&#37;) than other methods. Thus, our study shows that GCN is an appropriate tool for road network selection; its application in cartography must be further explored.
WOS:000727978700001
</snippet>
</document>

<document id="809">
<title>ERF-IMCS: An Efficient and Robust Framework with Image-Based Monte Carlo Scheme for Indoor Topological Navigation</title>
<url>http://dx.doi.org/10.3390/app10196829</url>
<snippet>Conventional approaches to global localization and navigation mainly rely on metric maps to provide precise geometric coordinates, which may cause the problem of large-scale structural ambiguity and lack semantic information of the environment. This paper presents a scalable vision-based topological mapping and navigation method for a mobile robot to work robustly and flexibly in large-scale environment. In the vision-based topological navigation, an image-based Monte Carlo localization method is presented to realize global topological localization based on image retrieval, in which fine-tuned local region features from an object detection convolutional neural network (CNN) are adopted to perform image matching. The combination of image retrieval and Monte Carlo provide the robot with the ability to effectively avoid perceptual aliasing. Additionally, we propose an effective visual localization method, simultaneously employing the global and local CNN features of images to construct discriminative representation for environment, which makes the navigation system more robust to the interference of occlusion, translation, and illumination. Extensive experimental results demonstrate that ERF-IMCS exhibits great performance in the robustness and efficiency of navigation.
WOS:000587209800001
</snippet>
</document>

<document id="810">
<title>Automated Retinal Layer Segmentation Using Graph-based Algorithm Incorporating Deep-learning-derived Information</title>
<url>http://dx.doi.org/10.1038/s41598-020-66355-5</url>
<snippet>Regular drusen, an accumulation of material below the retinal pigment epithelium (RPE), have long been established as a hallmark early feature of nonneovascular age-related macular degeneration (AMD). Advances in imaging have expanded the phenotype of AMD to include another extracellular deposit, reticular pseudodrusen (RPD) (also termed subretinal drusenoid deposits, SDD), which are located above the RPE. We developed an approach to automatically segment retinal layers associated with regular drusen and RPD in spectral domain (SD) optical coherence tomography (OCT) images. More specifically, a shortest-path algorithm enhanced with probability maps generated through a fully convolutional neural network was used to segment drusen and RPD, as well as 11 retinal layers in SD-OCT volumes. This algorithm achieves a mean difference that is within the subpixel accuracy range drusen and RPD, alongside the other 11 retinal layers, highlighting the high robustness of this algorithm for this dataset. To the best of our knowledge, this is the first report of a validated algorithm for the automated segmentation of the retinal layers including early AMD features of RPD and regular drusen separately on SD-OCT images.
WOS:000543969200022
</snippet>
</document>

<document id="811">
<title>PulseSatellite: A Tool Using Human-AI Feedback Loops for Satellite Image Analysis in Humanitarian Contexts</title>
<url>http://dx.doi.org/</url>
<snippet>Humanitarian response to natural disasters and conflicts can be assisted by satellite image analysis. In a humanitarian context, very specific satellite image analysis tasks must be done accurately and in a timely manner to provide operational support. We present PulseSatellite, a collaborative satellite image analysis tool which leverages neural network models that can be retrained on-the fly and adapted to specific humanitarian contexts and geographies. We present two case studies, in mapping shelters and floods respectively, that illustrate the capabilities of PulseSatellite.
WOS:000668126806022
</snippet>
</document>

<document id="812">
<title>M-2-Net: A Multi-scale Multi-level Feature Enhanced Network for Object Detection in Optical Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/DICTA51227.2020.9363420</url>
<snippet>Object detection in remote sensing images is a challenging task due to diversified orientation, complex background, dense distribution and scale variation of objects. In this paper, we tackle this problem by proposing a novel multi-scale multi-level feature enhanced network (M-2-Net) that integrates a Feature Map Enhancement (FME) module and a Feature Fusion Block (FFB) into Rotational RetinaNet. The FME module aims to enhance the weak features by factorizing the convolutional operation into two similar branches instead of one single branch, which helps to broaden receptive field with less parameters. This module is embedded into different layers in the backbone network to capture multi-scale semantics and location information for detection. The FFB module is used to shorten the information propagation path between low-level high-resolution features in shallow layers and high-level semantic features in deep layers, facilitating more effective feature fusion and object detection especially those with small sizes. Experimental results on three benchmark datasets show that our method not only outperforms many one-stage detectors but also achieves competitive accuracy with lower time cost than two-stage detectors.
WOS:000935148000053
</snippet>
</document>

<document id="813">
<title>Deeply supervised convolutional neural network for shadow detection based on a novel aerial shadow imagery dataset</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.07.016</url>
<snippet>Shadow detection is an essential work for remote sensing image analysis, as the presence of shadows in high resolution images not only degrades the radiometric information but also disturbs the image interpretation. In this paper, a convolutional neural network (CNN) based shadow detection framework for aerial remote sensing images is presented. We construct a publicly available Aerial Imagery dataset for Shadow Detection (AISD), which is the first aerial shadow imagery dataset, as far as we know. Based on AISD, we propose a novel Deeply Supervised convolutional neural network for Shadow Detection (DSSDNet). To solve the insufficient feature extraction problem of shadows, the DSSDNet model is designed to include two steps: (1) an encoder-decoder residual (EDR) structure is adopted to extract multi-level and discriminative shadow features; (2) a deeply supervised progressive fusion (DSPF) process is then imposed on EDR to further boost the detection performance by directly guiding the training of the network and fuse adjacent feature maps progressively. The proposed DSSDNet is compared with several state-of-the-art methods in both qualitative and quantitative analysis. Results show that the proposed DSSDNet is more accurate, and more consistent to the shape of the objects casting shadows, with the average F-score being 91.79&#37; on the testing images.
WOS:000561346200030
</snippet>
</document>

<document id="814">
<title>Remote Sensing and Social Sensing Data Fusion for Fine-Resolution Population Mapping With a Multimodel Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3086139</url>
<snippet>Mapping population distribution at fine spatial scales is significant and fundamental for resource utilization, assessment of city disaster, environmental regulation, and urbanization. Multisource data produced by remote and social sensing have been widely used to disaggregate census information to map population distributions at fine resolution. However, it is challenging to achieve accurate high-spatial-resolution population mapping by combining multisource data and considering geographic spatial heterogeneity. The existing approaches do not consider global and local spatial information simultaneously, resulting in low accuracy. This article proposes a multimodel fusion neural network for estimating fine-resolution population estimates from multisource data. Our approach takes into account the local spatial information and global information of each geographic unit. Specifically, a first-order space matrix of a geographic unit is used to characterize its local spatial information. We propose a multimodel neural network, which combines a convolutional neural network and a multilayer perceptron (MLP) model to estimate a fine-resolution population mapping. Using Shenzhen, China, as the experimental setting, a population distribution map was generated at a 100-m spatial resolution. The model was quantitatively validated by showing that it captured the relationship between the estimated population and the census population at the township level (R-2 = 0.77) more accurately than the WorldPop dataset (R-2 = 0.51) and the MLP-based model (R-2 = 0.63). Qualitatively, the proposed model can identify differences in population density in densely populated areas and some remote population clusters more accurately than the WorldPop population dataset.
WOS:000668882800003
</snippet>
</document>

<document id="815">
<title>Recurrently exploring class-wise attention in a hybrid convolutional and bidirectional LSTM network for multi-label aerial image classification</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.01.015</url>
<snippet>Aerial image classification is of great significance in the remote sensing community, and many researches have been conducted over the past few years. Among these studies, most of them focus on categorizing an image into one semantic label, while in the real world, an aerial image is often associated with multiple labels, e.g., multiple object-level labels in our case. Besides, a comprehensive picture of present objects in a given high-resolution aerial image can provide a more in-depth understanding of the studied region. For these reasons, aerial image multi-label classification has been attracting increasing attention. However, one common limitation shared by existing methods in the community is that the co-occurrence relationship of various classes, so-called class dependency, is underexplored and leads to an inconsiderate decision. In this paper, we propose a novel end-to end network, namely class-wise attention-based convolutional and bidirectional LSTM network (CA-Cony-BiLSTM), for this task. The proposed network consists of three indispensable components: (1) a feature extraction module, (2) a class attention learning layer, and (3) a bidirectional LSTM-based sub-network. Particularly, the feature extraction module is designed for extracting fine-grained semantic feature maps, while the class attention learning layer aims at capturing discriminative class-specific features. As the most important part, the bidirectional LSTM-based sub-network models the underlying class dependency in both directions and produce structured multiple object labels. Experimental results on UCM multi-label dataset and DFC15 multi label dataset validate the effectiveness of our model quantitatively and qualitatively.
WOS:000461535600015
</snippet>
</document>

<document id="816">
<title>Deep Learning Approach for Classifying the Built Year and Structure of Individual Buildings by Automatically Linking Street View Images and GIS Building Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2023.3237509</url>
<snippet>The built year and structure of individual buildings are crucial factors for estimating and assessing potential earthquake and tsunami damage. Recent advances in sensing and analysis technologies allow the acquisition of high-resolution street view images (SVIs) that present new possibilities for research and development. In this study, we developed a model to estimate the built year and structure of a building using omnidirectional SVIs captured using an onboard camera. We used geographic information system (GIS) building data and SVIs to generate an annotated built-year and structure dataset by developing a method to automatically combine the GIS data with images of individual buildings cropped through object detection. Furthermore, we trained a deep learning model to classify the built year and structure of buildings using the annotated image dataset based on a deep convolutional neural network (DCNN) and a vision transformer (ViT). The results showed that SVI accurately predicts the built year and structure of individual buildings using ViT (overall accuracies for structure = 0.94 [three classes] and 0.96 [two classes] and for age = 0.68 [six classes] and 0.90 [three classes]). Compared with DCNN-based networks, the proposed Swin transformer based on ViT architectures effectively improves prediction accuracy. The results indicate that multiple high-resolution images can be obtained for individual buildings using SVI, and the proposed method is an effective approach for classifying structures and determining building age. The automatic, accurate, and large-scale mapping of the built year and structure of individual buildings can help develop specific disaster prevention measures.
WOS:000935002300016
</snippet>
</document>

<document id="817">
<title>Distributed Deep Neural Networks over the Cloud, the Edge and End Devices</title>
<url>http://dx.doi.org/10.1109/ICDCS.2017.226</url>
<snippet>We propose distributed deep neural networks (DDNNs) over distributed computing hierarchies, consisting of the cloud, the edge (fog) and end devices. While being able to accommodate inference of a deep neural network (DNN) in the cloud, a DDNN also allows fast and localized inference using shallow portions of the neural network at the edge and end devices. When supported by a scalable distributed computing hierarchy, a DDNN can scale up in neural network size and scale out in geographical span. Due to its distributed nature, DDNNs enhance sensor fusion, system fault tolerance and data privacy for DNN applications. In implementing a DDNN, we map sections of a DNN onto a distributed computing hierarchy. By jointly training these sections, we minimize communication and resource usage for devices and maximize usefulness of extracted features which are utilized in the cloud. The resulting system has built-in support for automatic sensor fusion and fault tolerance. As a proof of concept, we show a DDNN can exploit geographical diversity of sensors to improve object recognition accuracy and reduce communication cost. In our experiment, compared with the traditional method of offloading raw sensor data to be processed in the cloud, DDNN locally processes most sensor data on end devices while achieving high accuracy and is able to reduce the communication cost by a factor of over 20x.
WOS:000412759500030
</snippet>
</document>

<document id="818">
<title>Building segmentation from satellite imagery using U-Net with ResNet encoder</title>
<url>http://dx.doi.org/10.1109/ICMCCE51767.2020.00431</url>
<snippet>As one of the most important types of artificial ground features, building images and outlines are widely used in map updating, GIS analysis, urban planning, and environmental modelling etc. Fast and accurate segmentation of buildings is one of the hot and difficult topics in remote sensing image processing research for many years. This paper tries to apply the U-Net deep neural network model with ResNet encoder to remote sensing image segmentation for building extraction. First, we obtain some remote sensing images through some open datasets, and then we outline the building images to get the building mask, then we divide the dataset into eight to two to form two datasets for training and validation, and then we use this dataset to train and validate the U-Net model. The results show that the models MIoU (Mean Intersection over Union) reached 0.83, and the model achieved a good building segmentation effect. This model can be used for building segmentation with clear boundaries, however, there are also a few improvements to be resolved in further studies, more accurate results, more straight outlines, less misclassification, etc.
WOS:000675598100423
</snippet>
</document>

<document id="819">
<title>RDQS: A Geospatial Data Analysis System for Improving Roads Directionality Quality</title>
<url>http://dx.doi.org/10.3390/ijgi11080448</url>
<snippet>With the increasing availability of smart devices, billions of users are currently relying on map services for many fundamental daily tasks such as obtaining directions and getting routes. It is becoming more and more important to verify the quality and consistency of route data presented by different map providers. However, verifying this consistency manually is a very time-consuming task. To address this problem, in this paper we introduce a novel geospatial data analysis system that is based on road directionality. We investigate our Road Directionality Quality System (RDQS) using multiple map providers, including: Bing Maps, Google Maps, and OpenStreetMap. Results from the experiments conducted show that our detection neural network is able to detect an arrows position and direction in map images with &gt;90&#37; F1-Score across each of the different providers. We then utilize this model to analyze map images in six different regions. Our findings show that our approach can reliably assess map quality and discover discrepancies in road directionality across the different providers. We report the percentage of discrepancies found between map providers using this approach in a proposed study area. These results can help determine areas needs to be revised and prioritized to improve the overall quality of the data within maps.
WOS:000846692800001
</snippet>
</document>

<document id="820">
<title>A CNN-based Super-resolution Technique for Active Fire Detection on Sentinel-2 Data</title>
<url>http://dx.doi.org/</url>
<snippet>Remote Sensing applications can benefit from a relatively fine spatial resolution multispectral (MS) images and a high revisit frequency ensured by the twin satellites Sentinel-2. Unfortunately, only four out of thirteen bands are provided at the highest resolution of 10 meters, and the others at 20 or 60 meters. For instance the Short-Wave Infrared (SWIR) bands, provided at 20 meters, are very useful to detect active fires. Aiming to a more detailed Active Fire Detection (AFD) maps, we propose a super-resolution data fusion method based on Convolutional Neural Network (CNN) to move towards the 10-m spatial resolution the SWIR bands. The proposed CNN-based solution is compared to alternative methods in terms of some accuracy metrics. Moreover we have tested the super-resolved bands from an application point of view by monitoring active fire through classic indices. Advantages and limits of our proposed approach are validated on specific geographical area (the mount Vesuvius, close to Naples) that was damaged by widespread fires during the summer of 2017.
WOS:000550769300060
</snippet>
</document>

<document id="821">
<title>Automatic stenosis recognition from coronary angiography using convolutional neural networks</title>
<url>http://dx.doi.org/10.1016/j.cmpb.2020.105819</url>
<snippet>Background and objective: Coronary artery disease, which is mostly caused by atherosclerotic narrowing of the coronary artery lumen, is a leading cause of death. Coronary angiography is the standard method to estimate the severity of coronary artery stenosis, but is frequently limited by intraand inter-observer variations. We propose a deep-learning algorithm that automatically recognizes stenosis in coronary angiographic images. Methods: The proposed method consists of key frame detection, deep learning model training for classification of stenosis on each key frame, and visualization of the possible location of the stenosis. Firstly, we propose an algorithm that automatically extracts key frames essential for diagnosis from 452 right coronary artery angiography movie clips. Our deep learning model is then trained with image-level annotations to classify the areas narrowed by over 50 &#37;. To make the model focus on the salient features, we apply a self-attention mechanism. The stenotic locations are visualized using the activated area of feature maps with gradient-weighted class activation mapping. Results: The automatically detected key frame was very close to the manually selected key frame (average distance (1.70 +/- 0.12) frame per clip). The model was trained with key frames on internal datasets, and validated with internal and external datasets. Our training method achieved high frame-wise area-under the-curve of 0.971, frame-wise accuracy of 0.934, and clip-wise accuracy of 0.965 in the average values of cross-validation evaluations. The external validation results showed high performances with the mean frame-wise area-under-the-curve of (0.925 and 0.956) in the single and ensemble model, respectively. Heat map visualization shows the location for different types of stenosis in both internal and external data sets. With the self-attention mechanism, the stenosis could be precisely localized, which helps to accurately classify the stenosis by type. Conclusions: Our automated classification algorithm could recognize and localize coronary artery stenosis highly accurately. Our approach might provide the basis for a screening and assistant tool for the interpretation of coronary angiography. (C) 2020 The Authors. Published by Elsevier B.V.
WOS:000597384600013
</snippet>
</document>

<document id="822">
<title>Improved Lane Detection Method Based on Convolutional Neural Network Using Self-attention Distillation</title>
<url>http://dx.doi.org/10.18494/SAM.2020.3128</url>
<snippet>With the rapid development of autopilot technology and various types of sensor, high-precision maps containing a large amount of information for assisting driving have been proposed. The standard lane line detection algorithm relies on the robust estimation of visible lane line markers from a camera image using vision and image processing algorithms. Although the recognition and detection technology for road marking lines is relatively mature, some problems still exist, such as poor detection accuracy and unsatisfactory real-time performance. To solve the problems of the poor robustness and low running speed of the current lane detection methods in complex environments, in this study, we improve current lane detection methods from the perspective of semantic segmentation and propose a DC-VGG-SAD network (VGG: visual geometry group), in which dilated convolution (DC) is used to reduce the complexity of the network to ensure detection accuracy. Furthermore, adding self-attention distillation (SAD) makes the information transmission faster. The proposed network was experimentally evaluated using two large-scale datasets. It was found that when dealing with lane lines in complex environments, the network offers higher detection accuracy and detection speed than most current mainstream networks.
WOS:000611407800013
</snippet>
</document>

<document id="823">
<title>A Novel CNN-Based Detector for Ship Detection Based on Rotatable Bounding Box in SAR Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3049851</url>
<snippet>Thanks to the excellent feature representation capabilities of neural networks, deep learning-based methods perform far better than traditional methods on target detection tasks such as ship detection. Although various network models have been proposed for SAR ship detection such as DRBox-v1, DRBox-v2, and MSR2N, there are still some problems such as mismatch of feature scale, contradictions between different learning tasks, and unbalanced distribution of positive samples, which have not been mentioned in these studies. In this article, an improved one-stage object detection framework based on RetinaNet and rotatable bounding box (RBox), which is referred as R-RetinaNet, is proposed to solve the above problems. The main improvements of R-RetinaNet as well as the contributions of this article are threefold. First, a scale calibration method is proposed to align the scale distribution of the output backbone feature map with the scale distribution of the targets. Second, a feature fusion network based on task-wise attention feature pyramid network is designed to decouple the feature optimization process of different tasks, which alleviates the conflict between different learning goals. Finally, an adaptive intersection over union (IoU) threshold training method is proposed for RBox-based model to correct the unbalanced distribution of positive samples caused by the fixed IoU threshold on RBox. Experimental results show that our method obtains 13.26&#37;, 9.49&#37;, 8.92&#37;, and 4.55&#37; gains in average precision under an IoU threshold of 0.5 on the public SAR ship detection dataset compared with four state-of-the-art RBox-based methods, respectively.
WOS:000613557400008
</snippet>
</document>

<document id="824">
<title>Review on remote sensing methods for landslide detection using machine and deep learning</title>
<url>http://dx.doi.org/10.1002/ett.3998</url>
<snippet>Landslide, one of the most critical natural hazards, is caused due to specific compositional slope movement. In the past decades, due to inflation of urbanized area and climate change, a compelling expansion in landslide prevalence took place which is also termed as mass/slope movement and mass wasting, causing extensive collapse around the world. The principal reason for its pursuance is a reduction in the internal resistance of soil and rocks, classified as a slide, topple, fall, and flow. Slopes can be differentiated based on earth material and the nature of its movements. The downward flow of landslides occurs due to excessive rainfall, snowmelt, earthquake, volcanic eruption, and so on. This review article revisits the conventional approaches for identification of landslides, predicting future risk, associated with slope failures, followed by emphasizing the advantages of modern geospatial techniques such as aerial photogrammetry, satellite remote sensing images (ie, panchromatic, multispectral, radar images), Terrestrial laser scanning, and High-Resolution Digital Elevation Model (HR-DEM) in updating landslide inventory maps. Machine learning techniques like Support Vector Machine, Artificial neural network, deep learning has been extensively used with geographical data producing effective results for assessment of natural hazard/resources and environmental research. Based on recent studies, deep learning is a reliable tool addressing remote sensing challenges such as trade-off in imaging system producing poor quality investigation, in addition, to expedite consequent task such as image recognition, object detection, classification, and so on. Conventional methods, like pixel and object-based machine learning methods, have been broadly explored. Advanced development in deep learning technique like CNN (Convolutional neural network) has been extensively successful in information extraction from an image and has exceeded other traditional approaches. Over the past few years, minor attempts have been made for landslide susceptibility mapping using CNN. In addition, small sample sizes for training purpose will be major drawback and notably remarkable while using deep learning techniques. Also, assessment of the models performance with diverse training and testing proportion other than commonly utilized ratio, that is, 70/30 needs to be explored further. The review article briefly highlights the remote sensing methods for landslide detection using machine learning and deep learning.
WOS:000542033600001
</snippet>
</document>

<document id="825">
<title>Identification of Sedimentary Strata by Segmentation Neural Networks of Oblique Photogrammetry of UAVs</title>
<url>http://dx.doi.org/10.1007/978-3-031-21753-1_4</url>
<snippet>The determination of the shape and extent of strata is one of the most important steps in the classification of architectural elements in an outcrop. However, mapping and accessing outcrops with broad lateral and vertical continuity can be difficult due to their geographic position. In this work, UAV-captured outcrop images are used to apply seeding methods to identify and measure strata size. A computational system based on Segmentation Neural Networks was used as an effective method to automatically identify outcrop strata. The results show that the DeepLabV3 semantic segmentation neural network can achieve interesting results when combined with the PointRend technique. The different combinations of architectures were compared with UAV images from outcrops with turbidite systems of the Itarar  e Group (Paran  a Basin, Brazil). The results show that machine learning approaches are alternative and efficient techniques that contribute to the improvement of traditional semi-supervised segmentation methods.
WOS:000904430900004
</snippet>
</document>

<document id="826">
<title>Landslide Inventory Mapping With Bitemporal Aerial Remote Sensing Images Based on the Dual-Path Fully Convolutional Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2980895</url>
<snippet>This article presents a novel dual-path full convolutional network (DP-FCN) model for constructing a landslide inventory map (LIM) with bitemporal very high-resolution (VHR) remote sensing images. Unlike traditional methods for drawing LIM, the proposed DP-FCN directly draws LIMs from the bitemporal aerial images with VHR through a trained deep neural network without generating the change magnitude map. Thus, the proposed approach can effectively reduce the effects of pseudo changes caused by phenological differences rather than landslide events. The proposed DP-FCN model contains two modules, namely, deep feature extraction, and joint feature learning networks. Deep feature extraction aims to reduce redundancy while extracting the high-level deep features from bitemporal images. Joint feature learning establishes the relationship between the deep features of bitemporal images and the ground reference map. Experiments on the real datasets of the landslide sites in Lantau Island of Hong Kong, China, demonstrate the feasibility and superiority of the proposed approach in drawing LIM with VHR remote sensing images. Moreover, compared with the results obtained by the state-of-the-art algorithms, the proposed DP-FCN method achieves the best performance in terms of accuracy for landslide inventory mapping.
WOS:000564184200006
</snippet>
</document>

<document id="827">
<title>Spatial Variability Aware Deep Neural Networks (SVANN): A General Approach</title>
<url>http://dx.doi.org/10.1145/3466688</url>
<snippet>Spatial variability is a prominent feature of various geographic phenomena such as climatic zones, USDA plant hardiness zones, and terrestrial habitat types (e.g., forest, grasslands, wetlands, and deserts). However, current deep learning methods follow a spatial-one-size-fits-all (OSFA) approach to train single deep neural network models that do not account for spatial variability. Quantification of spatial variability can be challenging due to the influence of many geophysical factors. In preliminary work, we proposed a spatial variability aware neural network (SVANN-I, formerly called SVANN) approach where weights are a function of location but the neural network architecture is location independent. In this work, we explore a more flexible SVANNE approach where neural network architecture varies across geographic locations. In addition, we provide a taxonomy of SVANN types and a physics inspired interpretation model. Experiments with aerial imagery based wetland mapping show that SVANN-I outperforms OSFA and SVANN-E performs the best of all.
WOS:000754524700009
</snippet>
</document>

<document id="828">
<title>Semisupervised Classification of Hyperspectral Image Based on Graph Convolutional Broad Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3062642</url>
<snippet>Hyperspectral image (HSI) classification has attracted much attention in the field of remote sensing. However, the lack of sufficient labeled training samples is a huge challenge for HSI classification. To face this challenge, we propose a semisupervised HSI classification method based on graph convolutional broad network (GCBN). First, to avoid the underfitting problem caused by the insufficient linear sparse feature representation ability of broad learning system (BLS), graph convolution operation is applied to extract nonlinear and discriminative spectral-spatial features from the original HSI to replace the linear mapping features in the traditional BLS. Second, to solve the problem of insufficient model classification ability caused by limited labeled samples, the combinatorial average method (CAM) is proposed to use valuable paired samples to generate sample expansion set for GCBN model training. Third, BLS is used to perform broad expansion on spectral-spatial features extracted by GCN and extended by CAM, which further enhances the feature representation ability. Finally, the output weights can be easily calculated by the ridge regression theory. Experimental results on three real HSI datasets demonstrate the effectiveness of our proposed GCBN.
WOS:000633636500009
</snippet>
</document>

<document id="829">
<title>Passing the Message: Representation Transfer in Modular Balanced Networks</title>
<url>http://dx.doi.org/10.3389/fncom.2019.00079</url>
<snippet>Neurobiological systems rely on hierarchical and modular architectures to carry out intricate computations using minimal resources. A prerequisite for such systems to operate adequately is the capability to reliably and efficiently transfer information across multiple modules. Here, we study the features enabling a robust transfer of stimulus representations in modular networks of spiking neurons, tuned to operate in a balanced regime. To capitalize on the complex, transient dynamics that such networks exhibit during active processing, we apply reservoir computing principles and probe the systems computational efficacy with specific tasks. Focusing on the comparison of random feed-forward connectivity and biologically inspired topographic maps, we find that, in a sequential set-up, structured projections between the modules are strictly necessary for information to propagate accurately to deeper modules. Such mappings not only improve computational performance and efficiency, they also reduce response variability, increase robustness against interference effects, and boost memory capacity. We further investigate how information from two separate input streams is integrated and demonstrate that it is more advantageous to perform non-linear computations on the input locally, within a given module, and subsequently transfer the result downstream, rather than transferring intermediate information and performing the computation downstream. Depending on how information is integrated early on in the system, the networks achieve similar task-performance using different strategies, indicating that the dimensionality of the neural responses does not necessarily correlate with nonlinear integration, as predicted by previous studies. These findings highlight a key role of topographic maps in supporting fast, robust, and accurate neural communication over longer distances. Given the prevalence of such structural feature, particularly in the sensory systems, elucidating their functional purpose remains an important challenge toward which this work provides relevant, new insights. At the same time, these results shed new light on important requirements for designing functional hierarchical spiking networks.
WOS:000503494900001
</snippet>
</document>

<document id="830">
<title>Building Footprint Generation Through Convolutional Neural Networks With Attraction Field Representation</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3109844</url>
<snippet>Building footprint generation is a vital task in a wide range of applications, including, to name a few, land use management, urban planning and monitoring, and geographical database updating. Most existing approaches addressing this problem fall back on convolutional neural networks (CNNs) to learn semantic masks of buildings. However, one limitation of their results is blurred building boundaries. To address this, we propose to learn attraction field representation for building boundaries, which is capable of providing an enhanced representation power. Our method comprises two elemental modules: an Img2AFM module and an AFM2Mask module. More specifically, the former aims at learning an attraction field representation conditioned on an input image, which is capable of enhancing building boundaries and suppressing the background. The latter module predicts segmentation masks of buildings using the learned attraction field map. The proposed method is evaluated on three datasets with different spatial resolutions: the ISPRS dataset, the INRIA dataset, and the Planet dataset. From experimental results, we find that the proposed framework can well preserve geometric shapes and sharp boundaries of buildings, which brings significant improvements over other competitors. The trained model and code are available at https://github.com/lqycrystal/AFM_building.
WOS:000732756700001
</snippet>
</document>

<document id="831">
<title>A Concentric Loop Convolutional Neural Network for Manual Delineation-Level Building Boundary Segmentation From Remote-Sensing Images</title>
<url>http://dx.doi.org/10.1109/TGRS.2021.3126704</url>
<snippet>To date, accurate building footprint delineation in the surveying, mapping, and geographic information system (GIS) communities has been dependent on human labor. In this article, to address this issue, we propose a concentric loop convolutional neural network (CLP-CNN) method for the automatic segmentation of building boundaries from remote-sensing images. The proposed method consists of three components: 1) a boundary detector to extract coarse polygonal boundaries of individual regions of interest; 2) a concentric loop-shaped convolutional network with bidirectional pairing loss to fine-tune the vertices of the polygons; and 3) a refinement block, which removes redundant vertices and regularizes the boundaries to polygons at the manual delineation level. We also demonstrate that the proposed CLP-CNN method is applicable to other generic objects in natural images. Experiments on two building datasets confirmed that more than 77&#37;/67&#37; of the building polygons predicted by the proposed method are on par with the manual delineation level, representing a significant saving in the labor cost of manual annotation. In generic object boundary delineation tests performed on the Semantic Boundaries Dataset (SBD), the proposed method outperformed the most recent state-of-the-art methods by at least 3.1&#37; in average precision (AP). Furthermore, compared with other vertex matching methods, the learning process of the proposed method converges faster. The source code will be available at http://gpcv.whu.edu.cn/data.
WOS:000757891700005
</snippet>
</document>

<document id="832">
<title>Mapping stone walls in Northeastern USA using deep learning and LiDAR data</title>
<url>http://dx.doi.org/10.1080/15481603.2023.2196117</url>
<snippet>Stone walls are widespread and iconic landforms found throughout forested terrain in the Northeastern USA that were built during the 17th to early 20th centuries to delineate property boundaries and the edges of agricultural fields and pastures. As linear, or broadly curved, features that are typically &gt; 0.5 m high, 1-2 m wide, and &gt; 4-8 m long, stone walls are highly visible in LiDAR data, and mapping them is of broad interest to the cultural heritage sector as well as to researchers specifically focused on historic landscape reconstruction. However, existing mapping attempts have commonly relied on field surveys and manual digitization, which is time-consuming, especially when trying to complete mapping at broader scales. In response to this limitation, this study: (1) presents a novel framework to automate stone wall mapping using Deep Convolutional Neural Networks (DCNN) models (U-Net and ResUnet) and high-resolution airborne LiDAR, (2) evaluates model performance in two test sites against field verified stone walls, (3) investigates the factors that can influence model performance in terms of the quality of LiDAR data (e.g. ground point spacing), and (4) suggests post-processing for town-level mapping of stone walls (similar to 120 km(2)). Both models performed well with respect to the Matthews Correlation Coefficient (MCC) score. U-Net scenario 3 achieved an MCC score of 0.87 at test site 1, while ResUnet scenario 3 (S3) had an MCC score of 0.80 at test site 2. In town-level test site 3, ResUnet S3 achieved the best F-1 score of 82&#37; after post-processing. This study demonstrates the potential of automated mapping of anthropogenic features using our models.
WOS:000963378900001
</snippet>
</document>

<document id="833">
<title>Height estimation from single aerial images using a deep convolutional encoder-decoder network</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.01.013</url>
<snippet>Extracting 3D information from aerial images is an important and still challenging topic in photogrammetry and remote sensing. Height estimation from only a single aerial image is an ambiguous and ill-posed problem. To address this challenging problem, in this paper, an architecture based on a deep convolutional neural network (CNN) is proposed in order to estimate the height values from a single aerial image. Methodologies for data preprocessing, selection of training data as well as data augmentation are presented. Subsequently, a deep CNN architecture is proposed consisting of encoding and decoding steps. In the encoding part, a deep residual learning is employed for extracting the local and global features. An up-sampling approach is proposed in the decoding part for increasing the output resolution and skip connections are employed in each scale to modify the estimated height values at the object boundaries. Finally, a post-processing approach is proposed to merge the predicted height image patches and generate a seamless continuous height map. The quantitative evaluation of the proposed approaches on the ISPRS datasets indicates relative and root mean square errors of approximately 0.9 m and 3.2 m, respectively.
WOS:000461535600005
</snippet>
</document>

<document id="834">
<title>Large-scale surface water change observed by Sentinel-2 during the 2018 drought in Germany</title>
<url>http://dx.doi.org/10.1080/01431161.2020.1723817</url>
<snippet>Monitoring and understanding the spatio-temporal dynamics of hydrological droughts with seamless geographical coverage over large areas is essential for an assessment of impacts on water resources, industry, transport, and human health. This became particularly relevant during the heat and drought of 2018 in Germany, which affected the country across various sectors and caused significant interruptions to ship traffic on rivers and lakes with negative impacts on tourism, transportation, and supply chains. In this study, we provide a spatially and temporally consistent view on the 2018 hydrological drought in Germany as seen from Sentinel-2 satellite images. We extract waterbodies with national coverage at different timestamps using an automated processing chain, which is based on a convolutional neural network and has originally been developed for near-real time flood monitoring. The method produces water segmentations with consistently high Overall Accuracy (&gt;= 0.95) and Kappa Coefficient (&gt;= 0.89), despite varying topography, land-use/land-cover and atmospheric conditions. Furthermore, we identify hotspots of change in water extent at a national scale by comparing monthly water maps for 2018 with the respective maps of the previous year 2017. For the change hotspots, we map change gradients and produce water extent time-series with mapping frequencies &lt;5 days along a timeline of 12 months.
WOS:000517363700001
</snippet>
</document>

<document id="835">
<title>Unsupervised Domain Adaptation with Adversarial Self-Training for Crop Classification Using Remote Sensing Images</title>
<url>http://dx.doi.org/10.3390/rs14184639</url>
<snippet>Crop type mapping is regarded as an essential part of effective agricultural management. Automated crop type mapping using remote sensing images is preferred for the consistent monitoring of crop types. However, the main obstacle to generating annual crop type maps is the collection of sufficient training data for supervised classification. Classification based on unsupervised domain adaptation, which uses prior information from the source domain for target domain classification, can solve the impractical problem of collecting sufficient training data. This study presents self-training with domain adversarial network (STDAN), a novel unsupervised domain adaptation framework for crop type classification. The core purpose of STDAN is to combine adversarial training to alleviate spectral discrepancy problems with self-training to automatically generate new training data in the target domain using an existing thematic map or ground truth data. STDAN consists of three analysis stages: (1) initial classification using domain adversarial neural networks; (2) the self-training-based updating of training candidates using constraints specific to crop classification; and (3) the refinement of training candidates using iterative classification and final classification. The potential of STDAN was evaluated by conducting six experiments reflecting various domain discrepancy conditions in unmanned aerial vehicle images acquired at different regions and times. In most cases, the classification performance of STDAN was found to be compatible with the classification using training data collected from the target domain. In particular, the superiority of STDAN was shown to be prominent when the domain discrepancy was substantial. Based on these results, STDAN can be effectively applied to automated cross-domain crop type mapping without analyst intervention when prior information is available in the target domain.
WOS:000856805300001
</snippet>
</document>

<document id="836">
<title>Deep Learning-Driven Detection and Mapping of Rockfalls on Mars</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2991588</url>
<snippet>The analysis of rockfall distribution and magnitude is a useful tool to study the past and current endogenic and exogenic activity of Mars. At the same time, tracks left by rockfalls provide insights into the mechanical properties of the Martian surface. While a wealth of high-resolution spaceborne image data are available, manual mapping of displaced boulders with tracks is inefficient and slow, resulting in: 1) a small total number of mapped features; 2) inadequate statistics; and 3) a suboptimal utilization of the available big data. This study implements a deep learning-driven approach to automatically detect and map Martian boulders with tracks in high resolution imaging science experiment (HiRISE) imagery. Six off-the-shelf neural networks have been trained either on Martian or lunar rockfall data, or a combination of both, and are able to achieve a maximum overall recall of up to 0.78 and a maximum overall precision of up to 1.0, with a mean average precision of 0.71. The fusion of training data from different planets and sensors results in an increased detection precision, highlighting the value of domain generalization and multidomain learning. Average processing time per HiRISE image is similar to 45 s using an NVIDIA Titan Xp, which is more than one order of magnitude faster than a human operator. The developed deep learning-driven infrastructure can be deployed to map Martian rockfalls on a global scale and within a realistic timeframe.
WOS:000544051400001
</snippet>
</document>

<document id="837">
<title>SSD Real-Time Illegal Parking Detection Based on Contextual Information Transmission</title>
<url>http://dx.doi.org/10.32604/cmc.2020.06427</url>
<snippet>With the improvement of the national economic level, the number of vehicles is still increasing year by year. According to the statistics of National Bureau of Statics, the number is approximately up to 327 million in China by the end of 2018, which makes urban traffic pressure continues to rise so that the negative impact of urban traffic order is growing. Illegal parking-the common problem in the field of transportation security is urgent to be solved and traditional methods to address it are mainly based on ground loop and manual supervision, which may miss detection and cost much manpower. Due to the rapidly developing deep learning sweeping the world in recent years, object detection methods relying on background segmentation cannot meet the requirements of complex and various scenes on speed and precision. Thus, an improved Single Shot MultiBox Detector (SSD) based on deep learning is proposed in our study, we introduce attention mechanism by spatial transformer module which gives neural networks the ability to actively spatially transform feature maps and add contextual information transmission in specified layer. Finally, we found out the best connection layer in the detection model by repeated experiments especially for small objects and increased the precision by 1.5&#37; than the baseline SSD without extra training cost. Meanwhile, we designed an illegal parking vehicle detection method by the improved SSD, reaching a high precision up to 97.3&#37; and achieving a speed of 40FPS, superior to most of vehicle detection methods, will make contributions to relieving the negative impact of illegal parking.
WOS:000512696500018
</snippet>
</document>

<document id="838">
<title>Appearance based deep domain adaptation for the classification of aerial images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2021.08.004</url>
<snippet>This paper addresses appearance based domain adaptation for the pixel-wise classification of remotely sensed data using deep neural networks (DNN) as a strategy to reduce the requirements of DNN with respect to the availability of training data. We focus on the setting in which labelled data are only available in a source domain D S, but not in a target domain D T, known as unsupervised domain adaptation in Computer Vision. Our method is based on adversarial training of an appearance adaptation network (AAN) that transforms images from D S such that they look like images from D T. Together with the original label maps from D S, the transformed images are used to adapt a DNN to D T. The AAN has to change the appearance of objects of a certain class such that they resemble objects of the same class in D T. Many approaches try to achieve this goal by incorporating cycle consistency in the adaptation process, but such approaches tend to hallucinate structures that occur frequently in one of the domains. In contrast, we propose a joint training strategy of the AAN and the classifier, which constrains the AAN to transform the images such that they are correctly classified. To further improve the adaptation performance, we propose a new regularization loss for the discriminator network used in adversarial training. We also address the problem of finding the optimal values of the trained network parameters, proposing a new unsupervised entropy based parameter selection criterion, which compensates for the fact that there is no validation set in D T that could be monitored. As a minor contribution, we present a new weighting strategy for the cross-entropy loss, addressing the problem of imbalanced class distributions. Our method is evaluated in 42 adaptation scenarios using datasets from 7 cities, all consisting of high-resolution digital orthophotos and height data. It achieves a positive transfer in all cases, and on average it improves the performance in the target domain by 4.3&#37; in overall accuracy. In adaptation scenarios between the Vaihingen and Potsdam datasets from the ISPRS semantic labelling benchmark our method outperforms those from recent publications by 10 20&#37; with respect to the mean intersection over union.
WOS:000697167200006
</snippet>
</document>

<document id="839">
<title>Using An Attention-Based LSTM Encoder-Decoder Network for Near Real-Time Disturbance Detection</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.2988324</url>
<snippet>Accurate prediction of future observations based on past data is the key to near real-time disturbance detection using satellite image time series (SITS). To overcome the limitations of existing methods, we present an attention-based long-short-term memory (LSTM) encoder-decoder model in which the historical time series of a pixel is encoded with a bidirectional LSTM encoder while the future time series is produced by another LSTM decoder. An attention mechanism is integrated into the encoder-decoder model to align the input time series with the output time series and to dynamically choose the most relevant contextual information while forecasting. Based on the proposed model, we develop a framework for near real-time disturbance detection and verify its effectiveness in the case of burned area mapping. The prediction accuracy of the proposed model is evaluated using moderate resolution imaging spectroradiometer (MODIS) time series and compared with state-of-the-art models. Experimental results show that our model achieves the best results in terms of lower prediction error and higher model fitness. We also evaluate the disturbance detection ability of the proposed framework. The proposed approach improves the detection rate of disturbances while suppressing false alarms, and increases the temporal accuracy. We suggest that the proposed methods provide new tools for enhancing current early warning systems in real time.
WOS:000537282600005
</snippet>
</document>

<document id="840">
<title>Hyperspectral Images Classification Based on Multi-Feature Fusion and Hybrid Convolutional Neural Networks</title>
<url>http://dx.doi.org/10.3788/LOP202158.0810010</url>
<snippet>Aiming at the problem that the classification accuracy of hyperspectral images is not ideal when the amount of training samples of three-dimensional convolutional network is limited, an efficient classification model based on multi-feature fusion and hybrid convolutional neural networks is proposed in this paper. First, after the dimensionality reduction processing is performed on hyperspectral images, the three-dimensional convolutional layer is used to extract deep hierarchical spatial-spectral joint features. Then, the residual connection is introduced to perform multi -feature fusion through feature map concatenation and pixel-wise addition to realize feature reuse and enhance information transmission. Finally, a two-dimensional convolutional layer is used to enhance the spatial information of the extracted features and realize image classification. The experimental results show that in the three publicly available hyperspectral data sets Indian Pines, Salinas and University of Pavia, 5&#37;, 1&#37; and 1&#37; of the labeled samples are used as training data, respectively, the classification accuracy of the model is 97.09&#37;, 99.30&#37; and 97. 60&#37;, respectively, which can effectively improve the classification accuracy of hyperspectral images for under small sample condition.
WOS:000686485400012
</snippet>
</document>

<document id="841">
<title>Hybrid Dense Network With Attention Mechanism for Hyperspectral Image Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3171586</url>
<snippet>The nonlinear relation between the spectral information and the corresponding objects (complex physiognomies) makes pixelwise classification challenging for conventional methods. To deal with nonlinearity issues in hyperspectral image classification (HSIC), convolutional neural networks (CNN) are more suitable, indeed. However, fixed kernel sizes make traditional CNN too specific, neither flexible nor conducive to feature learning, thus impacting on the classification accuracy. The convolution of different kernel size networks may overcome this problem by capturing more discriminating and relevant information. In light of this, the proposed solution aims at combining the core idea of 3-D and 2-D inception net with the attention mechanism to boost the HSIC CNN performance in a hybrid scenario. The resulting attention-fused hybrid network (AfNet) is based on three attention-fused parallel hybrid subnets with different kernels in each block repeatedly using high-level features to enhance the final ground-truth maps. In short, AfNet is able to selectively filter out the discriminative features critical for classification. Several tests on HSI datasets provided competitive results for AfNet compared to state-of-the-art models.
WOS:000803307300005
</snippet>
</document>

<document id="842">
<title>Cascaded Detection Framework Based on a Novel Backbone Network and Feature Fusion</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2924086</url>
<snippet>Due to the ability of powerful feature representation, deep-learning-based object detection has attracted considerable research attention, and many methods have been proposed for remote sensing images. However, there are still some problems that need to be addressed. In this paper, a novel and effective detection framework based on faster region-based convolutional neural network is designed. Specifically, first, in order to locate the boundaries of large objects and find the missing small objects, DetNet is incorporated into the detection framework as the backbone network. DetNet fixes the spatial resolution in deep layers and adopts dilated bottleneck with convolution projection to increase the divergence between input and output feature maps. Then, the proposed framework uses the backbone network to extract the scene features and region features simultaneously, which are both mapped to feature vectors and then fused together. The feature fusion operation can improve the feature representation of the generated region. Last, to improve the performance of localization, the cascade structure is adopted in the framework. The cascade structure has multiple phases and every phase has independent classifier and regressor. The results obtained from the previous phase are used as the regions of interest in the next phase. Therefore, the multiphase detector can increase the detection accuracy phase by phase. Comprehensive evaluations on a public ten-class object detection dataset demonstrate the effectiveness of the proposed framework. Moreover, ablation experiments are also implemented to show the respective influence of different parts of the framework on the performance improvement.
WOS:000489785800028
</snippet>
</document>

<document id="843">
<title>Building Extraction in Very High Resolution Imagery by Dense-Attention Networks</title>
<url>http://dx.doi.org/10.3390/rs10111768</url>
<snippet>Building extraction from very high resolution (VHR) imagery plays an important role in urban planning, disaster management, navigation, updating geographic databases, and several other geospatial applications. Compared with the traditional building extraction approaches, deep learning networks have recently shown outstanding performance in this task by using both high-level and low-level feature maps. However, it is difficult to utilize different level features rationally with the present deep learning networks. To tackle this problem, a novel network based on DenseNets and the attention mechanism was proposed, called the dense-attention network (DAN). The DAN contains an encoder part and a decoder part which are separately composed of lightweight DenseNets and a spatial attention fusion module. The proposed encoder-decoder architecture can strengthen feature propagation and effectively bring higher-level feature information to suppress the low-level feature and noises. Experimental results based on public international society for photogrammetry and remote sensing (ISPRS) datasets with only red-green-blue (RGB) images demonstrated that the proposed DAN achieved a higher score (96.16&#37; overall accuracy (OA), 92.56&#37; F1 score, 90.56&#37; mean intersection over union (MIOU), less training and response time and higher-quality value) when compared with other deep learning methods.
WOS:000451733800099
</snippet>
</document>

<document id="844">
<title>Constraining 135 years of mass balance with historic structure-from-motion photogrammetry on Storglaciaren, Sweden</title>
<url>http://dx.doi.org/10.1080/04353676.2019.1588543</url>
<snippet>Geodetic volume estimates of Storglaciaren in Sweden suggest a 28&#37; loss in total ice mass between 1910 and 2015. Terrestrial photographs from 1910 of Tarfala valley, where Storglaciaren is situated, allow for an accurate reconstruction of the glaciers surface using Structure-from-Motion photogrammetry, which we used for past volume and mass estimations. The glaciers yearly mass balance gradient and net mass balance was also estimated back to 1880 using weather data from Karesuando, 170 km north-east of Storglaciaren, through neural network regression. These combined reconstructions provide a continuous mass change series between the end of the Little Ice Age and 1946, when field data become available. The resultant reconstruction suggests a state close to equilibrium between 1880 and the 1910s, followed by drastic melt until the 1970s, constituting 76&#37; of the 1910-2015 ice loss. More favourable conditions subsequently stabilized the mass balance until the late 1990s, after which Storglaciaren started losing mass again. The 1910 reconstruction allows for a more accurate mass change series than previous estimates, and the methodology can be used on other glaciers where early photographic material exists.
WOS:000466080500001
</snippet>
</document>

<document id="845">
<title>Deep learning for dune pattern mapping with the AW3D30 global surface model</title>
<url>http://dx.doi.org/10.1002/esp.4888</url>
<snippet>In this paper we present a deep learning (U-Net)-based workflow for classifying linear dune landforms based on the discrete Laplacian convolution of a new global elevation dataset, the AW3D30 digital surface model. Crest vectors were then derived for landscape pattern analysis. The U-Net crest classification model was trained and evaluated on sample data from dunefields across the Australian continent. The resulting crest vectors and dune defect placement were then evaluated in typical semi-arid and arid dune landscapes in eastern central Australia where high-resolution (5 m horizontal) digital elevation models are available (for three out of our four study sites) as a reference dataset. The method was applied to quantify dune pattern metrics for the entire Simpson Desert dunefield, Australia. The U-Net does a very good job of segmenting dune crests, even where dunes are less clear in the Laplacian map (intersection over union score approximate to 0.68). When crest vectors and dune defects (network nodes) were derived, the defect predictions were typically correct (0.4 to 0.79 correctness) but incomplete (0.02 to 0.64 completeness). Much of the residual error was traced to the resolution of the input data. Through the application to the Simpson Desert, we nevertheless demonstrated that our method can effectively be used for regional-scale dune pattern analysis. Furthermore, we suggest that the combination of morphological filtering and a convolutional neural network could readily be adapted to target other geomorphic features, such as channel networks or geological lineaments. 
WOS:000535495300001
</snippet>
</document>

<document id="846">
<title>SEMANTIC SEGMENTATION OF AERIAL IMAGERY VIA MULTI-SCALE SHUFFLING CONVOLUTIONAL NEURAL NETWORKS WITH DEEP SUPERVISION</title>
<url>http://dx.doi.org/10.5194/isprs-annals-IV-1-29-2018</url>
<snippet>In this paper, we address the semantic segmentation of aerial imagery based on the use of multi-modal data given in the form of true orthophotos and the corresponding Digital Surface Models (DSMs). We present the Deeply-supervised Shuffling Convolutional Neural Network (DSCNN) representing a multi-scale extension of the Shuffling Convolutional Neural Network (SCNN) with deep supervision. Thereby, we take the advantage of the SCNN involving the shuffling operator to effectively upsample feature maps and then fuse multiscale features derived from the intermediate layers of the SCNN, which results in the Multi-scale Shuffling Convolutional Neural Network (MSCNN). Based on the MSCNN, we derive the DSCNN by introducing additional losses into the intermediate layers of the MSCNN. In addition, we investigate the impact of using different sets of hand-crafted radiometric and geometric features derived from the true orthophotos and the DSMs on the semantic segmentation task. For performance evaluation, we use a commonly used benchmark dataset. The achieved results reveal that both multi-scale fusion and deep supervision contribute to an improvement in performance. Furthermore, the use of a diversity of hand-crafted radiometric and geometric features as input for the DSCNN does not provide the best numerical results, but smoother and improved detections for several objects.
WOS:000467487300004
</snippet>
</document>

<document id="847">
<title>Analysis of Deep Learning Techniques for Maasai Boma Mapping in Tanzania</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3167373</url>
<snippet>Underdeveloped countries in sub-Saharan Africa often contain cultural subpopulations that are underserved in regard to health and education. This perpetuates the health challenges of the country as a whole, and it is therefore of interest to be able to automatically map the subpopulation for the health services delivery. International nonprofit health organizations have often taken the lead in these efforts, providing humanitarian aid (e.g., clean water and food) as well as health care. This is necessary, as the ethnic subpopulations are not well integrated into the society and the existing health care systems. In this study, we explicitly explore the Maasailand of Tanzania, to evaluate the use of deep neural networks (DNN) to aid in the automatic visual analysis of remote sensing data to geolocate Maasai boma structures. We investigate the performance of four state-of-the-art DNN as classifiers of boma presence within high-resolution imagery; all showing over 95&#37; F1 score performance. Additionally, we scan over 3900 km(2) of high-resolution imagery, combining a ProxylessNAS with broad area aggregation and mapping techniques and demonstrate the discovery of hundreds of boma, many that were not discovered by human analysts performing visual scans. The trained ProxylessNAS model generates a classified vector response field (CVRF). The CVRF is aggregated by a mode-seeking algorithm to detect potential locations of boma structures within the study area. The model detected numerous human false negatives (HFNs) and achieved 94.022&#37; TPR and 95.395&#37; F1 score using an aggregation aperture of 250 m within a 76.620 square kilometers area of interest.
WOS:000803307300003
</snippet>
</document>

<document id="848">
<title>SADA-Net: A Shape Feature Optimization and Multiscale Context Information-Based Water Body Extraction Method for High-Resolution Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3146275</url>
<snippet>Convolutional neural networks (CNNs) have significance in remote sensing image mapping, and pixel-level representation allows refined results. Due to inconsistencies within a class and different scales of water bodies, the water body mapping has challenges, such as insufficient integrity and rough shape segmentation. To resolve these issues, we proposed an intelligent water bodies extraction method (named SADA-Net) for high-resolution remote sensing images. This method considers multiscale information, context dependence, and shape features. The network framework integrates three critical components: shape feature optimization (SFO), atrous spatial pyramid pooling, and dual attention modules. SADA-Net can accurately extract an extensive range of water bodies in complex scenarios. SADA-Net has certain advantages regarding small and dense water bodies extraction, as the SFO module effectively solves the defects of the unified processing of low-level features in the encoder stage of CNNs, which highlights the shape information of a water body. Two data types (red, green, and blue bands and multispectral images) are employed to verify the performance of the proposed network. The best result achieved an evaluation index F1-Score of 96.14&#37; in large-scale image segmentation, and the structural similarity index measure reached 94.70&#37;. Overall, the proposed method achieves the purpose of maximizing the integrity and optimizing the shape of a water body. Additionally, the SADA-Net proposed in this article has a specific reference value for high-resolution remote sensing image water bodies mapping.
WOS:000757849300002
</snippet>
</document>

<document id="849">
<title>Using Self-Organizing Maps to find spatial relationships between wildlife-vehicle crashes and land use classes</title>
<url>http://dx.doi.org/10.1590/0001-3765202220210727</url>
<snippet>The construction and expansion of roads cause significant impacts on the environment. The main potential impacts to biotic environment are vegetation suppression, reduction of the amount and composition of animal distribution due to forest fragmentation and increasing risks of animal (domestic and wildlife) vehicle collisions. The objective of this work was to establish a relationship between the different spatial patterns in wildlife-vehicle crash, by using spatial analysis and machine learning tools. Self-Organizing Maps (SOM), an artificial neural network (ANN), was selected to reorganize the multi-dimensional data according to the similarity between them. The results of the spatial pattern analysis were important to perceive that the point data pattern varies from an animal type to another. The events occur spatially clustered and are not uniformly distributed along the highway. SOM was able to analyze the relationship between multiple variables, linear and non-linear, such as ecological data, and established distinct spatial patterns per each animal type. In the studied area, most of the wildlife was run over very close to forest area and water bodies, and not so close to sugarcane fields, forestry and built environment. A considerable part of the wildlife-vehicle collisions occurred in areas with diverse landscape.
WOS:000911020000004
</snippet>
</document>

<document id="850">
<title>Using Combination Technique for Land Cover Classification of Optical Multispectral Images</title>
<url>http://dx.doi.org/10.4018/IJAGR.2021100102</url>
<snippet>The need for efficient planning of the land is exponentially increasing because of the unplanned human activities, especially in the urban areas. A land cover map gives a detailed report on temporal dynamics of a given geographical area. The land cover map can be obtained by using machine learning classifiers on the raw satellite images. In this work, the authors propose a combination method for the land cover classification. This method combines the outputs of two classifiers, namely, random forests (RF) and support vector machines (SVM), using Dempster-Shafer combination theory (DSCT), also called the theory of evidence. This combination is possible because of the inherent uncertainties associated with the output of each classifier. The experimental results indicate an improved accuracy (89.6&#37;, kappa = 0.86 as versus accuracy of RF [87.31&#37;, kappa = 0.83] and SVM [82.144&#37;, kappa = 0.76]). The results are validated using the normalized difference vegetation index (NDVI), and the overall accuracy (OA) has been used as a comparison basis.
WOS:000724229300002
</snippet>
</document>

<document id="851">
<title>A Two-Stream Multiscale Deep Learning Architecture for Pan-Sharpening</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3021074</url>
<snippet>Pan-sharpening, which fuses the high-resolution panchromatic (PAN) image and the low-resolution multispectral image (MSI), is a hot topic in remote sensing. Recently, deep learning technology has been successfully applied in pan-sharpening. However, the existing methods ignore that the MSI and PAN image are at different resolutions and use the same networks to extract features of the two images. To address this problem, we propose a two-stream deep learning architecture, called coupled multiscale convolutional neural network, for pan-sharpening. The proposed network has three components, feature extraction subnetworks, fusion layer, and super-resolution subnetwork. In the feature extraction subnetworks, two subnetworks are used to extract the features of the MSI and PAN image separately. Different sizes of convolutional kernels are used in the first layers due to the different spatial resolutions. Thus, the source images are mapped to the similar scale. Then a multiscale asymmetric convolution factorization is used to extract features at different scales. In the fusion layer, the two feature extraction subnetworks are coupled. Features at the same scale are first summed, and then the features of all scales are concatenated as one feature map. At last, a super-resolution subnetwork is used to generate the high-resolution MSI. Experimental results on both synthetic and real data sets demonstrate that the proposed method outperforms the other state-of-the-art pan-sharpening methods.
WOS:000573802800006
</snippet>
</document>

<document id="852">
<title>Precipitation Nowcasting with Satellite Imagery</title>
<url>http://dx.doi.org/10.1145/3292500.3330762</url>
<snippet>Precipitation nowcasting is a short-range forecast of rain/snow (up to 2 hours), often displayed on top of the geographical map by the weather service. Modern precipitation nowcasting algorithms rely on the extrapolation of observations by ground-based radars via optical flow techniques or neural network models. Dependent on these radars, typical nowcasting is limited to the regions around their locations. We have developed a method for precipitation nowcasting based on geostationary satellite imagery and incorporated the resulting data into the Yandex. Weather precipitation map (including an alerting service with push notifications for products in the Yandex ecosystem), thus expanding its coverage and paving the way to a truly global nowcasting service.
WOS:000485562502075
</snippet>
</document>

<document id="853">
<title>BT-RoadNet: A boundary and topologically-aware neural network for road extraction from high-resolution remote sensing imagery</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.08.019</url>
<snippet>Automatic road extraction from high-resolution remote sensing imagery has various applications like urban planning and automatic navigation. Existing methods for automatic road extraction however, focus on regional accuracy but not on the boundary quality; and most of these road extraction methods yield discontinuous results due to noise and occlusions. To address these two problems, a Boundary and Topological-aware Road extraction Network (BT-RoadNet) is proposed. BT-RoadNet is a coarse-to-fine architecture composed of two encoder-to-decoder networks, a Coarse Map Predicting Module (CMPM) and Fine Map Predicting Module (FMPM). The CMPM learns to predict coarse road segmentation maps, in which a Spatial Context Module (SCM) is employed as a bridge to solve discontinuous problems. The FMPM is used to refine the coarse road maps by learning the difference between the coarse road extraction result and the ground truth. Experiments were conducted on the open Massachusetts Road Dataset, a newly annotated Wuhan University (WHU) Road Dataset, and three large satellite images. Quantitative and qualitative analysis demonstrate that the proposed BT-RoadNet can enhance road network extraction to deal with interruptions caused by shadows and occlusions, extract roads with different scales and materials, and handle roads under construction that have incomplete spectral and geometric properties.
WOS:000567932300022
</snippet>
</document>

<document id="854">
<title>Context pyramidal network for stereo matching regularized by disparity gradients</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2019.09.012</url>
<snippet>Also after many years of research, stereo matching remains to be a challenging task in photogrammetry and computer vision. Recent work has achieved great progress by formulating dense stereo matching as a pixel-wise learning task to be resolved with a deep convolutional neural network (CNN). However, most estimation methods, including traditional and deep learning approaches, still have difficulty to handle real-world challenging scenarios, especially those including large depth discontinuity and low texture areas. To tackle these problems, we investigate a recently proposed end-to-end disparity learning network, DispNet (Mayer et al., 2015), and improve it to yield better results in these problematic areas. The improvements consist of three major contributions. First, we use dilated convolutions to develop a context pyramidal feature extraction module. A dilated convolution expands the receptive field of view when extracting features, and aggregates more contextual information, which allows our network to be more robust in weakly textured areas. Second, we construct the matching cost volume with patch-based correlation to handle larger disparities. We also modify the basic encoder-decoder module to regress detailed disparity images with full resolution. Third, instead of using post-processing steps to impose smoothness in the presence of depth discontinuities, we incorporate disparity gradient information as a gradient regularizer into the loss function to preserve local structure details in large depth discontinuity areas. We evaluate our model in terms of end-point-error on several challenging stereo datasets including Scene Flow, Sintel and KITTI. Experimental results demonstrate that our model decreases the estimation error compared with DispNet on most datasets (e.g. we obtain an improvement of 46&#37; on Sintel) and estimates better structure-preserving disparity maps. Moreover, our proposal also achieves competitive performance compared to other methods.
WOS:000491613300014
</snippet>
</document>

<document id="855">
<title>Automatic Road Extraction from Historical Maps Using Deep Learning Techniques: A Regional Case Study of Turkey in a German World War II Map</title>
<url>http://dx.doi.org/10.3390/ijgi10080492</url>
<snippet>Scanned historical maps are available from different sources in various scales and contents. Automatic geographical feature extraction from these historical maps is an essential task to derive valuable spatial information on the characteristics and distribution of transportation infrastructures and settlements and to conduct quantitative and geometrical analysis. In this research, we used the Deutsche Heereskarte 1:200,000 Turkei (DHK 200 Turkey) maps as the base geoinformation source to construct the past transportation networks using the deep learning approach. Five different road types were digitized and labeled to be used as inputs for the proposed deep learning-based segmentation approach. We adapted U-Net++ and ResneXt50_32x4d architectures to produce multi-class segmentation masks and perform feature extraction to determine various road types accurately. We achieved remarkable results, with 98.73&#37; overall accuracy, 41.99&#37; intersection of union, and 46.61&#37; F1 score values. The proposed method can be implemented in DHK maps of different countries to automatically extract different road types and used for transfer learning of different historical maps.
WOS:000689308400001
</snippet>
</document>

<document id="856">
<title>Neural population geometry: An approach for understanding biological and artificial neural networks</title>
<url>http://dx.doi.org/10.1016/j.conb.2021.10.010</url>
<snippet>Advances in experimental neuroscience have transformed our ability to explore the structure and function of neural circuits. At the same time, advances in machine learning have unleashed the remarkable computational power of artificial neural networks (ANNs). While these two fields have different tools and applications, they present a similar challenge: namely, understanding how information is embedded and processed through high-dimensional representations to solve complex tasks. One approach to addressing this challenge is to utilize mathematical and computational tools to analyze the geometry of these high-dimensional representations, i.e., neural population geometry. We review examples of geometrical approaches providing insight into the function of biological and artificial neural networks: representation untangling in perception, a geometric theory of classification capacity, disentanglement, and abstraction in cognitive systems, topological representations underlying cognitive maps, dynamic untangling in motor systems, and a dynamical approach to cognition. Together, these findings illustrate an exciting trend at the intersection of machine learning, neuroscience, and geometry, in which neural population geometry provides a useful population-level mechanistic descriptor underlying task implementation. Importantly, geometric descriptions are applicable across sensory modalities, brain regions, network architectures, and timescales. Thus, neural population geometry has the potential to unify our understanding of structure and function in biological and artificial neural networks, bridging the gap between single neurons, population activities, and behavior.
WOS:000740926400016
</snippet>
</document>

<document id="857">
<title>GETNext: Trajectory Flow Map Enhanced Transformer for Next POI Recommendation</title>
<url>http://dx.doi.org/10.1145/3477495.3531983</url>
<snippet>Next POI recommendation intends to forecast users' immediate future movements given their current status and historical information, yielding great values for both users and service providers. However, this problem is perceptibly complex because various data trends need to be considered together. This includes the spatial locations, temporal contexts, user's preferences, etc. Most existing studies view the next POI recommendation as a sequence prediction problem while omitting the collaborative signals from other users. Instead, we propose a user-agnostic global trajectory flow map and a novel Graph Enhanced Transformer model (GETNext) to better exploit the extensive collaborative signals for a more accurate next POI prediction, and alleviate the cold start problem in the meantime. GETNext incorporates the global transition patterns, user's general preference, spatio-temporal context, and time-aware category embeddings together into a transformer model to make the prediction of user's future moves. With this design, our model outperforms the state-of-the-art methods with a large margin and also sheds light on the cold start challenges within the spatio-temporal involved recommendation problems.
WOS:000852715901021
</snippet>
</document>

<document id="858">
<title>Improved Swin Transformer-Based Semantic Segmentation of Postearthquake Dense Buildings in Urban Areas Using Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3225150</url>
<snippet>Timely acquiring the earthquake-induced damage of buildings is crucial for emergency assessment and post-disaster rescue. Optical remote sensing is a typical method for obtaining seismic data due to its wide coverage and fast response speed. Convolutional neural networks (CNNs) are widely applied for remote sensing image recognition. However, insufficient extraction and expression ability of global correlations between local image patches limit the performance of dense building segmentation. This paper proposes an improved Swin Transformer to segment dense urban buildings from remote sensing images with complex backgrounds. The original Swin Transformer is used as a backbone of the encoder, and a convolutional block attention module is employed in the linear embedding and patch merging stages to focus on significant features. Hierarchical feature maps are then fused to strengthen the feature extraction process and fed into the UPerNet (as the decoder) to obtain the final segmentation map. Collapsed and non-collapsed buildings are labeled from remote sensing images of the Yushu and Beichuan earthquakes. Data augmentations of horizontal and vertical flipping, brightness adjustment, uniform fogging, and non-uniform fogging are performed to simulate actual situations. The effectiveness and superiority of the proposed method over the original Swin Transformer and several mature CNN-based segmentation models are validated by ablation experiments and comparative studies. The results show that the mean intersection-over-union of the improved Swin Transformer reaches 88.53&#37;, achieving an improvement of 1.3&#37; compared to the original model. The stability, robustness, and generalization ability of dense building recognition under complex weather disturbances are also validated.
WOS:000900007800005
</snippet>
</document>

<document id="859">
<title>CGSANet: A Contour-Guided and Local Structure-Aware Encoder-Decoder Network for Accurate Building Extraction From Very High-Resolution Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3139017</url>
<snippet>Extracting buildings accurately from very high-resolution (VHR) remote sensing imagery is challenging due to diverse building appearances, spectral variability, and complex background in VHR remote sensing images. Recent studies mainly adopt a variant of the fully convolutional network (FCN) with an encoder-decoder architecture to extract buildings, which has shown promising improvement over conventional methods. However, FCN-based encoder-decoder models still fail to fully utilize the implicit characteristics of building shapes. This adversely affects the accurate localization of building boundaries, which is particularly relevant in building mapping. A contour-guided and local structure-aware encoder-decoder network (CGSANet) is proposed to extract buildings with more accurate boundaries. CGSANet is a multitask network composed of a contour-guided (CG) and a multiregion-guided (MRG) module. The CG module is supervised by a building contour that effectively learns building contour-related spatial features to retain the shape pattern of buildings. The MRG module is deeply supervised by four building regions that further capture multiscale and contextual features of buildings. In addition, a hybrid loss function was designed to improve the structure learning ability of CGSANet. These three improvements benefit each other synergistically to produce high-quality building extraction results. Experimental results on the WHU and NZ32km2 building datasets demonstrate that compared with the tested algorithms, CGSANet can produce more accurate building extraction results and achieve the best intersection over union value 91.55&#37; and 90.02&#37;, respectively. Experiments on the INRIA building dataset further demonstrate the ability for generalization of the proposed framework, indicating great practical potential.
WOS:000754245300001
</snippet>
</document>

<document id="860">
<title>BOOTSTRAPPED CNNS FOR BUILDING SEGMENTATION ON RGB-D AERIAL IMAGERY</title>
<url>http://dx.doi.org/10.5194/isprs-annals-IV-4-187-2018</url>
<snippet>Detection of buildings and other objects from aerial images has various applications in urban planning and map making. Automated building detection from aerial imagery is a challenging task, as it is prone to varying lighting conditions, shadows and occlusions. Convolutional Neural Networks (CNNs) are robust against some of these variations, although they fail to distinguish easy and difficult examples. We train a detection algorithm from RGB-D images to obtain a segmented mask by using the CNN architecture DenseNet. First, we improve the performance of the model by applying a statistical re-sampling technique called Bootstrapping and demonstrate that more informative examples are retained. Second, the proposed method outperforms the non-bootstrapped version by utilizing only one-sixth of the original training data and it obtains a precision-recall break-even of 95.10&#37; on our aerial imagery dataset.
WOS:000467484500025
</snippet>
</document>

<document id="861">
<title>Land Consumption Mapping with Convolutional Neural Network: Case Study in Italy</title>
<url>http://dx.doi.org/10.3390/land11111919</url>
<snippet>In recent years, deep learning (DL) algorithms have been widely integrated for remote sensing image classification, but fewer studies have applied it for land consumption (LC). LC is the main factor in land transformation dynamics and it is the first cause of natural habitat loss; therefore, monitoring this phenomenon is extremely important for establishing effective policies and sustainable planning. This paper aims to test a DL algorithm on high-resolution aerial images to verify its applicability to land consumption monitoring. For this purpose, we applied a convolutional neural networks (CNNs) architecture called ResNet50 on a reference dataset of six high-spatial-resolution aerial images for the automatic production of thematic maps with the aim of improving accuracy and reducing costs and time compared with traditional techniques. The comparison with the National Land Consumption Map (LCM) of ISPRA suggests that although deep learning techniques are not widely exploited to map consumed land and to monitor land consumption, it might be a valuable support for monitoring and reporting data on highly dynamic peri-urban areas, especially in view of the rapid evolution of these techniques.
WOS:000881115600001
</snippet>
</document>

<document id="862">
<title>Choosing an appropriate training set size when using existing data to train neural networks for land cover segmentation</title>
<url>http://dx.doi.org/10.1080/19475683.2020.1803402</url>
<snippet>Land cover data is an inventory of objects on the Earths surface, which is often derived from remotely sensed imagery. Deep Convolutional Neural Network (DCNN) is a competitive method in image semantic segmentation. Some scholars argue that the inadequacy of training set is an obstacle when applying DCNNs in remote sensing image segmentation. While existing land cover data can be converted to large training sets, the size of training data set needs to be carefully considered. In this paper, we used different portions of a high-resolution land cover map to produce different sizes of training sets to train DCNNs (SegNet and U-Net) and then quantitatively evaluated the impact of training set size on the performance of the trained DCNN. We also introduced a new metric, Edge-ratio, to assess the performance of DCNN in maintaining the boundary of land cover objects. Based on the experiments, we document the relationship between the segmentation accuracy and the size of the training set, as well as the nonstationary accuracies among different land cover types. The findings of this paper can be used to effectively tailor the existing land cover data to training sets, and thus accelerate the assessment and employment of deep learning techniques for high-resolution land cover map extraction.
WOS:000602734200002
</snippet>
</document>

<document id="863">
<title>Fine-grained landuse characterization using ground-based pictures: a deep learning solution based on globally available data</title>
<url>http://dx.doi.org/10.1080/13658816.2018.1542698</url>
<snippet>We study the problem of landuse characterization at the urban-object level using deep learning algorithms. Traditionally, this task is performed by surveys or manual photo interpretation, which are expensive and difficult to update regularly. We seek to characterize usages at the single object level and to differentiate classes such as educational institutes, hospitals and religious places by visual cues contained in side-view pictures from Google Street View (GSV). These pictures provide geo-referenced information not only about the material composition of the objects but also about their actual usage, which otherwise is difficult to capture using other classical sources of data such as aerial imagery. Since the GSV database is regularly updated, this allows to consequently update the landuse maps, at lower costs than those of authoritative surveys. Because every urban-object is imaged from a number of viewpoints with street-level pictures, we propose a deep-learning based architecture that accepts arbitrary number of GSV pictures to predict the fine-grained landuse classes at the object level. These classes are taken from OpenStreetMap. A quantitative evaluation of the area of ile-de-France, France shows that our model outperforms other deep learning-based methods, making it a suitable alternative to manual landuse characterization.
WOS:000530991600003
</snippet>
</document>

<document id="864">
<title>Automatic Detection of Aquatic Weeds: A Case Study in the Guadiana River, Spain</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3210373</url>
<snippet>The spread of aquatic invasive plants is a major concern in several zones of the worlds geography. These plants, which are not part of the natural ecosystem, cause a negative impact to the environment, as well as to economy and society. In Spain, large areas of Guadiana (the second-longest river in Spain) have been invaded by such plants. Among the strategies to address this problem, monitoring and detection play an important role to control the spatiotemporal distribution of the invasive plants. The main objective of this work is to develop a methodology able to automatically detect the geolocation of aquatic invasive plants using remote sensing and machine learning techniques. To this end, several classification algorithms have been applied to freely available multispectral satellite imagery, collected by ESAs Sentinel-2 satellite. A quantitative and comparative assessment is conducted using different machine and deep learning algorithms from classical methods, such as unsupervised K-means to supervised random forests and convolutional neural networks. This study also proposes a methodology for validating the obtained classification results, generating synthetic ground truth images based on available high spatial resolution imagery. The obtained results demonstrate the suitability of some of the considered algorithms for automatic detection of aquatic weeds in satellite images with medium spatial resolution.
WOS:000866530700014
</snippet>
</document>

<document id="865">
<title>Recognition and Mapping of Landslide Using a Fully Convolutional DenseNet and Influencing Factors</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3101203</url>
<snippet>The recognition and mapping of landslide (RML) is an important task in hazard and risk research and can provide a scientific basis for the prevention and control of landslide disasters. However, traditional RML methods are inefficient, costly, and not intuitive. With the rapid development of computer vision, methods based on convolutional neural networks have attracted great attention due to their numerous advantages. However, problems such as insufficient feature extraction, excessive parameters, and slow model testing have restricted the development of this technology. This research proposes a new RML framework based on a new semantic segmentation network termed the fully convolutional DenseNet (FC-DenseNet). In this network, the features extracted from each layer are repeatedly used in a dense connection, and the parameters are controlled by a bottle-neck structure. Meanwhile, the structure of the encoder-decoder solves the problem of the slowness of model testing. Finally, the landslide influencing factors are added, which enriches the training data. To verify the effectiveness of the proposed method, we focused on several deep networks for comparison and analysis. The results show that FC-DenseNet can better recognize the boundary and interior of landslides, and there are fewer missing and excessive recognition results. The kappa value of the new method is 94.72&#37; in Site 1, which is 6&#37; and 4&#37; higher than that of U-Net and ResU-Net, respectively, and 94.56&#37; in Site 2, which is 6&#37; and 3&#37; higher than that of U-Net and ResU-Net, respectively, indicating that FC-DenseNet has great potential in RML applications.
WOS:000686757500008
</snippet>
</document>

<document id="866">
<title>Building Extraction From Remote Sensing Images With DoG as Prior Constraint</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3195808</url>
<snippet>Building extraction from aerial and satellite remote sensing images is a basic component of social development. Compared to traditional feature extraction strategies, deep convolutional neural networks (CNNs) have the advantage of extracting deep high-level semantic features and efficient image processing capabilities. However, most works focus on developing end-to-end data-driven models without considering prior information, such as edges and the structures of buildings, which would cause loss of details and blurred boundaries in the prediction results. To alleviate this problem, we constructed a prior information module (PIM) as a constraint for feature map refinement in the training phase that uses the edge information extracted by the multiscale difference of Gaussian operator. We combined this module with the main network to form a dual-output training network. The module helps optimize the feature extraction process and solves the inaccurate and adhesion phenomenon of edge points in the building extraction results of the existing CNNs. Experiments illustrate that this PIM can improve the intersection over union of a lightweight network by 2.4&#37; and 2.0&#37; on an aerial remote sensing dataset and a satellite remote sensing dataset, respectively. The PIM can be embedded in other networks during the training phase to improve building extraction performance without any extra computational requirements during the inference phase. At the same time, we build a U-structure network with the proposed module, which outperforms another state-of-the-art building extraction approach with the extracted edge as prior information.
WOS:000842061200006
</snippet>
</document>

<document id="867">
<title>Registration of Multimodal Remote Sensing Image Based on Deep Fully Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2019.2916560</url>
<snippet>Multimodal image registration is the fundamental technique for scene analysis with series remote sensing images of different spectrum region. Due to the highly nonlinear radiometric relationship, it is quite challenging to find common features between images of different modal types. This paper resorts to the deep neural network, and tries to learn descriptors for multimodal image patch matching, which is the key issue of image registration. A Siamese fully convolutional network is set up and trained with a novel loss function, which adopts the strategy of maximizing the feature distance between positive and hard negative samples. The two branches of the Siamese network are connected by the convolutional operation, resulting in the similarity score between the two input image patches. The similarity score value is used, not only for correspondence point location, but also for outlier identification. A generalized workflow for deep feature based multimodal RS image registration is constructed, including the training data curation, candidate feature point generation, and outlier removal. The proposed network is tested on a variety of optical, near infrared, thermal infrared, SAR, and map images. Experiment results verify the superiority over other state-of-the-art approaches.
WOS:000487530100037
</snippet>
</document>

<document id="868">
<title>Integrating Gate and Attention Modules for High-Resolution Image Semantic Segmentation</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3071353</url>
<snippet>Semantic segmentation of high-resolution (HR) remote sensing images achieved great progress by utilizing deep convolutional neural networks (DCNNs) in recent years. However, the decrease of resolution in the feature map of DCNNs brings about the loss of spatial information and thus leads to the blurring of object boundary and misclassification of small objects. In addition, the class imbalance and the high diversity of geographic objects in HR images exacerbate the performance. To deal with the above problems, we proposed an end-to-end DCNN network named GAMNet to balance the contradiction between global semantic information and local details. An integration of attention and gate module (GAM) is specially designed to simultaneously realize multiscale feature extraction and boundary recovery. The integration module can be inserted in an encoder-decoder network with skip connection. Meanwhile, a composite loss function is designed to achieve deep supervision of GAM by adding an auxiliary loss, which can help improve the effectiveness of the integration module. The performance of GAMNet is quantitatively evaluated on the ISPRS 2-D semantic labeling datasets and achieves state-of-the-art performance in comparison with other representative methods.
WOS:000650468700008
</snippet>
</document>

<document id="869">
<title>Building Semantic Cognitive Maps with Text Embedding and Clustering</title>
<url>http://dx.doi.org/10.1109/IJCNN55064.2022.9892429</url>
<snippet>Text embedding using vector space models has recently emerged as the leading way to represent text in natural language processing. These embeddings can be at the level of words, sentences, or larger textual units including entire documents. However, sentence embeddings are especially useful because sentences are the most explicitly specified elements of individual thoughts or ideas comprising a document, discussion, or conversation. Analyzing text at the sentence level thus allows access to its fine-grained semantics while preserving the semantic structure that is lost in bag-of-words approaches. Several deep learning-based models such as BERT and USE provide such contextual representations. However, the resulting embeddings are very high-dimensional, and the individual dimensions are not amenable to interpretable labels. Thus, these embeddings define a semantic space but not an explicitly useful cognitive map. In this paper, we show that an adaptive clustering approach applied to the embeddings produced by a neural network-based language model can produce much lower-dimensional, readily interpretable semantic representations, thus creating a usable cognitive map for applications such as semantic tracking and visualization of discussions, or discerning the sequential semantic structure of long documents.
WOS:000867070904063
</snippet>
</document>

<document id="870">
<title>Attention_FPNet: Two-Branch Remote Sensing Image Pansharpening Network Based on Attention Feature Fusion</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3126645</url>
<snippet>Inspired by the impressive achievements of convolutional neural networks in various computer vision tasks and the effective role of attention mechanisms, this article proposes a two-branch fusion network based on attention feature fusion (AFF) called Attention_FPNet to solve the pansharpening problem. We reconstruct the spatial information of the image in the high-pass filter domain and fully consider the spatial information in the multispectral (MS) and panchromatic (PAN) images. At the same time, the input PAN image and the upsampled MS image are directly transmitted to the reconstructed image through a long skip connection. The spectral information of the PAN and MS images is considered to improve the spectral resolution of the fused image. It also supplements the loss of spatial information that may be caused by network deepening. Moreover, an AFF method is used to replace the existing simple channel concatenation method commonly used in pansharpening, which fully considers the relationship between different feature maps and improves the fusion quality. Through experiments on image datasets acquired by the Pleiades, SPOT-6 and Gaofen-2 satellites, the results show that this method can effectively fuse PAN and MS images and generate a fused image and outperforms existing methods.
WOS:000725801600006
</snippet>
</document>

<document id="871">
<title>Novel Machine Learning Method Integrating Ensemble Learning and Deep Learning for Mapping Debris-Covered Glaciers</title>
<url>http://dx.doi.org/10.3390/rs13132595</url>
<snippet>Glaciers in High Mountain Asia (HMA) have a significant impact on human activity. Thus, a detailed and up-to-date inventory of glaciers is crucial, along with monitoring them regularly. The identification of debris-covered glaciers is a fundamental and yet challenging component of research into glacier change and water resources, but it is limited by spectral similarities with surrounding bedrock, snow-affected areas, and mountain-shadowed areas, along with issues related to manual discrimination. Therefore, to use fewer human, material, and financial resources, it is necessary to develop better methods to determine the boundaries of debris-covered glaciers. This study focused on debris-covered glacier mapping using a combination of related technologies such as random forest (RF) and convolutional neural network (CNN) models. The models were tested on Landsat 8 Operational Land Imager (OLI)/Thermal Infrared Sensor (TIRS) data and the Advanced Spaceborne Thermal Emission and Reflection Radiometer Global Digital Elevation Model (ASTER GDEM), selecting Eastern Pamir and Nyainqentanglha as typical glacier areas on the Tibetan Plateau to construct a glacier classification system. The performances of different classifiers were compared, the different classifier construction strategies were optimized, and multiple single-classifier outputs were obtained with slight differences. Using the relationship between the surface area covered by debris and the machine learning model parameters, it was found that the debris coverage directly determined the performance of the machine learning model and mitigated the issues affecting the detection of active and inactive debris-covered glaciers. Various classification models were integrated to ascertain the best model for the classification of glaciers.
WOS:000671296600001
</snippet>
</document>

<document id="872">
<title>CBF-Net: An Adaptive Context Balancing and Feature Filtering Network for Point Cloud Classification</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3106376</url>
<snippet>Point cloud classification is regarded as a critical task in remote sensing data interpretation, which is widely used in many fields. Recently, many proposed methods tend to develop an end-to-end network to directly operate on the raw point cloud, which has shown great power. However, most of these methods abstract local features by equally considering the neighboring points. The features learned may neglect to distinguish contributions of different points especially the edge points and outliers, leading to a coarse classification result especially for boundaries. Moreover, the extracted features are high redundant and intercorrelated with similar categories, posing difficulty in identifying classes sharing similar characteristics especially in complex scenes. Therefore, we propose an adaptive context balancing and feature filtering network (CBF-Net) to tackle the aforementioned problems. First, we introduce a balanced context encoding module to balance semantically the features of neighboring points, which can help the model learn more from the edge points and, therefore, contribute to a finer classification. Then, considering that the interference for similar classes probably causes confusion among them, a filtered feature aggregating module is proposed to filter the extracted features by mapping them into a cleaner subspace with a lower rank. We have conducted thorough experiments on the International Society for Photogrammetry and Remote Sensing 3-D labeling dataset. Experimental results show that our CBF-Net can obtain high accuracy and achieve state-of-the-art level in the categories of Powerline, Car, and Facade. In addition, we also conduct experiments on the RueMonge2014 dataset, which further reveals the strong ability of our model.
WOS:000694698900016
</snippet>
</document>

<document id="873">
<title>Urban Building Extraction and Modeling Using GF-7 DLC and MUX Images</title>
<url>http://dx.doi.org/10.3390/rs13173414</url>
<snippet>Urban modeling and visualization are highly useful in the development of smart cities. Buildings are the most prominent features in the urban environment, and are necessary for urban decision support; thus, buildings should be modeled effectively and efficiently in three dimensions (3D). In this study, with the help of Gaofen-7 (GF-7) high-resolution stereo mapping satellite double-line camera (DLC) images and multispectral (MUX) images, the boundary of a building is segmented via a multilevel features fusion network (MFFN). A digital surface model (DSM) is generated to obtain the elevation of buildings. The building vector with height information is processed using a 3D modeling tool to create a white building model. The building model, DSM, and multispectral fused image are then imported into the Unreal Engine 4 (UE4) to complete the urban scene level, vividly rendered with environmental effects for urban visualization. The results of this study show that high accuracy of 95.29&#37; is achieved in building extraction using our proposed method. Based on the extracted building vector and elevation information from the DSM, building 3D models can be efficiently created in Level of Details 1 (LOD1). Finally, the urban scene is produced for realistic 3D visualization. This study shows that high-resolution stereo mapping satellite images are useful in 3D modeling for urban buildings and can support the generation and visualization of urban scenes in a large area for different applications.
WOS:000694480600001
</snippet>
</document>

<document id="874">
<title>Sentinel-2 Image Scene Classification: A Comparison between Sen2Cor and a Machine Learning Approach</title>
<url>http://dx.doi.org/10.3390/rs13020300</url>
<snippet>Given the continuous increase in the global population, the food manufacturers are advocated to either intensify the use of cropland or expand the farmland, making land cover and land usage dynamics mapping vital in the area of remote sensing. In this regard, identifying and classifying a high-resolution satellite imagery scene is a prime challenge. Several approaches have been proposed either by using static rule-based thresholds (with limitation of diversity) or neural network (with data-dependent limitations). This paper adopts the inductive approach to learning from surface reflectances. A manually labeled Sentinel-2 dataset was used to build a Machine Learning (ML) model for scene classification, distinguishing six classes (Water, Shadow, Cirrus, Cloud, Snow, and Other). This models was accessed and further compared to the European Space Agency (ESA) Sen2Cor package. The proposed ML model presents a Micro-F1 value of 0.84, a considerable improvement when compared to the Sen2Cor corresponding performance of 0.59. Focusing on the problem of optical satellite image scene classification, the main research contributions of this paper are: (a) an extended manually labeled Sentinel-2 database adding surface reflectance values to an existing dataset; (b) an ensemble-based and a Neural-Network-based ML models; 
WOS:000611558200001
</snippet>
</document>

<document id="875">
<title>MULTILEVEL SEMANTIC LABELING OF MOBILE HOMES FROM OVERHEAD IMAGERY</title>
<url>http://dx.doi.org/</url>
<snippet>Finding where people live and the vulnerabilities of man-made facilities during natural disasters is not only critical for rescue efforts but also essential for damage assessment in the aftermath. New advances from machine learning and high performance computing are leveraging on the availability of high resolution satellite imagery to generate geographical maps for man-made facilities at scale. Mapping from satellite imagery can be a daunting task due to the enormous amount of data to be processed over large areas. In this short paper we take advantage of annotated satellite imagery and automate the semantic labeling of mobile home parks using an efficient framework rooted in patch-based and pixel-level classification. This multilevel labeling effort is a precursor for deploying very large scale deep convolutional neural networks toward broad and finer characterization of man-made structures from one-meter resolution NAIP images.
WOS:000451039806170
</snippet>
</document>

<document id="876">
<title>Automatic Waterline Extraction and Topographic Mapping of Tidal Flats From SAR Images Based on Deep Learning</title>
<url>http://dx.doi.org/10.1029/2021GL096007</url>
<snippet>This study presented an intuitive approach to derive large-scale tidal flats Digital Elevation Model (DEM). We first developed an automated method for accurately extracting the waterline from Synthetic Aperture Radar images acquired in Subei Sandbanks along the Yellow Sea coast of China between 2015 and 2020 based on deep convolutional neural networks. The statistical results show this method has appreciable accuracy for efficient waterline extraction even under complex imaging conditions with a mean recall and precision of 0.90 and 0.80, respectively. Then the pixel-level extracted waterlines are calibrated with a global tide model to construct the large-scale tidal flats DEM in the study region. The comparison against in situ topographic data shows an error of 29 cm, demonstrating the usefulness of monitoring the morpho-sedimentary evolution in intertidal areas. Furthermore, the Subei Sandbanks remained stable from 2015 to 2020, while the coastal region changed drastically due to human activities.
WOS:000751642800047
</snippet>
</document>

<document id="877">
<title>Deep Learning for Automatic Outlining Agricultural Parcels: Exploiting the Land Parcel Identification System</title>
<url>http://dx.doi.org/10.1109/ACCESS.2019.2950371</url>
<snippet>Accurate and up-to-date information on the spatial and geographical characteristics of agricultural areas is an indispensable value for the various activities related to agriculture and research. Most agricultural studies and policies are carried out at the field level, for which precise boundaries are required. Today, high-resolution remote sensing images provide useful spatial information for plot delineation; however, manual processing is time-consuming and prone to human error. The objective of this paper is to explore the potential of deep learning (DL) approach, in particular a convolutional neural network (CNN) model, for the automatic outlining of agricultural plot boundaries from orthophotos over large areas with a heterogeneous landscape. Since DL approaches require a large amount of labeled data to learn, we have exploited the open data from the Land Parcel Identification System (LPIS) from the Chartered Community of Navarre, Spain. The boundaries of the agricultural plots obtained from our methodology were compared with those obtained using a state-of-the-art methodology known as gPb-UCM (global probability of boundary followed by ultrametric contour map) through an error measurement called the boundary displacement error index (BDE). In BDE terms, the results obtained by our method outperform those obtained from the gPb-UCM method. In this regard, CNN models trained with LPIS data are a useful and powerful tool that would reduce intensive manual labor in outlining agricultural plots.
WOS:000502256000001
</snippet>
</document>

<document id="878">
<title>Urban Functional Zone Mapping With a Bibranch Neural Network via Fusing Remote Sensing and Social Sensing Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3127246</url>
<snippet>Urban functional zones (UFZs) are the urban spaces divided by various functional activities and are the basic units of daily human activities. UFZ mapping, which identifies the UFZ categories in different spatial areas of a city, is of considerable significance to urban management, design, and sustainable development. Various deep learning-based (DL-based) methods, which achieved remarkable results in an end-to-end supervised process, were proposed for UFZ mapping. However, the excellent performance of DL-based models relies heavily on a large number of well-annotated samples, which is impossible to obtain in practical UFZ mapping scenarios. Obtaining these well-annotated samples requires a lot of manual costs, which greatly limits the outcome of these methods in practical UFZ mapping tasks. In this article, we proposed a UFZ mapping method using OpenStreetMap-based (OSM-based) sample generation and the bi-branch neural network (BibNet). By adopting the idea of OSM-based sample generation, the proposed method utilized large-scale crowdsourcing labeled data (source domain) in OSM to generate a UFZ dataset (target domain) from OSM using remote sensing and social sensing data. Considering the inconsistent response of UFZ to various data observations, it is difficult to fully reflect the characteristics of UFZs using only remote sensing or social sensing data. We further proposed the BibNet, which utilizes two different deep neural network branches to comprehensively harness remote sensing images and social sensing data to map the UFZ. Experiments were conducted in Shenzhen City and Hong Kong City (Yau Tsim Mong District, Sham Shui Po District and Kowloon City District). The proposed method achieved an overall accuracy (OA) of 94.46&#37; in the testing set of Shenzhen City and OA of 91.90&#37; in the testing set of Hong Kong City.
WOS:000724480200007
</snippet>
</document>

<document id="879">
<title>3-D Hybrid CNN Combined With 3-D Generative Adversarial Network for Wetland Classification With Limited Training Data</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3206143</url>
<snippet>Recently, deep learning algorithms, specifically convolutional neural networks (CNNs), have played an important role in remote sensing image classification, including wetland mapping. However, one limitation of deep CNN for classification is its requirement for a great number of training samples. This limitation is particularly enhanced when the classes of interest are spectrally similar, such as that of wetland types, and the training samples are limited. This article presents a novel approach named 3-D hybrid generative adversarial network (3-D hybrid GAN) that addresses the limited training sample issue in the classification of remote sensing imagery with a focus on complex wetland classification. We used a conditional map unit that generates synthetic training samples for only classes with a lower number of training samples to improve the per-class accuracy of wetlands. This procedure overcomes the issue of imbalanced data in conventional wetland mapping. Based on the achieved results, better classification accuracy is obtained by integrating a 3-D generative adversarial network (3-D GAN) and the CNN network of a 3-D hybrid CNN using both 3-D and 2-D convolutional filters. Experimental results on the avalon pilot site located in eastern Newfoundland, Canada, and covering five wetland types of bog, fen, marsh, swamp, and shallow water demonstrate that our model significantly outperforms other CNN models, including the HybridSN, SpectralNet, MLP-mixer, as well as a conventional algorithm of random forest for complex wetland classification by approximately 1&#37; to 51&#37; in terms of F-1 score.
WOS:000861443200004
</snippet>
</document>

<document id="880">
<title>Bathymetric Inversion and Mapping of Two Shallow Lakes Using Sentinel-2 Imagery and Bathymetry Data in the Central Tibetan Plateau</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3177227</url>
<snippet>High-accuracy lake bathymetry and mapping are crucial for estimating lake water storage on the Tibetan Plateau (TP). In this article, we constructed traditional empirical (TE) models and machine learning (ML) models to compare the prediction accuracy and remote sensing bathymetric mapping performance by using Sentinel-2 satellite imagery and in situ measured water depth from Caiduochaka (CK) and QiXiang Co in the central TP. We analyzed the relationship between the band reflectance and depth and explored the universality of the model in different lakes. The results indicated that when using the TE model, the mean absolute percentage error (MAPE) varied between 14.5&#37; and 26.5&#37; for the test dataset at different study sites. When using the ML models, the MAPE varied between 7.6&#37; and 18.9&#37;, and it was the better choice overall. For the test dataset of the random forest model with the highest accuracy, in the CK with the maximum depth of approximately 16 m, the mean absolute error (MAE) and root-mean-square error (RMSE) were 0.54 and 0.89 m, and the precision was the highest with an MAE of 1.13 m and RMSE of 1.67 m in QiXiang Co with a maximum depth of approximately 28 m, whereas the portability of the model was not satisfactory. Overall, the results indicated that the ML model can obtain bathymetric maps with high accuracy, good visual performance, and reliability, outperforming the TE model. It can be used effectively for deriving accurate and updated high-resolution bathymetric maps for shallow lakes.
WOS:000808062300002
</snippet>
</document>

<document id="881">
<title>Graph Convolutional Networks-Based Super-Resolution Land Cover Mapping</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3100400</url>
<snippet>Super-resolution mapping (SRM) is an effective technology to solve the problem of mixed pixels because it can be used to generate fine-resolution land cover maps from coarse-resolution remote sensing images. Current methods based on deep neural networks have been successfully applied to SRM, as they can learn complex spatial patterns from training data. However, they lack the ability to learn structural information between adjacent land cover classes, which is vital in the reconstruction of spatial distribution. In this article, an SRM method based on graph convolutional networks (GCNs), named SRMGCN, is proposed to improve SRM results by capturing structure information on the graph. In SRMGCN, a supervised inductive learning strategy with mini-graphs as input is considered, which is an extension of the GCN framework. Furthermore, two operations are designed in terms of adjacency matrix construction and an information propagation rule to help reconstruct detailed information of geographical objects. Experiments on three datasets with different spatial resolutions demonstrate the qualitative and quantitative superiority of SRMGCN over three other popular SRM methods.
WOS:000684698600009
</snippet>
</document>

<document id="882">
<title>Multi-nomenclature, multi-resolution joint translation: an application to land-cover mapping</title>
<url>http://dx.doi.org/10.1080/13658816.2022.2120996</url>
<snippet>Land-use/land-cover (LULC) maps describe the Earths surface with discrete classes at a specific spatial resolution. The chosen classes and resolution highly depend on peculiar uses, making it mandatory to develop methods to adapt these characteristics for a large range of applications. Recently, a convolutional neural network (CNN)-based method was introduced to take into account both spatial and geographical context to translate a LULC map into another one. However, this model only works for two maps: one source and one target. Inspired by natural language translation using multiple-language models, this article explores how to translate one LULC map into several targets with distinct nomenclatures and spatial resolutions. We first propose a new data set based on six open access LULC maps to train our CNN-based encoder-decoder framework. We then apply such a framework to convert each of these six maps into each of the others using our Multi-Landcover Translation network (MLCT-Net). Extensive experiments are conducted at a country scale (namely France). The results reveal that our MLCT-Net outperforms its semantic counterparts and gives on par results with mono-LULC models when evaluated on areas similar to those used for training. Furthermore, it outperforms the mono-LULC models when applied to totally new landscapes.
WOS:000865701600001
</snippet>
</document>

<document id="883">
<title>Crowd Density Estimation and Mapping Method Based on Surveillance Video and GIS</title>
<url>http://dx.doi.org/10.3390/ijgi12020056</url>
<snippet>Aiming at the problem that the existing crowd counting methods cannot achieve accurate crowd counting and map visualization in a large scene, a crowd density estimation and mapping method based on surveillance video and GIS (CDEM-M) is proposed. Firstly, a crowd semantic segmentation model (CSSM) and a crowd denoising model (CDM) suitable for high-altitude scenarios are constructed by transfer learning. Then, based on the homography matrix between the video and remote sensing image, the crowd areas in the video are projected to the map space. Finally, according to the distance from the crowd target to the camera, the camera inclination, and the area of the crowd polygon in the geographic space, a BP neural network for the crowd density estimation is constructed. The results show the following: (1) The test accuracy of the CSSM was 96.70&#37;, and the classification accuracy of the CDM was 86.29&#37;, which can achieve a high-precision crowd extraction in large scenes. (2) The BP neural network for the crowd density estimation was constructed, with an average error of 1.2 and a mean square error of 4.5. Compared to the density map method, the MAE and RMSE of the CDEM-M are reduced by 89.9 and 85.1, respectively, which is more suitable for a high-altitude camera. (3) The crowd polygons were filled with the corresponding number of points, and the symbol was a human icon. The crowd mapping and visual expression were realized. The CDEM-M can be used for crowd supervision in stations, shopping malls, and sports venues.
WOS:000944974000001
</snippet>
</document>

<document id="884">
<title>Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2020.01.025</url>
<snippet>Object detection plays an important role in the field of remote sensing imagery analysis. The most challenging issues in advancing this task are the large variation in object scales and the arbitrary orientation of objects. In this paper, we build a unified framework upon the region-based convolutional neural network for arbitrary-oriented and multi-scale object detection in remote sensing images. To handle the problem of multi-scale object detection, a feature-fusion architecture is proposed to generate a multi-scale feature hierarchy, which augments the features of shallow layers with semantic representations via a top-down pathway and combines the feature maps of top layers with low-level information by a bottom-up pathway. By combining features of different levels, we can form a powerful feature representation for multi-scale objects. Most previous methods locate objects with arbitrary orientations and dense spatial distributions via axis-aligned boxes, which may cover adjacent instances and background areas. We build a rotation-aware object detector that uses oriented boxes to localize objects in remote sensing images. The region proposal network augments the anchors with multiple default angles to cover oriented objects. It utilizes oriented proposal boxes to enclose objects rather than horizontal proposals that coarsely locate oriented objects. The orientation RoI pooling operation is introduced to extract the feature maps of oriented proposals for the following R-CNN subnetwork. We conduct comprehensive experiments on a public dataset for oriented object detection in remote sensing images. Our method achieves state-of-the-art performance, which demonstrates the effectiveness of the proposed methods.
WOS:000517849600023
</snippet>
</document>

<document id="885">
<title>Deep Learning for Facial Beauty Prediction</title>
<url>http://dx.doi.org/10.3390/info11080391</url>
<snippet>Facial beauty prediction (FBP) is a burgeoning issue for attractiveness evaluation, which aims to make assessment consistent with human opinion. Since FBP is a regression problem, to handle this issue, there are data-driven methods for finding the relations between facial features and beauty assessment. Recently, deep learning methods have shown its amazing capacity for feature representation and analysis. Convolutional neural networks (CNNs) have shown tremendous performance on facial recognition and comprehension, which are proved as an effective method for facial feature exploration. Lately, there are well-designed networks with efficient structures investigated for better representation performance. However, these designs concentrate on the effective block but do not build an efficient information transmission pathway, which led to a sub-optimal capacity for feature representation. Furthermore, these works cannot find the inherent correlations of feature maps, which also limits the performance. In this paper, an elaborate network design for FBP issue is proposed for better performance. A residual-in-residual (RIR) structure is introduced to the network for passing the gradient flow deeper, and building a better pathway for information transmission. By applying the RIR structure, a deeper network can be established for better feature representation. Besides the RIR network design, an attention mechanism is introduced to exploit the inner correlations among features. We investigate a joint spatial-wise and channel-wise attention (SCA) block to distribute the importance among features, which finds a better representation for facial information. Experimental results show our proposed network can predict facial beauty closer to a humans assessment than state-of-the-arts.
WOS:000577870900001
</snippet>
</document>

<document id="886">
<title>A comparison of statistical and machine learning methods for creating national daily maps of ambient PM2.5 concentration</title>
<url>http://dx.doi.org/10.1016/j.atmosenv.2019.117130</url>
<snippet>A typical challenge in air pollution epidemiology is to perform detailed exposure assessment for individuals for which health data are available. To address this problem, in the last few years, substantial research efforts have been placed in developing statistical methods or machine learning techniques to generate estimates of air pollution at fine spatial and temporal scales (daily, usually) with complete coverage. However, it is not clear how much the predicted exposures yielded by the various methods differ, and which method generates more reliable estimates. In this paper, we aim to address this gap by evaluating a variety of exposure modeling approaches, comparing their predictive performance. Using PM2.5 in year 2011 over the continental U.S. as a case study, we generate national maps of ambient PM2.5 concentration using: (i) ordinary least squares and inverse distance weighting; (ii) kriging; (iii) statistical downscaling models, that is, spatial statistical models that use the information contained in air quality model outputs; (iv) land use regression, that is, linear regression modeling approaches that leverage the information in Geographical Information System (GIS) covariates; and (v) machine learning methods, such as neural networks, random forests and support vector regression. We examine the various methods predictive performance via cross-validation using Root Mean Squared Error, Mean Absolute Deviation, Pearson correlation, and Mean Spatial Pearson Correlation. Additionally, we evaluated whether factors such as, season, urbanicity, and levels of PM2.5 concentration (low, medium or high) affected the performance of the different methods. Overall, statistical methods that explicitly modeled the spatial correlation, e.g. universal kriging and the downscaler model, outperform all the other exposure assessment approaches regardless of season, urbanicity and PM2.5 concentration level. We posit that the better predictive performance of spatial statistical models over machine learning methods is due to the fact that they explicitly account for spatial dependence, thus borrowing information from neighboring observations. In light of our findings, we suggest that future exposure assessment methods for regional PM2.5 incorporate information from neighboring sites when deriving predictions at unsampled locations or attempt to account for spatial dependence.
WOS:000510946800027
</snippet>
</document>

<document id="887">
<title>A Segmentation Map Difference-Based Domain Adaptive Change Detection Method</title>
<url>http://dx.doi.org/10.1109/JSTARS.2021.3113327</url>
<snippet>Deep neural network (DNN) has been widely used in remote sensing image change detection (CD) in recent years. Due to the scarcity of training data, a large number of labeled data onto other fields become the source of DNN concept learning in remote sensing image CD. However, the distribution of features of the CD data and other data varies greatly, which prevents DNN from being better applied for one task to another. To solve this problem, a domain adaptive CD method based on segmentation map difference is proposed to this article, which includes the pretraining stage and the CD stage. In the pretraining stage, the domain adaptive UNet (Ada-UNet) is applied as the basic network of remote sensing image segmentation for network training with the purpose of learning the concepts of different features. In the CD stage, strict threshold segmentation results are used to train the channel attention network, which makes it more efficient to utilize the high-dimensional feature map. The probabilistic map generated by the three-channel attention networks is evaluated, and then it is used to accurately classify the changing pixels. In this article, experiments are carried out on datasets with different feature distributions. The results show that this method has strong domain adaptability and can greatly reduce the influence of the difference in feature distributions of the CD results.
WOS:000704110600006
</snippet>
</document>

<document id="888">
<title>CG-SSD: Corner guided single stage 3D object detection from LiDAR point cloud</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2022.07.006</url>
<snippet>Detecting accurate 3D bounding boxes of the object from point clouds is a major task in autonomous driving perception. At present, the anchor-based or anchor-free models that use LiDAR point clouds for 3D object detection use the center assigner strategy to infer the 3D bounding boxes. However, in the real-world scene, due to the occlusions and the effective detection range of the LiDAR system, only part of the object surface can be covered by the collected point clouds, and there are no measured 3D points corresponding to the physical object center. Obtaining the object by aggregating the incomplete surface point clouds will bring a loss of accuracy in direction and dimension estimation. To address this problem, we propose a corner-guided anchor-free single-stage 3D object detection model (CG-SSD). Firstly, the point clouds within a single frame are assigned to regular 3D grids. 3D sparse convolution backbone network composed of residual layers and sub-manifold sparse convolutional layers are used to construct bird's eye view (BEV) features for further deeper feature mining by a lite U-shaped network; Secondly, a novel corner-guided auxiliary module (CGAM) with adaptive corner classification algorithm is proposed to incorporate corner supervision signals into the neural network. CGAM is explicitly designed and trained to estimate locations of partially visible and invisible corners to obtain a more accurate object feature representation, especially for small or partial occluded objects; Finally, the deep features from both the backbone networks and CGAM module are concatenated and fed into the head module to predict the classification and 3D bounding boxes of the objects in the scene. The experiments demonstrate that CG-SSD achieves the state-of-art performance on the ONCE benchmark for supervised 3D object detection using single frame point cloud data, with 62.77&#37; mAP. Additionally, the experiments on ONCE and Waymo Open Dataset show that CGAM can be extended to most anchor-based models which use the BEV feature to detect objects, as a plug-in and bring +1.17&#37;similar to+14.23&#37; AP improvement. The code is available at https://github.com/mrqrs/CG-SSD.
WOS:000831127700001
</snippet>
</document>

<document id="889">
<title>Automatic generation of land use maps using aerial orthoimages and building floor data with a Conv-Depth Block (CDB) ResU-Net architecture</title>
<url>http://dx.doi.org/10.1016/j.jag.2022.102678</url>
<snippet>Unlike land classification maps, it is difficult to automate the generation of land use (LU) maps. The deep learning approach is a state-of-the-art methodology that can expedite the creation of LU maps. However, as the deep learning output depends on the training input, it is critical to decide upon the input that should be selected. In this study, a method for securing accurate LU information is established and used for ground truthing, using data on the number of building floors extracted from a digital topographic map and a 51 cm resolution aerial orthoimages as inputs. To this end, we developed a Conv-Depth Block (CDB) ResU-Net architecture. To verify the versatility of the proposed network, our neural network was applied to three complex metropolitan areas with different LU characteristics in Korea. The accuracy of LU maps for these cities was improved by combining convolution layers and depth-wise separable convolution as well as by including numerical building floor data. The proposed CDB ResU-Net achieved an overall accuracy of 83.7 &#37; for the test samples. Our network exhibited an improved performance compared to Deeplab v3+, ResUnet, ResASPP-Unet, and context-based ResU-Net in classifying residential classes, which is crucial for estimating the degree of exposure in urban risk analyses.
WOS:000911789200001
</snippet>
</document>

<document id="890">
<title>Learning Point-Guided Localization for Detection in Remote Sensing Images</title>
<url>http://dx.doi.org/10.1109/JSTARS.2020.3036685</url>
<snippet>Object detection in remote sensing images is challenging due to the dense distribution and arbitrary angle of the objects. It is a consensus that the oriented bounding box (OBB) is more suitable to represent the aerial objects. However, there are some extreme cases in regression-based OBB detection that make the regression target discontinuous, resulting in the poor performance. In this article, an analysis of the formats of OBB and the problems in its regression is presented, following with an exploration of transform localization from regression to keypoint estimation, which could be applied to avoid the problem of discontinuous regression target. Our novel method is called Object-wise Point-guided Localization Detector (OPLD). Continuously, a new prediction of center-point is introduced to refine the results, as the truncation problem caused by the cut graph. Lastly, in order to figure the problem of inconsistency between the localization quality and the classification score, both the endpoint scores and the classification score are adopted weighting as a result score. Experimental results are based on two widely used datasets, i.e., DOTA and HRSC2016. OPLD achieve 76.43&#37; mAP and 78.35&#37; mAP in OBB and horizontal bounding boxes tasks of DOTA-v1.0, which achieves state-of-the-art performance, respectively. Project page at https://github.com/yf19970118/OPLD-Pytorch.
WOS:000696430600021
</snippet>
</document>

<document id="891">
<title>PIXEL-BASED AND OBJECT-BASED TERRACE EXTRACTION USING FEED-FORWARD DEEP NEURAL NETWORK</title>
<url>http://dx.doi.org/10.5194/isprs-annals-IV-3-W1-1-2019</url>
<snippet>In this paper, we present the identification of terrace field by using Feed-forward back propagation deep neural network in pixel-based and several cases of object-based approaches. Terrace field of Lao Cai area in Vietnam is identified from 5-meter RapidEye image. The image includes 5 bands: red, green, blue, rededge and nir-infrared. Reference data are set of terrace points and nonterrace points, which are generated by randomly selected from reference map. The reference data is separated into three sets: training set for training processing, validation set for generating optimal parameters of deep neural network model, and test set for assessing the accuracy of classification. Six optimal thresholds (T): 0.06, 0.09, 0.12, 0.14, 0.2 and 0.22 are chosen from Rate of Change graph, and then used to generate six cases of object-based classification. Deep neural network (DNN) model is built with 8 hidden layers, input units are 5 bands of RapidEye, and output is terrace and non-terrace classes. Each hidden layer includes 256 units a large number, to avoid under-fitting. Activation function is Rectifier. Dropout and two regularization parameters are applied to avoid overfitting. Seven terrace maps are generated. The classification results show that the DNN is able to identify terrace field effectively in both pixel-based and object-based approaches. Pixel-based classification is the most accurate approach, achieves 90&#37; accuracy. The values of object-based approaches are 88.5&#37;, 87.3&#37;, 86.7&#37;, 86.6&#37;, 85&#37; and 85.3&#37; correspond to the segmentation thresholds.
WOS:000582727200001
</snippet>
</document>

<document id="892">
<title>Personalized Atrophy Risk Mapping in Age-Related Macular Degeneration</title>
<url>http://dx.doi.org/10.1167/tvst.10.13.18</url>
<snippet>Purpose: To develop and validate an automatic retinal pigment epithelial and outer retinal atrophy (RORA) progression prediction model for nonexudative age-related macular degeneration (AMD) cases in optical coherence tomography (OCT) scans. Methods: Longitudinal OCT data from 129 eyes/119 patients with RORA was collected and separated into training and testing groups. RORA was automatically segmented in all scans and additionally manually annotated in the test scans. OCT-based features such as layers thicknesses, mean reflectivity, and a drusen height map served as an input to the deep neural network. Based on the baseline OCT scan or the previous visit OCT, en face RORA predictions were calculated for future patient visits. The performance was quantified over time with the means of Dice scores and square root area errors. Results: The average Dice score for segmentations at baseline was 0.85. When predicting progression from baseline OCTs, the Dice scores ranged from 0.73 to 0.80 for total RORA area and from 0.46 to 0.72 for RORA growth region. The square root area error ranged from 0.13 mm to 0.33 mm. By providing continuous time output, the model enabled creation of a patient-specific atrophy risk map. Conclusions: We developed a machine learning method for RORA progression prediction, which provides continuous-time output. It was used to compute atrophy risk maps, which indicate time-to-RORA-conversion, a novel and clinically relevant way of representing disease progression. Translational Relevance: Application of recent advances in artificial intelligence to predict patient-specific progression of atrophic AMD.
WOS:000734334400015
</snippet>
</document>

<document id="893">
<title>Spatial Disaggregation of Historical Census Data Leveraging Multiple Sources of Ancillary Information</title>
<url>http://dx.doi.org/10.3390/ijgi8080327</url>
<snippet>High-resolution population grids built from historical census data can ease the analyses of geographical population changes, at the same time also facilitating the combination of population data with other GIS layers to perform analyses on a wide range of topics. This article reports on experiments with a hybrid spatial disaggregation technique that combines the ideas of dasymetric mapping and pycnophylactic interpolation, using modern machine learning methods to combine different types of ancillary variables, in order to disaggregate historical census data into a 200 m resolution grid. We specifically report on experiments related to the disaggregation of historical population counts from three different national censuses which took place around 1900, respectively in Great Britain, Belgium, and the Netherlands. The obtained results indicate that the proposed method is indeed highly accurate, outperforming simpler disaggregation schemes based on mass-preserving areal weighting or pycnophylactic interpolation. The best results were obtained using modern regression methods (i.e., gradient tree boosting or convolutional neural networks, depending on the case study), which previously have only seldom been used for spatial disaggregation.
WOS:000482985000026
</snippet>
</document>

<document id="894">
<title>Target Detection Model Distillation Using Feature Transition and Label Registration for Remote Sensing Imagery</title>
<url>http://dx.doi.org/10.1109/JSTARS.2022.3188252</url>
<snippet>Deep convolution networks have been widely used in remote sensing target detection for various applications in recent years. Target detection models with many parameters provide better results but are not suitable for resource-constrained devices due to their high computational cost and storage requirements. Furthermore, current lightweight target detection models for remote sensing imagery rarely have the advantages of existing models. Knowledge distillation can improve the learning ability of a small student network from a large teacher network due to acceleration and compression. However, current knowledge distillation methods typically use mature backbones as teacher and student networks are unsuitable for target detection in remote sensing imagery. In this article, we propose a target detection model distillation (TDMD) framework using feature transition and label registration for remote sensing imagery. A lightweight attention network is designed by ranking the importance of the convolutional feature layers in the teacher network. Multiscale feature transition based on a feature pyramid is utilized to constrain the feature maps of the student network. A label registration procedure is proposed to improve the TDMD models learning ability of the output distribution of the teacher network. The proposed method is evaluated on the DOTA and NWPU VHR-10 remote sensing image datasets. The results show that the TDMD achieves a mean Average Precision (mAP) of 75.47&#37; and 93.81&#37; on the DOTA and NWPU VHR-10 datasets, respectively. Moreover, the model size is 43&#37; smaller than that of the predecessor model (11.8 MB and 11.6 MB for the two datasets).
WOS:000828222600001
</snippet>
</document>

<document id="895">
<title>A Citizen Science Unmanned Aerial System Data Acquisition Protocol and Deep Learning Techniques for the Automatic Detection and Mapping of Marine Litter Concentrations in the Coastal Zone</title>
<url>http://dx.doi.org/10.3390/drones5010006</url>
<snippet>Marine litter (ML) accumulation in the coastal zone has been recognized as a major problem in our time, as it can dramatically affect the environment, marine ecosystems, and coastal communities. Existing monitoring methods fail to respond to the spatiotemporal changes and dynamics of ML concentrations. Recent works showed that unmanned aerial systems (UAS), along with computer vision methods, provide a feasible alternative for ML monitoring. In this context, we proposed a citizen science UAS data acquisition and annotation protocol combined with deep learning techniques for the automatic detection and mapping of ML concentrations in the coastal zone. Five convolutional neural networks (CNNs) were trained to classify UAS image tiles into two classes: (a) litter and (b) no litter. Testing the CCNs generalization ability to an unseen dataset, we found that the VVG19 CNN returned an overall accuracy of 77.6&#37; and an f-score of 77.42&#37;. ML density maps were created using the automated classification results. They were compared with those produced by a manual screening classification proving our approachs geographical transferability to new and unknown beaches. Although ML recognition is still a challenging task, this study provides evidence about the feasibility of using a citizen science UAS-based monitoring method in combination with deep learning techniques for the quantification of the ML load in the coastal zone using density maps.
WOS:000633102700001
</snippet>
</document>

</searchresult>
