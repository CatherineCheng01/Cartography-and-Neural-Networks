<?xml version="1.0" encoding="UTF-8"?>
<searchresult>
<query>Global</query>
<document id="1">
<title>Mapping stone walls in Northeastern USA using deep learning and LiDAR data</title>
<url>http://dx.doi.org/10.1080/15481603.2023.2196117</url>
<snippet>Stone walls are widespread and iconic landforms found throughout forested terrain in the Northeastern USA that were built during the 17th to early 20th centuries to delineate property boundaries and the edges of agricultural fields and pastures. As linear, or broadly curved, features that are typically &gt; 0.5 m high, 1-2 m wide, and &gt; 4-8 m long, stone walls are highly visible in LiDAR data, and mapping them is of broad interest to the cultural heritage sector as well as to researchers specifically focused on historic landscape reconstruction. However, existing mapping attempts have commonly relied on field surveys and manual digitization, which is time-consuming, especially when trying to complete mapping at broader scales. In response to this limitation, this study: (1) presents a novel framework to automate stone wall mapping using Deep Convolutional Neural Networks (DCNN) models (U-Net and ResUnet) and high-resolution airborne LiDAR, (2) evaluates model performance in two test sites against field verified stone walls, (3) investigates the factors that can influence model performance in terms of the quality of LiDAR data (e.g. ground point spacing), and (4) suggests post-processing for town-level mapping of stone walls (similar to 120 km(2)). Both models performed well with respect to the Matthews Correlation Coefficient (MCC) score. U-Net scenario 3 achieved an MCC score of 0.87 at test site 1, while ResUnet scenario 3 (S3) had an MCC score of 0.80 at test site 2. In town-level test site 3, ResUnet S3 achieved the best F-1 score of 82&#37; after post-processing. This study demonstrates the potential of automated mapping of anthropogenic features using our models.
WOS:000963378900001
</snippet>
</document>

<document id="2">
<title>Potential of eye-tracking for interactive geovisual exploration aided by machine learning</title>
<url>http://dx.doi.org/10.1080/23729333.2022.2150379</url>
<snippet>ABSTRAITECet article de synthese recueille des connaissances sur lutilisation de methodes de suivi oculaire et dapprentissage automatique appliquees a des systemes de geovisualisation automatises et interactifs. Nous nous concentrons sur la lecture exploratoire de geovisualisation (abrege en geoexploration) et sur les outils dapprentissage automatique pour lexploration de donnees geospatiales vectorielles. Nous examinons particulierement les donnees geospatiales non etiquetees, sujettes a confusion ou inconnues de lutilisateur. La contribution de cet article est dans 1) la definition de principes et de besoins pour permettre linteraction de lutilisateur avec les outils de geovisualisation qui apprennent de lutilisateur et sadaptent a son comportement 2) lanalyse de lutilisation de methodes de suivi oculaire et dapprentissage pour la conception de systemes de cartes interactives sensibles au regard (GAIMS). Dans ce contexte, nous examinons la litterature sur 1) la conception des interfaces homme-machine (HCI) pour lexploration de donnees geospatiales 2) lutilisation du suivi oculaire pour des experiences cartographiques, et 3) lapprentissage applique aux donnees geospatiales vectorielles. Letat de lart montre que la combinaison du suivi oculaire et de lapprentissage est prometteuse pour assister la geoexploration. Il manque cependant des recherches sur le suivi oculaire pour les interactions et la personnalisation des interfaces cartographiques ainsi que sur lapprentissage automatique pour la detection de geometries vectorielles.
WOS:000911965800001
</snippet>
</document>

<document id="3">
<title>A point selection method in map generalization using graph convolutional network model</title>
<url>http://dx.doi.org/10.1080/15230406.2023.2187886</url>
<snippet>For point clusters, the conflict and crowding of map symbols is an inevitable problem during the transition from large to small scales. The cartographic generalization involved in this problem as a spatial decision-making process is usually related to the analysis of spatial context, the choice of abstraction operators, and the judgment of the resulting data quality. The rules summarized by traditional generalization methods usually require manual setting of conditions or thresholds and sometimes encounter special cases that make it difficult to directly match certain rules or integrate different rules together. An alternative method is using a data-driven strategy under AI technology background to simulate cartographer behaviors through typical sample training, such as deep learning. The integration of cartography domain knowledge and deep learning is a better choice to settle generalization decisions. This study uses a combination of domain knowledge and a data-driven approach to introduce graph neural networks into point cluster generalization. First, we construct a virtual graph structure of point clusters using Delaunay triangulation, secondly, we extract spatial features, contextual features, and attributes of each point separately, and then propose a generalization model based on the TAGCN network. Finally, this model is trained with the manually generalized sample to realize the automatic point cluster generalization. The results demonstrate that the proposed model is valid and efficient for point cluster generalization and that this algorithm can better maintain various characteristics of the point cluster in both the local area and the overall map compared to other methods.
WOS:000950985900001
</snippet>
</document>

<document id="4">
<title>A Siamese neural network for learning the similarity metrics of linear features</title>
<url>http://dx.doi.org/10.1080/13658816.2022.2143505</url>
<snippet>Measuring similarity is essential for classifying, clustering, retrieving, and matching linear features in geospatial data. However, the complexity of linear features challenges the formalization of characteristics and determination of the weight of each characteristic in similarity measurements. Additionally, traditional methods have limited adaptability to the variety of linear features. To address these challenges, this study proposes a metric learning model that learns similarity metrics directly from linear features. The models ability to learn allows no pre-determined characteristics and supports adaptability to different levels of complex linear features. LineStringNet functions as a feature encoder that maps vector lines to embeddings without format conversion or feature engineering. With a Siamese architecture, the learning process minimizes the contrastive loss, which brings similar pairs closer and pushes dissimilar pairs away in the embedding space. Finally, the proposed model calculates the Euclidean distance to measure the similarity between learned embeddings. Experiments on common linear features and building shapes indicated that the learned similarity metrics effectively supported retrieving, matching, and classifying lines and polygons, with higher precision and accuracy than traditional measures. Furthermore, the model ensures desired metric properties, including rotation and starting point invariances, by adjusting labeling strategies or preprocessing input data.
WOS:000881869600001
</snippet>
</document>

<document id="5">
<title>Fuzzy Cognitive Maps: Their Role in Explainable Artificial Intelligence</title>
<url>http://dx.doi.org/10.3390/app13063412</url>
<snippet>Currently, artificial intelligence is facing several problems with its practical implementation in various application domains. The explainability of advanced artificial intelligence algorithms is a topic of paramount importance, and many discussions have been held recently. Pioneering and classical machine learning and deep learning models behave as black boxes, constraining the logical interpretations that the end users desire. Artificial intelligence applications in industry, medicine, agriculture, and social sciences require the users trust in the systems. Users are always entitled to know why and how each method has made a decision and which factors play a critical role. Otherwise, they will always be wary of using new techniques. This paper discusses the nature of fuzzy cognitive maps (FCMs), a soft computational method to model human knowledge and provide decisions handling uncertainty. Though FCMs are not new to the field, they are evolving and incorporate recent advancements in artificial intelligence, such as learning algorithms and convolutional neural networks. The nature of FCMs reveals their supremacy in transparency, interpretability, transferability, and other aspects of explainable artificial intelligence (XAI) methods. The present study aims to reveal and defend the explainability properties of FCMs and to highlight their successful implementation in many domains. Subsequently, the present study discusses how FCMs cope with XAI directions and presents critical examples from the literature that demonstrate their superiority. The study results demonstrate that FCMs are both in accordance with the XAI directives and have many successful applications in domains such as medical decision-support systems, precision agriculture, energy savings, environmental monitoring, and policy-making for the public sector.
WOS:000953855200001
</snippet>
</document>

<document id="6">
<title>Interpreting convolutional neural network decision for earthquake detection with feature map visualization, backward optimization and layer-wise relevance propagation methods</title>
<url>http://dx.doi.org/10.1093/gji/ggac369</url>
<snippet>In the recent years, the seismological community has adopted deep learning (DL) models for many diverse tasks such as discrimination and classification of seismic events, identification of P- and S-phase wave arrivals or earthquake early warning systems. Numerous models recently developed are showing high accuracy values, and it has been attested for several tasks that DL models perform better than the classical seismological state-of-art models. However, their performances strongly depend on the DL architecture, the training hyperparameters, and the training data sets. Moreover, due to their complex nature, we are unable to understand how the model is learning and therefore how it is making a prediction. Thus, DL models are usually referred to as a black-box. In this study, we propose to apply three complementary techniques to address the interpretability of a convolutional neural network (CNN) model for the earthquake detection. The implemented techniques are: feature map visualization, backward optimization and layer-wise relevance propagation. Since our model reaches a good accuracy performance (97&#37;), we can suppose that the CNN detector model extracts relevant characteristics from the data, however a question remains: can we identify these characteristics? The proposed techniques help to answer the following questions: How is an earthquake processed by a CNN model? What is the optimal earthquake signal according to a CNN? Which parts of the earthquake signal are more relevant for the model to correctly classify an earthquake sample? The answer to these questions help understand why the model works and where it might fail, and whether the model is designed well for the predefined task. The CNN used in this study had been trained for single-station detection, where an input sample is a 25 s three-component waveform. The model outputs a binary target: earthquake (positive) or noise (negative) class. The training database contains a balanced number of samples from both classes. Our results shows that the CNN model correctly learned to recognize where is the earthquake within the sample window, even though the position of the earthquake in the window is not explicitly given during the training. Moreover, we give insights on how a neural network builds its decision process: while some aspects can be linked to clear physical characteristics, such as the frequency content and the P and S waves, we also see how different a DL detection is compared to a visual expertise or an STA/LTA detection. On top of improving our model designs, we also think that understanding how such models work, how they perceive an earthquake, can be useful for the comprehension of events that are not fully understood yet such as tremors or low frequency earthquakes.
WOS:000866173400007
</snippet>
</document>

<document id="7">
<title>Towards general-purpose representation learning of polygonal geometries</title>
<url>http://dx.doi.org/10.1007/s10707-022-00481-2</url>
<snippet>Neural network representation learning for spatial data (e.g., points, polylines, polygons, and networks) is a common need for geographic artificial intelligence (GeoAI) problems. In recent years, many advancements have been made in representation learning for points, polylines, and networks, whereas little progress has been made for polygons, especially complex polygonal geometries. In this work, we focus on developing a general-purpose polygon encoding model, which can encode a polygonal geometry (with or without holes, single or multipolygons) into an embedding space. The result embeddings can be leveraged directly (or finetuned) for downstream tasks such as shape classification, spatial relation prediction, building pattern classification, cartographic building generalization, and so on. To achieve model generalizability guarantees, we identify a few desirable properties that the encoder should satisfy: loop origin invariance, trivial vertex invariance, part permutation invariance, and topology awareness. We explore two different designs for the encoder: one derives all representations in the spatial domain and can naturally capture local structures of polygons; the other leverages spectral domain representations and can easily capture global structures of polygons. For the spatial domain approach we propose ResNet1D, a 1D CNN-based polygon encoder, which uses circular padding to achieve loop origin invariance on simple polygons. For the spectral domain approach we develop NUFTspec based on Non-Uniform Fourier Transformation (NUFT), which naturally satisfies all the desired properties. We conduct experiments on two different tasks: 1) polygon shape classification based on the commonly used MNIST dataset; 2) polygon-based spatial relation prediction based on two new datasets (DBSR-46K and DBSR-cplx46K) constructed from OpenStreetMap and DBpedia. Our results show that NUFTspec and ResNet1D outperform multiple existing baselines with significant margins. While ResNet1D suffers from model performance degradation after shape-invariance geometry modifications, NUFTspec is very robust to these modifications due to the nature of the NUFT representation. NUFTspec is able to jointly consider all parts of a multipolygon and their spatial relations during prediction while ResNet1D can recognize the shape details which are sometimes important for classification. This result points to a promising research direction of combining spatial and spectral representations.
WOS:000870986700001
</snippet>
</document>

<document id="8">
<title>Analysis of Airglow Image Classification Based on Feature Map Visualization</title>
<url>http://dx.doi.org/10.3390/app13063671</url>
<snippet>All-sky airglow imagers (ASAIs) are used in the Meridian Project to observe the airglow in the middle and upper atmosphere to study the atmospheric perturbation. However, the ripples of airglow caused by the perturbation are only visible in the airglow images taken on a clear night. It is a problem to effectively select images suitable for scientific analysis from the enormous amount of airglow images captured under various environments due to the low efficiency and subjectivity of traditional manual classification. We trained a classification model based on convolutional neural network to distinguish between airglow images from clear nights and unclear nights. The data base contains 1688 images selected from the airglow images captured at Xinglong station (40.4 degrees N, 30.5 degrees E). The entire training process was tracked by feature maps which visualized every resulting classification model. The classification models with the clearest feature maps were saved for future use. We cropped the central part of the airglow images to avoid disturbance from the artificial lights at the edge of the vision field according to the feature maps of our first training. The accuracy of the saved model is 99&#37;. The feature maps of five categories also indicate the reliability of the classification model.
WOS:000957417400001
</snippet>
</document>

<document id="9">
<title>Computational Cartographic Recognition: Identifying Maps, Geographic Regions, and Projections from Images Using Machine Learning</title>
<url>http://dx.doi.org/10.1080/24694452.2023.2166010</url>
<snippet>Map reading is a challenging task for computer programs. This article explores how artificial intelligence and machine learning methods can be used to understand maps, an area we broadly refer to as computational cartographic recognition. Specifically, we use machine learning methods to (1) identify whether an image is a map, (2) recognize the geographic region on the map, and (3) recognize the projection used on the map. Four machine learning models-support vector machine, multilayer perceptrons, convolutional neural networks (CNNs) developed from scratch using our own architecture (CNNS), and pretrained CNN models through transfer learning (CNNT)-are applied in these tasks. We use 2,200 online map images, 500 nonmap images, and 1,050 synthetic map images to train and evaluate the models. Results show that the CNNT models achieve the highest performance among all models, with an accuracy rate above 90 percent for the tasks. The CNNS models come in second. We also conduct a round of stress tests using 3,600 additional synthetic maps where the shape and layout are systematically distorted and test if the models can still identify the maps and recognize the region and projection on the maps. The results of the stress tests show that the models can reliably recognize some of the modified maps even when exhibiting performance inferior to even random models for other maps. This unpredictable nature of the methods when applied to maps that are not represented in the training data suggests both promises and limitations of the current machine learning approaches to cartographic recognition.
WOS:000942715400001
</snippet>
</document>

</searchresult>
