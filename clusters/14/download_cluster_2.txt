FN Thomson Reuters Web of Scienceâ„¢
VR 1.0
PT J
AU Shao, Z
   Zhang, Linjing
   Wang, Lei
TI Stacked Sparse Autoencoder Modeling Using the Synergy of Airborne LiDAR and Satellite Optical and SAR Data to Map Forest Above-Ground Biomass
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
DE biomass; deep learning (dl); landsat 8; light detection and ranging (lidar); stacked sparse autoencoder network (ssae); sentinel-1a
AB Timely, spatially complete, and reliable forest aboveground biomass (AGB) data are a prerequisite to support forest management and policy formulation. Traditionally, forest AGB is spatially estimated by integrating satellite images, in particular, optical data, with field plots from forest inventory programs. However, field data are limited in remote and unmanaged areas. In addition, optical reflectance usually saturates at high-density biomass level and is subject to cloud contaminations. Thus, this study aimed to developa deep learning based workflow for mapping forest AGB by integrating Landsat 8 and Sentinel-1A images with airborne light detection and ranging (LiDAR) data. A reference AGB map was derived from the wall-to-wall LiDAR data and field measurements. The LiDAR plots-stratified random samples of forest biomass extracted from the LiDAR simulated strips in the reference map-were adopted as a surrogate for traditional field plots. In addition to the deep learning model, i.e., stacked sparse autoencoder network (SSAE), five different prediction techniques including multiple stepwise linear regressions, K-nearest neighbor, support vector machine, back propagation neural networks, and random forest were individually used to establish the relationship between LiDAR-derived forest biomass and the satellite predictors. Optical variables (Landsat 8 OLI), SAR variables (Sentinel-1A), and their combined variables were individually input to the six prediction models. Results showed that the SSAE model had the best performance forthe forest biomass estimation. The combined optical and microwave dataset as explanatory variables improved the modeling performance compared to either the optical-only or microwave-only data, regardless of prediction algorithms. The best mapping accuracy was obtained by the SSAE model with inputs of optical and microwave integrated metrics that yielded R-2 of 0.812, root mean squared error (RMSE) of 21.753 Mg/ha, and relative RMSE (RMSEr) of 14.457%. Overall, the SSAE model with inputs of combined Landsat 8 OLI and Sentinel-1A information could result in accurate estimation of forest biomass by using the stratification-sampled and LiDAR-derived AGB as ground reference data. The modeling workflow has the potential to promote future forest growth monitoring and carbon stock assessment across large areas.
C1 [Shao, Zhenfeng; Zhang, Linjing; Wang, Lei] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Hubei, Peoples R China.   
[Shao, Zhenfeng; Zhang, Linjing; Wang, Lei] Wuhan Univ, Collaborat Innovat Ctr Geospatial Technol, Wuhan 430079, Hubei, Peoples R China.
RP Zhang, LJ (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Hubei, Peoples R China.; Zhang, LJ (corresponding author), Wuhan Univ, Collaborat Innovat Ctr Geospatial Technol, Wuhan 430079, Hubei, Peoples R China.
FU National Key Technologies Research and Development Program [2016YFB0502603]; Fundamental Research Funds for the Central Universities [2042016kf0179, 2042016kf1019]; Guangzhou Science and Technology Project [201604020070]; National Administration of Surveying, Mapping and Geoinformation [2015NGCM]; Wuhan Chen Guang Project [2016070204010114]; Special task of technical innovation in Hubei Province [2016AAA018]
CR Ahmed OS, 2015, ISPRS J PHOTOGRAMM, V101, P89, DOI 10.1016/j.isprsjprs.2014.11.007
   Attarchi S, 2014, REMOTE SENS-BASEL, V6, P3693, DOI 10.3390/rs6053693
   Basuki TM, 2013, INT J REMOTE SENS, V34, P4871, DOI 10.1080/01431161.2013.777486
   CRTeam, 2013, TEAM RDC R LANG ENV, V1, P12
   Carreiras JMB, 2012, REMOTE SENS ENVIRON, V121, P426, DOI 10.1016/j.rse.2012.02.012
   Chang KT, 2012, INT GEOSCI REMOTE SE, V0, PP6376, DOI 10.1109/IGARSS.2012.6352718
   Chen BQ, 2015, ISPRS J PHOTOGRAMM, V102, P148, DOI 10.1016/j.isprsjprs.2014.12.011
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Chirici G, 2016, REMOTE SENS ENVIRON, V174, P1, DOI 10.1016/j.rse.2015.11.010
   Cougo MF, 2015, REMOTE SENS-BASEL, V7, P17097, DOI 10.3390/rs71215873
   Deng SQ, 2014, REMOTE SENS-BASEL, V6, P7878, DOI 10.3390/rs6097878
   Dube T, 2015, ISPRS J PHOTOGRAMM, V101, P36, DOI 10.1016/j.isprsjprs.2014.11.001
   Dube T, 2015, ISPRS J PHOTOGRAMM, V108, P12, DOI 10.1016/j.isprsjprs.2015.06.002
   El-Askary H, 2014, INT J REMOTE SENS, V35, P2327, DOI 10.1080/01431161.2014.894656
   Fassnacht FE, 2014, REMOTE SENS ENVIRON, V154, P102, DOI 10.1016/j.rse.2014.07.028
   Gao S, 2013, INT J APPL EARTH OBS, V24, P1, DOI 10.1016/j.jag.2013.02.002
   Gatziolis D, 2012, COMPARISON LIDAR PHO, V0, P0
   Godwin C, 2015, LANDSCAPE URBAN PLAN, V136, P97, DOI 10.1016/j.landurbplan.2014.12.007
   Hame T, 2013, IEEE J-STARS, V6, P92, DOI 10.1109/JSTARS.2013.2241020
   He QS, 2012, INT J REMOTE SENS, V33, P710, DOI 10.1080/01431161.2011.577829
   Hyyppa J, 2012, REMOTE SENS-BASEL, V4, P1190, DOI 10.3390/rs4051190
   Kattenborn T, 2015, INT J APPL EARTH OBS, V35, P359, DOI 10.1016/j.jag.2014.10.008
   Kelsey KC, 2014, REMOTE SENS-BASEL, V6, P6407, DOI 10.3390/rs6076407
   Korhonen L, 2013, INT J REMOTE SENS, V34, P8172, DOI 10.1080/01431161.2013.833361
   Latifi H, 2012, INT J REMOTE SENS, V33, P6668, DOI 10.1080/01431161.2012.693969
   Li M, 2014, IEEE J-STARS, V7, P3143, DOI 10.1109/JSTARS.2014.2304642
   Li W, 2015, INT J APPL EARTH OBS, V41, P88, DOI 10.1016/j.jag.2015.04.020
   Liang H, 2016, NEURAL NETWORK FEATU, V8, P99, DOI 10.3390/rs8020099
   Pahlevan N, 2013, IEEE J-STARS, V6, P360, DOI 10.1109/JSTARS.2012.2235174
   Shao ZF, 2016, SENSORS-BASEL, V16, P0, DOI 10.3390/s16060834
   Singh KK, 2015, ISPRS J PHOTOGRAMM, V101, P310, DOI 10.1016/j.isprsjprs.2014.12.021
   Singh KK, 2016, IEEE J-STARS, V9, P3210, DOI 10.1109/JSTARS.2016.2522960
   Su YJ, 2016, REMOTE SENS ENVIRON, V173, P187, DOI 10.1016/j.rse.2015.12.002
   Tian X, 2012, INT J APPL EARTH OBS, V14, P160, DOI 10.1016/j.jag.2011.09.010
   Tian X, 2014, INT J REMOTE SENS, V35, P7339, DOI 10.1080/01431161.2014.967888
   Tsui OW, 2013, REMOTE SENS ENVIRON, V139, P340, DOI 10.1016/j.rse.2013.08.012
   Verrelst J, 2012, REMOTE SENS ENVIRON, V118, P127, DOI 10.1016/j.rse.2011.11.002
   Wulder MA, 2012, REMOTE SENS ENVIRON, V121, P196, DOI 10.1016/j.rse.2012.02.001
   Yu YT, 2015, IEEE J-STARS, V8, P709, DOI 10.1109/JSTARS.2014.2347276
   Yu YT, 2016, IEEE T GEOSCI REMOTE, V54, P4130, DOI 10.1109/TGRS.2016.2537830
   Zald HSJ, 2014, REMOTE SENS ENVIRON, V143, P26, DOI 10.1016/j.rse.2013.12.013
   Zald HSJ, 2016, REMOTE SENS ENVIRON, V176, P188, DOI 10.1016/j.rse.2016.01.015
   Zhang L, 2015, IEEE J-STARS, V8, P4895, DOI 10.1109/JSTARS.2015.2467377
   Zhang L, 2016, IEEE GEOSCI REMOTE S, V13, P1359, DOI 10.1109/LGRS.2016.2586109
NR 44
TC 65
Z9 0
U1 9
U2 82.0
J9 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
PD DEC
PY 2017
VL 10
BP 5569
EP 5582
DI 10.1109/JSTARS.2017.2748341
PG 14
SC ENGINEERING; PHYSICAL GEOGRAPHY; REMOTE SENSING; IMAGING SCIENCE & PHOTOGRAPHIC TECHNOLOGY
UT WOS:000418871200021
PM 
ER

PT J
AU Ghamisi, P
   Hoefle, Bernhard
   Zhu, Xiao Xiang
TI Hyperspectral and LiDAR Data Fusion Using Extinction Profiles and Deep Convolutional Neural Network
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
DE convolutional neural network (cnn); deep learning; extinction profile (ep); graph-based feature fusion (gbff); hyperspectral; light detection and ranging (lidar); random forest (rf); support vector machines (svms)
AB This paper proposes a novel framework for the fusion of hyperspectral and light detection and ranging-derived rasterized data using extinction profiles (EPs) and deep learning. In order to extract spatial and elevation information from both the sources, EPs that include different attributes (e.g., height, area, volume, diagonal of the bounding box, and standard deviation) are taken into account. Then, the derived features are fused via either feature stacking or graph-based feature fusion. Finally, the fused features are fed to a deep learning-based classifier (convolutional neural network with logistic regression) to ultimately produce the classification map. The proposed approach is applied to two datasets acquired in Houston, TX, USA, and Trento, Italy. Results indicate that the proposed approach can achieve accurate classification results compared to other approaches. It should be noted that, in this paper, the concept of deep learning has been used for the first time to fuse LiDAR and hyperspectral features, which provides new opportunities for further research.
C1 [Ghamisi, Pedram; Zhu, Xiao Xiang] Tech Univ Munich, German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-80333 Munich, Germany.   
[Ghamisi, Pedram; Zhu, Xiao Xiang] Tech Univ Munich, Signal Proc Earth Observat, D-80333 Munich, Germany.   
[Hoefle, Bernhard] Heidelberg Univ, Inst Geog, GIScience, D-69120 Heidelberg, Germany.
RP Ghamisi, P; Zhu, XX (corresponding author), Tech Univ Munich, German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-80333 Munich, Germany.; Ghamisi, P; Zhu, XX (corresponding author), Tech Univ Munich, Signal Proc Earth Observat, D-80333 Munich, Germany.
FU Alexander von Humboldt Fellowship; Helmholtz Young Investigators Group [VH-NG-1018]; Directorate For Geosciences; Division Of Earth Sciences [1339015] Funding Source: National Science Foundation
CR Belgiu M, 2014, REMOTE SENS-BASEL, V6, P1347, DOI 10.3390/rs6021347
   Benediktsson JA, 2015, ARTECH HSE REMOTE SE, V0, P1
   Bernabe S, 2014, IEEE GEOSCI REMOTE S, V11, P288, DOI 10.1109/LGRS.2013.2256336
   Carlinet E, 2014, IEEE T IMAGE PROCESS, V23, P3885, DOI 10.1109/TIP.2014.2336551
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107
   Debes C, 2014, IEEE J-STARS, V7, P2405, DOI 10.1109/JSTARS.2014.2305441
   Ghamisi P, 2014, IEEE J-STARS, V7, P2147, DOI 10.1109/JSTARS.2014.2298876
   Ghamisi P, 2014, IEEE T GEOSCI REMOTE, V52, P2382, DOI 10.1109/TGRS.2013.2260552
   Ghamisi P, 2014, IEEE T GEOSCI REMOTE, V52, P2565, DOI 10.1109/TGRS.2013.2263282
   Ghamisi P, 2014, IEEE T GEOSCI REMOTE, V52, P5771, DOI 10.1109/TGRS.2013.2292544
   Ghamisi P, 2015, IEEE GEOSCI REMOTE S, V12, P309, DOI 10.1109/LGRS.2014.2337320
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2335, DOI 10.1109/TGRS.2014.2358934
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2935, DOI 10.1109/TGRS.2014.2367010
   Ghamisi P, 2015, INT J IMAGE DATA FUS, V6, P189, DOI 10.1080/19479832.2015.1055833
   Ghamisi P, 2015, THESIS, V0, P0
   Ghamisi P, 2016, IEEE GEOSCI REMOTE S, V13, P1537, DOI 10.1109/LGRS.2016.2595108
   Ghamisi P, 2016, IEEE GEOSCI REMOTE S, V13, P1641, DOI 10.1109/LGRS.2016.2600244
   Ghamisi P, 2016, IEEE T GEOSCI REMOTE, V54, P5631, DOI 10.1109/TGRS.2016.2561842
   Heiden U, 2012, LANDSCAPE URBAN PLAN, V105, P361, DOI 10.1016/j.landurbplan.2012.01.001
   Hofle B, 2012, ISPRS J PHOTOGRAMM, V67, P134, DOI 10.1016/j.isprsjprs.2011.12.003
   Khodadadzadeh M, 2015, IEEE J-STARS, V8, P2971, DOI 10.1109/JSTARS.2015.2432037
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Liao W, 2014, P 5 WORKSH EARSEL SP, V0, P34
   Liao WZ, 2015, IEEE GEOSCI REMOTE S, V12, P552, DOI 10.1109/LGRS.2014.2350263
   Pedergnana M, 2012, IEEE J-STSP, V6, P856, DOI 10.1109/JSTSP.2012.2208177
   Souza R, 2015, IEEE IMAGE PROC, V0, PP3620, DOI 10.1109/ICIP.2015.7351479
   Souza R, 2015, LECT NOTES COMPUT SC, V9082, P63, DOI 10.1007/978-3-319-18720-4, 6
   Tomljenovic I, 2015, REMOTE SENS-BASEL, V7, P3826, DOI 10.3390/rs70403826
NR 30
TC 103
Z9 0
U1 9
U2 11.0
J9 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
PD JUN
PY 2017
VL 10
BP 3011
EP 3024
DI 10.1109/JSTARS.2016.2634863
PG 14
SC ENGINEERING; PHYSICAL GEOGRAPHY; REMOTE SENSING; IMAGING SCIENCE & PHOTOGRAPHIC TECHNOLOGY
UT WOS:000406419400055
PM 
ER

PT J
AU Adep, RN
   Shetty, Amba
   Ramesh, H
TI EXhype: A tool for mineral classification using hyperspectral data
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
DE artificial neural network; expert system; human expertise; knowledge base; segmented upper hull method; spectral angle mapper
AB Various supervised classification algorithms have been developed to classify earth surface features using hyperspectral data. Each algorithm is modelled based on different human expertises. However, the performance of conventional algorithms is not satisfactory to map especially the minerals in view of their typical spectral responses. This study introduces a new expert system named EXhype (Expert system for hyperspectral data classification) to map minerals. The system incorporates human expertise at several stages of its implementation: (i) to deal with intra-class variation; (ii) to identify absorption features; (iii) to discriminate spectra by considering absorption features, non-absorption features and by full spectra comparison; and (iv) finally takes a decision based on learning and by emphasizing most important features. It is developed using a knowledge base consisting of an Optimal Spectral Library, Segmented Upper Hull method, Spectral Angle Mapper (SAM) and Artificial Neural Network. The performance of the EXhype is compared with a traditional, most commonly used SAM algorithm using Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) data acquired over Cuprite, Nevada, USA. A virtual verification method is used to collect samples information for accuracy assessment. Further, a modified accuracy assessment method is used to get a real users accuracies in cases where only limited or desired classes are considered for classification. With the modified accuracy assessment method, SAM and EXhype yields an overall accuracy of 60.35% and 90.75% and the kappa coefficient of 0.51 and 0.89 respec-tively. It was also found that the virtual verification method allows to use most desired stratified random sampling method and eliminates all the difficulties associated with it. The experimental results show that EXhype is not only producing better accuracy compared to traditional SAM but, can also rightly classify the minerals. It is proficient in avoiding misclassification between target classes when applied on minerals. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
C1 [Adep, Ramesh Nityanand; Shetty, Amba; Ramesh, H.] Natl Inst Technol Karnataka, Dept Appl Mech & Hydraul, Surathkal 575025, Karnataka, India.   
[Adep, Ramesh Nityanand] 276-1 Padmanagar, Bhiwani 421305, Maharashtra, India.
RP Adep, RN (corresponding author), Natl Inst Technol Karnataka, Dept Appl Mech & Hydraul, Surathkal 575025, Karnataka, India.; Adep, RN (corresponding author), 276-1 Padmanagar, Bhiwani 421305, Maharashtra, India.
CR Brossard M, 2016, REMOTE SENS LETT, V7, P581, DOI 10.1080/2150704X.2016.1168946
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Geng XR, 2013, ISPRS J PHOTOGRAMM, V79, P211, DOI 10.1016/j.isprsjprs.2013.02.020
   Koerting F, 2015, INT ARCH PHOTOGRAMM, V41, P417, DOI 10.5194/isprsarchives-XL-1-W5-417-2015
   Kumar C, 2014, INT ARCH PHOTOGRAMM, V40-8, P455, DOI 10.5194/isprsarchives-XL-8-455-2014
   Li H, 2014, PRECIS AGRIC, V15, P162, DOI 10.1007/s11119-013-9325-6
   Liu J, 2015, ISPRS J PHOTOGRAMM, V101, P145, DOI 10.1016/j.isprsjprs.2014.11.009
   Lokman G, 2015, 2015 7TH INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN SPACE TECHNOLOGIES (RAST), V0, PP239, DOI 10.1109/RAST.2015.7208348
   Marshall M, 2015, ISPRS J PHOTOGRAMM, V108, P205, DOI 10.1016/j.isprsjprs.2015.08.001
   Mielke C, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8020127
   Molan YE, 2014, INT J APPL EARTH OBS, V27, P117, DOI 10.1016/j.jag.2013.09.014
   Murphy RJ, 2013, ISPRS J PHOTOGRAMM, V75, P29, DOI 10.1016/j.isprsjprs.2012.09.014
   Padma S, 2014, INT ARCH PHOTOGRAMM, V40-8, P1403, DOI 10.5194/isprsarchives-XL-8-1403-2014
   Pan ZK, 2013, INT J APPL EARTH OBS, V25, P21, DOI 10.1016/j.jag.2013.03.002
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379
   Shanmugam S, 2014, INT J REMOTE SENS, V35, P8217, DOI 10.1080/01431161.2014.980922
   Swayze GA, 2014, ECON GEOL, V109, P1179, DOI 10.2113/econgeo.109.5.1179
   Tits L, 2012, ISPRS J PHOTOGRAMM, V74, P163, DOI 10.1016/j.isprsjprs.2012.09.013
   Yu K, 2014, ISPRS J PHOTOGRAMM, V97, P58, DOI 10.1016/j.isprsjprs.2014.08.005
   Zhong YF, 2016, ISPRS J PHOTOGRAMM, V119, P49, DOI 10.1016/j.isprsjprs.2016.04.008
   Zhu FY, 2014, ISPRS J PHOTOGRAMM, V88, P101, DOI 10.1016/j.isprsjprs.2013.11.014
NR 22
TC 13
Z9 0
U1 0
U2 29.0
J9 ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
PD FEB
PY 2017
VL 124
BP 106
EP 118
DI 10.1016/j.isprsjprs.2016.12.012
PG 13
SC PHYSICAL GEOGRAPHY; GEOLOGY; REMOTE SENSING; IMAGING SCIENCE & PHOTOGRAPHIC TECHNOLOGY
UT WOS:000394082400008
PM 
ER

PT J
AU Gong, M
   Yang, Hailun
   Zhang, Puzhao
TI Feature learning and change feature classification based on deep learning for ternary change detection in SAR images
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
DE ternary change detection; deep learning; synthetic aperture radar; representation learning; sparse autoencoder; convolutional neural networks
AB Ternary change detection aims to detect changes and group the changes into positive change and negative change. It is of great significance in the joint interpretation of spatial-temporal synthetic aperture radar images. In this study, sparse autoencoder, convolutional neural networks (CNN) and unsupervised clustering are combined to solve ternary change detection problem without any supervison. Firstly, sparse autoencoder is used to transform log-ratio difference image into a suitable feature space for extracting key changes and suppressing outliers and noise. And then the learned features are clustered into three classes, which are taken as the pseudo labels for training a CNN model as change feature classifier. The reliable training samples for CNN are selected from the feature maps learned by sparse autoencoder with certain selection rules. Having training samples and the corresponding pseudo labels, the CNN model can be trained by using back propagation with stochastic gradient descent. During its training procedure, CNN is driven to learn the concept of change, and more powerful model is established to distinguish different types of changes. Unlike the traditional methods, the proposed framework integrates the merits of sparse autoencoder and CNN to learn more robust difference representations and the concept of change for ternary change detection. Experimental results on real datasets validate the effectiveness and superiority of the proposed framework. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
C1 [Gong, Maoguo; Yang, Hailun; Zhang, Puzhao] Xidian Univ, Minist Educ, Int Res Ctr Intelligent Percept & Computat, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Shaanxi Provinc, Peoples R China.
RP Gong, MG (corresponding author), Xidian Univ, Minist Educ, Int Res Ctr Intelligent Percept & Computat, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Shaanxi Provinc, Peoples R China.
FU National Natural Science Foundation of China [61422209]; National Program for Support of Top-notch Young Professionals of China; Specialized Research Fund for the Doctoral Program of Higher Education [20130203110011]
CR Ban YF, 2012, IEEE J-STARS, V5, P1087, DOI 10.1109/JSTARS.2012.2201135
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bovolo F, 2012, IEEE T GEOSCI REMOTE, V50, P2196, DOI 10.1109/TGRS.2011.2171493
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Giustarini L, 2013, IEEE T GEOSCI REMOTE, V51, P2417, DOI 10.1109/TGRS.2012.2210901
   Goh H, 2014, IEEE T NEUR NET LEAR, V25, P2212, DOI 10.1109/TNNLS.2014.2307532
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Gong MG, 2014, IEEE T FUZZY SYST, V22, P98, DOI 10.1109/TFUZZ.2013.2249072
   Gong MG, 2015, IEEE T NEUR NET LEAR, V26, P3263, DOI 10.1109/TNNLS.2015.2469673
   Gong MG, 2016, IEEE T GEOSCI REMOTE, V54, P7077, DOI 10.1109/TGRS.2016.2594952
   Gong MG, 2016, IEEE T NEUR NET LEAR, V27, P125, DOI 10.1109/TNNLS.2015.2435783
   Haixia Yang, 2014, 2014 IEEE GEOSCIENCE AND REMOTE SENSING SYMPOSIUM. (IGARSS). PROCEEDINGS, V0, PP4272, DOI 10.1109/IGARSS.2014.6947433
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hou B, 2014, IEEE J-STARS, V7, P3297, DOI 10.1109/JSTARS.2014.2328344
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Jia L, 2015, IEEE T GEOSCI REMOTE, V53, P3960, DOI 10.1109/TGRS.2015.2388495
   Jin SM, 2013, REMOTE SENS ENVIRON, V132, P159, DOI 10.1016/j.rse.2013.01.012
   Li H, 2016, APPL SOFT COMPUT, V46, P767, DOI 10.1016/j.asoc.2015.10.044
   Li N, 2014, IEEE J-STARS, V7, P3200, DOI 10.1109/JSTARS.2014.2345417
   Liu J, 2016, SOFT COMPUT, V20, P4645, DOI 10.1007/s00500-014-1460-0
   Liu M, 2014, IEEE T GEOSCI REMOTE, V52, P7483, DOI 10.1109/TGRS.2014.2310451
   Lu J, 2015, IEEE J-STARS, V8, P3486, DOI 10.1109/JSTARS.2015.2416635
   Parrilli S, 2012, IEEE T GEOSCI REMOTE, V50, P606, DOI 10.1109/TGRS.2011.2161586
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P2303, DOI 10.1109/TNNLS.2014.2308519
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Su LZ, 2017, PATTERN RECOGN, V66, P213, DOI 10.1016/j.patcog.2017.01.002
   Tang JX, 2015, IEEE T GEOSCI REMOTE, V53, P1174, DOI 10.1109/TGRS.2014.2335751
   Toshev A, 2014, PROC CVPR IEEE, V0, PP1653, DOI 10.1109/CVPR.2014.214
   Wang YT, 2014, IEEE T FUZZY SYST, V22, P1557, DOI 10.1109/TFUZZ.2014.2298244
   Wu C, 2014, IEEE T GEOSCI REMOTE, V52, P2858, DOI 10.1109/TGRS.2013.2266673
   Yu P, 2012, IEEE T GEOSCI REMOTE, V50, P1302, DOI 10.1109/TGRS.2011.2164085
   Yuan Y, 2015, IEEE T NEUR NET LEAR, V26, P2222, DOI 10.1109/TNNLS.2014.2359471
   Yun Lei, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, V0, P1695, DOI 10.1109/ICASSP.2014.6853887
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang H, 2016, IEEE GEOSCI REMOTE S, V13, P1666, DOI 10.1109/LGRS.2016.2601930
   Zhang PZ, 2016, ISPRS J PHOTOGRAMM, V116, P24, DOI 10.1016/j.isprsjprs.2016.02.013
   [Anonymous], 2013, PATTERN RECOGN, V0, P0
NR 38
TC 139
Z9 0
U1 4
U2 121.0
J9 ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
PD JUL
PY 2017
VL 129
BP 212
EP 225
DI 10.1016/j.isprsjprs.2017.05.001
PG 14
SC PHYSICAL GEOGRAPHY; GEOLOGY; REMOTE SENSING; IMAGING SCIENCE & PHOTOGRAPHIC TECHNOLOGY
UT WOS:000403860600017
PM 
ER

