<?xml version="1.0" encoding="UTF-8"?>
<searchresult>
<query>Global</query>
<document id="1">
<title>Stacked Sparse Autoencoder Modeling Using the Synergy of Airborne LiDAR and Satellite Optical and SAR Data to Map Forest Above-Ground Biomass</title>
<url>http://dx.doi.org/10.1109/JSTARS.2017.2748341</url>
<snippet>Timely, spatially complete, and reliable forest aboveground biomass (AGB) data are a prerequisite to support forest management and policy formulation. Traditionally, forest AGB is spatially estimated by integrating satellite images, in particular, optical data, with field plots from forest inventory programs. However, field data are limited in remote and unmanaged areas. In addition, optical reflectance usually saturates at high-density biomass level and is subject to cloud contaminations. Thus, this study aimed to developa deep learning based workflow for mapping forest AGB by integrating Landsat 8 and Sentinel-1A images with airborne light detection and ranging (LiDAR) data. A reference AGB map was derived from the wall-to-wall LiDAR data and field measurements. The LiDAR plots-stratified random samples of forest biomass extracted from the LiDAR simulated strips in the reference map-were adopted as a surrogate for traditional field plots. In addition to the deep learning model, i.e., stacked sparse autoencoder network (SSAE), five different prediction techniques including multiple stepwise linear regressions, K-nearest neighbor, support vector machine, back propagation neural networks, and random forest were individually used to establish the relationship between LiDAR-derived forest biomass and the satellite predictors. Optical variables (Landsat 8 OLI), SAR variables (Sentinel-1A), and their combined variables were individually input to the six prediction models. Results showed that the SSAE model had the best performance forthe forest biomass estimation. The combined optical and microwave dataset as explanatory variables improved the modeling performance compared to either the optical-only or microwave-only data, regardless of prediction algorithms. The best mapping accuracy was obtained by the SSAE model with inputs of optical and microwave integrated metrics that yielded R-2 of 0.812, root mean squared error (RMSE) of 21.753 Mg/ha, and relative RMSE (RMSEr) of 14.457&#37;. Overall, the SSAE model with inputs of combined Landsat 8 OLI and Sentinel-1A information could result in accurate estimation of forest biomass by using the stratification-sampled and LiDAR-derived AGB as ground reference data. The modeling workflow has the potential to promote future forest growth monitoring and carbon stock assessment across large areas.
WOS:000418871200021
</snippet>
</document>

<document id="2">
<title>Hyperspectral and LiDAR Data Fusion Using Extinction Profiles and Deep Convolutional Neural Network</title>
<url>http://dx.doi.org/10.1109/JSTARS.2016.2634863</url>
<snippet>This paper proposes a novel framework for the fusion of hyperspectral and light detection and ranging-derived rasterized data using extinction profiles (EPs) and deep learning. In order to extract spatial and elevation information from both the sources, EPs that include different attributes (e.g., height, area, volume, diagonal of the bounding box, and standard deviation) are taken into account. Then, the derived features are fused via either feature stacking or graph-based feature fusion. Finally, the fused features are fed to a deep learning-based classifier (convolutional neural network with logistic regression) to ultimately produce the classification map. The proposed approach is applied to two datasets acquired in Houston, TX, USA, and Trento, Italy. Results indicate that the proposed approach can achieve accurate classification results compared to other approaches. It should be noted that, in this paper, the concept of deep learning has been used for the first time to fuse LiDAR and hyperspectral features, which provides new opportunities for further research.
WOS:000406419400055
</snippet>
</document>

<document id="3">
<title>EXhype: A tool for mineral classification using hyperspectral data</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2016.12.012</url>
<snippet>Various supervised classification algorithms have been developed to classify earth surface features using hyperspectral data. Each algorithm is modelled based on different human expertises. However, the performance of conventional algorithms is not satisfactory to map especially the minerals in view of their typical spectral responses. This study introduces a new expert system named EXhype (Expert system for hyperspectral data classification) to map minerals. The system incorporates human expertise at several stages of its implementation: (i) to deal with intra-class variation; (ii) to identify absorption features; (iii) to discriminate spectra by considering absorption features, non-absorption features and by full spectra comparison; and (iv) finally takes a decision based on learning and by emphasizing most important features. It is developed using a knowledge base consisting of an Optimal Spectral Library, Segmented Upper Hull method, Spectral Angle Mapper (SAM) and Artificial Neural Network. The performance of the EXhype is compared with a traditional, most commonly used SAM algorithm using Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) data acquired over Cuprite, Nevada, USA. A virtual verification method is used to collect samples information for accuracy assessment. Further, a modified accuracy assessment method is used to get a real users accuracies in cases where only limited or desired classes are considered for classification. With the modified accuracy assessment method, SAM and EXhype yields an overall accuracy of 60.35&#37; and 90.75&#37; and the kappa coefficient of 0.51 and 0.89 respec-tively. It was also found that the virtual verification method allows to use most desired stratified random sampling method and eliminates all the difficulties associated with it. The experimental results show that EXhype is not only producing better accuracy compared to traditional SAM but, can also rightly classify the minerals. It is proficient in avoiding misclassification between target classes when applied on minerals. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000394082400008
</snippet>
</document>

<document id="4">
<title>Feature learning and change feature classification based on deep learning for ternary change detection in SAR images</title>
<url>http://dx.doi.org/10.1016/j.isprsjprs.2017.05.001</url>
<snippet>Ternary change detection aims to detect changes and group the changes into positive change and negative change. It is of great significance in the joint interpretation of spatial-temporal synthetic aperture radar images. In this study, sparse autoencoder, convolutional neural networks (CNN) and unsupervised clustering are combined to solve ternary change detection problem without any supervison. Firstly, sparse autoencoder is used to transform log-ratio difference image into a suitable feature space for extracting key changes and suppressing outliers and noise. And then the learned features are clustered into three classes, which are taken as the pseudo labels for training a CNN model as change feature classifier. The reliable training samples for CNN are selected from the feature maps learned by sparse autoencoder with certain selection rules. Having training samples and the corresponding pseudo labels, the CNN model can be trained by using back propagation with stochastic gradient descent. During its training procedure, CNN is driven to learn the concept of change, and more powerful model is established to distinguish different types of changes. Unlike the traditional methods, the proposed framework integrates the merits of sparse autoencoder and CNN to learn more robust difference representations and the concept of change for ternary change detection. Experimental results on real datasets validate the effectiveness and superiority of the proposed framework. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.
WOS:000403860600017
</snippet>
</document>

</searchresult>
