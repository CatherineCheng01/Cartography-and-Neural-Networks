
PT J
AU Sugg, JW
AF Sugg, Johnathan W.
TI Exploratory Geovisualization of the Character and Distribution of American Climate Change Beliefs
SO WEATHER CLIMATE AND SOCIETY
LA English
DT Article
DE Social Science; Climate change; Communications; decision making; Societal impacts; Clustering; Neural networks
ID self-organizing maps; audience segmentation; geographic-variation; public-opinion; perceptions; visualization; communication; proximity; usability; extremes
AB Americans remain polarized about climate change. However, recent scholarship reveals a plurality of climate change opinions among the public, with nontrivial support for a range of awareness, risk perceptions, and policy prescriptions. This study uses publicly available opinion estimates to examine the geographic variability of American climate change opinions and maps them as regions that share similarities or differences in the character of their beliefs. The exploratory geovisual environment of a self-organizing map is used to compare the support for 56 different climate opinions across all counties in the United States and arrange them into a spatially coherent grid of nodes. To facilitate the exploration of the patterns, a statistical cluster analysis groups together counties with the most similar climate beliefs. Choropleth maps visualize the clustering results from the self-organizing map. This study finds six groups of climate beliefs in which member counties exhibit a distinct regionality across the United States and share similarities in the magnitude of support for specific opinions. Groups that generally exhibit high or low levels of support for climate change awareness, risk perceptions, and policy prescriptions vary in their relative support for specific opinions. The results provide a nuanced understanding of different types of climate change opinions and where they exist geographically.
C1 [Sugg, Johnathan W.] Appalachian State Univ, Dept Geog & Planning, Boone, NC 28608 USA.
C3 University of North Carolina; Appalachian State University
RP Sugg, JW (corresponding author), Appalachian State Univ, Dept Geog & Planning, Boone, NC 28608 USA.
EM suggjw@appstate.edu
CR AGARWAL P, 2008, SELF ORG MAPS APPL G, V0, P0
   Andrienko G, 2010, COMPUT GRAPH FORUM, V29, P913, DOI 10.1111/j.1467-8659.2009.01664.x
   Barnes AP, 2012, CLIMATIC CHANGE, V112, P507, DOI 10.1007/s10584-011-0226-2
   Bashir MF, 2020, SCI TOTAL ENVIRON, V728, P0, DOI 10.1016/j.scitotenv.2020.138835
   Boykoff MT, 2013, AM BEHAV SCI, V57, P796, DOI 10.1177/0002764213476846
   Brugger A, 2015, NAT CLIM CHANGE, V5, P1031, DOI 10.1038/NCLIMATE2760
   Budayan C, 2009, EXPERT SYST APPL, V36, P11772, DOI 10.1016/j.eswa.2009.04.022
   Capstick S, 2015, WIRES CLIM CHANGE, V6, P35, DOI 10.1002/wcc.321
   Carmichael JT, 2017, CLIMATIC CHANGE, V141, P599, DOI 10.1007/s10584-017-1908-1
   Chryst B, 2018, ENVIRON COMMUN, V12, P1109, DOI 10.1080/17524032.2018.1508047
   Crampton JW, 2002, CARTOGR GEOGR INF SC, V29, P85, DOI 10. 1559/152304002782053314
   Delmelle E, 2013, URBAN STUD, V50, P923, DOI 10.1177/0042098012458003
   Detenber BH, 2016, INT J COMMUN-US, V10, P4736
   Dunlap RE, 2011, OXFORD HDB CLIMATE C, V0, PP144, DOI 10.1093/OXFORDHB/9780199566600.003.0010
   Dunlap RE, 2013, AM BEHAV SCI, V57, P691, DOI 10.1177/0002764213477097
   Feldman L, 2017, PUBLIC UNDERST SCI, V26, P481, DOI 10.1177/0963662515595348
   Feldman L, 2012, INT J PRESS/POLIT, V17, P3, DOI 10.1177/1940161211425410
   Fielding KS, 2016, FRONT PSYCHOL, V7, P0, DOI 10.3389/fpsyg.2016.00121
   Gao PC, 2019, J SPAT SCI, V64, P239, DOI 10.1080/14498596.2018.1440649
   Gibson PB, 2016, CLIM DYNAM, V47, P2235, DOI 10.1007/s00382-015-2961-y
   Goldberg M., 2020, 1 TIME ALARMED ARE N, V0, P0
   Hagenauer J, 2013, INT J GEOGR INF SCI, V27, P2026, DOI 10.1080/13658816.2013.788249
   Hamilton LC, 2016, SOCIOLOGY, V50, P913, DOI 10.1177/0038038516648547
   Hamilton LC, 2009, INT J CLIMATOL, V29, P2348, DOI 10.1002/joc.1930
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Hart PS, 2012, COMMUN RES, V39, P701, DOI 10.1177/0093650211416646
   Hepburn C., 2020, SMITH SCH WORKING PA, V0, P0
   Hine DW, 2014, WIRES CLIM CHANGE, V5, P441, DOI 10.1002/wcc.279
   Howe PD, 2015, NAT CLIM CHANGE, V5, P596, DOI 10.1038/nclimate2583
   Howe PD, 2013, GLOBAL ENVIRON CHANG, V23, P1488, DOI 10.1016/j.gloenvcha.2013.09.014
   Hulme M, 2009, WHY WE DISAGREE ABOUT CLIMATE CHANGE: UNDERSTANDING CONTROVERSY, V0, P1
   Hulme M, 2020, WIRES CLIM CHANGE, V11, P0, DOI 10.1002/wcc.619
   Hulme M, 2019, ISSUES SCI TECHNOL, V36, P23
   Janetzko H., 2013, P 3 INT C ADV INF MI, V0, P12
   Jones MD, 2014, POLIT PSYCHOL, V35, P447, DOI 10.1111/pops.12057
   Joslyn S, 2019, WEATHER CLIM SOC, V11, P651, DOI 10.1175/WCAS-D-18-0126.1
   Kahan D, 2012, NATURE, V488, P255, DOI 10.1038/488255a
   Kahan DM, 2015, POLIT PSYCHOL, V36, P1, DOI 10.1111/pops.12244
   Kaufman L.R.P., 1990, FINDING GROUPS DATA, V0, P0, DOI DOI 10.1002/9780470316801.CH2
   Kohonen T., 2001, SELF ORG MAPS, V0, P0, DOI DOI 10.1007/978-3-642-56927-2
   Koua EL, 2006, INT J GEOGR INF SCI, V20, P425, DOI 10.1080/13658810600607550
   Koua E.L., 2003, P 21 INT CART REN IC, V0, P1694
   Koua EL, 2004, CARTOGR J, V41, P217, DOI 10.1179/000870404X13283
   Lee TM, 2015, NAT CLIM CHANGE, V5, P1014, DOI 10.1038/NCLIMATE2728
   Leiserowitz A., 2017, TRUMP VOTERS GLOBAL, V0, P0
   LEISEROWITZ A, 2010, RACE ETHNICITY PUBLI, V0, P0
   Leiserowitz AA, 2013, AM BEHAV SCI, V57, P818, DOI 10.1177/0002764212458272
   Lin CT, 1999, J MANAGE INFORM SYST, V16, P57, DOI 10.1080/07421222.1999.11518256
   Lyons BA, 2018, ENVIRON COMMUN, V12, P876, DOI 10.1080/17524032.2018.1520735
   Maibach EW, 2011, PLOS ONE, V6, P0, DOI 10.1371/journal.pone.0017571
   Marquart-Pyatt ST, 2014, GLOBAL ENVIRON CHANG, V29, P246, DOI 10.1016/j.gloenvcha.2014.10.004
   Marquart-Pyatt ST, 2011, ENVIRONMENT, V53, P38, DOI 10.1080/00139157.2011.588555
   McCright AM, 2016, TOP COGN SCI, V8, P76, DOI 10.1111/tops.12171
   McCright AM, 2011, GLOBAL ENVIRON CHANG, V21, P1163, DOI 10.1016/j.gloenvcha.2011.06.003
   Merkley E, 2018, SCI COMMUN, V40, P258, DOI 10.1177/1075547018760334
   Mildenberger M, 2017, CLIMATIC CHANGE, V145, P539, DOI 10.1007/s10584-017-2103-0
   Milfont TL, 2014, PLOS ONE, V9, P0, DOI 10.1371/journal.pone.0103180
   Murtagh F, 2014, J CLASSIF, V31, P274, DOI 10.1007/s00357-014-9161-z
   Neme A, 2011, J COMPUT SCI-NETH, V2, P345, DOI 10.1016/j.jocs.2011.08.003
   Nisbet MC, 2009, ENVIRONMENT, V51, P12, DOI 10.3200/ENVT.51.2.12-23
   Pearce W, 2017, ENVIRON COMMUN, V11, P723, DOI 10.1080/17524032.2017.1333965
   Pew, 2014, POLITICAL POLARIZATI, V0, P0
   R Core Team, 2020, R LANG ENV STAT COMP, V0, P0
   Ratcliffe M., 2016, DEFINING RURAL US CE, V0, P0
   Rentfrow PJ, 2008, PERSPECT PSYCHOL SCI, V3, P339, DOI 10.1111/j.1745-6924.2008.00084.x
   Retchless DP, 2018, ENVIRON BEHAV, V50, P483, DOI 10.1177/0013916517709043
   Reusch DB, 2005, POLAR GEOGR, V29, P188, DOI 10.1080/789610199
   Rolfe-Redding J.C., 2011, REPUBLICANS CLIMATE, V0, P0, DOI DOI 10.2139/ssrn.2026002
   Roser-renouf C., 2009, GLOBAL WARMINGS 6 AM, V0, P0
   Roth RE, 2010, CARTOGR J, V47, P130, DOI 10.1179/000870409X12488753453372
   Sahin M, 2020, SCI TOTAL ENVIRON, V728, P0, DOI 10.1016/j.scitotenv.2020.138810
   Sester M, 2005, INT J GEOGR INF SCI, V19, P871, DOI 10.1080/13658810500161179
   Sharma A., 2013, ARXIV13093946, V0, P0
   Smith TW, 2017, INT J SOCIOL, V47, P62, DOI 10.1080/00207659.2017.1264837
   Spence A, 2012, RISK ANAL, V32, P957, DOI 10.1111/j.1539-6924.2011.01695.x
   Stover D, 2017, B ATOM SCI, V73, P364, DOI 10.1080/00963402.2017.1388661
   Tosepu R, 2020, SCI TOTAL ENVIRON, V725, P0, DOI 10.1016/j.scitotenv.2020.138436
   Unsworth KL, 2014, GLOBAL ENVIRON CHANG, V27, P131, DOI 10.1016/j.gloenvcha.2014.05.002
   Van Boven L, 2018, PERSPECT PSYCHOL SCI, V13, P492, DOI 10.1177/1745691617748966
   Vesanto J, 2000, IEEE T NEURAL NETWOR, V11, P586, DOI 10.1109/72.846731
   Vesanto J., 1999, INTELL DATA ANAL, V3, P111, DOI 10.1016/S1088-467X(99)00013-X
   Wehrens R, 2018, J STAT SOFTW, V87, P1, DOI 10.18637/iss.v087.i07
   Wehrens R, 2007, J STAT SOFTW, V21, P1
NR 83
TC 2
Z9 2
U1 0
U2 4
PU AMER METEOROLOGICAL SOC
PI BOSTON
PA 45 BEACON ST, BOSTON, MA 02108-3693, UNITED STATES
SN 1948-8327
EI 1948-8335
J9 WEATHER CLIM SOC
JI Weather Clim. Soc.
PD JAN 15
PY 2021
VL 13
IS 1
BP 67
EP 82
DI 10.1175/WCAS-D-20-0071.1
PG 16
WC Environmental Studies; Meteorology & Atmospheric Sciences
SC Environmental Sciences & Ecology; Meteorology & Atmospheric Sciences
GA SY3JS
UT WOS:000665787800006
DA 2023-04-26
ER

PT J
AU Wang, YM
   Zhang, Z
   Feng, LW
   Ma, YC
   Du, QY
AF Wang, Yumiao
   Zhang, Zhou
   Feng, Luwei
   Ma, Yuchi
   Du, Qingyun
TI A new attention-based CNN approach for crop mapping using time series Sentinel-2 images
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
LA English
DT Article
DE Crop classification; Geographic heterogeneity; Attention-based CNN; Sentinel-2
ID red-edge bands; land-cover; neural-networks; classification; intensification; maize
AB Accurate crop mapping is of great importance for agricultural applications, and deep learning methods have been applied on multi-temporal remotely sensed images to classify crops. However, due to the geographic heterogeneity, the spectral profiles of the same crop can vary spatially, and thus using the spectral features alone can limit the model performance in mapping crops in large scales. Moreover, it is a challenge for traditional deep learning models to accurately capture the important information from a large number of features. To address these issues, in this study, we developed a novel attention-based convolutional neural network (CNN) approach (Geo-CBAM-CNN) for crop classification using time series Sentinel-2 images. Specifically, geographic information of crops was first integrated into an advanced attention module, Convolutional Block Attention Module (CBAM) to form a Geo-CBAM module which can help mitigate the impacts of geographic heterogeneity and restrain unnecessary information. Then, the developed Geo-CBAM module was embedded into a CNN model to boost the model?s attention both spectrally and spatially. The proposed Geo-CBAM-CNN model was validated on four main crops over six counties with different geographic environments in the U.S. Also, it was compared to three other state-of-the-art machine learning approaches, including CBAM-CNN, CNN and Random Forest (RF). The results showed that the proposed model achieved the best performance, reaching 97.82% overall accuracy, 96.82% Kappa coefficient and 96.96% Macro-average F1 score. Moreover, the developed Geo-CBAM-CNN model showed strong spatial adaptability, indicating its superior performance in large scale applications. Furthermore, by visualizing the structure of the Geo-CBAM-CNN, we found that the model automatically allocated different weights to the features, and generally, the red-edge features in the middle of the year obtained more attention.
C1 [Wang, Yumiao; Feng, Luwei; Du, Qingyun] Wuhan Univ, Sch Resources & Environm Sci, Wuhan 430079, Peoples R China.
   [Wang, Yumiao; Zhang, Zhou; Feng, Luwei; Ma, Yuchi] Univ Wisconsin, Biol Syst Engn, Madison, WI 53706 USA.
   [Du, Qingyun] Wuhan Univ, Key Lab Geog Informat Syst, Minist Educ, Wuhan 430079, Peoples R China.
   [Du, Qingyun] Wuhan Univ, Key Lab Digital Mapping & Land Informat Applicat, Minist Nat Resources, Wuhan 430079, Peoples R China.
C3 Wuhan University; University of Wisconsin System; University of Wisconsin Madison; Wuhan University; Ministry of Natural Resources of the People's Republic of China; Wuhan University
RP Zhang, Z (corresponding author), Univ Wisconsin, Biol Syst Engn, Madison, WI 53706 USA.
EM wymfrank@whu.edu.cn; zzhang347@wisc.edu; lwfeng@whu.edu.cn; ma286@wisc.edu; qydu@whu.edu.cn
FU University of Wisconsin-Madison, Office of the Vice Chancellor for Research and Graduate Education; Wisconsin Alumni Research Foundation
CR Abadi Martin, 2016, ARXIV, V0, P0
   Abdi AM, 2020, GISCI REMOTE SENS, V57, P1, DOI 10.1080/15481603.2019.1650447
   Adelabu S, 2014, ISPRS J PHOTOGRAMM, V95, P34, DOI 10.1016/j.isprsjprs.2014.05.013
   BADHWAR GD, 1984, REMOTE SENS ENVIRON, V14, P15, DOI 10.1016/0034-4257(84)90004-X
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bajzelj B, 2014, NAT CLIM CHANGE, V4, P924, DOI 10.1038/NCLIMATE2353
   Bargiel D, 2017, REMOTE SENS ENVIRON, V198, P369, DOI 10.1016/j.rse.2017.06.022
   Bijlsma S, 2006, ANAL CHEM, V78, P567, DOI 10.1021/ac051495j
   Boryan C, 2011, GEOCARTO INT, V26, P341, DOI 10.1080/10106049.2011.562309
   Bradley B., 2007, CURVE FITTING PROCED, V0, P0
   Carletto C, 2015, J AFR ECON, V24, P593, DOI 10.1093/jae/ejv011
   Claverie M., 2018, HARMONIZED LANDSAT S, V0, P0
   Clevers JGPW, 2013, INT J APPL EARTH OBS, V23, P344, DOI 10.1016/j.jag.2012.10.008
   Deng M, 2018, T GIS, V22, P183, DOI 10.1111/tgis.12302
   Dong JW, 2016, REMOTE SENS ENVIRON, V185, P142, DOI 10.1016/j.rse.2016.02.016
   Foley JA, 2005, SCIENCE, V309, P570, DOI 10.1126/science.1111772
   Forkuor G, 2018, GISCI REMOTE SENS, V55, P331, DOI 10.1080/15481603.2017.1370169
   Geerken RA, 2009, ISPRS J PHOTOGRAMM, V64, P422, DOI 10.1016/j.isprsjprs.2009.03.001
   Gorelick N, 2017, REMOTE SENS ENVIRON, V202, P18, DOI 10.1016/j.rse.2017.06.031
   Gourlay S., 2017, COULD DEBATE BE OVER, V0, P0
   Hao PY, 2020, SCI TOTAL ENVIRON, V733, P0, DOI 10.1016/j.scitotenv.2020.138869
   He K., 2015, PROC CVPR IEEE, V5, P6
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hou P, 2014, FIELD CROP RES, V158, P55, DOI 10.1016/j.fcr.2013.12.021
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jonsson P, 2002, IEEE T GEOSCI REMOTE, V40, P1824, DOI 10.1109/TGRS.2002.802519
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128
   Li ZT, 2020, INFRARED PHYS TECHN, V105, P0, DOI 10.1016/j.infrared.2019.103152
   LICHTENTHALER HK, 1987, METHOD ENZYMOL, V148, P350
   Liu YE, 2013, FIELD CROP RES, V144, P192, DOI 10.1016/j.fcr.2013.01.003
   Mathur A, 2008, IEEE GEOSCI REMOTE S, V5, P241, DOI 10.1109/LGRS.2008.915597
   Matson PA, 1997, SCIENCE, V277, P504, DOI 10.1126/science.277.5325.504
   Moghimi A, 2020, COMPUT ELECTRON AGR, V172, P0, DOI 10.1016/j.compag.2020.105299
   OLSSON L, 1994, INT J REMOTE SENS, V15, P3735, DOI 10.1080/01431169408954355
   Pax-Lenney M, 1997, MONITORING AGR LANDS, V0, P0
   Pedregosa F., 2011, J MACH LEARN RES, V12, P2825
   REED BC, 1994, J VEG SCI, V5, P703, DOI 10.2307/3235884
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, V0, PP618, DOI 10.1109/ICCV.2017.74
   Tilman D, 2011, P NATL ACAD SCI USA, V108, P20260, DOI 10.1073/pnas.1116437108
   U. NASS, 2019, CROP PRODUCTION 2018, V0, P0
   USDA, 2020, JUNE AREA, V0, P0
   Vuolo F, 2018, INT J APPL EARTH OBS, V72, P122, DOI 10.1016/j.jag.2018.06.007
   Waldner F, 2015, REMOTE SENS-BASEL, V7, P7959, DOI 10.3390/rs70607959
   Wang F, 2017, PROC CVPR IEEE, V0, PP6450, DOI 10.1109/CVPR.2017.683
   Wang HY, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11141639
   Wang Y., 2020, A HYBRID MODEL CONSI, V0, P0
   Wang Z, 2016, INT J PLANT PROD, V10, P509
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang CH, 2011, COMPUT ELECTRON AGR, V75, P347, DOI 10.1016/j.compag.2010.12.012
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738
   Zhong LH, 2019, REMOTE SENS ENVIRON, V221, P430, DOI 10.1016/j.rse.2018.11.032
   Zhong LH, 2011, INT J REMOTE SENS, V32, P7777, DOI 10.1080/01431161.2010.527397
NR 59
TC 26
Z9 27
U1 26
U2 121
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0168-1699
EI 1872-7107
J9 COMPUT ELECTRON AGR
JI Comput. Electron. Agric.
PD MAY 15
PY 2021
VL 184
IS 
BP 
EP 
DI 10.1016/j.compag.2021.106090
EA MAR 2021
PG 12
WC Agriculture, Multidisciplinary; Computer Science, Interdisciplinary Applications
SC Agriculture; Computer Science
GA RO9HU
UT WOS:000641351000001
DA 2023-04-26
ER

PT J
AU Ran, Q
   Wang, Q
   Zhao, BY
   Wu, YF
   Pu, SL
   Li, ZJ
AF Ran, Qiong
   Wang, Qing
   Zhao, Boya
   Wu, Yuanfeng
   Pu, Shengliang
   Li, Zijin
TI Lightweight Oriented Object Detection Using Multiscale Context and Enhanced Channel Attention in Remote Sensing Images
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Feature extraction; Object detection; Remote sensing; Training; Proposals; Detectors; Visualization; Channel attention; lightweight convolutional neural network (CNN); multiscale context; object detection; remote sensing
AB Object detection is a focal point in remote sensing applications. Remote sensing images typically contain a large number of small objects and a wide range of orientations across objects. This results in great challenges to small object detection approaches based on remote sensing images. Methods directly employ channel relations with equal weights to construct information features leads to inadequate feature representation in complex image small object detection tasks. Multiscale detection methods improve the speed and accuracy of detection, while small objects themselves contain limited information, and the features are easily lost following down-sampling. During the detection, the feature images are independent across scales, resulting in a discontinuity at the detection scale. In this article, we propose the multiscale context and enhanced channel attention (MSCCA) model. MSCCA employs PeleeNet as the backbone network. In particular, the feature image channel attention is enhanced and the multiscale context information is fused with multiscale detection methods to improve the characterization ability of the convolutional neural network. The proposed MSCCA method is evaluated on two real datasets. Results show that for 512 x 512 input images, MSCCA was able to achieve 80.4% and 94.4% mAP on the DOTA and NWPU VHR-10, respectively. Meanwhile, the model size of MSCCA is 21% smaller than that of its predecessor. MSCCA can be considered as a practical lightweight oriented object detection model in remote sensing images.
C1 [Ran, Qiong; Wang, Qing] Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 100029, Peoples R China.
   [Zhao, Boya; Wu, Yuanfeng; Pu, Shengliang; Li, Zijin] Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Digital Earth Sci, Beijing 100094, Peoples R China.
   [Wu, Yuanfeng; Li, Zijin] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100049, Peoples R China.
C3 Beijing University of Chemical Technology; Chinese Academy of Sciences; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Wu, YF (corresponding author), Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Digital Earth Sci, Beijing 100094, Peoples R China.; Wu, YF (corresponding author), Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100049, Peoples R China.
EM ranqiong@mail.buct.edu.cn; 2019210520@mail.buct.edu.cn; zhaoby@aircas.ac.cn; wuyf@radi.ac.cn; pusl@aircas.ac.cn; 1700012409@pku.edu.cn
FU National Natural Science Foundation of China [41871245, 62001455]
CR Acatay O, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), V0, P163
   [Anonymous], 2017, P IEEE C COMP VIS PA, V0, P0, DOI DOI 10.1109/CVPR.2017.106
   Chen Yinpeng, 2020, IEEE C COMP VIS PATT, V0, P0
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Cheng G, 2014, ISPRS J PHOTOGRAMM, V98, P119, DOI 10.1016/j.isprsjprs.2014.10.002
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu C.-Y., 2017, DSSD DECONVOLUTIONAL, V0, P0
   Fu K, 2021, IEEE T GEOSCI REMOTE, V59, P4370, DOI 10.1109/TGRS.2020.3020165
   Gao Z, 2019, IEEE J-STARS, V12, P3552, DOI 10.1109/JSTARS.2019.2933501
   Ghanbari H, 2021, IEEE J-STARS, V14, P3602, DOI 10.1109/JSTARS.2021.3065569
   Girshick R, 2015, IEEE I CONF COMP VIS, V0, PP1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, V0, PP580, DOI 10.1109/CVPR.2014.81
   He, 2021, P IEEE AAAI, V0, P11207
   He KM, 2017, IEEE I CONF COMP VIS, V0, PP2980, DOI 10.1109/TPAMI.2018.2844175
   Hoang T. M., 2020, IEEE ACCESS, V8, P0
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P4340, DOI 10.1109/TGRS.2020.3016820
   Hong DF, 2020, ISPRS J PHOTOGRAMM, V167, P12, DOI 10.1016/j.isprsjprs.2020.06.014
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Jeong J, 2017, BRIT MACH VIS C, V0, P0
   Kong T, 2016, PROC CVPR IEEE, V0, PP845, DOI 10.1109/CVPR.2016.98
   Lee YW, 2019, IEEE COMPUT SOC CONF, V0, PP752, DOI 10.1109/CVPRW.2019.00103
   Li YH, 2019, IEEE I CONF COMP VIS, V0, PP6053, DOI 10.1109/ICCV.2019.00615
   Li YY, 2021, IEEE J-STARS, V14, P2148, DOI 10.1109/JSTARS.2020.3046482
   Li Z., 2017, FSSD FEATURE FUSION, V0, P0
   LIN T, 2018, IEEE T PATTERN ANAL, V42, P2999
   Ma N., 2018, P EUR C COMP VIS ECC, V0, PP122, DOI 10.1007/978-3-030-01264-9_8
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, V0, PP89, DOI 10.1109/ICCV.2011.6126229
   Ming Q., 2020, ARXIV201204150, V0, P0
   Pan X., 2020, P IEEECVF C COMPUTER, V0, P11204
   Pang JM, 2019, PROC CVPR IEEE, V0, PP821, DOI 10.1109/CVPR.2019.00091
   Redmon J., 2016, P IEEE C COMP VIS PA, V0, P0, DOI DOI 10.1109/CVPR.2017.690
   REDMON J, 2016, PROC CVPR IEEE, V0, PP779, DOI 10.1109/CVPR.2016.91
   Redmon J, 2018, ARXIV, V0, P0, DOI DOI 10.1109/CVPR.2017.690
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Singh B, 2018, ADV NEUR IN, V31, P0
   Singh B, 2018, PROC CVPR IEEE, V0, PP3578, DOI 10.1109/CVPR.2018.00377
   Su JH, 2021, IEEE J-STARS, V14, P1389, DOI 10.1109/JSTARS.2020.3044733
   Tang HY, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, V0, P1, DOI 10.1109/QRS.2016.11
   TIAN Z, 2017, IEEE I CONF COMP VIS, V0, P9627
   Tian ZZ, 2019, IEEE J-STARS, V12, P3480, DOI 10.1109/JSTARS.2019.2924086
   Triggs, 2005, PROC CVPR IEEE, V1, P886, DOI 10.1109/CVPR.2005.177
   Wang C, 2019, IEEE GEOSCI REMOTE S, V16, P310, DOI 10.1109/LGRS.2018.2872355
   Wang J, 2021, IEEE J-STARS, V14, P283, DOI 10.1109/JSTARS.2020.3041859
   Wang PJ, 2020, IEEE T GEOSCI REMOTE, V58, P3377, DOI 10.1109/TGRS.2019.2954328
   Wang RJ, 2018, ADV NEUR IN, V31, P0
   Wei Liu, 2016, COMPUTER VISION - ECCV 2016. 14TH EUROPEAN CONFERENCE. PROCEEDINGS: LNCS 9905, V0, PP21, DOI 10.1007/978-3-319-46448-0_2
   Xia GS, 2018, PROC CVPR IEEE, V0, PP3974, DOI 10.1109/CVPR.2018.00418
   Xu CY, 2020, IEEE T GEOSCI REMOTE, V58, P4353, DOI 10.1109/TGRS.2019.2963243
   Yang X., 2020, ARXIV200413316, V0, P0
   Ye J, 2018, ARXIV190505055V1, V0, P0
   Zhang S, 2018, PROC CVPR IEEE, V0, PP4203, DOI 10.1109/CVPR.2018.00442
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, V0, P9259
NR 52
TC 5
Z9 5
U1 7
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2021
VL 14
IS 
BP 5786
EP 5795
DI 10.1109/JSTARS.2021.3079968
PG 10
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA SV0RU
UT WOS:000663535500012
DA 2023-04-26
ER

PT J
AU Roy, SK
   Mondal, R
   Paoletti, ME
   Haut, JM
   Plaza, A
AF Roy, Swalpa Kumar
   Mondal, Ranjan
   Paoletti, Mercedes E.
   Haut, Juan M.
   Plaza, Antonio
TI Morphological Convolutional Neural Networks for Hyperspectral Image Classification
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Feature extraction; Convolution; Data mining; Morphological operations; Kernel; Data models; Three-dimensional displays; Classification; convolutional neural networks (CNNs); deep learning (DL); hyperspectral images (HSIs); latent feature space transfer; morphological transformations
AB Convolutional neural networks (CNNs) have become quite popular for solving many different tasks in remote sensing data processing. The convolution is a linear operation, which extracts features from the input data. However, nonlinear operations are able to better characterize the internal relationships and hidden patterns within complex remote sensing data, such as hyperspectral images (HSIs). Morphological operations are powerful nonlinear transformations for feature extraction that preserve the essential characteristics of the image, such as borders, shape, and structural information. In this article, a new end-to-end morphological deep learning framework (called MorphConvHyperNet) is introduced. The proposed approach efficiently models nonlinear information during the training process of HSI classification. Specifically, our method includes spectral and spatial morphological blocks to extract relevant features from the HSI input data. These morphological blocks consist of two basic 2-D morphological operators (erosion and dilation) in the respective layers, followed by a weighted combination of the feature maps. Both layers can successfully encode the nonlinear information related to shape and size, playing an important role in classification performance. Our experimental results, obtained on five widely used HSIs, reveal that our newly proposed MorphConvHyperNet offers comparable (and even superior) performance when compared to traditional 2-D and 3-D CNNs for HSI classification.
C1 [Roy, Swalpa Kumar] Jalpaiguri Govt Engn Coll, Dept Comp Sci & Engn, Jalpaiguri 735102, India.
   [Mondal, Ranjan] Indian Stat Inst, Elect & Commun Sci Unit, Kolkata 700108, India.
   [Haut, Juan M.] Univ Extremadura, Dept Technol Comp & Commun, Hyperspectral Comp Lab, Caceres 28015, Spain.
   [Paoletti, Mercedes E.; Plaza, Antonio] Natl Distance Educ Univ, Higher Sch Comp Engn, Dept Commun & Control Syst, Madrid 10003, Spain.
C3 Jalpaiguri Government Engineering College; Indian Statistical Institute; Indian Statistical Institute Kolkata; Universidad de Extremadura; Universidad Nacional de Educacion a Distancia (UNED)
RP Plaza, A (corresponding author), Natl Distance Educ Univ, Higher Sch Comp Engn, Dept Commun & Control Syst, Madrid 10003, Spain.
EM swalpa@cse.jgec.ac.in; ranjan15_r@isical.ac.in; mpaoletti@unex.es; jmhaut@scc.uned.es; aplaza@unex.es
FU Junta de Extremadura [GR18060]; Spanish Ministerio de Ciencia e Innovacion [PID2019-110315RB-I00]; European Unionas Horizon 2020 Research and Innovation Program [734541]
CR Akcay HG, 2008, IEEE T GEOSCI REMOTE, V46, P2097, DOI 10.1109/TGRS.2008.916644
   Alipour-Fard T, 2021, IEEE GEOSCI REMOTE S, V18, P1089, DOI 10.1109/LGRS.2020.2990971
   Ben Hamida A, 2018, IEEE T GEOSCI REMOTE, V56, P4420, DOI 10.1109/TGRS.2018.2818945
   Benediktsson JA, 2005, IEEE T GEOSCI REMOTE, V43, P480, DOI 10.1109/TGRS.2004.842478
   Bera S, 2020, INT J REMOTE SENS, V41, P2664, DOI 10.1080/01431161.2019.1694725
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Cho K., 2014, PROC 8 WORKSHOP SYNT, V0, PP103, DOI 10.3115/V1/W14-4012
   Dalla Mura M, 2011, IEEE GEOSCI REMOTE S, V8, P542, DOI 10.1109/LGRS.2010.2091253
   Dalla Mura M, 2010, IEEE T GEOSCI REMOTE, V48, P3747, DOI 10.1109/TGRS.2010.2048116
   Dos Santos J.A., 2019, ARXIV190601751, V0, P0
   Foody GM, 2002, REMOTE SENS ENVIRON, V80, P185, DOI 10.1016/S0034-4257(01)00295-4
   Franchi G, 2020, PATTERN RECOGN, V102, P0, DOI 10.1016/j.patcog.2020.107246
   Green RO, 1998, REMOTE SENS ENVIRON, V65, P227, DOI 10.1016/S0034-4257(98)00064-9
   Haut J. M., 2017, P 17 INT C COMPUTATI, V0, P1063
   Hu W, 2015, J SENSORS, V2015, P0, DOI 10.1155/2015/258619
   Huang X, 2009, INT J REMOTE SENS, V30, P3205, DOI 10.1080/01431160802559046
   King DB, 2015, ACS SYM SER, V1214, P1
   Li ST, 2019, IEEE T GEOSCI REMOTE, V57, P6690, DOI 10.1109/TGRS.2019.2907932
   Makantasis K, 2015, INT GEOSCI REMOTE SE, V0, PP4959, DOI 10.1109/IGARSS.2015.7326945
   Manna S., 2021, IEEE T GEOSCI REMOTE, V59, P7831, DOI 10.1109/TGRS.2020.3043267
   Mei SH, 2019, IEEE T GEOSCI REMOTE, V57, P6808, DOI 10.1109/TGRS.2019.2908756
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Mellouli D, 2019, IEEE T NEUR NET LEAR, V30, P2876, DOI 10.1109/TNNLS.2018.2890334
   Mondal R., 2019, ARXIV190100109, V0, P0
   Paoletti ME, 2019, ISPRS J PHOTOGRAMM, V158, P279, DOI 10.1016/j.isprsjprs.2019.09.006
   Paoletti ME, 2018, ISPRS J PHOTOGRAMM, V145, P120, DOI 10.1016/j.isprsjprs.2017.11.021
   Paoletti ME, 2020, J SUPERCOMPUT, V76, P8866, DOI 10.1007/s11227-020-03187-0
   Ramamurthy M, 2020, MICROPROCESS MICROSY, V79, P0, DOI 10.1016/j.micpro.2020.103280
   Roy Swalpa Kumar, 2022, IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, V60, P0, DOI 10.1109/TGRS.2021.3052048
   Roy SK, 2020, IEEE GEOSCI REMOTE S, V17, P277, DOI 10.1109/LGRS.2019.2918719
   Roy SK, 2018, DIGIT SIGNAL PROCESS, V82, P152, DOI 10.1016/j.dsp.2018.06.016
   Shen Y., 2019, ARXIV190901532, V1909, P01532
   Soille P., 2013, MORPHOLOGICAL IMAGE, V0, P0
   Spinoulas L, 2015, IEEE COMPUT SOC CONF, V0, P0
   Tobar MC, 2007, LECT NOTES COMPUT SC, V4478, P467
   VANDENBOOMGAARD R, 1994, IEEE T PATTERN ANAL, V16, P1101, DOI 10.1109/34.334389
   Wang JW, 2020, IEEE J-STARS, V13, P4133, DOI 10.1109/JSTARS.2020.3008949
   Xu X, 2016, INT GEOSCI REMOTE SE, V0, PP3575, DOI 10.1109/IGARSS.2016.7729926
   Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798
   Zhao WZ, 2016, IEEE T GEOSCI REMOTE, V54, P4544, DOI 10.1109/TGRS.2016.2543748
   Zhong ZL, 2018, IEEE T GEOSCI REMOTE, V56, P847, DOI 10.1109/TGRS.2017.2755542
NR 41
TC 24
Z9 23
U1 3
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2021
VL 14
IS 
BP 8689
EP 8702
DI 10.1109/JSTARS.2021.3088228
PG 14
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA UO4XN
UT WOS:000694698900015
DA 2023-04-26
ER

PT J
AU Kim, E
   Cho, S
AF Kim, Eunji
   Cho, Sungzoon
TI Exposing Fake Faces Through Deep Neural Networks Combining Content and Trace Feature Extractors
SO IEEE ACCESS
LA English
DT Article
DE Feature extraction; Face recognition; Videos; Faces; Information integrity; Image forensics; Media; Convolutional neural networks; DeepFake; Face2Face; fake face detection; fake face image forensics; multi-channel constrained convolution; transfer learning
ID contrast enhancement; forensic detection; jpeg compression; image
AB With the breakthrough of computer vision and deep learning, there has been a surge of realistic-looking fake face media manipulated by AI such as DeepFake or Face2Face that manipulate facial identities or expressions. The fake faces were mostly created for fun, but abuse has caused social unrest. For example, some celebrities have become victims of fake pornography made by DeepFake. There are also growing concerns about fake political speech videos created by Face2Face. To maintain individual privacy as well as social, political, and international security, it is imperative to develop models that detect fake faces in media. Previous research can be divided into general-purpose image forensics and face image forensics. While the former has been studied for several decades and focuses on extracting hand-crafted features of traces left in the image after manipulation, the latter is based on convolutional neural networks mainly inspired by object detection models specialized to extract images' content features. This paper proposes a hybrid face forensics framework based on a convolutional neural network combining the two forensics approaches to enhance the manipulation detection performance. To validate the proposed framework, we used a public Face2Face dataset and a custom DeepFake dataset collected on our own. Experimental results using the two datasets showed that the proposed model is more accurate and robust at various video compression rates compared to the previous methods. Throughout class activation map visualization, the proposed framework provided information on which face parts are considered important and revealed the tempering traces invisible to naked eyes.
C1 [Kim, Eunji] Chung Ang Univ, Sch Business Adm, Seoul 06974, South Korea.
   [Cho, Sungzoon] Seoul Natl Univ, Dept Ind Engn, Seoul 08826, South Korea.
   [Cho, Sungzoon] Seoul Natl Univ, Inst Ind Syst Innovat, Seoul 08826, South Korea.
C3 Chung Ang University; Seoul National University (SNU); Seoul National University (SNU)
RP Cho, S (corresponding author), Seoul Natl Univ, Dept Ind Engn, Seoul 08826, South Korea.; Cho, S (corresponding author), Seoul Natl Univ, Inst Ind Syst Innovat, Seoul 08826, South Korea.
EM zoon@snu.ac.kr
FU National Research Foundation of Korea (NRF) - Korean Government (MSIT) [2021R1G1A1093263]
CR Afchar D, 2018, IEEE INT WORKS INFOR, V0, P0
   Alexander O., 2009, 2009 C VIS MED PROD, V0, PP176, DOI 10.1109/CVMP.2009.29
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bethge M., 2016, PROC CVPR IEEE, V0, PP2414, DOI 10.1109/CVPR.2016.265
   Bianchi T, 2012, IEEE T INF FOREN SEC, V7, P1003, DOI 10.1109/TIFS.2012.2187516
   Bitouk D, 2008, ACM T GRAPHIC, V27, P0, DOI 10.1145/1360612.1360638
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Bohme R., 2013, DIGITAL IMAGE FORENS, V0, P327
   Bregler C., 1997, P353, V0, P0
   Bulat A., 2017, IEEE I CONF COMP VIS, V0, P0, DOI DOI 10.1109/ICCV.2017.116
   Busch C., 2017, IEEE COMPUT SOC CONF, V0, PP1822, DOI 10.1109/CVPRW.2017.228
   Cao G., 2010, IEEE INT CON MULTI, V0, P89
   Chen C., 2011, P INT WORK DIG WAT, V0, P361
   Chollet F., 2017, PROC CVPR IEEE, V0, PP1251, DOI 10.1109/CVPR.2017.195
   Cozzolino D., 2017, P 5 ACM WORKSH INF H, V0, P159
   Dale K, 2011, ACM T GRAPHIC, V30, P0, DOI 10.1145/2024156.2024164
   Dalgaard N., 2010, IEEE IMAGE PROC, V0, P1753
   Dambre J., 2017, IEEE I CONF COMP VIS, V0, PP3697, DOI 10.1109/ICCV.2017.397
   Dang LM, 2019, EXPERT SYST APPL, V129, P156, DOI 10.1016/j.eswa.2019.04.005
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Fridrich, 2015, PROC SPIE, V9409, P0, DOI 10.1117/12.2078399
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Garrido P, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12552
   Garrido P., 2014, PROC CVPR IEEE, V0, PP4217, DOI 10.1109/CVPR.2014.537
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jain AK, 2006, IEEE T INF FOREN SEC, V1, P125, DOI 10.1109/TIFS.2006.873653
   Jones M., 2001, PROC CVPR IEEE, V1, P0
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Kemelmacher-Shlizerman I, 2016, ACM T GRAPHIC, V35, P0, DOI 10.1145/2897824.2925871
   Khodabakhsh A, 2018, 2018 INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP (BIOSIG), V0, P0
   King DB, 2015, ACS SYM SER, V1214, P1
   Kirchner M., 2008, MMSEC08 P MULT SEC, V0, P11
   Kirchner M, 2010, PROC SPIE, V7541, P0, DOI 10.1117/12.839100
   Krizhevsky Alex, 2017, COMMUNICATIONS OF THE ACM, V60, P84, DOI 10.1145/3065386
   Liu W., 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/cvpr.2015.7298594
   Mahdian B, 2008, IEEE T INF FOREN SEC, V3, P529, DOI 10.1109/TIFS.2004.924603
   Neelamani R, 2006, IEEE T IMAGE PROCESS, V15, P1365, DOI 10.1109/TIP.2005.864171
   Oliva A., 2016, PROC CVPR IEEE, V0, PP2921, DOI 10.1109/CVPR.2016.319
   Perez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Qiu X., 2014, P 2 ACM WORKSH INF H, V0, P165
   Qu Z., 2008, INT CONF ACOUST SPEE, V0, P1661
   Rossler A., 2019, IEEE I CONF COMP VIS, V0, PP1, DOI 10.1109/ICCV.2019.00009
   Rossler Andreas, 2018, FACEFORENSICS LARGE, V1, P0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shlens J., 2016, PROC CVPR IEEE, V0, PP2818, DOI 10.1109/CVPR.2016.308
   Simonyan K, 2015, ARXIV, V0, P0
   Stamm M., 2008, IEEE IMAGE PROC, V0, P3112
   Stamm MC, 2010, IEEE T INF FOREN SEC, V5, P492, DOI 10.1109/TIFS.2010.2053202
   Sun JY, 2018, SIGNAL PROCESS-IMAGE, V63, P149, DOI 10.1016/j.image.2018.02.001
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, P0, DOI 10.1145/3072959.3073640
   Thies J, 2015, ACM T GRAPHIC, V34, P0, DOI 10.1145/2816795.2818056
   Wolf L., 2010, PROC CVPR IEEE PROC CVPR IEEE, V0, PP817, DOI 10.1109/CVPR.2010.5540133
   Yao H., 2009, P94, V0, P0
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhou P., 2017, IEEE COMPUT SOC CONF, V0, PP1831, DOI 10.1109/CVPRW.2017.229
NR 60
TC 2
Z9 2
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
EI 
J9 IEEE ACCESS
JI IEEE Access
PD JUN 15
PY 2021
VL 9
IS 
BP 123493
EP 123503
DI 10.1109/ACCESS.2021.3110859
PG 11
WC Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA UQ4ZU
UT WOS:000696074000001
DA 2023-04-26
ER
