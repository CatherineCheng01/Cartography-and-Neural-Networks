
PT J
AU Carreaud, A
   Mariani, F
   Gressin, A
AF Carreaud, A.
   Mariani, F.
   Gressin, A.
TI AUTOMATING THE UNDERGROUND CADASTRAL SURVEY: A PROCESSING CHAIN PROPOSAL
SO XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II
LA English
DT Proceedings Paper
DE Deep learning; Photogrammetry; Automatic 3D segmentation
AB In order to ensure the proper functioning and evolution of underground networks (water, gas, etc.) over time, municipal services need to maintain accurate and up-to-date maps. Such maps are generally updated using traditional data acquisition methods (total station or GNSS), which are time-consuming, expensive, and require several teams of surveyors in the field. In this context, an important topic of research is the automation of the updating of the underground cadastre in order to save time, money, and human effort. In this paper, we present a new method that we developed ranging from the choice of the acquisition system, the tests carried out in the field to the detection of objects and the automatic segmentation in a 3D point cloud. We have chosen to use a convolutional neural network on images for the detection of objects that are part of the underground cadastre. As the next step, objects are projected to obtain a 3D point cloud segmented based on the object type. The vectorization step is still under development so that objects can be converted to vector format and therefore be used for updating the cadastre. The results based on excavation sites with well-represented objects in our training database are excellent, approaching 96% accuracy. However, the detection of rare objects is much less good and thus remains a topic for future research. Overall, the complete processing chain allowing to automate as much as possible the update of an underground cadastre is presented in this paper.
C1 [Carreaud, A.; Gressin, A.] Univ Appl Sci Western Switzerland HES SO HEIG VD, Insit Inst, CH-1400 Yverdon, Switzerland.
   [Mariani, F.] Geneva Ind Serv, Vernier, Switzerland.
C3 University of Applied Sciences & Arts Western Switzerland
RP Carreaud, A (corresponding author), Univ Appl Sci Western Switzerland HES SO HEIG VD, Insit Inst, CH-1400 Yverdon, Switzerland.
EM antoine.carreaud@heig-vd.ch; fabio.mariani@sig-ge.ch; adrien.gressin@heig-vd.ch
FU National Geographic Data Infrastructure (INDG)
CR Barsi A., 2018, INT ARCH PHOTOGRAMM, VXLII-3, P61, DOI 10.5194/isprs-archives-XLII-3-61-2018
   Beniaouf S., 2021, RAPPORT TRAVAIL FIN, V0, P0
   Borrmann A., 2015, ISARC P INT S AUTOMA, V0, PP1, DOI 10.22260/ISARC2015/0034
   Carreaud A., 2021, SWISS GEOSCIENCE M 2, V0, P0
   Carreaud A., 2021, 26 GAZETTE LIGSO, V0, P0
   Cerioni A., 2021, SWIMMING POOL DETECT, V0, P0
   Crommelinck S., 2016, REMOTE SENS-BASEL, V0, P0
   Forlani G, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10020311
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, V0, P1
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hafiz AM, 2020, INT J MULTIMED INF R, V9, P171, DOI 10.1007/s13735-020-00195-x
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Ng A., 2022, DEEP LEARNING SPECIA, V0, P0
   Pellis E., 1900, P429, V0, P0
   Picterra, 2022, GEOSPATIAL INSIGHTS, V0, P0
   Qi C. R., 2016, ARXIV, V0, P0
   Qi CR, 2017, ARXIV, V0, P0
   Wang CS, 2021, DISPLAYS, V70, P0, DOI 10.1016/j.displa.2021.102080
   Wu Y., 2019, DETECTRON 2, V0, P0
NR 19
TC 0
Z9 0
U1 5
U2 5
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2022
VL 43-B2
IS 
BP 565
EP 570
DI 10.5194/isprs-archives-XLIII-B2-2022-565-2022
PG 6
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BT8VT
UT WOS:000855635300078
DA 2023-04-26
ER

PT J
AU Steffen, L
   Weyer, T
   Ulbrich, S
   Roennau, A
   Dillmann, R
AF Steffen, Lea
   Weyer, Tobias
   Ulbrich, Stefan
   Roennau, Arne
   Dillmann, Ruediger
TI Reactive Neural Path Planning with Dynamic Obstacle Avoidance in a Condensed Configuration Space
SO 2022 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS)
LA English
DT Proceedings Paper
ID maps
AB We present a biologically inspired approach for path planning with dynamic obstacle avoidance. Path planning is performed in a condensed configuration space of a robot generated by self-organizing neural networks (SONN). The robot itself and static as well as dynamic obstacles are mapped from the Cartesian task to the configuration space by precomputed kinematics. The condensed space represents a cognitive map of the environment, which is inspired by place cells and the concept of cognitive maps in mammalian brains. Generation of training data as well as the evaluation are performed on a real industrial robot accompanied by simulations. To evaluate reactive collision-free online planning within a changing environment, a demonstrator was realized. Then, a comparative study regarding sample-based planners was carried out. The robot is able to operate in dynamically changing environments and re-plan its motion trajectories within impressing 0.02 seconds, which proofs the real-time capability of our concept.
C1 [Steffen, Lea; Weyer, Tobias; Ulbrich, Stefan; Roennau, Arne; Dillmann, Ruediger] FZI Res Ctr Informat Technol, D-76131 Karlsruhe, Germany.
RP Steffen, L (corresponding author), FZI Res Ctr Informat Technol, D-76131 Karlsruhe, Germany.
EM steffen@fzi.de
FU European Union [945539, SGA3]
CR Baoling Han, 2021, ICGG 2020 - PROCEEDINGS OF THE 19TH INTERNATIONAL CONFERENCE ON GEOMETRY AND GRAPHICS. ADVANCES IN INTELLIGENT SYSTEMS AND COMPUTING (AISC 1296), V0, PP604, DOI 10.1007/978-3-030-63403-2_54
   BROST RC, 1989, PROCEEDINGS - 1989 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOL 1-3, P170, DOI 10.1109/ROBOT.1989.99985
   Cheng K.-T, 2009, ELECT DESIGN AUTOMAT, V0, P173
   dAvella A, 2003, NAT NEUROSCI, V6, P300, DOI 10.1038/nn1010
   Derdikman D, 2010, TRENDS COGN SCI, V14, P561, DOI 10.1016/j.tics.2010.09.004
   Estevez PA, 2009, LECT NOTES COMPUT SC, V5629, P63, DOI 10.1007/978-3-642-02397-2_8
   Fritzke B., 1995, ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 7, V0, P625
   Hermann A, 2014, IEEE INT C INT ROBOT, V0, PP4154, DOI 10.1109/IROS.2014.6943148
   Hsu D, 2003, IEEE INT CONF ROBOT, V0, P4420
   Kiesel S, 2017, IEEE INT C INT ROBOT, V0, P2864
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kuffner J. J. Jr., 2000, PROCEEDINGS 2000 ICRA. MILLENNIUM CONFERENCE. IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION. SYMPOSIA PROCEEDINGS (CAT. NO.00CH37065), V0, PP995, DOI 10.1109/ROBOT.2000.844730
   Miljkovic D, 2017, 2017 40TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, V0, P1061, DOI 10.23919/MIPRO.2017.7973581
   MILTON C, 1994, J INTELL ROBOT SYST, V11, P67, DOI 10.1007/BF01258294
   OKeefe J., 1978, HIPPOCAMPUS COGNITIV, V0, P0
   Orthey A., 2021, IEEE T ROBOTICS, V0, P0
   Pan J., 2015, ENGINEERING, V1, P0
   Quigley M, 2009, IEEE INT CONF ROBOT, V0, P3604
   Ravankar A, 2018, SENSORS-BASEL, V18, P0, DOI 10.3390/s18093170
   Steffen L., 2021, ICARM, V0, P0
   Steffen L., 2022, COMP SELF ORG UNPUB, V17, P0
   Sucan IA, 2012, IEEE ROBOT AUTOM MAG, V19, P72, DOI 10.1109/MRA.2012.2205651
   Szkandera J, 2020, LECT NOTES COMPUT SC, V12137, P459, DOI 10.1007/978-3-030-50371-0_34
   Thomas D., 2014, ROSCON, V0, P0
   TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626
   Van Hulle M. M., 2012, TECH REP, V0, P0
   Varadhan G, 2006, IEEE INT CONF ROBOT, V0, PP3041, DOI 10.1109/ROBOT.2006.1642164
   Wang CY, 2021, FRONT ENV SCI ENG, V15, P0, DOI 10.1007/s11783-021-1398-2
   Ward J., 2007, ICRA, V0, P0
   Wise K. D., 2001, INT J ROBOT RES, V19, P762
   Wu XJ, 2005, 2005 12TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS, V0, P90
   Xie YW, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20092640
NR 32
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2153-0858
EI 
J9 IEEE INT C INT ROBOT
PD JUN 15
PY 2022
VL 0
IS 
BP 8101
EP 8108
DI 10.1109/IROS47612.2022.9981453
PG 8
WC Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Robotics
SC Automation & Control Systems; Computer Science; Engineering; Robotics
GA BU4YE
UT WOS:000909405301001
DA 2023-04-26
ER

PT J
AU Ghanghas, A
   Dey, S
   Merwade, V
AF Ghanghas, Ankit
   Dey, Sayan
   Merwade, Venkatesh
TI Evaluating the reliability of synthetic rating curves for continental scale flood mapping
SO JOURNAL OF HYDROLOGY
LA English
DT Article
DE Synthetic rating curve; HAND; Flood mapping; Deep learning application; Continental scale
ID nearest drainage; geometry; hand; regions; height; amazon; model
AB Empirical approaches such as the Height Above Nearest Drainage method in conjunction with Synthetic Rating Curves (HAND-SRC) have emerged as particularly appealing alternatives to the traditional flood mapping techniques due to their lower complexity and fewer data requirements. However, SRCs use Digital Elevation Model (DEM) derived reach averaged hydraulic properties and assume one dimensional steady state flow condition with normal depth. These implicit model assumptions may introduce errors in flood stage and extent estimates using the HAND-SRC approach. This study investigates the reliability of SRC across continental United States (CONUS) by comparing them to the United States Geological Survey's (USGS) gauge rating curves. Results from this comparison show that the implicit model assumptions used in the SRC-HAND approach add significant error in the SRC derivation. The accuracy of the SRC is found to be related to the stream characteristics, including the bathymetry area, slope of the main channel two-year flow and drainage area. Results also show that SRCs in coastal areas, characterized by low slopes and large drainage areas, have higher error and tend to overpredict the stage height in comparison to the USGS rating curves; whereas they tend to underpredict stage height in the mountainous regions. The SRCs are most reliable for the midwestern plains of Ohio, Mid Atlantic, Tennessee and Upper Mississippi regions, and least reliable (higher error) for the Rocky Mountains. Further, the study finds that Deep Neural Network models can be effectively used to judge the performance of SRC for ungauged river reaches.
C1 [Ghanghas, Ankit; Dey, Sayan; Merwade, Venkatesh] Purdue Univ, Lyles Sch Civil Engn, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University; Purdue University West Lafayette Campus
RP Ghanghas, A (corresponding author), Purdue Univ, Lyles Sch Civil Engn, W Lafayette, IN 47907 USA.
EM aghangha@purdue.edu
CR Biancamaria S, 2016, SURV GEOPHYS, V37, P307, DOI 10.1007/s10712-015-9346-y
   Bieger K, 2015, J AM WATER RESOUR AS, V51, P842, DOI 10.1111/jawr.12282
   Blackburn-Lynch W, 2017, J AM WATER RESOUR AS, V53, P903, DOI 10.1111/1752-1688.12540
   Canova M. G., 2016, **DATA OBJECT**, V0, P0, DOI DOI 10.5066/F7D798H6
   Dey S., 2016, THESIS, V0, P940
   Dey S, 2019, J HYDROL, V575, P838, DOI 10.1016/j.jhydrol.2019.05.085
   Dunne T, 1978, WATER ENV PLANNING, V0, P0
   Fekete B. M, 2007, PREDICTIONS UNGAUGED, V0, P129
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Garousi-Nejad I, 2019, WATER RESOUR RES, V55, P7983, DOI 10.1029/2019WR024837
   Godbout L, 2019, J AM WATER RESOUR AS, V55, P952, DOI 10.1111/1752-1688.12783
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, V0, P163
   Grimaldi S, 2018, WATER RESOUR RES, V54, P1031, DOI 10.1002/2017WR021765
   Hallegatte S, 2013, NAT CLIM CHANGE, V3, P802, DOI 10.1038/NCLIMATE1979
   Hinton G.E., 2012, ARXIV PREPRINT ARXIV, V0, P0
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jafarzadegan K, 2020, J HYDROL, V589, P0, DOI 10.1016/j.jhydrol.2020.125165
   Johnson J. M., 2017, NATL WATER CTR INNOV, V0, P0, DOI DOI 10.4211/technical.20171009
   Johnson JM, 2019, NAT HAZARD EARTH SYS, V19, P2405, DOI 10.5194/nhess-19-2405-2019
   Jung HC, 2010, EARTH SURF PROC LAND, V35, P294, DOI 10.1002/esp.1914
   Khandelwal A., 2020, ARXIV PREPRINT ARXIV, V0, P0
   King DB, 2015, ACS SYM SER, V1214, P1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leopold L. B., 1953, US GEOLOGICAL SURVEY, V0, P0
   Liu YY, 2018, J AM WATER RESOUR AS, V54, P770, DOI 10.1111/1752-1688.12660
   Loshchilov I., 2016, INT C LEARN REPR, V0, P0
   Maidment DR, 2017, J AM WATER RESOUR AS, V53, P245, DOI 10.1111/1752-1688.12474
   Neal JC, 2015, J HYDROL, V529, P169, DOI 10.1016/j.jhydrol.2015.07.026
   Nobre AD, 2011, J HYDROL, V404, P13, DOI 10.1016/j.jhydrol.2011.03.051
   Nobre AD, 2016, HYDROL PROCESS, V30, P320, DOI 10.1002/hyp.10581
   Rahmati O, 2018, ENVIRON MODELL SOFTW, V102, P1, DOI 10.1016/j.envsoft.2018.01.004
   Renno CD, 2008, REMOTE SENS ENVIRON, V112, P3469, DOI 10.1016/j.rse.2008.03.018
   Rodda HJE, 2005, NAT HAZARDS, V36, P207, DOI 10.1007/s11069-004-4549-4
   Ruder S., 2016, OVERVIEW GRADIENT DE, V2016, P0
   Scriven BWG, 2021, NAT HAZARDS, V109, P1629, DOI 10.1007/s11069-021-04892-6
   Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159
   Trigg MA, 2009, J HYDROL, V374, P92, DOI 10.1016/j.jhydrol.2009.06.004
   U.S. Environmental Protection Agency (USEPA) and the U.S. Geological Survey (USGS), 2012, NAT HYDR DAT PLUS NH, V0, P0
   U.S. Geological Survey, 2013, 3D EL PROGR 1 3 ARC, V0, P0
   U.S. Geological Survey, 2019, **DATA OBJECT**, V0, P0, DOI DOI 10.5066/P937PN4Z
   U.S. Geological Survey U.S. Department of Agriculture-Natural Resource Conservation Service (NRCS) & U.S. Environmental Protection Agency (EPA), 2020, USGS NAT WAT BOUND D, V0, P0
   US Army Corps of Hydraulic Engineers, 2002, HYDR ENG CTR RIV AN, V0, P0
   Yoon Y, 2012, J HYDROL, V464, P363, DOI 10.1016/j.jhydrol.2012.07.028
   Zheng X, 2018, J AM WATER RESOUR AS, V54, P785, DOI 10.1111/1752-1688.12661
NR 44
TC 1
Z9 1
U1 1
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0022-1694
EI 1879-2707
J9 J HYDROL
JI J. Hydrol.
PD MAR 15
PY 2022
VL 606
IS 
BP 
EP 
DI 10.1016/j.jhydrol.2022.127470
EA JAN 2022
PG 13
WC Engineering, Civil; Geosciences, Multidisciplinary; Water Resources
SC Engineering; Geology; Water Resources
GA YV6BC
UT WOS:000752810500002
DA 2023-04-26
ER

PT J
AU Zhang, K
   Chen, ZZ
   Li, SN
   Liu, S
AF Zhang, Kao
   Chen, Zhenzhong
   Li, Songnan
   Liu, Shan
TI An efficient saliency prediction model for Unmanned Aerial Vehicle video
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
LA English
DT Article
DE Visual saliency; UAV video analysis; Spatial-temporal features; Prior information
ID visual-attention; spatiotemporal saliency; object detection; neural-network; targets
AB Visual saliency prediction plays an important role in Unmanned Aerial Vehicle (UAV) video analysis tasks. In this paper, an efficient saliency prediction model for UAV video is proposed based on spatial-temporal features, prior information and the relationship of frames. It can achieve high efficiency by designing a simplified network model. Since UAV videos usually cover a wide range of scenes containing various background disturbances, a cascading architecture module is proposed for feature extraction from coarse to fine, in which a saliency related feature sub-network is utilized to obtain useful clues from each frame, then a new convolution block is designed to capture spatial-temporal features. This structure can achieve advanced performance and high speed based on a 2D CNN framework. Moreover, a multi-stream prior module is proposed to model the bias phenomenon in viewing behavior for UAV video scenes. It can automatically learn prior information based on the video context, and can also combine other priors. Finally, based on the spatial-temporal features and learned priors, a temporal weighted average module is proposed to model the inter-frame relationship and generate the final saliency map, which can make the generated saliency maps look smoother in the temporal dimension. The proposed method is compared with 17 state-of-the-art models on two public UAV video saliency prediction datasets. The experimental results demonstrate that our model outperforms other competitors. Source code is available at: https://github.com/zhangkao/IIP_UAVSal_Saliency.
C1 [Zhang, Kao; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Li, Songnan; Liu, Shan] Tencent Media Lab, Shenzhen 518057, Peoples R China.
C3 Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
EM zzchen@ieee.org
FU National Natural Science Foundation of China [62036005]; China Postdoctoral Science Foundation [2021M692463]; Postdoctoral Innovative Research Position Funding of Hubei Province of China, LIESMARS Special Research Funding; Tencent
CR Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Bellitto G, 2021, INT J COMPUT VISION, V129, P3216, DOI 10.1007/s11263-021-01519-y
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Borji A, 2012, PROC CVPR IEEE, V0, PP438, DOI 10.1109/CVPR.2012.6247706
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cong RM, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2021.3123984
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, V0, PP3488, DOI 10.1109/ICPR.2016.7900174
   Droste Richard, 2020, COMPUTER VISION - ECCV 2020 16TH EUROPEAN CONFERENCE. PROCEEDINGS. LECTURE NOTES IN COMPUTER SCIENCE (LNCS 12350), V0, PP419, DOI 10.1007/978-3-030-58558-7_25
   Fan DP, 2019, PROC CVPR IEEE, V0, PP8546, DOI 10.1109/CVPR.2019.00875
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fu CH, 2020, IEEE T GEOSCI REMOTE, V58, P8940, DOI 10.1109/TGRS.2020.2992301
   Fu K, 2020, IEEE T IMAGE PROCESS, V29, P7117, DOI 10.1109/TIP.2020.2998977
   Geng J, 2019, IEEE T GEOSCI REMOTE, V57, P7365, DOI 10.1109/TGRS.2019.2913095
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Gorji S, 2018, PROC CVPR IEEE, V0, PP7501, DOI 10.1109/CVPR.2018.00783
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Han JW, 2014, ISPRS J PHOTOGRAMM, V89, P37, DOI 10.1016/j.isprsjprs.2013.12.011
   Harel J., 2007, P C NEURAL INFORM PR, V0, PP545, DOI 10.7551/MITPRESS/7503.003.0073
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Jiang BY, 2019, IEEE I CONF COMP VIS, V0, PP2000, DOI 10.1109/ICCV.2019.00209
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Jiang M, 2015, PROC CVPR IEEE, V0, PP1072, DOI 10.1109/CVPR.2015.7298710
   Judd T, 2009, IEEE I CONF COMP VIS, V0, PP2106, DOI 10.1109/ICCV.2009.5459462
   Khatoonabadi SH, 2017, MULTIMED TOOLS APPL, V76, P26297, DOI 10.1007/s11042-016-4124-5
   Khatoonabadi SH, 2015, MULTIMED TOOLS APPL, V74, P10057, DOI 10.1007/s11042-015-2802-3
   Khatoonabadi SH, 2015, PROC CVPR IEEE, V0, PP5501, DOI 10.1109/CVPR.2015.7299189
   Kiimmerer M., 2014, ARXIV14111045, V0, P0
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krassanakis V, 2018, DRONES-BASEL, V2, P0, DOI 10.3390/drones2040036
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Kummerer M, 2017, IEEE I CONF COMP VIS, V0, PP4799, DOI 10.1109/ICCV.2017.513
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Leboran V, 2017, IEEE T PATTERN ANAL, V39, P893, DOI 10.1109/TPAMI.2016.2567391
   Li CY, 2019, IEEE T GEOSCI REMOTE, V57, P9156, DOI 10.1109/TGRS.2019.2925070
   Li J, 2020, IEEE T IMAGE PROCESS, V29, P1902, DOI 10.1109/TIP.2019.2946102
   Linardos P., 2019, ARXIV190701869, V0, P0
   Liu D, 2021, IEEE T MULTIMEDIA, V23, P967, DOI 10.1109/TMM.2020.2991523
   Marat S, 2009, INT J COMPUT VISION, V82, P231, DOI 10.1007/s11263-009-0215-3
   Min K, 2019, IEEE I CONF COMP VIS, V0, PP2394, DOI 10.1109/ICCV.2019.00248
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Pan J., 2017, CVPR SCEN UND WORKSH, V0, P0
   Perrin AF, 2020, DRONES-BASEL, V4, P0, DOI 10.3390/drones4010002
   Perrin AF, 2019, LECT NOTES COMPUT SC, V11678, P311, DOI 10.1007/978-3-030-29888-3_25
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Qiu ZF, 2017, IEEE I CONF COMP VIS, V0, PP5534, DOI 10.1109/ICCV.2017.590
   Riche N, 2013, IEEE I CONF COMP VIS, V0, PP1153, DOI 10.1109/ICCV.2013.147
   Sandler M, 2018, PROC CVPR IEEE, V0, PP4510, DOI 10.1109/CVPR.2018.00474
   Seo HJ, 2009, J VISION, V9, P0, DOI 10.1167/9.12.15
   Sun DQ, 2018, PROC CVPR IEEE, V0, PP8934, DOI 10.1109/CVPR.2018.00931
   Sun SY, 2018, PROC CVPR IEEE, V0, PP1390, DOI 10.1109/CVPR.2018.00151
   Tatler BW, 2007, J VISION, V7, P0, DOI 10.1167/7.14.4
   Tran D, 2018, PROC CVPR IEEE, V0, PP6450, DOI 10.1109/CVPR.2018.00675
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Vig E, 2014, PROC CVPR IEEE, V0, PP2798, DOI 10.1109/CVPR.2014.358
   Wan X, 2019, IEEE T GEOSCI REMOTE, V57, P1311, DOI 10.1109/TGRS.2018.2865961
   Wang M, 2019, IEEE T GEOSCI REMOTE, V57, P1245, DOI 10.1109/TGRS.2018.2856923
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2018, PROC CVPR IEEE, V0, PP4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   White BJ, 2017, NAT COMMUN, V8, P0, DOI 10.1038/ncomms14263
   Wu HL, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2020.3042515
   Wu XY, 2020, AAAI CONF ARTIF INTE, V34, P12410
   Yang HY, 2019, ISPRS J PHOTOGRAMM, V147, P65, DOI 10.1016/j.isprsjprs.2018.10.017
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhang K, 2021, IEEE T IMAGE PROCESS, V30, P572, DOI 10.1109/TIP.2020.3036749
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhang LB, 2017, IEEE GEOSCI REMOTE S, V14, P2433, DOI 10.1109/LGRS.2017.2768070
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang LY, 2008, J VISION, V8, P0, DOI 10.1167/8.7.32
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhao JX, 2019, IEEE I CONF COMP VIS, V0, PP8778, DOI 10.1109/ICCV.2019.00887
   Zhong YF, 2019, ISPRS J PHOTOGRAMM, V151, P207, DOI 10.1016/j.isprsjprs.2019.02.021
   Zhou SC, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), V0, PP406, DOI 10.1109/BigMM.2015.16
   Zhou XF, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2021.3091312
   Zhu GB, 2016, AAAI CONF ARTIF INTE, V0, P3690
NR 79
TC 0
Z9 0
U1 17
U2 17
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0924-2716
EI 1872-8235
J9 ISPRS J PHOTOGRAMM
JI ISPRS-J. Photogramm. Remote Sens.
PD DEC 15
PY 2022
VL 194
IS 
BP 152
EP 166
DI 10.1016/j.isprsjprs.2022.10.008
EA OCT 2022
PG 15
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA 6D0TD
UT WOS:000882413500002
DA 2023-04-26
ER

PT J
AU Fang, Y
   Xu, LL
   Chen, YH
   Zhou, W
   Wong, A
   Clausi, DA
AF Fang, Yuan
   Xu, Linlin
   Chen, Yuhao
   Zhou, Wei
   Wong, Alexander
   Clausi, David A.
TI A Bayesian Deep Image Prior Downscaling Approach for High-Resolution Soil Moisture Estimation
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Spatial resolution; Data models; Bayes methods; Image resolution; Electronics packaging; Training; Inverse problems; Bayesian; convolutional neural network; deep image prior; MODIS; SMAP; soil moisture downscaling
ID regression; model
AB Soil moisture (SM) estimation is a critical part of environmental and agricultural monitoring, with satellite-based microwave remote sensing being the main SM source. However, the limited spatial resolution of most current remote sensing SM products reduces their utility for many applications, such as evapotranspiration modeling and agriculture management. In this article, to address this issue, we propose a Bayesian deep image prior (BDIP) downscaling approach to estimate the high-resolution SM from satellite products. More specifically, the high-resolution SM estimation problem is formulated as a maximum a posteriori problem, and solved via a neural network comprising of a deep fully convolutional neural network (FCNN) for modeling the prior spatial correlation distribution of the underlying high-resolution SM variables, and a forward model characterizing the SM map degeneration process for modeling the data likelihood. As such, the proposed BDIP approach provides a statistical framework that integrates deep learning with forward modeling in a coherent manner for combining different sources of information, i.e., the knowledge in the forward model, the spatial correlation prior in FCNN architecture, and the remote sensing data and products. Experiments on the downscaling of SM active passive SM products using the moderate resolution imaging spectroradiometer products show that SM maps estimated using the proposed method provide greater spatial detail information than other downscaling methods, with the SM estimates very close to in situ measurements.
C1 [Fang, Yuan; Xu, Linlin; Chen, Yuhao; Wong, Alexander; Clausi, David A.] Univ Waterloo, Dept Syst Design Engn, Waterloo, ON N2L 3G1, Canada.
   [Zhou, Wei] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
C3 University of Waterloo; University of Waterloo
RP Xu, LL (corresponding author), Univ Waterloo, Dept Syst Design Engn, Waterloo, ON N2L 3G1, Canada.
EM yuan.fang@uwaterloo.ca; l44xu@uwaterloo.ca; yuhao.chen1@uwaterloo.ca; wei.zhou@uwaterloo.ca; alexander.wong@uwaterloo.ca; dclausi@uwaterloo.ca
FU Natural Sciences and Engineering Research Council of Canada (NSERC) [RGPIN-2017-04869, DGDND-2017-00078, RGPAS2017-50794, RGPIN-2019-06744]
CR Alemohammad SH, 2018, HYDROL EARTH SYST SC, V22, P5341, DOI 10.5194/hess-22-5341-2018
   Bai YN, 2020, SIGNAL PROCESS, V177, P0, DOI 10.1016/j.sigpro.2020.107729
   Batson J, 2019, PR MACH LEARN RES, V97, P0
   Bostan E, 2020, OPTICA, V7, P559, DOI 10.1364/OPTICA.389314
   CHAVEZ PS, 1989, PHOTOGRAMM ENG REM S, V55, P339
   Fang B, 2020, J HYDROL, V588, P0, DOI 10.1016/j.jhydrol.2020.125043
   Fang Y, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2022.3151004
   Fang Y, 2018, IEEE J-STARS, V11, P3325, DOI 10.1109/JSTARS.2018.2858008
   Gong K, 2019, IEEE T MED IMAGING, V38, P1655, DOI 10.1109/TMI.2018.2888491
   Hassaballa A. Abdalla, 2013, CASP J APPL SCI RES, V2, P182
   Iwasaki A., 2011, PROC 3 WORKSHOP HYPE, V0, PP1, DOI 10.1109/WHISPERS.2011.6080924
   Jiao LC, 2017, IEEE T GEOSCI REMOTE, V55, P5585, DOI 10.1109/TGRS.2017.2710079
   Jin Y, 2018, IEEE T GEOSCI REMOTE, V56, P2362, DOI 10.1109/TGRS.2017.2778420
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaheil YH, 2008, IEEE T GEOSCI REMOTE, V46, P1375, DOI 10.1109/TGRS.2008.916086
   Kim G, 2002, REMOTE SENS ENVIRON, V83, P400, DOI 10.1016/S0034-4257(02)00044-5
   Kingma Diederik P, 2013, ARXIV13126114, V0, P0
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Krull A, 2019, PROC CVPR IEEE, V0, PP2124, DOI 10.1109/CVPR.2019.00223
   Li YS, 2017, NEUROCOMPUTING, V266, P29, DOI 10.1016/j.neucom.2017.05.024
   Long J., 2015, P IEEE C COMP VIS PA, V0, P3431
   Ma XF, 2020, INT J REMOTE SENS, V41, P2818, DOI 10.1080/01431161.2019.1698079
   McLachlan G. J., 2007, EM ALGORITHM EXTENSI, V382, P0
   Mirza M., 2014, ARXIV14111784, V0, P0
   Narayan U, 2008, WATER RESOUR RES, V44, P0, DOI 10.1029/2006WR005817
   Njoku EG, 2002, IEEE T GEOSCI REMOTE, V40, P2659, DOI 10.1109/TGRS.2002.807008
   Ozkan S, 2018, IEEE IMAGE PROC, V0, PP3313, DOI 10.1109/ICIP.2018.8451420
   Pellenq J, 2003, J HYDROL, V276, P112, DOI 10.1016/S0022-1694(03)00066-0
   Peng J, 2017, WATER-SUI, V9, P0, DOI 10.3390/w9070530
   Rhee J, 2010, REMOTE SENS ENVIRON, V114, P2875, DOI 10.1016/j.rse.2010.07.005
   Ronneberger O., 2015, P MED IM COMP COMP A, V0, P234
   Sabaghy S, 2020, REMOTE SENS ENVIRON, V239, P0, DOI 10.1016/j.rse.2019.111586
   SHETTIGARA VK, 1992, PHOTOGRAMM ENG REM S, V58, P561
   Srivastava PK, 2013, WATER RESOUR MANAG, V27, P3127, DOI 10.1007/s11269-013-0337-9
   Ulyanov D, 2018, PROC CVPR IEEE, V0, PP9446, DOI 10.1109/CVPR.2018.00984
   Vivone G, 2015, IEEE T GEOSCI REMOTE, V53, P2565, DOI 10.1109/TGRS.2014.2361734
   Wang H, 2020, ACM COMPUT SURV, V53, P0, DOI 10.1145/3409383
   Wang H, 2015, AAAI CONF ARTIF INTE, V0, P3052
   Wang TT, 2019, IEEE T IMAGE PROCESS, V28, P227, DOI 10.1109/TIP.2018.2866954
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei ZS, 2019, REMOTE SENS ENVIRON, V225, P30, DOI 10.1016/j.rse.2019.02.022
   Wei-Cheng Liao, 2015, 2015 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA). PROCEEDINGS, V0, PP1, DOI 10.1109/WASPAA.2015.7336916
   Wen FP, 2020, IEEE T GEOSCI REMOTE, V58, P913, DOI 10.1109/TGRS.2019.2941696
   Xu W, 2021, IEEE J-STARS, V14, P4051, DOI 10.1109/JSTARS.2021.3069774
   Yokoya N, 2013, IEEE J-STARS, V6, P419, DOI 10.1109/JSTARS.2012.2208449
   Yokoya N, 2012, IEEE T GEOSCI REMOTE, V50, P528, DOI 10.1109/TGRS.2011.2161320
   Zhang JW, 2017, PROC CVPR IEEE, V0, PP6969, DOI 10.1109/CVPR.2017.737
   Zhao XM, 2018, MED IMAGE ANAL, V43, P98, DOI 10.1016/j.media.2017.10.002
   Zhou KC, 2020, OPT EXPRESS, V28, P12872, DOI 10.1364/OE.379200
NR 49
TC 3
Z9 3
U1 10
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 4571
EP 4582
DI 10.1109/JSTARS.2022.3177081
PG 12
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 2E9HO
UT WOS:000812531800005
DA 2023-04-26
ER
