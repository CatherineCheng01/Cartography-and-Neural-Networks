
PT J
AU Zheng, XT
   Chen, XM
   Lu, XQ
   Sun, BY
AF Zheng, Xiangtao
   Chen, Xiumei
   Lu, Xiaoqiang
   Sun, Bangyong
TI Unsupervised Change Detection by Cross-Resolution Difference Learning
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Image resolution; Feature extraction; Image segmentation; Remote sensing; Mutual information; Learning systems; Data mining; Coupled deep neural network (CDNN); cross-resolution difference; mutual information distance; unsupervised change detection (CD)
ID threshold selection method; multiple-change detection; change vector analysis; images; landsat
AB Change detection (CD) aims to identify the differences between multitemporal images acquired over the same geographical area at different times. With the advantages of requiring no cumbersome labeled change information, unsupervised CD has attracted extensive attention of researchers. Multitemporal images tend to have different resolutions as they are usually captured at different times with different sensor properties. It is difficult to directly obtain one pixelwise change map for two images with different resolutions, so current methods usually resize multitemporal images to a unified size. However, resizing operations change the original information of pixels, which limits the final CD performance. This article aims to detect changes from multitemporal images in the originally different resolutions without resizing operations. To achieve this, a cross-resolution difference learning method is proposed. Specifically, two cross-resolution pixelwise difference maps are generated for the two different resolution images and fused to produce the final change map. First, the two input images are segmented into individual homogeneous regions separately due to different resolutions. Second, each pixelwise difference map is produced according to two measure distances, the mutual information distance and the deep feature distance, between image regions in which the pixel lies. Third, the final binary change map is generated by fusing and binarizing the two cross-resolution difference maps. Extensive experiments on four datasets demonstrate the effectiveness of the proposed method for detecting changes from different resolution images.
C1 [Zheng, Xiangtao; Chen, Xiumei; Lu, Xiaoqiang; Sun, Bangyong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision Mechanics, CAS
RP Lu, XQ (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Peoples R China.
EM luxq666666@gmail.com
FU National Science Fund for Distinguished Young Scholars [61925112]; National Natural Science Foundation of China [61806193, 61772510]; Innovation Capability Support Program of Shaanxi [2020KJXX-091, 2020TD-015]; Key Research and Development Program of Shaanxi [2020ZDLGY04-03]; Funds for International Cooperation and Exchange of the National Natural Science Foundation of China [62011530021]
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Alphan H, 2009, ENVIRON MONIT ASSESS, V151, P327, DOI 10.1007/s10661-008-0274-x
   An L, 2015, IEEE GEOSCI REMOTE S, V12, P1863, DOI 10.1109/LGRS.2015.2432071
   Bao TF, 2020, IEEE GEOSCI REMOTE S, V17, P1797, DOI 10.1109/LGRS.2019.2955309
   Bergamasco L, 2019, PROC SPIE, V11155, P0, DOI 10.1117/12.2533812
   Bovolo F, 2007, IEEE T GEOSCI REMOTE, V45, P218, DOI 10.1109/TGRS.2006.885408
   Carlotto MJ, 2005, IEEE T GEOSCI REMOTE, V43, P374, DOI 10.1109/TGRS.2004.841481
   Celik T, 2009, IEEE GEOSCI REMOTE S, V6, P772, DOI 10.1109/LGRS.2009.2025059
   Chen G, 2014, ISPRS J PHOTOGRAMM, V87, P19, DOI 10.1016/j.isprsjprs.2013.10.007
   Deng JS, 2008, INT J REMOTE SENS, V29, P4823, DOI 10.1080/01431160801950162
   Du B, 2019, IEEE T GEOSCI REMOTE, V57, P9976, DOI 10.1109/TGRS.2019.2930682
   Gao F, 2016, IEEE GEOSCI REMOTE S, V13, P1792, DOI 10.1109/LGRS.2016.2611001
   Gong MG, 2017, IEEE T GEOSCI REMOTE, V55, P2658, DOI 10.1109/TGRS.2017.2650198
   Gueguen L, 2011, IEEE T GEOSCI REMOTE, V49, P4503, DOI 10.1109/TGRS.2011.2141999
   Hou B, 2020, IEEE T GEOSCI REMOTE, V58, P1790, DOI 10.1109/TGRS.2019.2948659
   Huo CL, 2010, IEEE GEOSCI REMOTE S, V7, P118, DOI 10.1109/LGRS.2009.2028438
   Hussain M, 2013, ISPRS J PHOTOGRAMM, V80, P91, DOI 10.1016/j.isprsjprs.2013.03.006
   Kalinicheva E, 2019, LECT NOTES COMPUT SC, V11729, P637, DOI 10.1007/978-3-030-30508-6_50
   Li L, 2016, INT GEOSCI REMOTE SE, V0, PP2873, DOI 10.1109/IGARSS.2016.7729742
   Liu J, 2020, IEEE T NEUR NET LEAR, V31, P876, DOI 10.1109/TNNLS.2019.2910571
   Lu J, 2015, IEEE J-STARS, V8, P3486, DOI 10.1109/JSTARS.2015.2416635
   Lu XQ, 2020, IEEE T GEOSCI REMOTE, V58, P2504, DOI 10.1109/TGRS.2019.2951779
   Lu XQ, 2017, IEEE T CYBERNETICS, V47, P884, DOI 10.1109/TCYB.2016.2531179
   Lv N, 2018, IEEE T IND INFORM, V14, P5530, DOI 10.1109/TII.2018.2873492
   Lv PY, 2018, IEEE T GEOSCI REMOTE, V56, P4002, DOI 10.1109/TGRS.2018.2819367
   Lv ZY, 2021, IEEE GEOSCI REMOTE S, V18, P1284, DOI 10.1109/LGRS.2020.2998684
   Marinelli D, 2019, IEEE T GEOSCI REMOTE, V57, P4913, DOI 10.1109/TGRS.2019.2894339
   Meddens AJH, 2013, REMOTE SENS ENVIRON, V132, P49, DOI 10.1016/j.rse.2013.01.002
   Mercier G, 2006, INT GEOSCI REMOTE SE, V0, PP204, DOI 10.1109/IGARSS.2006.57
   Mignotte M, 2020, IEEE T GEOSCI REMOTE, V58, P8046, DOI 10.1109/TGRS.2020.2986239
   Nielsen AA, 2007, IEEE T IMAGE PROCESS, V16, P463, DOI 10.1109/TIP.2006.888195
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Peng DF, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11111382
   Saha S, 2021, IEEE T GEOSCI REMOTE, V59, P1917, DOI 10.1109/TGRS.2020.3000296
   Saha S, 2019, INT GEOSCI REMOTE SE, V0, PP5033, DOI 10.1109/IGARSS.2019.8900173
   Saha S, 2019, IEEE T GEOSCI REMOTE, V57, P3677, DOI 10.1109/TGRS.2018.2886643
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Theiler J., 2006, ICML WORKSH MACH LEA, V0, P7
   Touati R, 2018, IEEE T GEOSCI REMOTE, V56, P1046, DOI 10.1109/TGRS.2017.2758359
   Volpi M, 2012, IEEE GEOSCI REMOTE S, V9, P1026, DOI 10.1109/LGRS.2012.2189092
   Wan L, 2019, IEEE T GEOSCI REMOTE, V57, P9941, DOI 10.1109/TGRS.2019.2930322
   Wang Q, 2007, IEEE T IMAGE PROCESS, V16, P889, DOI 10.1109/TIP.2007.891794
   Wu TJ, 2018, IEEE GEOSCI REMOTE S, V15, P63, DOI 10.1109/LGRS.2017.2773118
   Xiong BL, 2012, IEEE GEOSCI REMOTE S, V9, P287, DOI 10.1109/LGRS.2011.2166149
   Zhan T, 2020, IEEE T GEOSCI REMOTE, V58, P5653, DOI 10.1109/TGRS.2020.2968098
   Zhang M, 2020, IEEE T GEOSCI REMOTE, V58, P7232, DOI 10.1109/TGRS.2020.2981051
   Zhang PZ, 2019, IEEE T GEOSCI REMOTE, V57, P2277, DOI 10.1109/TGRS.2018.2872509
   Zhao WZ, 2020, IEEE T GEOSCI REMOTE, V58, P2720, DOI 10.1109/TGRS.2019.2953879
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P4799, DOI 10.1109/TGRS.2019.2893115
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P2596, DOI 10.1109/TGRS.2018.2875304
NR 50
TC 42
Z9 42
U1 5
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD JUN 15
PY 2022
VL 60
IS 
BP 
EP 
DI 10.1109/TGRS.2021.3079907
EA MAY 2021
PG 16
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science & Photographic Technology
GA YD8DY
UT WOS:000732756300001
DA 2023-04-26
ER

PT J
AU Zhang, B
   Naya, Y
AF Zhang, Bo
   Naya, Yuji
TI A dataset of human fMRI/MEG experiments with eye tracking for spatial memory research using virtual reality
SO DATA IN BRIEF
LA English
DT Article; Data Paper
DE fMRI; MEG; Cognitive map; Navigation; Episodic memory; Parietal lobe; Medial temporal lobe; Egocentric space
AB A dataset consisting of whole-brain fMRI (functional magnetic resonance imaging)/MEG (magnetoencephalography) images, eye tracking files, and behavioral records from healthy adult human participants when they performed a spatial-memory paradigm in a virtual environment was collected to investigate the neural representation of the cognitive map defined by unique spatial relationship of three objects, as well as the neural dynamics of the cognitive map following the task demand from localizing self-location to remembering the target location relative to the self-body. The dataset, including both fMRI and MEG, was also used to investigate the neural networks involved in representing a target within and outside the visual field. The dataset included 19 and 12 university students at Peking University for fMRI and MEG experiments, respectively (fMRI: 12 women, 7 men; MEG: 4 women, 8 men). The average ages of those participants were 24.9 years (MRI: 18-30 years) and 22.5 years (MEG: 19-25 years), respectively. fMRI BOLD and T1-weighted images were acquired using a 3T Siemens Prisma scanner (Siemens, Erlangen, Germany) equipped with a 20-channel receiver head coil. MEG neuromagnetic data were acquired using a 275-channel MEG system (CTF MEG, Canada). The dataset could be further used to investigate a range of neural mechanisms involved in human spatial cognition or to develop a bioinspired deep neural network to enhance machines' abilities in spatial processing. (C) 2022 The Author(s). Published by Elsevier Inc.
C1 [Naya, Yuji] Peking Univ, Sch Psychol & Cognit Sci, 52 Haidian Rd, Beijing 100805, Peoples R China.
   [Naya, Yuji] Peking Univ, IDG McGovern Inst Brain Res, 52 Haidian Rd, Beijing 100805, Peoples R China.
   [Zhang, Bo] Beijing Acad Artificial Intelligence, Beijing 100084, Peoples R China.
   [Zhang, Bo] Tsinghua Univ, Tsinghua Lab Brain & Intelligence, 160 Chengfu Rd,SanCaiTang Bldg, Beijing 100084, Peoples R China.
   [Naya, Yuji] Peking Univ, Ctr Life Sci, 52 Haidian Rd, Beijing 100805, Peoples R China.
   [Naya, Yuji] Peking Univ, Beijing Key Lab Behav & Mental Hlth, 52 Haidian Rd, Beijing 100805, Peoples R China.
C3 Peking University; Peking University; Tsinghua University; Peking University; Peking University
RP Naya, Y (corresponding author), Peking Univ, Sch Psychol & Cognit Sci, 52 Haidian Rd, Beijing 100805, Peoples R China.; Zhang, B (corresponding author), Beijing Acad Artificial Intelligence, Beijing 100084, Peoples R China.
EM zhangbo@baai.ac.cn; yujin@pku.edu.cn
FU National Natural Science Foundation of China [31421003]; Fundamental Research Funds for the Central Universities, PKU [7100602954]; Innovations of Science and Technology 2030, China [2021ZD0203600]
CR Gorgolewski KJ, 2016, SCI DATA, V3, P0, DOI 10.1038/sdata.2016.44
   McCubbin J, 2004, NEUROL CLIN NEUROPHYSIOL, V2004, P69
   Mildenberger P, 2002, EUR RADIOL, V12, P920, DOI 10.1007/s003300101100
   Nystrom M, 2010, BEHAV RES METHODS, V42, P188, DOI 10.3758/BRM.42.1.188
   SR Research, 2021, EYELINK PROGR GUID D, V0, P0
   VSM MedTech Ltd, 2006, CTF MEG FIL FORM BRO, V0, P0
   Zhang B, 2022, NEUROIMAGE, V252, P0, DOI 10.1016/j.neuroimage.2022.119041
   Zhang B, 2020, CEREB CORTEX, V30, P5356, DOI 10.1093/cercor/bhaa117
NR 8
TC 0
Z9 0
U1 6
U2 11
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 2352-3409
EI 
J9 DATA BRIEF
JI Data Brief
PD AUG 15
PY 2022
VL 43
IS 
BP 
EP 
DI 10.1016/j.dib.2022.108380
PG 8
WC Multidisciplinary Sciences
SC Science & Technology - Other Topics
GA 3H8WH
UT WOS:000832310400018
PM 35789905
DA 2023-04-26
ER

PT J
AU Diao, WX
   Zhang, F
   Wang, HT
   Sun, JD
   Zhang, K
AF Diao, Wenxiu
   Zhang, Feng
   Wang, Haitao
   Sun, Jiande
   Zhang, Kai
TI Pansharpening via Triplet Attention Network With Information Interaction
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Feature extraction; Spatial resolution; Pansharpening; Degradation; Data mining; High frequency; Correlation; Image fusion; information interaction; pansharpening; remote sensing; triplet attention network
ID pan-sharpening method; remote-sensing images; panchromatic images; fusion; transform; cnn
AB Pansharpening aims to obtain high spatial resolution multispectral (MS) images by fusing the spatial and spectral information in low spatial resolution (LR) MS and panchromatic (PAN) images. Recently, deep neural network (DNN) based pansharpening methods have been advanced extensively. Although most DNN-based methods show good performance, it is difficult for them to preserve the spatial details in the fused image. In this article, we propose a new pansharpening method based on a triplet attention network with information interaction to efficiently enhance the spatial and spectral information in the fused image. First, different attention mechanisms are designed to model the spatial and spectral feature properties in LR MS and PAN images. Then, the complementarity among different feature maps is enhanced by information interaction, which promotes the compatibility of features from subnetworks. Finally, we utilize a graph attention module to capture the similarity within feature maps. According to the graph, the informative feature maps are selected to provide more details for the reconstruction of the fused image. Extensive experiments on QuickBird and GeoEye-1 satellite datasets show that the proposed method can produce competitive fused images when compared with some state-of-the-art methods.
C1 [Diao, Wenxiu; Zhang, Feng; Wang, Haitao; Sun, Jiande; Zhang, Kai] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
C3 Shandong Normal University
RP Zhang, K (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM diaowx0920@163.com; fengzhangpl@163.com; haitaowang1996@hotmail.com; jiandesun@hotmail.com; zhangkainuc@163.com
FU Natural Science Foundation of China [61901246]; China Postdoctoral Science Foundation [2019TQ0190, 2019M662432]; Scientific Research Leader Studio of Jinan [2021GXRC081]; Joint Project for Smart Computing of Shandong Natural Science Foundation [ZR2020LZH015]; Ningxia Natural Science Foundation [2021AAC03045]
CR Alparone L, 2008, PHOTOGRAMM ENG REM S, V74, P193, DOI 10.14358/PERS.74.2.193
   Alparone L, 2004, IEEE GEOSCI REMOTE S, V1, P313, DOI 10.1109/LGRS.2004.836784
   Ballester C, 2006, INT J COMPUT VISION, V69, P43, DOI 10.1007/s11263-006-6852-x
   Chang CI, 2000, IEEE T INFORM THEORY, V46, P1927, DOI 10.1109/18.857802
   CHAVEZ PS, 1991, PHOTOGRAMM ENG REM S, V57, P295
   Deng LJ, 2019, INFORM FUSION, V52, P76, DOI 10.1016/j.inffus.2018.11.014
   Dian RW, 2021, INFORM FUSION, V69, P40, DOI 10.1016/j.inffus.2020.11.001
   Dian RW, 2021, IEEE T NEUR NET LEAR, V32, P1124, DOI 10.1109/TNNLS.2020.2980398
   Dian RW, 2019, IEEE T IMAGE PROCESS, V28, P5135, DOI 10.1109/TIP.2019.2916734
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   El-Mezouar MC, 2014, IEEE J-STARS, V7, P1806, DOI 10.1109/JSTARS.2014.2306332
   Fang FM, 2013, IEEE T IMAGE PROCESS, V22, P2822, DOI 10.1109/TIP.2013.2258355
   Garzelli A, 2008, IEEE T GEOSCI REMOTE, V46, P228, DOI 10.1109/TGRS.2007.907604
   Gastineau A, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2021.3060958
   Han XH, 2019, IEEE INT CONF COMP V, V0, PP4330, DOI 10.1109/ICCVW.2019.00533
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   He L, 2019, IEEE J-STARS, V12, P1188, DOI 10.1109/JSTARS.2019.2898574
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Huang W, 2015, IEEE GEOSCI REMOTE S, V12, P1037, DOI 10.1109/LGRS.2014.2376034
   Khan MM, 2008, IEEE GEOSCI REMOTE S, V5, P98, DOI 10.1109/LGRS.2007.909934
   Kim Y, 2017, IEEE GEOSCI REMOTE S, V14, P2295, DOI 10.1109/LGRS.2017.2762427
   Leung Y, 2014, IEEE GEOSCI REMOTE S, V11, P985, DOI 10.1109/LGRS.2013.2284282
   Li H, 2016, IEEE J-STARS, V9, P5715, DOI 10.1109/JSTARS.2016.2584142
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2011, IEEE T GEOSCI REMOTE, V49, P738, DOI 10.1109/TGRS.2010.2067219
   Liu PF, 2016, IEEE T GEOSCI REMOTE, V54, P2235, DOI 10.1109/TGRS.2015.2497966
   Liu QJ, 2021, IEEE T GEOSCI REMOTE, V59, P10227, DOI 10.1109/TGRS.2020.3042974
   Liu XY, 2020, INFORM FUSION, V55, P1, DOI 10.1016/j.inffus.2019.07.010
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Ma JY, 2020, INFORM FUSION, V62, P110, DOI 10.1016/j.inffus.2020.04.006
   Masi G, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8070594
   Meng XC, 2019, INFORM FUSION, V46, P102, DOI 10.1016/j.inffus.2018.05.006
   Ozcelik F, 2021, IEEE T GEOSCI REMOTE, V59, P3486, DOI 10.1109/TGRS.2020.3010441
   Paris C, 2019, IEEE T GEOSCI REMOTE, V57, P4259, DOI 10.1109/TGRS.2018.2890404
   Peng JY, 2021, IEEE T GEOSCI REMOTE, V59, P4957, DOI 10.1109/TGRS.2020.3020162
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Shandoosti HR, 2016, INFORM FUSION, V27, P150, DOI 10.1016/j.inffus.2015.06.006
   Shao ZF, 2018, IEEE J-STARS, V11, P1656, DOI 10.1109/JSTARS.2018.2805923
   Shi Y, 2014, INFORM FUSION, V20, P213, DOI 10.1016/j.inffus.2014.02.005
   Te-Ming Tu, 2001, INFORMATION FUSION, V2, P177, DOI 10.1016/S1566-2535(01)00036-7
   Tu TM, 2004, IEEE GEOSCI REMOTE S, V1, P309, DOI 10.1109/LGRS.2004.834804
   Vivone G, 2015, IEEE T GEOSCI REMOTE, V53, P2565, DOI 10.1109/TGRS.2014.2361734
   Wald L., 2000, PROC 3 C FUSION EART, V0, P99
   Wang LZ, 2017, IEEE T PATTERN ANAL, V39, P2104, DOI 10.1109/TPAMI.2016.2621050
   Wang WQ, 2015, IEEE GEOSCI REMOTE S, V12, P781, DOI 10.1109/LGRS.2014.2361834
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Yang JX, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10050800
   Yang JF, 2017, IEEE I CONF COMP VIS, V0, PP1753, DOI 10.1109/ICCV.2017.193
   Yang SY, 2018, IEEE T NEUR NET LEAR, V29, P3647, DOI 10.1109/TNNLS.2017.2736011
   Yang SY, 2012, INFORM FUSION, V13, P177, DOI 10.1016/j.inffus.2010.09.003
   Yuhas R.H., 1992, PROC SUMMARIES 3 ANN, V1, P147
   Zhang K, 2019, IEEE T GEOSCI REMOTE, V57, P1117, DOI 10.1109/TGRS.2018.2864750
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang K, 2016, IEEE J-STARS, V9, P5740, DOI 10.1109/JSTARS.2015.2475754
   Zhang WW, 2021, IEEE J-STARS, V14, P3719, DOI 10.1109/JSTARS.2021.3068274
   Zheng S, 2008, IEEE T GEOSCI REMOTE, V46, P1313, DOI 10.1109/TGRS.2007.912737
   Zhou HY, 2021, IEEE J-STARS, V14, P6316, DOI 10.1109/JSTARS.2021.3090252
NR 58
TC 2
Z9 2
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 3576
EP 3588
DI 10.1109/JSTARS.2022.3171423
PG 13
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 1I8CP
UT WOS:000797454600006
DA 2023-04-26
ER

PT J
AU Yao, S
   Chen, HN
   Thompson, EJ
   Cifelli, R
AF Yao, Shun
   Chen, Haonan
   Thompson, Elizabeth J.
   Cifelli, Robert
TI An Improved Deep Learning Model for High-Impact Weather Nowcasting
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Radar; Hurricanes; Meteorology; Reflectivity; Deep learning; Predictive models; Meteorological radar; Deep learning; high-impact weather; precipita- tion nowcasting; weather radar
ID identification; tracking; rainfall; titan
AB Accurate nowcasting (short-term prediction, 0-6 h) of high-impact weather, such as landfalling hurricanes and extreme convective precipitation, plays a critical role in natural disaster monitoring and mitigation. A number of nowcasting approaches have been developed in the past few decades, such as optical flow and the tracking radar echoes by correlation system. Most of these mainstream operational techniques are based on radar echo map extrapolation, which determines the velocity and direction of precipitation systems using historical and current radar observations. However, the skill of the traditional extrapolation method decreases rapidly within the first hour. In order to improve nowcasting skill, recent studies have proposed using deep learning methods, such as convolutional recurrent neural network and trajectory gate recurrent unit. But none of these methods focuses on high-impact weather events, and the deep learning models trained based on general precipitation events cannot meet the demand of accurate warnings and decision-making at the scales required for high-impact weather events, such as hurricanes. Using multiradar observations, this article introduces the idea of self-attention and develops a self-attention-based gate recurrent unit (SaGRU) to enhance its generalization capability and scalability in predicting high-impact weather events. In particular, two types of high-impact weather systems, namely, landfalling hurricanes and extreme convective precipitation events, are investigated. Three models are trained based on hurricane events, heavy rainfall (i.e., nonhurricane) events, and all events combined in the southeast United States during 2015 and 2020. The impacts of different data sources on the nowcasting performance are quantified. The evaluation results of nowcasting products show that our SaGRU performs very well in predicting hurricane-induced rainfall. In the new methodology, the data from nonhurricane events are shown to provide useful information in enhancing the nowcasting performance during hurricane events as the model trained by combining all the hurricane and nonhurricane events has the best performance. In addition, this article quantifies the impact of the sequence length of input radar observations on the nowcasting performance, which shows that five consecutive observations are sufficient to obtain a stable model, and even two consecutive observations can produce reasonable results.
C1 [Yao, Shun; Chen, Haonan] Colorado State Univ, Dept Elect & Comp Engn, Ft Collins, CO 80523 USA.
   [Thompson, Elizabeth J.; Cifelli, Robert] NOAA, Phys Sci Lab, Boulder, CO 80305 USA.
C3 Colorado State University; National Oceanic Atmospheric Admin (NOAA) - USA
RP Chen, HN (corresponding author), Colorado State Univ, Dept Elect & Comp Engn, Ft Collins, CO 80523 USA.
EM s.yao@colostate.edu; haonan.chen@colostate.edu; elizabeth.thompson@noaa.gov; rob.cifelli@noaa.gov
FU National Oceanic and Atmospheric Administration (NOAA) [NA19OAR4320073]; Oak Ridge Associated Universities (ORAU) Ralph E. Powe Junior Faculty Enhancement Award
CR Bowler NEH, 2004, J HYDROL, V288, P74, DOI 10.1016/j.jhydrol.2003.11.011
   Chen HN, 2020, IEEE T GEOSCI REMOTE, V58, P982, DOI 10.1109/TGRS.2019.2942280
   Chen HN, 2019, GEOPHYS RES LETT, V46, P10669, DOI 10.1029/2019GL084771
   DIXON M, 1993, J ATMOS OCEAN TECH, V10, P785, DOI 10.1175/1520-0426(1993)010<0785:TTITAA>2.0.CO;2
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Han L, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2021.3100847
   Han L, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2021.3056470
   Han L, 2009, J ATMOS OCEAN TECH, V26, P719, DOI 10.1175/2008JTECHA1084.1
   He KM, 2017, IEEE I CONF COMP VIS, V0, PP2980, DOI 10.1109/TPAMI.2018.2844175
   Kim S, 2017, PROC 7 INT WORKSHOP, V0, P1
   Kingma D. P., 2015, PROC INT C LEARN REP, V0, PP1, DOI 10.1145/1830483.1830503
   Lin ZH, 2020, AAAI CONF ARTIF INTE, V34, P11531
   Long J., 2015, P IEEE C COMPUTER VI, V0, PP3431, DOI 10.48550/ARXIV.1411.4038
   NationalHurricane Center Public Affairs, 1999, NAT OCEAN ATMOSPHERI, V0, P0
   Nielsen-Gammon JW, 2005, PHYS GEOGR, V26, P340, DOI 10.2747/0272-3646.26.5.340
   Pan X, 2021, GEOPHYS RES LETT, V48, P0, DOI 10.1029/2021GL095302
   Paszke A, 2019, ADV NEUR IN, V32, P0
   Peduzzi P, 2012, NAT CLIM CHANGE, V2, P289, DOI 10.1038/NCLIMATE1410
   Radhakrishnan C, 2020, J ATMOS OCEAN TECH, V37, P211, DOI 10.1175/JTECH-D-18-0192.1
   Ramachandran P, 2019, ADV NEUR IN, V32, P0
   RINEHART RE, 1978, NATURE, V273, P287, DOI 10.1038/273287a0
   Sharif HO, 2015, NAT HAZARDS REV, V16, P0, DOI 10.1061/(ASCE)NH.1527-6996.0000145
   Shi XJ, 2015, ADV NEUR IN, V28, P0
   Shi XJ, 2017, ADV NEUR IN, V30, P0
   Smith JA, 2000, J HYDROMETEOROL, V1, P5, DOI 10.1175/1525-7541(2000)001<0005:CRAFIT>2.0.CO;2
   Vaswani A, 2017, ADV NEUR IN, V30, P0
   Wang YB, 2017, ADV NEUR IN, V30, P0
   Zelinsky D.A., 2018, TROPICAL CYCLONE REP, V0, P0
   Zhang H, 2019, PR MACH LEARN RES, V97, P0
   Zhang YH, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13163157
   Zhao H., 2020, 2020 IEEE CVF C COMP, V0, PP10073, DOI 10.1109/CVPR42600.2020.01009
NR 31
TC 2
Z9 2
U1 10
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 7400
EP 7413
DI 10.1109/JSTARS.2022.3203398
PG 14
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 4N2SY
UT WOS:000853871300006
DA 2023-04-26
ER

PT J
AU Bahrami, H
   Homayouni, S
   McNairn, H
   Hosseini, M
   Mahdianpari, M
AF Bahrami, Hazhir
   Homayouni, Saeid
   McNairn, Heather
   Hosseini, Mehdi
   Mahdianpari, Masoud
TI Regional Crop Characterization Using Multi-Temporal Optical and Synthetic Aperture Radar Earth Observations Data
SO CANADIAN JOURNAL OF REMOTE SENSING
LA English
DT Article
ID leaf-area index; spectral reflectance; vegetation index; biomass; wheat; corn; classification; sentinel-1; derivation; networks
AB Crop biophysical parameters, such as Leaf Area Index (LAI) and biomass, are essential for estimating crop productivity, yield modeling, and agronomic management. This study used several features extracted from multi-temporal Sentinel-1 Synthetic Aperture Radar (SAR) and spectral vegetation indices extracted from Sentinel-2 optical data to estimate crop LAI and wet and dry biomass. Various machine learning algorithms, including Random Forest Regression (RFR), Support Vector Regression (SVR), and Artificial Neural Network (ANN), were trained and assessed for three major crops (wheat, soybeans and canola). ANN provided the best accuracy for all wheat parameters and soybean LAI and canola wet biomass and LAI. RFR led to higher accuracy for soybean dry and wet biomass. However, SVR could accurately estimate only canola dry biomass. All data were then pooled to investigate if a single algorithm could estimate biophysical parameters for all crops. The RFR model accurately estimated wet and dry biomass and LAI across all crop types in this scenario. This generic model is fast and accurate and can be easily applied for crop mapping and monitoring over large geographies using cloud computing platforms, such as Google Earth Engine.
C1 [Bahrami, Hazhir; Homayouni, Saeid] Ctr Eau Terre Environm, Inst Natl Rech Sci, Quebec City, PQ, Canada.
   [McNairn, Heather] Agr & Agri Food Canada, Sci & Technol Branch, Ottawa, ON, Canada.
   [Hosseini, Mehdi] Univ Maryland, Dept Geog Sci, College Pk, MD 20742 USA.
   [Mahdianpari, Masoud] C CORE, St John, NF, Canada.
   [Mahdianpari, Masoud] Mem Univ Newfoundland, Dept Elect & Comp Engn, St John, NF, Canada.
C3 University of Quebec; Institut national de la recherche scientifique (INRS); Agriculture & Agri Food Canada; University System of Maryland; University of Maryland College Park; Memorial University Newfoundland
RP Bahrami, H; Homayouni, S (corresponding author), Ctr Eau Terre Environm, Inst Natl Rech Sci, Quebec City, PQ, Canada.
EM hazhir.bahrami@inrs.ca; saeid.homayouni@ete.inrs.ca
FU Quebec Ministry of International Relations and Francophonie
CR Ackermann N., 2014, GROWING STOCK VOLUME, V0, P0
   ASRAR G, 1984, AGRON J, V76, P300, DOI 10.2134/agronj1984.00021962007600020029x
   Baghdadi N, 2009, REMOTE SENS ENVIRON, V113, P1724, DOI 10.1016/j.rse.2009.04.005
   Basak Debasish, 2007, NEURAL INFORM PROCES, V11, P203, DOI 10.1007/978-1-4302-5990-9_4
   Betbeder J, 2016, IEEE J-STARS, V9, P2540, DOI 10.1109/JSTARS.2016.2541169
   Breiman L., 2001, MACHINE LEARNING, V45, P5, DOI 10.1023/A:1010933404324
   Campos-Taberner M, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10081167
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dangerti P., 2017, STAT MACHINE LEARNIN, V0, P0
   Daughtry CST, 2000, REMOTE SENS ENVIRON, V74, P229, DOI 10.1016/S0034-4257(00)00113-9
   Fensholt R, 2003, REMOTE SENS ENVIRON, V87, P111, DOI 10.1016/j.rse.2003.07.002
   Fieuzal R., 2013, ADV REMOTE SENS, V2, P0, DOI 10.4236/ars.2013.22020
   Gahrouei OR, 2020, CAN J REMOTE SENS, V46, P84, DOI 10.1080/07038992.2020.1740584
   Gao BC, 1996, REMOTE SENS ENVIRON, V58, P257, DOI 10.1016/S0034-4257(96)00067-3
   Ghasemi Nafiseh, 2011, INTERNATIONAL JOURNAL OF GEOMATICS AND GEOSCIENCES, V1, P776
   Gitelson AA, 1996, REMOTE SENS ENVIRON, V58, P289, DOI 10.1016/S0034-4257(96)00072-7
   Gitelson AA, 2003, J PLANT PHYSIOL, V160, P271, DOI 10.1078/0176-1617-00887
   Harfenmeister K, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11131569
   Homayouni S, 2019, INT J APPL EARTH OBS, V74, P78, DOI 10.1016/j.jag.2018.09.009
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hosseini M, 2017, INT J APPL EARTH OBS, V58, P50, DOI 10.1016/j.jag.2017.01.006
   Hosseini M, 2015, REMOTE SENS ENVIRON, V170, P77, DOI 10.1016/j.rse.2015.09.002
   Huang GB, 2003, IEEE T NEURAL NETWOR, V14, P274, DOI 10.1109/TNN.2003.809401
   Huang YC, 2016, IEEE T GEOSCI REMOTE, V54, P981, DOI 10.1109/TGRS.2015.2471803
   Huete A, 2002, REMOTE SENS ENVIRON, V83, P195, DOI 10.1016/S0034-4257(02)00096-2
   HUETE A R, 1988, REMOTE SENSING OF ENVIRONMENT, V25, P295
   Hunt ML, 2019, REMOTE SENS ENVIRON, V233, P0, DOI 10.1016/j.rse.2019.111410
   Inoue Y, 2002, REMOTE SENS ENVIRON, V81, P194, DOI 10.1016/S0034-4257(01)00343-1
   Jia MQ, 2014, IEEE J-STARS, V7, P469, DOI 10.1109/JSTARS.2013.2282641
   Jiao XF, 2014, ISPRS J PHOTOGRAMM, V96, P38, DOI 10.1016/j.isprsjprs.2014.06.014
   Jiao XF, 2011, CAN J REMOTE SENS, V37, P69, DOI 10.5589/m11-023
   Jin XL, 2015, REMOTE SENS-BASEL, V7, P13251, DOI 10.3390/rs71013251
   Jin ZN, 2019, REMOTE SENS ENVIRON, V228, P115, DOI 10.1016/j.rse.2019.04.016
   JORDAN CF, 1969, ECOLOGY, V50, P663, DOI 10.2307/1936256
   Kross A, 2015, INT J APPL EARTH OBS, V34, P235, DOI 10.1016/j.jag.2014.08.002
   Luntz A, 1969, TECHNICHESKAYA KIBER, V3, P0
   Madhiarasan M, 2017, ARTIF INTELL REV, V48, P449, DOI 10.1007/s10462-016-9506-6
   Mahdianpari M, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11131582
   Mandal D, 2019, INT J APPL EARTH OBS, V79, P24, DOI 10.1016/j.jag.2019.02.007
   Mandal D, 2018, INT GEOSCI REMOTE SE, V0, P6611
   Mansaray LR, 2020, COMPUT ELECTRON AGR, V176, P0, DOI 10.1016/j.compag.2020.105674
   Mao HH, 2019, APPL SCI-BASEL, V9, P0, DOI 10.3390/app9071459
   McNairn H, 2004, CAN J REMOTE SENS, V30, P517, DOI 10.5589/m03-068
   Mcnairn H., 2016, EXPT PLAN SMAP VALID, V0, P0
   McNairn H, 2016, REMOTE SENS DIGIT IM, V20, P317, DOI 10.1007/978-3-319-47037-5_15
   Ouattara B, 2020, INT J REMOTE SENS, V41, P6527, DOI 10.1080/01431161.2020.1739355
   Phan TN, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12152411
   Punalekar SM, 2018, REMOTE SENS ENVIRON, V218, P207, DOI 10.1016/j.rse.2018.09.028
   Reisi-Gahrouei O, 2019, INT J REMOTE SENS, V40, P6822, DOI 10.1080/01431161.2019.1594436
   Richards JA, 2009, SIGNALS COMMUN TECHN, V0, PP1, DOI 10.1007/978-3-642-02020-9_1
   RICHARDSON AJ, 1977, PHOTOGRAMM ENG REM S, V43, P1541
   Rondeaux G, 1996, REMOTE SENS ENVIRON, V55, P95, DOI 10.1016/0034-4257(95)00186-7
   ROUSE JW, 1974, MONITORING VERNAL AD, V0, P0
   Sammut C., 2011, ENCY MACHINE LEARNIN, V0, P0
   Sharifi A, 2020, J INDIAN SOC REMOTE, V48, P11, DOI 10.1007/s12524-019-01057-8
   Shelestov A, 2017, INT GEOSCI REMOTE SE, V0, PP3696, DOI 10.1109/IGARSS.2017.8127801
   Sheykhmousa M, 2020, IEEE J-STARS, V13, P6308, DOI 10.1109/JSTARS.2020.3026724
   Thieme A, 2020, REMOTE SENS ENVIRON, V248, P0, DOI 10.1016/j.rse.2020.111943
   Tian HF, 2019, CURR SCI INDIA, V116, P291, DOI 10.18520/cs/v116/i2/291-298
   Venkatappa M, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12183110
   Vincini M, 2008, PRECIS AGRIC, V9, P303, DOI 10.1007/s11119-008-9075-z
   Wang J, 2019, ISPRS J PHOTOGRAMM, V154, P189, DOI 10.1016/j.isprsjprs.2019.06.007
   Wiseman G, 2014, IEEE J-STARS, V7, P4461, DOI 10.1109/JSTARS.2014.2322311
   Wu F, 2011, IEEE GEOSCI REMOTE S, V8, P196, DOI 10.1109/LGRS.2010.2055830
   Xiong J, 2017, ISPRS J PHOTOGRAMM, V126, P225, DOI 10.1016/j.isprsjprs.2017.01.019
   Zeng HW, 2020, CHINESE GEOGR SCI, V30, P397, DOI 10.1007/s11769-020-1119-y
   Zhang Xiaomei., 2019, 2019 8 INT C AGRO GE, V0, P0
   Zhu GL, 2014, PLOS ONE, V9, P0, DOI 10.1371/journal.pone.0102560
   Zocca Valentino., 2019, PYTHON DEEP LEARNING, V0, P0
NR 69
TC 4
Z9 4
U1 4
U2 20
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 0703-8992
EI 1712-7971
J9 CAN J REMOTE SENS
JI Can. J. Remote Sens.
PD MAR 4
PY 2022
VL 48
IS 2
BP 258
EP 277
DI 10.1080/07038992.2021.2011180
EA DEC 2021
PG 20
WC Remote Sensing
SC Remote Sensing
GA 0T6LB
UT WOS:000742300200001
DA 2023-04-26
ER
