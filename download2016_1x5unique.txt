
PT J
AU Ustuner, M
   Sanli, FB
   Abdikan, S
AF Ustuner, M.
   Sanli, F. B.
   Abdikan, S.
TI BALANCED VS IMBALANCED TRAINING DATA: CLASSIFYING RAPIDEYE DATA WITH SUPPORT VECTOR MACHINES
SO XXIII ISPRS CONGRESS, COMMISSION VII
LA English
DT Proceedings Paper
DE Land cover classification; Imbalanced training data; Support Vector Machines; RapidEye; Agriculture
ID land-cover classification; extreme-learning-machine; image classification; neural-network; forest; performance; africa; crop
AB The accuracy of supervised image classification is highly dependent upon several factors such as the design of training set (sample selection, composition, purity and size), resolution of input imagery and landscape heterogeneity. The design of training set is still a challenging issue since the sensitivity of classifier algorithm at learning stage is different for the same dataset. In this paper, the classification of RapidEye imagery with balanced and imbalanced training data for mapping the crop types was addressed. Classification with imbalanced training data may result in low accuracy in some scenarios. Support Vector Machines (SVM), Maximum Likelihood (ML) and Artificial Neural Network (ANN) classifications were implemented here to classify the data. For evaluating the influence of the balanced and imbalanced training data on image classification algorithms, three different training datasets were created. Two different balanced datasets which have 70 and 100 pixels for each class of interest and one imbalanced dataset in which each class has different number of pixels were used in classification stage. Results demonstrate that ML and NN classifications are affected by imbalanced training data in resulting a reduction in accuracy (from 90.94% to 85.94% for ML and from 91.56% to 88.44% for NN) while SVM is not affected significantly (from 94.38% to 94.69%) and slightly improved. Our results highlighted that SVM is proven to be a very robust, consistent and effective classifier as it can perform very well under balanced and imbalanced training data situations. Furthermore, the training stage should be precisely and carefully designed for the need of adopted classifier.
C1 [Ustuner, M.; Sanli, F. B.] Yildiz Tech Univ, Dept Geomat Engn, Istanbul, Turkey.
   [Abdikan, S.] Bulent Ecevit Univ, Dept Geomat Engn, Zonguldak, Turkey.
C3 Yildiz Technical University; Bulent Ecevit University
RP Sanli, FB (corresponding author), Yildiz Tech Univ, Dept Geomat Engn, Istanbul, Turkey.
EM mustuner@yildiz.edu.tr; tbalik@yildiz.edu.tr; sabdikan@beun.edu.tr
CR Adelabu S, 2014, ISPRS J PHOTOGRAMM, V95, P34, DOI 10.1016/j.isprsjprs.2014.05.013
   BENEDIKTSSON JA, 1990, IEEE T GEOSCI REMOTE, V28, P540, DOI 10.1109/TGRS.1990.572944
   Demir B, 2007, IEEE GEOSCI REMOTE S, V4, P586, DOI 10.1109/LGRS.2007.903069
   Foody GM, 2006, REMOTE SENS ENVIRON, V104, P1, DOI 10.1016/j.rse.2006.03.004
   Forkuor G, 2014, REMOTE SENS-BASEL, V6, P6472, DOI 10.3390/rs6076472
   Gartner P, 2016, REMOTE SENS ENVIRON, V177, P237, DOI 10.1016/j.rse.2016.01.028
   Immitzer M, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8030166
   Kavzoglu T, 2003, INT J REMOTE SENS, V24, P4907, DOI 10.1080/0143116031000114851
   Kavzoglu T, 2009, INT J APPL EARTH OBS, V11, P352, DOI 10.1016/j.jag.2009.06.002
   Kavzoglu T, 2009, ENVIRON MODELL SOFTW, V24, P850, DOI 10.1016/j.envsoft.2008.11.012
   Khatami R, 2016, REMOTE SENS ENVIRON, V177, P89, DOI 10.1016/j.rse.2016.02.028
   Kim HO, 2015, GISCI REMOTE SENS, V52, P1, DOI 10.1080/15481603.2014.1001666
   Lu D, 2007, INT J REMOTE SENS, V28, P823, DOI 10.1080/01431160600746456
   Mather P.M., 2010, COMPUTER PROCESSING, V0, PP229, DOI 10.1002/9780470666517.ch8
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Omer G, 2015, IEEE J-STARS, V8, P4825, DOI 10.1109/JSTARS.2015.2461136
   Pal M, 2013, REMOTE SENS LETT, V4, P853, DOI 10.1080/2150704X.2013.805279
   Pal M, 2009, INT J REMOTE SENS, V30, P3835, DOI 10.1080/01431160902788636
   Pozi MSM, 2015, REMOTE SENS LETT, V6, P568, DOI 10.1080/2150704X.2015.1062159
   Roscher R, 2012, IMAGE VISION COMPUT, V30, P263, DOI 10.1016/j.imavis.2012.04.004
   Schuster C, 2012, INT J REMOTE SENS, V33, P5583, DOI 10.1080/01431161.2012.666812
   TOWNSHEND JRG, 1992, INT J REMOTE SENS, V13, P1319, DOI 10.1080/01431169208904193
   Trebar M, 2008, COMPUT ELECTRON AGR, V63, P119, DOI 10.1016/j.compag.2008.02.001
   Wang D., 2015, AGR AGR 2015 4 INT C, V0, P312
   Waske B, 2009, LECT NOTES COMPUT SC, V5519, P375, DOI 10.1007/978-3-642-02326-2_38
   Xia JS, 2016, IEEE T GEOSCI REMOTE, V54, P1519, DOI 10.1109/TGRS.2015.2481938
NR 26
TC 6
Z9 6
U1 0
U2 6
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 2194-9034
EI 
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2016
VL 41
IS B7
BP 379
EP 384
DI 10.5194/isprsarchives-XLI-B7-379-2016
PG 6
WC Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BG9GD
UT WOS:000393155900059
DA 2023-04-26
ER

PT J
AU Alidoost, F
   Arefi, H
AF Alidoost, F.
   Arefi, H.
TI KNOWLEDGE BASED 3D BUILDING MODEL RECOGNITION USING CONVOLUTIONAL NEURAL NETWORKS FROM LIDAR AND AERIAL IMAGERIES
SO XXIII ISPRS CONGRESS, COMMISSION III
LA English
DT Proceedings Paper
DE Deep Learning; Convolutional Neural Network; Pattern Recognition; LiDAR; 3D Building Model
ID extraction; fusion
AB In recent years, with the development of the high resolution data acquisition technologies, many different approaches and algorithms have been presented to extract the accurate and timely updated 3D models of buildings as a key element of city structures for numerous applications in urban mapping. In this paper, a novel and model-based approach is proposed for automatic recognition of buildings' roof models such as flat, gable, hip, and pyramid hip roof models based on deep structures for hierarchical learning of features that are extracted from both LiDAR and aerial ortho-photos. The main steps of this approach include building segmentation, feature extraction and learning, and finally building roof labeling in a supervised pre-trained Convolutional Neural Network (CNN) framework to have an automatic recognition system for various types of buildings over an urban area. In this framework, the height information provides invariant geometric features for convolutional neural network to localize the boundary of each individual roofs. CNN is a kind of feed-forward neural network with the multilayer perceptron concept which consists of a number of convolutional and subsampling layers in an adaptable structure and it is widely used in pattern recognition and object detection application. Since the training dataset is a small library of labeled models for different shapes of roofs, the computation time of learning can be decreased significantly using the pre-trained models. The experimental results highlight the effectiveness of the deep learning approach to detect and extract the pattern of buildings' roofs automatically considering the complementary nature of height and RGB information.
C1 [Alidoost, F.; Arefi, H.] Univ Tehran, Coll Engn, Sch Surveying & Geospatial Engn, Tehran, Iran.
C3 University of Tehran
RP Alidoost, F (corresponding author), Univ Tehran, Coll Engn, Sch Surveying & Geospatial Engn, Tehran, Iran.
EM falidoost@ut.ac.ir; hossein.arefi@ut.ac.ir
CR Arefi H, 2013, REMOTE SENS-BASEL, V5, P1681, DOI 10.3390/rs5041681
   Awrangjeb M, 2013, ISPRS J PHOTOGRAMM, V83, P1, DOI 10.1016/j.isprsjprs.2013.05.006
   Bastien Frederic, 2012, NIPS 2012 WORKSH, V0, P0
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, V0, P0, DOI DOI 10.5244/C.25.76
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Deng J, 2009, PROC CVPR IEEE, V0, PP248, DOI 10.1109/CVPRW.2009.5206848
   Filipe S., 2014, IEEE T GEOSCI REMOTE, V52, P1
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Guo L, 2011, ISPRS J PHOTOGRAMM, V66, P56, DOI 10.1016/j.isprsjprs.2010.08.007
   Haala N, 1999, ISPRS J PHOTOGRAMM, V54, P130, DOI 10.1016/S0924-2716(99)00010-6
   Hermosilla T, 2011, REMOTE SENS-BASEL, V3, P1188, DOI 10.3390/rs3061188
   Jia Yangqing, 2013, CAFFE OPEN SOURCE CO, V0, P0
   Karantzalos K., 2015, ISPRS INT ARCH PHOTO, VXL-1/W4, P293, DOI 10.5194/isprsarchives-XL-1-W4-293-2015
   Khurana M., 2015, INT J ADV RES COMPUT, V4, P0
   Kim CJ, 2009, SENSORS-BASEL, V9, P5679, DOI 10.3390/s90705679
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu T., 2015, COMPUTER VISION PATT, V0, P0
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Maltezos E., 2015, ISPRS ANN PHOTOGRAMM, VII-3, P0
   Matikainen L., 2004, INT ARCH PHOTOGRAMM, V35, P434
   Menegatti E., 2016, ADV INTELLIGENT SYST, V301, P889
   Mongus D, 2014, ISPRS J PHOTOGRAMM, V93, P145, DOI 10.1016/j.isprsjprs.2013.12.002
   Ngo TT, 2015, IEEE IMAGE PROC, V0, PP1483, DOI 10.1109/ICIP.2015.7351047
   Partovi Tahmineh, 2014, 2014 IEEE GEOSCIENCE AND REMOTE SENSING SYMPOSIUM. (IGARSS). PROCEEDINGS, V0, PP3168, DOI 10.1109/IGARSS.2014.6947150
   Phung S.L., 2009, TECHNICAL REPORT, V0, P0
   Salah M, 2009, J SPAT SCI, V54, P15, DOI 10.1080/14498596.2009.9635176
   Schwalbe E., 2005, ISPRS WORKSH LAS SCA, V0, P0
   Sen Maitra D, 2015, PROC INT CONF DOC, V0, PP1021, DOI 10.1109/ICDAR.2015.7333916
   Singh G., 2015, P SPIE INT SOC OPTIC, V9401, P0
   Socher R, 2013, DEEP LEARNING NLP, V0, P0
   Sohn G, 2007, ISPRS J PHOTOGRAMM, V62, P43, DOI 10.1016/j.isprsjprs.2007.01.001
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vakalopoulou M, 2015, INT GEOSCI REMOTE SE, V0, PP1873, DOI 10.1109/IGARSS.2015.7326158
   Vedaldi A, 2015, MM15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, V0, PP689, DOI 10.1145/2733373.2807412
   Vu TT, 2009, INT J APPL EARTH OBS, V11, P281, DOI 10.1016/j.jag.2009.03.005
   Wichmann A., 2015, ISPRS ANN PHOTOGRAMM, VII, P0
   Yang X., 2012, ADV MAPPING REMOTE S, V0, P33
   Yu BL, 2010, LANDSCAPE URBAN PLAN, V98, P210, DOI 10.1016/j.landurbplan.2010.08.004
   Yuan J., 2016, ARXIV PREPRINT ARXIV, V0, P0
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang YT, 2015, PROC CVPR IEEE, V0, PP249, DOI 10.1109/CVPR.2015.7298621
   Zhou GQ, 2014, IEEE T GEOSCI REMOTE, V52, P7393, DOI 10.1109/TGRS.2014.2311991
   Zou Q., 2015, IEEE GEOSCIENCE REMO, V12, P0
NR 45
TC 16
Z9 16
U1 0
U2 9
PU COPERNICUS GESELLSCHAFT MBH
PI GOTTINGEN
PA BAHNHOFSALLE 1E, GOTTINGEN, 37081, GERMANY
SN 1682-1750
EI 2194-9034
J9 INT ARCH PHOTOGRAMM
PD JUN 15
PY 2016
VL 41
IS B3
BP 833
EP 840
DI 10.5194/isprsarchives-XLI-B3-833-2016
PG 8
WC Computer Science, Interdisciplinary Applications; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Computer Science; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA BG8WM
UT WOS:000392743800121
DA 2023-04-26
ER

PT J
AU Langkvist, M
   Kiselev, A
   Alirezaie, M
   Loutfi, A
AF Langkvist, Martin
   Kiselev, Andrey
   Alirezaie, Marjan
   Loutfi, Amy
TI Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks
SO REMOTE SENSING
LA English
DT Article
DE remote sensing; orthoimagery; convolutional neural network; per-pixel classification; segmentation; region merging
ID land-cover; spatial classification; feature-selection; aerial imagery; urban; representation; extraction; algorithm; features; scale
AB The availability of high-resolution remote sensing (HRRS) data has opened up the possibility for new interesting applications, such as per-pixel classification of individual objects in greater detail. This paper shows how a convolutional neural network (CNN) can be applied to multispectral orthoimagery and a digital surface model (DSM) of a small city for a full, fast and accurate per-pixel classification. The predicted low-level pixel classes are then used to improve the high-level segmentation. Various design choices of the CNN architecture are evaluated and analyzed. The investigated land area is fully manually labeled into five categories (vegetation, ground, roads, buildings and water), and the classification accuracy is compared to other per-pixel classification works on other land areas that have a similar choice of categories. The results of the full classification and segmentation on selected segments of the map show that CNNs are a viable tool for solving both the segmentation and object recognition task for remote sensing data.
C1 [Langkvist, Martin; Kiselev, Andrey; Alirezaie, Marjan; Loutfi, Amy] Univ Orebro, Appl Autonomous Sensor Syst, Fakultetsgatan 1, S-70182 Orebro, Sweden.
C3 Orebro University
RP Langkvist, M (corresponding author), Univ Orebro, Appl Autonomous Sensor Syst, Fakultetsgatan 1, S-70182 Orebro, Sweden.
EM martin.langkvist@oru.se; andrey.kiselev@oru.se; marjan.alirezaie@oru.se; amy.loutfi@oru.se
FU Swedish Knowledge Foundation [20140033]
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Benediktsson JA, 2013, P IEEE, V101, P566, DOI 10.1109/JPROC.2012.2237076
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004
   Boggess J.E., 1993, IDENTIFICATION ROADS, V0, P0
   Chang CI, 1999, IEEE T GEOSCI REMOTE, V37, P2631, DOI 10.1109/36.803411
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chi MM, 2008, ADV SPACE RES, V41, P1793, DOI 10.1016/j.asr.2008.02.012
   Coates A., 2011, AISTATS, V0, P0
   Couprie C., 2013, P INT C LEARN REPR S, V0, P0
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   dos Santos JA, 2010, VISAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P203
   Ester M, 1996, P 2 INT C KNOWL DISC, V96, P226
   Farabet C., 2012, P 29 INT C MACHINE L, VVolume 1, P575, DOI 10.48550/arxiv.1202.2160
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fauvel M, 2008, IEEE T GEOSCI REMOTE, V46, P3804, DOI 10.1109/TGRS.2008.922034
   Gamba P, 2002, INT J REMOTE SENS, V23, P4439, DOI 10.1080/01431160110114952
   Glenn NF, 2005, REMOTE SENS ENVIRON, V95, P399, DOI 10.1016/j.rse.2005.01.003
   Glorot X., 2011, P 14 INT C ART INT S, V0, P315
   Graves A, 2013, P 38 INT C AC SPEECH, V0, P0
   Guyon I, 2003, J MACH LEARN RES, V3, P1157, DOI 10.1162/153244303322753616
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Hu F, 2014, INT GEOSCI REMOTE SE, V0, PP1273, DOI 10.1109/IGARSS.2014.6946665
   Huang MJ, 2008, PHOTOGRAMM ENG REM S, V74, P1473, DOI 10.14358/PERS.74.12.1473
   Hudjakov Robert, 2011, ESTONIAN JOURNAL OF ENGINEERING, V17, P17, DOI 10.3176/eng.2011.1.03
   Ishii T, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), V0, PP341, DOI 10.1109/MVA.2015.7153200
   Jarrett K, 2009, IEEE I CONF COMP VIS, V0, PP2146, DOI 10.1109/ICCV.2009.5459469
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Lai K, 2011, IEEE INT CONF ROBOT, V0, P1817
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee HT, 2009, ACM T WEB, V3, P0, DOI 10.1145/1541822.1541823
   Li T, 2014, IEEE IMAGE PROC, V0, PP5132, DOI 10.1109/ICIP.2014.7026039
   Li XX, 2014, REMOTE SENS-BASEL, V6, P11372, DOI 10.3390/rs61111372
   Lucieer VL, 2008, INT J REMOTE SENS, V29, P905, DOI 10.1080/01431160701311309
   Matikainen L, 2011, REMOTE SENS-BASEL, V3, P1777, DOI 10.3390/rs3081777
   Midhun M., 2014, P 2014 INT C INT ADV, V0, P0
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   Mylonas SK, 2015, REMOTE SENS-BASEL, V7, P2474, DOI 10.3390/rs70302474
   Nair V, 2010, ICML, V27, P807
   Netanyahu NS, 2004, IEEE T GEOSCI REMOTE, V42, P1586, DOI 10.1109/TGRS.2004.826822
   Pena-Barragan JM, 2011, REMOTE SENS ENVIRON, V115, P1301, DOI 10.1016/j.rse.2011.01.009
   Penatti Otavio A. B., 2015, 2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW), V0, PP44, DOI 10.1109/CVPRW.2015.7301382
   Pinheiro PO, 2014, PR MACH LEARN RES, V32, P0
   PLAZA A, 2009, MACHINE LEARNING SIG, V2009, P1, DOI 10.1109/MLSP.2009.5306202
   Pradhan B, 2010, ENVIRON EARTH SCI, V60, P1037, DOI 10.1007/s12665-009-0245-8
   Qian YT, 2013, IEEE J-STARS, V6, P499, DOI 10.1109/JSTARS.2012.2232904
   Quigley M, 2009, IEEE INT CONF ROBOT, V0, P3604
   REED BC, 1994, J VEG SCI, V5, P703, DOI 10.2307/3235884
   Running SW, 2004, BIOSCIENCE, V54, P547, DOI 10.1641/0006-3568(2004)054[0547:ACSMOG]2.0.CO;2
   Samadzadegan F, 2012, CAN J REMOTE SENS, V38, P139, DOI 10.5589/m12-022
   Sanchez C.R, 2007, PAPEL TRAB, V0, P1
   Serpico SB, 2001, IEEE T GEOSCI REMOTE, V39, P1360, DOI 10.1109/36.934069
   Socher R., 2012, ADV NEURAL INFORM PR, V3, P665
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taylor G., 2010, P EUR C COMP VIS ECC, V0, P0
   Nguyen T, 2013, AIP CONF PROC, V1558, P2237, DOI 10.1063/1.4825984
   Thomas N, 2003, PHOTOGRAMM ENG REM S, V69, P963, DOI 10.14358/PERS.69.9.963
   Vaduva C, 2013, IEEE T GEOSCI REMOTE, V51, P2770, DOI 10.1109/TGRS.2012.2219314
   Vincent P, 2008, P 25 INT C MACH LEAR, V0, PP1096, DOI 10.1145/1390156.1390294
   Wang J, 2015, INT J REMOTE SENS, V36, P3144, DOI 10.1080/01431161.2015.1054049
   Weng QH, 2004, REMOTE SENS ENVIRON, V89, P467, DOI 10.1016/j.rse.2003.11.005
   Xia GS, 2010, INT ARCH PHOTOGRAMM, V38, P298
   Yang Y, 2010, PROC 18 SIGSPATIAL I, V0, P0, DOI DOI 10.1145/1869790.1869829
   Yang Y, 2013, IEEE T GEOSCI REMOTE, V51, P818, DOI 10.1109/TGRS.2012.2205158
   Yang Y, 2008, IEEE IMAGE PROC, V0, PP1852, DOI 10.1109/ICIP.2008.4712139
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhu Z, 2012, REMOTE SENS ENVIRON, V117, P72, DOI 10.1016/j.rse.2011.07.020
NR 72
TC 205
Z9 213
U1 13
U2 141
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD APR 15
PY 2016
VL 8
IS 4
BP 
EP 
DI 10.3390/rs8040329
PG 21
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA DK8DD
UT WOS:000375156500062
DA 2023-04-26
ER

PT J
AU Grinblat, Y
   Gilichinsky, M
   Benenson, I
AF Grinblat, Yulia
   Gilichinsky, Michael
   Benenson, Itzhak
TI Cellular Automata Modeling of Land-Use/Land-Cover Dynamics: Questioning the Reliability of Data Sources and Classification Methods
SO ANNALS OF THE AMERICAN ASSOCIATION OF GEOGRAPHERS
LA English
DT Article
DE cellular automata; Landsat images; land-use/land-cover changes; Markov transition probabilities matrices; validation of remote sensing classification methods
ID urban-growth; spatial dynamics; neural-network; use scenarios; calibration; simulation; urbanization; integration; location; region
AB Based on four time intervals within a forty-year period of observation, we construct land-use/land-cover (LULC) maps and estimate the transition probabilities between six LULC states. The maps and transition probability matrices (TPMs) were built based on the high-resolution aerial photos and 30-m multispectral Landsat images for the same years. We considered the TPM constructed from manual classification of the aerial photos as a reference and compared it to the TPM constructed from the Landsat image classified with several methods: mean-shift segmentation followed by random forest classification and three pixel-based methods popular in cellular automata (CA) studies: K-means, iterative self-organizing data analysis techniques (ISO-DATA), and maximum likelihood. For each classification method, the TPMs were constructed and compared to the TPMs for the aerial photos. We prove that the goodness-of-fit of maps obtained with the three pixel-based methods was insufficient for estimating the LULC TPM. The LULC maps obtained with the object-based classification fit well to those based on the aerial photos, but the estimates of TPM were yet qualitatively different. This article raises doubts regarding the adequacy of Landsat data and standard classification methods for establishing LULC CA model rules and calls for the careful reexamination of the entire land-use CA framework. We appeal for a new view of the CA modeling methodology: It should be based on a long-term series of carefully validated LULC maps that portray different types of land-use dynamics and land planning systems over long and representative periods of population and economic growth.
C1 [Grinblat, Yulia; Benenson, Itzhak] Tel Aviv Univ, Dept Geog & Human Environm, IL-69978 Tel Aviv, Israel.
   [Grinblat, Yulia] Tel Aviv Univ, Porter Sch Environm Studies, IL-69978 Tel Aviv, Israel.
   [Gilichinsky, Michael] Ariel Univ, Ctr Res & Dev, IL-40700 Ariel, Israel.
C3 Tel Aviv University; Tel Aviv University; Ariel University
RP Grinblat, Y (corresponding author), Tel Aviv Univ, Dept Geog & Human Environm, IL-69978 Tel Aviv, Israel.; Grinblat, Y (corresponding author), Tel Aviv Univ, Porter Sch Environm Studies, IL-69978 Tel Aviv, Israel.
EM juliagri@post.tau.ac.il; gilichinsky@gmail.com; bennya@post.tau.ac.il
CR Ahmad A., 2012, CONTR SYST COMP ENG, V0, P0
   Akin A, 2014, INT J APPL EARTH OBS, V27, P156, DOI 10.1016/j.jag.2013.10.002
   [Anonymous], 2015, MODELING CITIES REGI, V0, P0
   [Anonymous], 2000, REMOTE SENSING IMAGE, V0, P0
   Araya YH, 2010, REMOTE SENS-BASEL, V2, P1549, DOI 10.3390/rs2061549
   Arsanjani JJ, 2011, INT J IMAGE DATA FUS, V2, P329, DOI 10.1080/19479832.2011.605397
   Balmann A, 1997, EUR REV AGRIC ECON, V24, P85, DOI 10.1093/erae/24.1.85
   Balzter H, 1998, ECOL MODEL, V107, P113, DOI 10.1016/S0304-3800(97)00202-0
   Batty M., 1999, COMPUTERS, V0, P0
   BAUER ME, 1994, PHOTOGRAMM ENG REM S, V60, P287
   Benediktsson J. A., 1989, GEOSC REM SENS S 12, V0, P0
   Benenson I, 2004, GEOSIMULATION AUTOMA, V0, P0
   Blanchard SD, 2015, J ENVIRON INFORM, V25, P1, DOI 10.3808/jei.201400284
   Breiman L., 2001, MACHINE LEARNING, V45, P5, DOI 10.1023/A:1010933404324
   Candau J. C., 2000, 38 ANN C URB REG INF, V0, P0
   Canty M., 2010, IMAGE ANAL CLASSIFIC, V0, P0
   Central Bureau of Statistics (CBS), 2009, STAT ABSTR ISR, V60, P0
   Chen QW, 2003, SIMUL MODEL PRACT TH, V11, P609, DOI 10.1016/j.simpat.2003.08.006
   Clarke KC, 1997, ENVIRON PLANN B, V24, P247, DOI 10.1068/b240247
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   de Almeida C. M., 2003, COMPUTERS, V0, P0
   Dietzel C., 2007, T GIS, V11, P29
   Dietzel C, 2006, COMPUT ENVIRON URBAN, V30, P78, DOI 10.1016/j.compenvurbsys.2005.04.001
   Engelen G., 2003, INTEGRATED ASSESSMEN, V4, P97
   European Environmental Agency (EEA), 2007, TECHNICAL REPORT, V17, P0
   Fan FL, 2008, ENVIRON MONIT ASSESS, V137, P127, DOI 10.1007/s10661-007-9734-y
   Feranec J, 2007, LAND USE POLICY, V24, P234, DOI 10.1016/j.landusepol.2006.02.002
   Flamenco-Sandoval A, 2007, BIOL CONSERV, V138, P131, DOI 10.1016/j.biocon.2007.04.022
   Foody GM, 2002, REMOTE SENS ENVIRON, V80, P185, DOI 10.1016/S0034-4257(01)00295-4
   Gao Y, 2006, INT J REMOTE SENS, V27, P4039, DOI 10.1080/01431160600702632
   Global Land Cover Facility (GLCF), 2013, GLOB LAND COV FAC, V0, P0
   Guan DJ, 2011, ECOL MODEL, V222, P3761, DOI 10.1016/j.ecolmodel.2011.09.009
   Hagen-Zanker A, 2005, INT J GEOGR INF SCI, V19, P769, DOI 10.1080/13658810500072137
   Hagen-Zanker A., 2005, 14 EUR C QUANT GEOGR, V0, P0
   HALL FG, 1991, ECOLOGY, V72, P628, DOI 10.2307/2937203
   Herold M, 2003, REMOTE SENS ENVIRON, V86, P286, DOI 10.1016/S0034-4257(03)00075-0
   Jantz CA, 2005, INT J GEOGR INF SCI, V19, P217, DOI 10.1080/13658810410001713425
   Kamusoko C, 2009, APPL GEOGR, V29, P435, DOI 10.1016/j.apgeog.2008.10.002
   Li X, 2001, ENVIRON PLANN A, V33, P1445, DOI 10.1068/a33210
   Liu X. H., 2004, COMPUTERS, V0, P0
   Logofet DO, 2002, ECOL MODEL, V151, P51, DOI 10.1016/S0304-3800(01)00486-0
   Lu D, 2007, INT J REMOTE SENS, V28, P823, DOI 10.1080/01431160600746456
   Puertas OL, 2014, LAND USE POLICY, V38, P415, DOI 10.1016/j.landusepol.2013.11.024
   Mitsova D, 2011, LANDSCAPE URBAN PLAN, V99, P141, DOI 10.1016/j.landurbplan.2010.10.001
   Moghadam HS, 2013, APPL GEOGR, V40, P140, DOI 10.1016/j.apgeog.2013.01.009
   Mondal P, 2010, FOREST ECOL MANAG, V260, P1716, DOI 10.1016/j.foreco.2010.08.017
   Myint SW, 2006, CAN J REMOTE SENS, V32, P390, DOI 10.5589/m06-032
   Norman LM, 2012, LANDSCAPE URBAN PLAN, V107, P225, DOI 10.1016/j.landurbplan.2012.06.015
   Orenstein DE, 2014, ENVIRON PLANN B, V41, P3, DOI 10.1068/b38017
   Paegelow M, 2005, INT J GEOGR INF SCI, V19, P697, DOI 10.1080/13658810500076443
   PAOLA JD, 1995, IEEE T GEOSCI REMOTE, V33, P981, DOI 10.1109/36.406684
   Petrov LO, 2009, LANDSCAPE URBAN PLAN, V92, P10, DOI 10.1016/j.landurbplan.2009.01.011
   Pijanowski B. C., 2002, COMPUTERS, V0, P0
   Pijanowski BC, 2009, INT J ENVIRON RES, V3, P493
   Pijanowski BC, 2005, INT J GEOGR INF SCI, V19, P197, DOI 10.1080/13658810410001713416
   Pontius Jr R. G., 2001, AGR ECOSYST ENVIRON, V1175, P1
   Pontius RG, 2000, PHOTOGRAMM ENG REM S, V66, P1011
   Pontius RG, 2004, LECT NOTES COMPUT SC, V3234, P251
   Pontius RG, 2004, ECOL MODEL, V179, P445, DOI 10.1016/j.ecolmodel.2004.05.010
   Pontius RG, 2002, PHOTOGRAMM ENG REM S, V68, P1041
   Pontius RG, 2010, ENVIRON MODELL SOFTW, V25, P299, DOI 10.1016/j.envsoft.2009.09.005
   Silva E. A., 2002, COMPUTERS, V0, P0
   Soares BS, 2002, ECOL MODEL, V154, P217, DOI 10.1016/S0304-3800(02)00059-5
   Stevens D, 2007, ENVIRON MODELL SOFTW, V22, P761, DOI 10.1016/j.envsoft.2006.02.004
   Straatman B., 2004, COMPUTERS, V0, P0
   Takada T, 2010, LANDSCAPE ECOL, V25, P561, DOI 10.1007/s10980-009-9433-x
   Thapa RB, 2011, COMPUT ENVIRON URBAN, V35, P25, DOI 10.1016/j.compenvurbsys.2010.07.005
   Torrens P.M, 2011, URBAN REMOTE SENSING, V0, PP335, DOI 10.1002/9780470979563.ch23
   van der Kwast J., 2009, 7 INT URB REM SENS C, V0, P0
   Vaz ED, 2012, LANDSCAPE URBAN PLAN, V104, P201, DOI 10.1016/j.landurbplan.2011.10.007
   VENKATESWARLU NB, 1992, PATTERN RECOGN, V25, P335, DOI 10.1016/0031-3203(92)90114-X
   Verbeiren B, 2013, INT J APPL EARTH OBS, V21, P92, DOI 10.1016/j.jag.2012.08.011
   Verburg PH, 2002, ENVIRON MANAGE, V30, P391, DOI 10.1007/s00267-002-2630-x
   Wang SQ, 2012, PROCEDIA ENVIRON SCI, V13, P1238, DOI 10.1016/j.proenv.2012.01.117
   WHITE R, 1993, ENVIRON PLANN A, V25, P1175, DOI 10.1068/a251175
   White R, 1997, ENVIRON PLANN B, V24, P323, DOI 10.1068/b240323
   White R, 2012, INT J GEOGR INF SCI, V26, P1251, DOI 10.1080/13658816.2011.635146
   Wickham JD, 2013, REMOTE SENS ENVIRON, V130, P294, DOI 10.1016/j.rse.2012.12.001
   Wu F, 1998, ENVIRON PLANN B, V25, P103, DOI 10.1068/b250103
   Wu F, 1998, J ENVIRON MANAGE, V53, P293, DOI 10.1006/jema.1998.0195
   Wu FL, 2002, INT J GEOGR INF SCI, V16, P795, DOI 10.1080/13658810210157769
   Xi FM, 2010, INT J URBAN SUSTAIN, V1, P111, DOI 10.1080/19463130903458326
   Yunyan D, 2010, EXPERT SYST APPL, V37, P5745, DOI 10.1016/j.eswa.2010.02.035
NR 83
TC 18
Z9 18
U1 1
U2 17
PU ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND
SN 2469-4452
EI 2469-4460
J9 ANN AM ASSOC GEOGR
JI Ann. Am. Assoc. Geogr.
PD JUN 15
PY 2016
VL 106
IS 6
BP 1299
EP 1320
DI 10.1080/24694452.2016.1213154
PG 22
WC Geography
SC Geography
GA DY8RS
UT WOS:000385398600007
DA 2023-04-26
ER

PT J
AU Davydova, K
   Cui, SY
   Reinartz, P
AF Davydova, Ksenia
   Cui, Shiyong
   Reinartz, Peter
TI Building footprint extraction from Digital Surface Models using Neural Networks
SO IMAGE AND SIGNAL PROCESSING FOR REMOTE SENSING XXII
LA English
DT Proceedings Paper
DE Building footprint extraction; binary mask; Digital Surface Model; neural networks; Markov Random Fields; Normalized Difference Vegetation Index
ID imagery; system
AB Two-dimensional building footprints are a basis for many applications: from cartography to three-dimensional building models generation. Although, many methodologies have been proposed for building footprint extraction, this topic remains an open research area. Neural networks are able to model the complex relationships between the multivariate input vector and the target vector. Based on these abilities we propose a methodology using neural networks and Markov Random Fields (MRF) for automatic building footprint extraction from normalized Digital Surface Model (nDSM) and satellite images within urban areas. The proposed approach has mainly two steps. In the first step, the unary terms are learned for the MRF energy function by a four-layer neural network. The neural network is learned on a large set of patches consisting of both nDSM and Normalized Difference Vegetation Index (NDVI). Then prediction is performed to calculate the unary terms that are used in the MRF. In the second step, the energy function is minimized using a maxflow algorithm, which leads to a binary building mask. The building extraction results are compared with available ground truth. The comparison illustrates the efficiency of the proposed algorithm which can extract approximately 80% of buildings from nDSM with high accuracy.
C1 [Davydova, Ksenia; Cui, Shiyong; Reinartz, Peter] German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, Oberpfaffenhofen, Germany.
C3 Helmholtz Association; German Aerospace Centre (DLR)
RP Davydova, K (corresponding author), German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, Oberpfaffenhofen, Germany.
EM Ksenia.Davydova@dlr.de
CR Ahmadi S, 2010, INT J APPL EARTH OBS, V12, P150, DOI 10.1016/j.jag.2010.02.001
   Anand SS, 2006, 2006 INTERNATIONAL CONFERENCE ON BIOMEDICAL AND PHARMACEUTICAL ENGINEERING, VOLS 1 AND 2, P44
   Benediktsson JA, 2003, IEEE T GEOSCI REMOTE, V41, P1940, DOI 10.1109/TGRS.2003.814625
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bredif M, 2013, ISPRS J PHOTOGRAMM, V77, P57, DOI 10.1016/j.isprsjprs.2012.11.007
   Held K, 1997, IEEE T MED IMAGING, V16, P878, DOI 10.1109/42.650883
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V, 2006, LECT NOTES COMPUT SC, V3952, P1
   Lari Z., 2007, ISPRS HANN WORKSH, V0, P0
   Lee DH, 2008, PHOTOGRAMM ENG REM S, V74, P215, DOI 10.14358/PERS.74.2.215
   Lee DS, 2003, PHOTOGRAMM ENG REM S, V69, P143, DOI 10.14358/PERS.69.2.143
   Li Y., 2008, REMOTE SENSING SPATI, V37, P197, DOI 10.1109/TGRS.2010.2041783
   Lu Y. H., 2002, SCH SURVEYING SPATIA, V0, P0
   Marmanis D, 2015, ISPRS ANN PHOTO REM, V2-3, P103, DOI 10.5194/isprsannals-II-3-W4-103-2015
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   Ortner M, 2007, INT J COMPUT VISION, V72, P107, DOI 10.1007/s11263-005-5033-7
   Peng J, 2005, PATTERN RECOGN LETT, V26, P587, DOI 10.1016/j.patrec.2004.09.033
   Qin R., 2015, INT J REMOTE SENS, V0, P1
   Rottensteiner F., 2005, INFORMATION FUSION, V6, P283, DOI 10.1016/j.inffus.2004.06.004
   San D. K., 2010, INT ARCH PHOTOGRAM 8, V38, P0
   San D.K., 2006, P ISPRS WORKSH TOP M, V0, P0
   Shorter N, 2009, REMOTE SENS-BASEL, V1, P731, DOI 10.3390/rs1040731
   Sohn G, 2001, AUTOMATIC EXTRACTION OF MAN-MADE OBJECTS FROM AERIAL AND SPACE IMAGES (III), V0, P345
   Tian JJ, 2014, IEEE T GEOSCI REMOTE, V52, P406, DOI 10.1109/TGRS.2013.2240692
   Unsalan C, 2005, COMPUT VIS IMAGE UND, V98, P423, DOI 10.1016/j.cviu.2004.10.006
   Zeng CQ, 2013, IEEE J-STARS, V6, P1640, DOI 10.1109/JSTARS.2013.2256882
NR 28
TC 10
Z9 10
U1 0
U2 4
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
J9 PROC SPIE
PD JUN 15
PY 2016
VL 10004
IS 
BP 
EP 
DI 10.1117/12.2240727
PG 10
WC Engineering, Electrical & Electronic; Remote Sensing; Optics
SC Engineering; Remote Sensing; Optics
GA BG9FP
UT WOS:000393154600018
DA 2023-04-26
ER
