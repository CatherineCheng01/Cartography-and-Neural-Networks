
PT J
AU Gomez, LF
AF Fabiana Gomez, Laura
TI Simulation of future scenarios of land use changes using Artificial Neural Networks. Application in Curuzu Cuatia city, Corrientes, Argentina
SO ESTUDIOS SOCIOTERRITORIALES
LA Spanish
DT Article
DE Simulation; Future Scenarios; Land use change; Artificial Neural Network
AB Simulations of future land use scenarios allow obtaining results that can become valuable information for land planners while decreasing the degree of uncertainty. The aim of this contribution is to simulate a linear path land use scenario with a linear trajectory for the year 2030 in the city of Curuzu Cuatia, Corrientes. Topographic factors, distances and flood areas defined by current regulations were chosen as explanatory variables of the changes, a model based on artificial neural networks, included in the Land Change Modeler -LCM- in the Idrisi Selva software was used, in addition to two maps of land uses of five categories elaborated for two moments of the past. The results obtained show the maps of transition potentials towards the Consolidated Urban Area with accuracies greater than 72%; simulated scenarios for the year 2030 (hard and soft prediction), and the surfaces of each class at each moment were determined, finding the increase only of the Urban Consolidated class in all of them. The study was completed with the validation of the model by class, using the global reliability and the Kappa index.
C1 [Fabiana Gomez, Laura] Univ Nacl Nordeste, Consejo Nacl Invest Cient & Tecn UNNE CONICET, CONICET, Dept Agrimensura,Fac Ciencias Exactas Nat & Agrim, 9 Julio 1449, RA-3400 Corrientes, Argentina.
C3 Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET)
RP Gomez, LF (corresponding author), Univ Nacl Nordeste, Consejo Nacl Invest Cient & Tecn UNNE CONICET, CONICET, Dept Agrimensura,Fac Ciencias Exactas Nat & Agrim, 9 Julio 1449, RA-3400 Corrientes, Argentina.
EM laura.f.gomez@gmail.com
CR Aguilera Benavente F., 2009, REV INT SOSTENIBILID, V4, P57
   [Anonymous], 2008, ENV FUTURES PRACTICE, V0, P0
   Baluja A. J., 2010, TECNOLOGIAS INFORM G, V0, P640
   Buzai G.D., 2011, ANALISIS SOCIOESPACI, V1, P0
   Camacho Olmedo M., 2010, TECNOLOGIAS INFORM G, V0, P658
   Chuvieco Salinero E, 2008, TELEDETECCION AMBIEN, V0, P0
   Cifuentes Ruiz P., 2009, REV INT SOSTENIBILID, V0, P81
   Da Silva C., 2014, REV GEOGRAFICA DIGIT, V11, P0
   Diaz-Pacheco J., 2013, GEOFOCUS, V14, P1
   Eastman J., 2012, IDRISI SELVA GUIA SI, V0, P0
   Gallardo Beltran M., 2014, THESIS U COMPLUTENSE, V0, P0
   Gomez L., 2019, REV ESTUDIOS MARITIM, V0, P0
   Instituto Nacional de Estadistica y Censos, 2010, CENS NAC POBL HOG VI, V0, P0
   La Macchia L, 2014, GEOGRAFIA SISTEMAS I, V6, P66
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Mishra VN, 2014, J GEOGR INST JOVAN C, V64, P111, DOI 10.2298/IJGI1401111M
   OLAYA V., 2014, SISTEMAS INFORM GEOG, V0, P0
   Pijanowski B. C., 2002, COMPUTERS, V0, P0
   Plata Rocha W., 2011, GEOGRAFIA SISTEMAS I, V0, P201
   Research Institute For Knowledge Systems (RIKS),, 2010, MAP COMP KIT 3, V0, P0
   Santos Preciado J., 2012, ANALISIS DINAMICA UR, V0, P81
   Veldkamp A, 2001, AGR ECOSYST ENVIRON, V85, P1, DOI 10.1016/S0167-8809(01)00199-2
   Wegener M, 2000, INT J APPL EARTH OBS, V3, P1
NR 23
TC 0
Z9 0
U1 0
U2 1
PU UNIV NACL CENTRO PROVINCIA BUENOS AIRES, CENTRO INVESTIGACIONES GEOGRAFICA
PI BUENOS AIRES
PA CAMPUS UNIV PARAJE ARROYO SECO, BUENOS AIRES, TANDIL 7000, ARGENTINA
SN 1515-6206
EI 1853-4392
J9 ESTUD SOCIOTERRITORI
JI Estud. Socioterritoriales
PD JUL-DEC 15
PY 2020
VL 0
IS 28
BP 
EP 
DI 10.37838/unicen/est.28-054
PG 21
WC Geography
SC Geography
GA YY9AN
UT WOS:000755077800001
DA 2023-04-26
ER

PT J
AU Yin, W
   He, CY
   Wu, QB
AF Yin, Wu
   He, Chenying
   Wu, Qibao
TI A Tomato Quality Identification Method Based on Raman Spectroscopy and Convolutional Neural Network
SO 2019 4TH INTERNATIONAL CONFERENCE ON COMMUNICATION, IMAGE AND SIGNAL PROCESSING (CCISP 2019)
LA English
DT Proceedings Paper
AB in recent years, more and more technologies have been applied in monitoring growth and production efficiency of plants, i.e. agricultural Internet of Things (IOT) and new information- aware technologies. The architecture of the IOT is divided into four layers, i.e., the sensing layer, network layer, processing layer and application layer[1-4]. Among them, the perception layer is the facial features and the skin of the IOT, which is the basis of the IOT [5]. Raman spectroscopy technology has the advantages of fast, simplicity, accuracy, non-destructive and automatic identification, which has become a powerful analytical verifying method. The method of tomato quality identification that based on the Raman spectroscopy combined with convolutional neural network (CNN)[6]was explored. The Raman spectrum of tomato was collected by Raman sensor to construct a neural network with deep network structure. Through repeatedly learning and training in Raman map, we can determine the map recognition model of high quality tomatoes and use matplotlib to realize the identification simulation.
C1 [Yin, Wu] Shenzhen Wissea Elect Technol Co Ltd, 1202,2nd Chinese Overseas Scholar Venture Bldg, Shenzhen, Guangdong, Peoples R China.
   [He, Chenying; Wu, Qibao] Shenzhen Inst Informat Technol, Sch Intelligent Mfg & Equipment, Shenzhen, Peoples R China.
C3 Shenzhen Institute of Information Technology
RP Yin, W (corresponding author), Shenzhen Wissea Elect Technol Co Ltd, 1202,2nd Chinese Overseas Scholar Venture Bldg, Shenzhen, Guangdong, Peoples R China.
EM yinwu@wissea.com; hechenying@wisse.com; wuqb@sziit.edu.cn
CR [Anonymous], 2014, ARXIV14095185, V0, P0
   Huang Y., 2017, INTERNET THINGS TECH, V7, P33
   Ji M.S., 2012, NEW AGR, V0, P45
   LECUN Y, 1989, IEEE COMMUN MAG, V27, P41, DOI 10.1109/35.41400
   Liu Y.J., 2015, AGR PRODUCTS WEEKLY, V0, P0
   Mashal I., 2015, AD HOC NETW, V28, P68, DOI 10.1016/j.adhoc.2014.12.006
   Miao Wu, 2010, 2010 3RD INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER THEORY AND ENGINEERING (ICACTE 2010), V0, PP484, DOI 10.1109/ICACTE.2010.5579493
   Said O., 2013, INT J COMPUTER NETWO, V5, P1
   Simonyan K, 2015, ARXIV, V0, P0
   Szegedy C, 2016, PROC CVPR IEEE, V0, PP2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, V0, PP1, DOI 10.1109/CVPR.2015.7298594
   Wu Z, 2013, P IFMBE P, V0, P1317
   Xiao X.Q., 2016, COMPUTER AGE, V0, P0
   Zhang Yun, 2010, CEMENT ENGINEERING, V0, P50
NR 14
TC 2
Z9 2
U1 0
U2 13
PU IOP PUBLISHING LTD
PI BRISTOL
PA DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND
SN 1742-6588
EI 1742-6596
J9 J PHYS CONF SER
PD JUN 15
PY 2020
VL 1438
IS 
BP 
EP 
DI 10.1088/1742-6596/1438/1/012029
PG 6
WC Imaging Science & Photographic Technology; Telecommunications
SC Imaging Science & Photographic Technology; Telecommunications
GA BQ7RM
UT WOS:000618445200029
DA 2023-04-26
ER

PT J
AU Anwar, AM
   Eldeib, AM
AF Anwar, Ayman M.
   Eldeib, Ayman M.
TI EEG Signal Classification Using Convolutional Neural Networks on Combined Spatial and Temporal Dimensions for BCI Systems
SO 42ND ANNUAL INTERNATIONAL CONFERENCES OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY: ENABLING INNOVATIVE TECHNOLOGIES FOR GLOBAL HEALTHCARE EMBC'20
LA English
DT Proceedings Paper
AB EEG signal classification is an important task to build an accurate Brain Computer Interface (BCI) system. Many machine learning and deep learning approaches have been used to classify EEG signals. Besides, many studies have involved the time and frequency domain features to classify EEG signals. On the other hand, a very limited number of studies combine the spatial and temporal dimensions of the EEG signal. Brain dynamics are very complex across different mental tasks, thus it is difficult to design efficient algorithms with features based on prior knowledge. Therefore, in this study, we utilized the 2D AlexNet Convolutional Neural Network (CNN) to learn EEG features across different mental tasks without prior knowledge. First, this study adds spatial and temporal dimensions of EEG signals to a 2D EEG topographic map. Second, topographic maps at different time indices were cascaded to populate a 2D image for a given time window. Finally, the topographic maps enabled the AlexNet to learn features from the spatial and temporal dimensions of the brain signals. The classification performance was obtained by the proposed method on a multiclass dataset from BCI Competition IV dataset 2a. The proposed system obtained an average classification accuracy of 81.09%, outperforming the previous state-of-the-art methods by a margin of 4% for the same dataset. The results showed that converting the EEG classification problem from a (1D) time series to a (2D) image classification problem improves the classification accuracy for BCI systems. Also, our EEG topographic maps enabled CNN to learn subtle features from spatial and temporal dimensions, which better represent mental tasks than individual time or frequency domain features.
C1 [Anwar, Ayman M.; Eldeib, Ayman M.] Cairo Univ, Fac Engn, Dept Syst & Biomed Engn, Giza, Egypt.
C3 Egyptian Knowledge Bank (EKB); Cairo University
RP Anwar, AM (corresponding author), Cairo Univ, Fac Engn, Dept Syst & Biomed Engn, Giza, Egypt.
EM ayman.anwar@ieee.org; eldeib@ieee.org
CR Aggarwal S., 2019, ARRAY, V1, P0, DOI 10.1016/J.ARRAY.2019.100003
   An X, 2014, LECT N BIOINFORMAT, V8590, P203, DOI 10.1007/978-3-319-09330-7_25
   Ang KK, 2012, FRONT NEUROSCI-SWITZ, V6, P0, DOI 10.3389/fnins.2012.00039
   [Anonymous], 1981, DYNAMICAL SYSTEMS TU, V0, P0, DOI DOI 10.1007/BFb0091924
   Baig M. Z., 2015, SCI INT, V27, P1165
   Brunner C., 2008, BCI COMPETITION IV 2, V0, P1
   Geron A., 2016, OREILLY MEDIA, V0, P489
   Graimann B, 2010, FRONT COLLECT, V0, PP1, DOI 10.1007/978-3-642-02091-9_1
   Jafri SRA, 2019, WIRELESS PERS COMMUN, V106, P2163, DOI 10.1007/s11277-018-5932-x
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V0, PP1097, DOI 10.2165/00129785-200404040-00005
   Lemm S, 2004, IEEE T BIO-MED ENG, V51, P1077, DOI 10.1109/TBME.2004.827076
   Tabar YR, 2017, J NEURAL ENG, V14, P0, DOI 10.1088/1741-2560/14/1/016003
   Wang YJ, 2008, IEEE ENG MED BIOL, V27, P64, DOI 10.1109/MEMB.2008.923958
   Yang HJ, 2015, IEEE ENG MED BIO, V0, PP2620, DOI 10.1109/EMBC.2015.7318929
   Zhang PB, 2019, IEEE T NEUR SYS REH, V27, P31, DOI 10.1109/TNSRE.2018.2884641
   Zhou SM, 2008, INFORM SCIENCES, V178, P1629, DOI 10.1016/j.ins.2007.11.012
NR 17
TC 8
Z9 8
U1 2
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1557-170X
EI 1558-4615
J9 IEEE ENG MED BIO
PD JUN 15
PY 2020
VL 0
IS 
BP 434
EP 437
DI 
PG 4
WC Engineering, Biomedical; Engineering, Electrical & Electronic
SC Engineering
GA BQ8TK
UT WOS:000621592200106
PM 33018021
DA 2023-04-26
ER

PT J
AU Maxwell, AE
   Pourmohammadi, P
   Poyner, JD
AF Maxwell, Aaron E.
   Pourmohammadi, Pariya
   Poyner, Joey D.
TI Mapping the Topographic Features of Mining-Related Valley Fills Using Mask R-CNN Deep Learning and Digital Elevation Data
SO REMOTE SENSING
LA English
DT Article
DE light detection and ranging; LiDAR; deep learning; convolutional neural networks; CNNs; mask regional-convolutional neural networks; mask R-CNN; digital terrain analysis; resource extraction
ID convolutional neural-networks; lidar data; scene classification; mountaintop removal; tree classification; accuracy assessment; land-cover; landslides; forest; quality
AB Modern elevation-determining remote sensing technologies such as light-detection and ranging (LiDAR) produce a wealth of topographic information that is increasingly being used in a wide range of disciplines, including archaeology and geomorphology. However, automated methods for mapping topographic features have remained a significant challenge. Deep learning (DL) mask regional-convolutional neural networks (Mask R-CNN), which provides context-based instance mapping, offers the potential to overcome many of the difficulties of previous approaches to topographic mapping. We therefore explore the application of Mask R-CNN to extract valley fill faces (VFFs), which are a product of mountaintop removal (MTR) coal mining in the Appalachian region of the eastern United States. LiDAR-derived slopeshades are provided as the only predictor variable in the model. Model generalization is evaluated by mapping multiple study sites outside the training data region. A range of assessment methods, including precision, recall, and F1 score, all based on VFF counts, as well as area- and a fuzzy area-based user's and producer's accuracy, indicate that the model was successful in mapping VFFs in new geographic regions, using elevation data derived from different LiDAR sensors. Precision, recall, and F1-score values were above 0.85 using VFF counts while user's and producer's accuracy were above 0.75 and 0.85 when using the area- and fuzzy area-based methods, respectively, when averaged across all study areas characterized with LiDAR data. Due to the limited availability of LiDAR data until relatively recently, we also assessed how well the model generalizes to terrain data created using photogrammetric methods that characterize past terrain conditions. Unfortunately, the model was not sufficiently general to allow successful mapping of VFFs using photogrammetrically-derived slopeshades, as all assessment metrics were lower than 0.60; however, this may partially be attributed to the quality of the photogrammetric data. The overall results suggest that the combination of Mask R-CNN and LiDAR has great potential for mapping anthropogenic and natural landscape features. To realize this vision, however, research on the mapping of other topographic features is needed, as well as the development of large topographic training datasets including a variety of features for calibrating and testing new methods.
C1 [Maxwell, Aaron E.; Poyner, Joey D.] West Virginia Univ, Dept Geol & Geog, Morgantown, WV 26506 USA.
   [Pourmohammadi, Pariya] West Virginia Univ, Davis Coll Agr Nat Resources & Design, Dept Design & Community Dev, Morgantown, WV 26506 USA.
C3 West Virginia University; West Virginia University
RP Maxwell, AE (corresponding author), West Virginia Univ, Dept Geol & Geog, Morgantown, WV 26506 USA.
EM Aaron.Maxwell@mail.wvu.edu; papourmohammadi@mix.wvu.edu; jpoyner@mix.wvu.edu
CR Abdulwahid WM, 2017, LANDSLIDES, V14, P1057, DOI 10.1007/s10346-016-0744-0
   Ardizzone F, 2007, NAT HAZARD EARTH SYS, V7, P637, DOI 10.5194/nhess-7-637-2007
   Arundel S. T., 2015, CARTOGR GEOGRAPH INF, V42, P40, DOI 10.1080/15230406.2015.1057229
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Ball JE, 2017, J APPL REMOTE SENS, V11, P0, DOI 10.1117/1.JRS.11.042609
   Behrens T, 2018, SCI REP-UK, V8, P0, DOI 10.1038/s41598-018-33516-6
   Bernhardt ES, 2011, ANN NY ACAD SCI, V1223, P39, DOI 10.1111/j.1749-6632.2011.05986.x
   Brandtberg T, 2003, REMOTE SENS ENVIRON, V85, P290, DOI 10.1016/S0034-4257(03)00008-7
   Chase AF, 2012, P NATL ACAD SCI USA, V109, P12916, DOI 10.1073/pnas.1205198109
   Chen G, 2018, GISCI REMOTE SENS, V55, P159, DOI 10.1080/15481603.2018.1426092
   Chen WT, 2014, REMOTE SENS ENVIRON, V152, P291, DOI 10.1016/j.rse.2014.07.004
   DeWitt JD, 2015, GISCI REMOTE SENS, V52, P179, DOI 10.1080/15481603.2015.1019708
   ESRI, 2018, ARCGIS PRO 2 2, V0, P0
   Fritz KM, 2010, J N AM BENTHOL SOC, V29, P673, DOI 10.1899/09-060.1
   Ghorbanzadeh O, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11020196
   Gold RD, 2013, J GEOPHYS RES-SOL EA, V118, P3753, DOI 10.1002/jgrb.50238
   Goutte C., 2005, P ADV INF RETR, V0, P0
   Griffith MB, 2012, SCI TOTAL ENVIRON, V417, P1, DOI 10.1016/j.scitotenv.2011.12.042
   Guan HY, 2015, REMOTE SENS LETT, V6, P864, DOI 10.1080/2150704X.2015.1088668
   Hackel T., 2017, SEMANTIC3D NET NEW L, V0, P0
   Haneberg WC, 2009, B ENG GEOL ENVIRON, V68, P263, DOI 10.1007/s10064-009-0204-3
   Hartman KJ, 2005, HYDROBIOLOGIA, V532, P91, DOI 10.1007/s10750-004-9019-1
   He K., 2016, P IEEE C COMP VIS PA, V2016, P1512.03385, DOI 10.1109/CVPR.2016.90
   Henderson P., 2017, COMPUTER VISION ACCV, V0, P0
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Hu W, 2015, J SENSORS, V2015, P0, DOI 10.1155/2015/258619
   Hu XY, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8090730
   Jaboyedoff M, 2012, NAT HAZARDS, V61, P5, DOI 10.1007/s11069-010-9634-2
   Janssens-Coron E., 2019, INT ARCH PHOTOGRAMM, V42, P1559, DOI 10.5194/isprs-archives-XLII-2-W13-1559-2019
   Ji SP, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10010075
   Kim M, 2011, INT J REMOTE SENS, V32, P2825, DOI 10.1080/01431161003745608
   Kim M, 2009, PHOTOGRAMM ENG REM S, V75, P819, DOI 10.14358/PERS.75.7.819
   Kussul N, 2017, IEEE GEOSCI REMOTE S, V14, P778, DOI 10.1109/LGRS.2017.2681128
   KWEON IS, 1994, CVGIP-IMAG UNDERSTAN, V59, P171, DOI 10.1006/ciun.1994.1011
   Latif Z.A., 2012, P 2012 IEEE 8 INT C, V0, P0
   Li W, 2017, SENSORS-BASEL, V17, P0, DOI 10.3390/s17102392
   Li WJ, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9010022
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Li Z, 2004, DIGITAL TERRAIN MODE, V0, P0
   Liu F., 2017, P 2017 IEEE INT C CO, V0, P0
   Maggiori E., 2016, P 2016 IEEE INT GEOS, V0, P0
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821
   Maxwell A. E., 2013, NATURAL SCIENCE, V5, P229
   Maxwell AE, 2015, INT J REMOTE SENS, V36, P4384, DOI 10.1080/01431161.2015.1083632
   Miller AJ, 2016, LAND-BASEL, V5, P0, DOI 10.3390/land5030022
   Miller AJ, 2014, WATER-SUI, V6, P472, DOI 10.3390/w6030472
   Passalacqua P, 2010, WATER RESOUR RES, V46, P0, DOI 10.1029/2009WR008812
   Penatti Otavio A. B., 2015, 2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW), V0, PP44, DOI 10.1109/CVPRW.2015.7301382
   Reed M., 2018, WILL ANTHROPOGENIC V, V0, P0
   Ren SQ, 2015, ADV NEUR IN, V28, P0, DOI 10.1109/TPAMI.2016.2577031
   Rizaldi A., 2018, ISPRS ANN PHOTOGRAMM, V4, P231, DOI 10.5194/ISPRS-ANNALS-IV-2-231-2018
   Ross MRV, 2016, ENVIRON SCI TECHNOL, V50, P2064, DOI 10.1021/acs.est.5b04532
   Stehman SV, 2013, REMOTE SENS ENVIRON, V132, P202, DOI 10.1016/j.rse.2013.01.016
   STEHMAN SV, 1995, INT J REMOTE SENS, V16, P589, DOI 10.1080/01431169508954425
   STEHMAN SV, 1992, PHOTOGRAMM ENG REM S, V58, P1343
   Sterenczak K, 2016, EUR J REMOTE SENS, V49, P599, DOI 10.5721/EuJRS20164932
   Stewart EL, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11192209
   Stumpf A, 2011, REMOTE SENS ENVIRON, V115, P2564, DOI 10.1016/j.rse.2011.05.013
   Trier OD, 2019, ARCHAEOL PROSPECT, V26, P165, DOI 10.1002/arp.1731
   Trier OD, 2015, J ARCHAEOL SCI-REP, V2, P69, DOI 10.1016/j.jasrep.2015.01.005
   Van Den Eeckhaut M, 2007, EARTH SURF PROC LAND, V32, P754, DOI 10.1002/esp.1417
   Van Den Eeckhaut M, 2012, GEOMORPHOLOGY, V173, P30, DOI 10.1016/j.geomorph.2012.05.024
   Verhagen P, 2012, J ARCHAEOL SCI, V39, P698, DOI 10.1016/j.jas.2011.11.001
   Wickham JD, 2007, LANDSCAPE ECOL, V22, P179, DOI 10.1007/s10980-006-9040-z
   Wickham J, 2013, BIOSCIENCE, V63, P335, DOI 10.1525/bio.2013.63.5.7
   Wood PB, 2013, J HERPETOL, V47, P119, DOI 10.1670/11-187
   You YN, 2019, IEEE ACCESS, V7, P128431, DOI 10.1109/ACCESS.2019.2940102
   Youssef AM, 2016, LANDSLIDES, V13, P839, DOI 10.1007/s10346-015-0614-1
   Yu XR, 2017, GISCI REMOTE SENS, V54, P741, DOI 10.1080/15481603.2017.1323377
   Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798
   Zhang WX, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10091487
   Zhang YX, 2018, CONSTRUCTION RESEARCH CONGRESS 2018: CONSTRUCTION INFORMATION TECHNOLOGY, V0, P22
   Zhao T, 2018, INT SOC OPTICS PHOTO, V1078, P0
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
   Zou XH, 2017, IEEE GEOSCI REMOTE S, V14, P2360, DOI 10.1109/LGRS.2017.2764938
   Zullig KJ, 2011, AM J PUBLIC HEALTH, V101, P848, DOI 10.2105/AJPH.2010.300073
NR 83
TC 33
Z9 33
U1 4
U2 29
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 
EI 2072-4292
J9 REMOTE SENS-BASEL
JI Remote Sens.
PD FEB 15
PY 2020
VL 12
IS 3
BP 
EP 
DI 10.3390/rs12030547
PG 23
WC Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology
SC Environmental Sciences & Ecology; Geology; Remote Sensing; Imaging Science & Photographic Technology
GA KO2QH
UT WOS:000515393800206
DA 2023-04-26
ER

PT J
AU Xu, SW
   Zhang, SH
   Zeng, J
   Li, TY
   Guo, QH
   Jin, SC
AF Xu, Shiwu
   Zhang, Shihui
   Zeng, Jue
   Li, Tingyu
   Guo, Qinghua
   Jin, Shichao
TI A Framework for Land Use Scenes Classification Based on Landscape Photos
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Semantics; Remote sensing; Image analysis; Image segmentation; Object recognition; Machine learning; Forestry; Deep convolutional neural networks (DCNNs); landscape photos; land survey; land use scene classification
ID convolutional neural-networks; selection; gradient
AB Space-earth integrated stereoscopic mapping promotes the progress of earth observation technologies. The method which combined remote sensing images with zenith perspectives and ground-level landscape photos with slanted viewing angles improves the efficiency and accuracy of land surveys. Recently, numerous efforts have been devoted to combining deep learning and remote sensing images for the classification of land use scenes. However, improvement of classification accuracy has been limited because of the lack of sectional representation. Landscape photos can describe the cross-sections in detail. For this reason, this study constructed a land-use semantic photo dataset (LSPD) and proposed a land-use classification framework for photos (LUCFP) based on Inception-v4. LSPD was constructed through semantic planning, scene segmentation, supervised iteration transfer learning, and augmentation of photos. LSPD has 1.4 million photos collected from seven geographic regions of China, and covers 13 land-use categories and 44 semantic categories. LUCFP adapts scene segmentation based on depth of field, multisemantic block labeling, and weighting of semantic joint spatial ranges to determine the land use category. To validate LUCFP, nine semantic samples (9x3x2000 photos) were chosen from LSPD, obtaining an overall accuracy of 97.64%. The best photo cropping method was masking, which crops the boundary of the scene labeled by the photo, leading to an accuracy of 90.32%. The optimal pixel size that balances speed and accuracy is 675x675, with speed reaching 30 photos per second with an average accuracy of 93.80%. LUCFP has been successfully applied to the automatic verification of land surveys in China.
C1 [Xu, Shiwu; Zhang, Shihui; Li, Tingyu] China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Hubei, Peoples R China.
   [Zeng, Jue] China Univ Geosci, Wuhan 430074, Hubei, Peoples R China.
   [Zeng, Jue] China Inst Land Surveying & Planning, Beijing 100035, Peoples R China.
   [Guo, Qinghua; Jin, Shichao] Chinese Acad Sci, Inst Bot, State Key Lab Vegetat & Environm Change, Beijing 100093, Peoples R China.
   [Guo, Qinghua; Jin, Shichao] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Jin, Shichao] Nanjing Agr Univ, Plant Phen Res Ctr, Nanjing 210095, Jiangsu, Peoples R China.
C3 China University of Geosciences; China University of Geosciences; Chinese Academy of Sciences; Institute of Botany, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Nanjing Agricultural University
RP Zhang, SH (corresponding author), China Univ Geosci, Sch Geog & Informat Engn, Wuhan 430074, Hubei, Peoples R China.
EM xushiwu1973@126.com; 1284989128@qq.com; 598722949@qq.com; 493163208@qq.com; qguo@ibcas.ac.cn; jinshichao1993@gmail.com
FU China Institute of Land Surveying and Planning [2019114017]
CR [Anonymous], 2010, 18 SIGSPATIAL INT C, V0, P0, DOI DOI 10.1145/1869790.1869829
   [Anonymous], 2017, IEEE I CONF COMP VIS, V0, P0, DOI DOI 10.1109/ICCV.2017.322
   Bahmanyar R, 2015, IEEE GEOSCI REMOTE S, V12, P1357, DOI 10.1109/LGRS.2015.2402391
   Chakraborty S, 2015, IEEE T PATTERN ANAL, V37, P1945, DOI 10.1109/TPAMI.2015.2389848
   Chen L, 2017, PROC CVPR IEEE, V0, PP6298, DOI 10.1109/CVPR.2017.667
   Chen Q, 2017, IEEE T EVOLUT COMPUT, V21, P792, DOI 10.1109/TEVC.2017.2683489
   Cheng G, 2015, IEEE T GEOSCI REMOTE, V53, P4238, DOI 10.1109/TGRS.2015.2393857
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P281, DOI 10.1109/TPAMI.2003.1177159
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Girshick R, 2014, PROC CVPR IEEE, V0, PP580, DOI 10.1109/CVPR.2014.81
   Guo QH, 2020, SCI CHINA EARTH SCI, V63, P1457, DOI 10.1007/s11430-019-9584-9
   Han JF, 2019, IEEE I CONF COMP VIS, V0, PP5137, DOI 10.1109/ICCV.2019.00524
   Hu F, 2016, INT CONF SIGN PROCES, V0, PP198, DOI 10.1109/ICSP.2016.7877823
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Hu F, 2015, IEEE J-STARS, V8, P2015, DOI 10.1109/JSTARS.2015.2444405
   Huang B, 2018, REMOTE SENS ENVIRON, V214, P73, DOI 10.1016/j.rse.2018.04.050
   Keskar N. S., 2017, 5 INT C LEARN REPR T, V0, P0
   Krizhevsky Alex, 2017, COMMUNICATIONS OF THE ACM, V60, P84, DOI 10.1145/3065386
   Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821
   Miao ZQ, 2011, 2011 INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING AND MULTIMEDIA COMMUNICATION, V0, P1
   Nobi M. N., 2011, J SCI RES, V3, P81, DOI 10.3329/JSR.V3I1.5544
   Ozdarici-Ok A, 2015, REMOTE SENS-BASEL, V7, P5611, DOI 10.3390/rs70505611
   Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), V0, PP1, DOI 10.1109/ICPHM.2017.7998297
   Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155
   Shi H, 2015, IEEE GEOSCI REMOTE S, V12, P1948, DOI 10.1109/LGRS.2015.2439696
   Shorten C, 2019, J BIG DATA-GER, V6, P0, DOI 10.1186/s40537-019-0197-0
   Szegedy C, 2017, AAAI CONF ARTIF INTE, V0, P4278
   Tong XY, 2020, REMOTE SENS ENVIRON, V237, P0, DOI 10.1016/j.rse.2019.111322
   Wang Zhao-Hu, 2005, CHINESE JOURNAL OF COMPUTERS, V28, P1686
   Watkins JL, 1996, ICES J MAR SCI, V53, P339, DOI 10.1006/jmsc.1996.0046
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Xiao JX, 2010, PROC CVPR IEEE, V0, PP3485, DOI 10.1109/CVPR.2010.5539970
   Xu RD, 2020, FRONT BIOENG BIOTECH, V8, P0, DOI 10.3389/fbioe.2020.01026
   [许夙晖 Xu Suhui], 2016, 测绘学报 ACTA GEODETICA ET CARTOGRAPHICA SINICA, V45, P834
   Yang Y, 2011, IEEE I CONF COMP VIS, V0, PP1465, DOI 10.1109/ICCV.2011.6126403
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang F, 2016, IEEE T GEOSCI REMOTE, V54, P1793, DOI 10.1109/TGRS.2015.2488681
   Zhang XY, 2015, REMOTE SENS ENVIRON, V169, P37, DOI 10.1016/j.rse.2015.07.017
   Zhao B, 2016, IEEE T GEOSCI REMOTE, V54, P2108, DOI 10.1109/TGRS.2015.2496185
   Zhao B, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8020157
   Zhong QY, 2020, NEUROCOMPUTING, V395, P170, DOI 10.1016/j.neucom.2017.12.070
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu X.-F., 2009, SCI SURVEYING MAPPIN, V5, P132
   [朱秀芳 ZHU Xiufang], 2007, 遥感学报 JOURNAL OF REMOTE SENSING, V11, P826
NR 44
TC 7
Z9 8
U1 6
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2020
VL 13
IS 
BP 6124
EP 6141
DI 10.1109/JSTARS.2020.3028158
PG 18
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA OC7MU
UT WOS:000579341600009
DA 2023-04-26
ER
