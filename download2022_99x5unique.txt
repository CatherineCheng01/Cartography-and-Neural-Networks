
PT J
AU Ma, XP
   Zhang, XK
   Pun, MO
AF Ma, Xianping
   Zhang, Xiaokang
   Pun, Man-On
TI A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Transformers; Remote sensing; Semantics; Image segmentation; Feature extraction; Fuses; Decoding; Combined squeeze-and-excitation (CSE); cross attention; crossmodal multiscale fusion; transformer
AB Driven by the rapid development of Earth observation sensors, semantic segmentation using multimodal fusion of remote sensing data has drawn substantial research attention in recent years. However, existing multimodal fusion methods based on convolutional neural networks cannot capture long-range dependencies across multiscale feature maps of remote sensing data in different modalities. To circumvent this problem, this work proposes a crossmodal multiscale fusion network (CMFNet) by exploiting the transformer architecture. In contrast to the conventional early, late, or hybrid fusion networks, the proposed CMFNet fuses information of different modalities at multiple scales using the cross-attention mechanism. More specifically, the CMFNet utilizes a novel cross-modal attention architecture to fuse multiscale convolutional feature maps of optical remote sensing images and digital surface model data through a crossmodal multiscale transformer (CMTrans) and a multiscale context augmented transformer (MCATrans). The CMTrans can effectively model long-range dependencies across multiscale feature maps derived from multimodal data, while the MCATrans can learn discriminative integrated representations for semantic segmentation. Extensive experiments on two large-scale fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam, confirm the excellent performance of the proposed CMFNet as compared to other multimodal fusion methods.
C1 [Ma, Xianping; Zhang, Xiaokang; Pun, Man-On] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Zhang, Xiaokang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
C3 Chinese University of Hong Kong, Shenzhen; Chinese Academy of Sciences; University of Science & Technology of China, CAS
RP Zhang, XK; Pun, MO (corresponding author), Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
EM xianpingma@link.cuhk.edu.cn; zhangxiaokang@cuhk.edu.cn; simonpun@cuhk.edu.cn
FU National Natural Science Foundation of China [41801323]; China Postdoctoral Science Foundation [2020M682038]; Shenzhen Science and Technology Innovation Committee [JCYJ20190813170803617]
CR Audebert N, 2018, ISPRS J PHOTOGRAMM, V140, P20, DOI 10.1016/j.isprsjprs.2017.11.011
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Cao H., 2021, ARXIV210505537, V2021, P0
   Chen J, 2021, ARXIV210204306, V0, P0
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Chen R, 2019, KDD19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, V0, PP2145, DOI 10.1145/3292500.3330690
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Dosovitskiy A., 2020, IMAGE IS WORTH 16X16, V0, P0
   Fukui Akira, 2016, P C EMP METH NAT LAN, V0, P0, DOI DOI 10.18653/v1/d16-1044
   Gao LR, 2015, IEEE GEOSCI REMOTE S, V12, P349, DOI 10.1109/LGRS.2014.2341044
   Ghamisi P, 2019, IEEE GEOSC REM SEN M, V7, P6, DOI 10.1109/MGRS.2018.2890023
   Ghassemian H, 2016, INFORM FUSION, V32, P75, DOI 10.1016/j.inffus.2016.03.003
   Gislason PO, 2006, PATTERN RECOGN LETT, V27, P294, DOI 10.1016/j.patrec.2005.08.011
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He KM, 2015, IEEE I CONF COMP VIS, V0, PP1026, DOI 10.1109/ICCV.2015.123
   Hong DF, 2022, IEEE GEOSCI REMOTE S, V19, P0, DOI 10.1109/LGRS.2020.3017414
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Kim JH, 2018, ADV NEUR IN, V31, P0
   Kim Jinhwa, 2017, ICLR, V0, P0
   Li H, 2018, INT C PATT RECOG, V0, PP2705, DOI 10.1109/ICPR.2018.8546006
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), V0, PP9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lu XC, 2015, IEEE J-STARS, V8, P3546, DOI 10.1109/JSTARS.2015.2442594
   Mahmood F, 2019, PROC SPIE, V10950, P0, DOI 10.1117/12.2513117
   Ngiam J, 2011, IEEE INT C MACH LEAR, V0, PP689, DOI 10.5555/3104482.3104569
   Prakash A., 1900, V2021, V0, P7077
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seichter D, 2021, IEEE INT CONF ROBOT, V0, PP13525, DOI 10.1109/ICRA48506.2021.9561675
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
   Simonyan K, 2015, ARXIV, V0, P0
   Sun YW, 2023, IEEE T NEUR NET LEAR, V34, P1035, DOI 10.1109/TNNLS.2021.3107375
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), V0, PP6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEURAL INFORM PR, V30, P5998, DOI 10.48550/ARXIV.1706.03762
   Wang H., 2021, ARXIV210904335, V0, P0
   Wang LB, 2022, IEEE GEOSCI REMOTE S, V19, P0, DOI 10.1109/LGRS.2022.3143368
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Xiaokang Chen, 2020, COMPUTER VISION - ECCV 2020 16TH EUROPEAN CONFERENCE. PROCEEDINGS. LECTURE NOTES IN COMPUTER SCIENCE (LNCS 12356), V0, PP561, DOI 10.1007/978-3-030-58621-8_33
   Xu DD, 2020, IEEE ACCESS, V8, P206445, DOI 10.1109/ACCESS.2020.3037770
   Xu XD, 2018, IEEE T GEOSCI REMOTE, V56, P937, DOI 10.1109/TGRS.2017.2756851
   Zadeh A., 2017, EMNLP 2017 C EMPIRIC, V0, P0, DOI DOI 10.18653/V1/D17-1115
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zhao HS, 2017, PROC CVPR IEEE, V0, PP6230, DOI 10.1109/CVPR.2017.660
   Zheng S., 2021, IEEE GEOSCI REMOTE S, V19, P1
NR 46
TC 8
Z9 8
U1 24
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 3463
EP 3474
DI 10.1109/JSTARS.2022.3165005
PG 12
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 1F4CW
UT WOS:000795117800003
DA 2023-04-26
ER

PT J
AU Wang, SX
   Wang, ZY
   Vejalla, S
   Ganegoda, A
   Nittala, MG
   Sadda, SR
   Hu, ZJ
AF Wang, Shuxian
   Wang, Ziyuan
   Vejalla, Srimanasa
   Ganegoda, Anushika
   Nittala, Muneeswar Gupta
   Sadda, SriniVas Reddy
   Hu, Zhihong Jewel
TI Reverse engineering for reconstructing baseline features of dry age-related macular degeneration in optical coherence tomography
SO SCIENTIFIC REPORTS
LA English
DT Article
AB Age-related macular degeneration (AMD) is the most widespread cause of blindness and the identification of baseline AMD features or biomarkers is critical for early intervention. Optical coherence tomography (OCT) imaging produces a 3D volume consisting of cross sections of retinal tissue while fundus fluorescence (FAF) imaging produces a 2D mapping of retina. FAF has been a good standard for assessing dry AMD late-stage geographic atrophy (GA) while OCT has been used for assessing early AMD biomarkers beyond as well. However, previous approaches in large extent defined AMD features subjectively based on clinicians' observation. Deep learning-an objective artificial intelligence approach, may enable to discover 'true' salient AMD features. We develop a novel reverse engineering approach which bases on the backbone of a fully convolutional neural network to objectively identify and visualize AMD early biomarkers in OCT from baseline exams before significant atrophy occurs. Utilizing manually annotated GA regions on FAF from a follow-up visit as ground truth, we segment GA regions and reconstruct early AMD features in baseline OCT volumes. In this preliminary exploration, compared with ground truth, we achieve baseline GA segmentation accuracy of 0.95 and overlapping ratio of 0.65. The reconstructions consistently highlight that large druse and druse clusters with or without mixed hyper-reflective focus lesion on baseline OCT cause the conversion of GA after 12 months. However, hyper-reflective focus lesions and subretinal drusenoid deposit lesions alone are not seen such conversion after 12 months. Further research with larger dataset would be needed to verify these findings.
C1 [Wang, Shuxian; Wang, Ziyuan; Vejalla, Srimanasa; Ganegoda, Anushika; Nittala, Muneeswar Gupta; Sadda, SriniVas Reddy; Hu, Zhihong Jewel] Doheny Eye Inst, 150 North Orange Grove Blvd,Room 251, Pasadena, CA 91103 USA.
   [Wang, Shuxian] Univ North Carolina Chapel Hill, Chapel Hill, NC 27514 USA.
C3 Doheny Eye Institute; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina School of Medicine
RP Hu, ZJ (corresponding author), Doheny Eye Inst, 150 North Orange Grove Blvd,Room 251, Pasadena, CA 91103 USA.
EM jhu@doheny.org
FU National Eye Institute of the National Institutes of Health [R21EY030619]
CR Coleman HR, 2008, LANCET, V372, P1835, DOI 10.1016/S0140-6736(08)61759-6
   Zeiler MD, 2013, ARXIV, V0, P0
   HUANG D, 1991, SCIENCE, V254, P1178, DOI 10.1126/science.1957169
   Lei JQ, 2017, GRAEF ARCH CLIN EXP, V255, P1551, DOI 10.1007/s00417-017-3693-y
   Ma X, 2020, IEEE J BIOMED HEALTH, V24, P3443, DOI 10.1109/JBHI.2020.2999588
   Mishra Z, 2020, SCI REP-UK, V10, P0, DOI 10.1038/s41598-020-66355-5
   Ronneberger O, 2015, ARXIV, V0, P0, DOI DOI 10.48550/ARXIV.1505.04597
   Saha Sajib, 2020, APPL AI LETT, V1, P0, DOI 10.1002/ail2.16
   Saha S, 2019, SCI REP-UK, V9, P0, DOI 10.1038/s41598-019-47390-3
   Schmidt-Erfurth U, 2020, AM J OPHTHALMOL, V216, P257, DOI 10.1016/j.ajo.2020.03.042
   Schmitz-Valckenberg S, 2008, RETINA-J RET VIT DIS, V28, P385, DOI 10.1097/IAE.0b013e318164a907
   Stahl A, 2020, DTSCH ARZTEBL INT, V117, P513, DOI 10.3238/arztebl.2020.0513
   Wang ZY, 2019, PROC SPIE, V10950, P0, DOI 10.1117/12.2511538
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NATURE PORTFOLIO
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN 2045-2322
EI 
J9 SCI REP-UK
JI Sci Rep
PD DEC 31
PY 2022
VL 12
IS 1
BP 
EP 
DI 10.1038/s41598-022-27140-8
PG 10
WC Multidisciplinary Sciences
SC Science & Technology - Other Topics
GA 9D0CT
UT WOS:000935774900005
PM 36587062
DA 2023-04-26
ER

PT J
AU Gao, F
   Tu, J
   Wang, J
   Hussain, A
   Zhou, HY
AF Gao, Fei
   Tu, Jun
   Wang, Jun
   Hussain, Amir
   Zhou, Huiyu
TI RoadSeg-CD: A Network With Connectivity Array and Direction Map for Road Extraction From SAR Images
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
LA English
DT Article
DE Roads; Feature extraction; Radar polarimetry; Image segmentation; Synthetic aperture radar; Image edge detection; Decoding; Connectivity array; direction map; neural network; road extraction; synthetic aperture radar (SAR)
ID remote-sensing images
AB Road extraction from synthetic aperture radar (SAR) images has attracted much attention in the field of remote sensing image processing. General road extraction algorithms, affected by shadows of buildings and trees, are prone to producing fragmented road segments. To improve the accuracy and completeness of road extraction, we propose a neural network-based algorithm, which takes the connectivity and direction features of roads into consideration, named RoadSeg-CD. It consists of two branches: one is the main branch for road segmentation; the other is the auxiliary branch for learning road directions. In the main branch, a connectivity array is designed to utilize local contextual information and construct a connectivity loss based on the predicted probabilities of neighboring pixels. In the auxiliary branch, we proposed a novel road direction map, which is used for learning the directions of roads. The two branches are connected by specific feature fusion process, and the output from the main branch is taken as the road extraction result. Experiments on real radar images are implemented to validate the effectiveness of our method. The experimental results demonstrate that our method can obtain more continuous and more complete roads than several state-of-the-art road extraction algorithms.
C1 [Gao, Fei; Tu, Jun; Wang, Jun] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Hussain, Amir] Edinburgh Napier Univ, Cyber & Big Data Res Lab, Edinburgh EH11 4BN, Midlothian, Scotland.
   [Hussain, Amir] Taibah Univ, Medina 30001, Saudi Arabia.
   [Zhou, Huiyu] Univ Leicester, Dept Informat, Leicester LE1 7RH, Leics, England.
C3 Beihang University; Edinburgh Napier University; Taibah University; University of Leicester
RP Wang, J (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM feigao2000@163.com; tujun@buaa.edu.cn; wangj203@buaa.edu.cn; a.hussain@napier.ac.uk; hz143@leicester.ac.uk
FU National Natural Science Foundation of China [61771027, 61071139, 61471019, 61501011, 61171122]; U.K. Engineering and Physical Sciences Research Council (EPSRC) [EP/M026981/1, EP/T021063/1, EP/T024917/1]; Royal Society-Newton Advanced Fellowship [NA160342]; European Union [720325]
CR Abdollahi A, 2021, GISCI REMOTE SENS, V58, P1151, DOI 10.1080/15481603.2021.1972713
   Abdollahi A, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12091444
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Batra A, 2019, PROC CVPR IEEE, V0, PP10377, DOI 10.1109/CVPR.2019.01063
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), V0, P0
   Chen L.-C., 2018, P EUR C COMP VIS ECC, V0, PP801, DOI 10.1007/978-3-030-01234-2_49
   Chen Y, 2006, IEEE IMAGE PROC, V0, PP2337, DOI 10.1109/ICIP.2006.312855
   Cheng GL, 2017, IEEE T GEOSCI REMOTE, V55, P3322, DOI 10.1109/TGRS.2017.2669341
   da Silva CR, 2010, CAN J REMOTE SENS, V36, P737, DOI 10.5589/m11-006
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   DellAcqua F, 2001, IEEE T GEOSCI REMOTE, V39, P2287, DOI 10.1109/36.957292
   Ding L, 2021, IEEE T GEOSCI REMOTE, V59, P10243, DOI 10.1109/TGRS.2020.3034011
   Douglas D. H., 1973, CARTOGRAPHICA INT J, V10, P112, DOI 10.3138/FM57-6770-U75U-7727
   Fjortoft R, 1998, IEEE T GEOSCI REMOTE, V36, P793, DOI 10.1109/36.673672
   Gamba P, 2006, IEEE GEOSCI REMOTE S, V3, P387, DOI 10.1109/LGRS.2006.873875
   Gao F, 2019, COGN COMPUT, V11, P809, DOI 10.1007/s12559-018-9563-z
   Grinias I, 2016, ISPRS J PHOTOGRAMM, V122, P145, DOI 10.1016/j.isprsjprs.2016.10.010
   Guo Y, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS
   He, 1900, V13, V0, P2021
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   He YS, 2021, IEEE J-STARS, V14, P3846, DOI 10.1109/JSTARS.2021.3068530
   Henry C, 2018, IEEE GEOSCI REMOTE S, V15, P1867, DOI 10.1109/LGRS.2018.2864342
   Jia CL, 2005, INT GEOSCI REMOTE SE, V0, P336
   Kampffmeyer M, 2019, IEEE T IMAGE PROCESS, V28, P2518, DOI 10.1109/TIP.2018.2886997
   Li JY, 2018, INT J REMOTE SENS, V39, P2421, DOI 10.1080/01431161.2018.1425563
   Li JJ, 2021, IEEE J-STARS, V14, P10535, DOI 10.1109/JSTARS.2021.3094673
   Li XF, 2010, INT J REMOTE SENS, V31, P5041, DOI 10.1080/01431160903283835
   Lian RB, 2020, IEEE J-STARS, V13, P5489, DOI 10.1109/JSTARS.2020.3023549
   Liu NY, 2018, IEEE GEOSCI REMOTE S, V15, P434, DOI 10.1109/LGRS.2018.2792421
   Liu WF, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9060590
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Ma F, 2022, IEEE T GEOSCI REMOTE, V60, P0, DOI 10.1109/TGRS.2021.3108585
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   Paszke A, 2019, ADV NEUR IN, V32, P0
   Perciano T, 2011, INT GEOSCI REMOTE SE, V0, PP1159, DOI 10.1109/IGARSS.2011.6049403
   Ramer U., 1972, COMPUT VISION GRAPH, V1, P244, DOI 10.1016/S0146-664X(72)80017-0
   Ronneberger O., 2015, P MED IM COMP COMP A, V0, P234
   Saito S, 2016, J IMAGING SCI TECHN, V60, P0, DOI 10.2352/J.ImagingSci.Technol.2016.60.1.010402
   Tao C, 2019, ISPRS J PHOTOGRAMM, V158, P155, DOI 10.1016/j.isprsjprs.2019.10.001
   Tupin F, 1998, IEEE T GEOSCI REMOTE, V36, P434, DOI 10.1109/36.662728
   Van Etten A, 2020, INT GEOSCI REMOTE SE, V0, PP3920, DOI 10.1109/IGARSS39084.2020.9324091
   Wiedemann C., 1998, EMPIRICAL EVALUATION, V12, P172
   Xiao FH, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11232733
   Xiao FH, 2016, INT GEOSCI REMOTE SE, V0, PP1266, DOI 10.1109/IGARSS.2016.7729321
   Yue ZY, 2021, COGN COMPUT, V13, P795, DOI 10.1007/s12559-019-09639-x
   Zhang QQ, 2019, EUR J REMOTE SENS, V52, P572, DOI 10.1080/22797254.2019.1694447
   Zhang W, 2022, IEEE GEOSCI REMOTE S, V19, P0, DOI 10.1109/LGRS.2020.3022478
   Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587
   Zhou LC, 2018, IEEE COMPUT SOC CONF, V0, PP192, DOI 10.1109/CVPRW.2018.00034
   Zhou MT, 2020, ISPRS J PHOTOGRAMM, V168, P288, DOI 10.1016/j.isprsjprs.2020.08.019
NR 50
TC 1
Z9 1
U1 3
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN 15
PY 2022
VL 15
IS 
BP 3992
EP 4003
DI 10.1109/JSTARS.2022.3175594
PG 12
WC Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science & Photographic Technology
GA 1R3YB
UT WOS:000803307300008
DA 2023-04-26
ER

PT J
AU Lu, YC
   James, T
   Schillaci, C
   Lipani, A
AF Lu, Yichen
   James, Thomas
   Schillaci, Calogero
   Lipani, Aldo
TI Snow detection in alpine regions with Convolutional Neural Networks: discriminating snow from cold clouds and water body
SO GISCIENCE & REMOTE SENSING
LA English
DT Article
DE Snow; Alps; Convolutional Neural Networks; mapping; DeepLabV3+
ID segmentation; sentinel-2; resolution; mask
AB Accurately monitoring the variation of snow cover from remote sensing is vital since it assists in various fields including prediction of floods, control of runoff values, and the ice regime of rivers. Spectral indices methods are traditional ways to realize snow segmentation, including the most common one - the Normalized Difference Snow Index (NDSI), which utilizes the combination of green and short-wave infrared (SWIR) bands. In addition, spectral indices methods heavily depend on the optimal threshold to determine the accuracy, making it time-consuming to find optimal values for different places. Convolutional neural networks ensemble model with DeepLabV3+ was employed as sub-models for snow segmentation using (Sentinel-2), which aims to distinguish clouds and water body from snow. The imagery dataset generated in this article contains sites in global alpine regions such as Tibetan Plateau in China, the Alps in Switzerland, Alaska in the United States, Southern Patagonian Icefield in Chile, Tsylos Provincial Park, Tatsamenie Peak, and Dalton Peak in Canada. To overcome the limitation of DeepLabV3+, which only accepts three channels as input features, and the need to use six features: green, red, blue, near-infraRed, SWIR, and NDSI, 20 three-channel DeepLabV3+ sub-models, were constructed with different combinations of three features and then ensembled together. The proposed ensemble model showed superior performance than benchmark spectral indices method, with mIoU values ranging from 0.8075 to 0.9538 in different test sites. The results of this project contribute to the development of automated snow segmentation tools to assist earth observation applications.
C1 [Lu, Yichen; Lipani, Aldo] UCL, Civil Environm & Geomat Engn Dept, London, England.
   [James, Thomas] WeGaw SA, Vaud, Switzerland.
   [Schillaci, Calogero] European Commiss, Joint Res Ctr JRC, Ispra, Italy.
   [Schillaci, Calogero] Univ Milan, Agr & Environm Sci Prod Landscape Agroenergy, Milan, Italy.
C3 University of London; University College London; European Commission Joint Research Centre; EC JRC ISPRA Site; University of Milan
RP Lipani, A (corresponding author), UCL, Civil Environm & Geomat Engn Dept, London, England.
EM aldo.lipani@ucl.ac.uk
CR [Anonymous], 2012, REMOTE SENSING DROUG, V0, P0, DOI DOI 10.1201/B11863
   Awasthi S, 2021, GISCI REMOTE SENS, V58, P852, DOI 10.1080/15481603.2021.1946938
   Bianchi FM, 2021, IEEE J-STARS, V14, P75, DOI 10.1109/JSTARS.2020.3036914
   Chen L.C., 2014, ARXIV 14127062, V0, P0
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, V0, PP1, DOI 10.1109/NANOARCH.2017.8053709
   Coluzzi R, 2018, REMOTE SENS ENVIRON, V217, P426, DOI 10.1016/j.rse.2018.08.009
   Dharpure JK, 2020, GISCI REMOTE SENS, V57, P882, DOI 10.1080/15481603.2020.1821150
   Dietz AJ, 2012, INT J REMOTE SENS, V33, P4094, DOI 10.1080/01431161.2011.640964
   Dronner J, 2018, REMOTE SENS-BASEL, V10, P0, DOI 10.3390/rs10111782
   Drusch M, 2012, REMOTE SENS ENVIRON, V120, P25, DOI 10.1016/j.rse.2011.11.026
   Eckerstorfer M, 2022, COLD REG SCI TECHNOL, V198, P0, DOI 10.1016/j.coldregions.2022.103549
   ESA, 2009, SPECTR SIGN, V0, P0
   [方墨人 Fang Moren], 2005, 地球信息科学 GEO-INFORMATION SCIENCE, V7, P10
   GANDHI A., 2021, DATA AUGMENTATION US, V0, P0
   Gascoin S, 2019, EARTH SYST SCI DATA, V11, P493, DOI 10.5194/essd-11-493-2019
   HALL DK, 1995, REMOTE SENS ENVIRON, V54, P127, DOI 10.1016/0034-4257(95)00137-P
   Hao SJ, 2020, NEUROCOMPUTING, V406, P302, DOI 10.1016/j.neucom.2019.11.118
   James T, 2021, INT J REMOTE SENS, V42, P5342, DOI 10.1080/01431161.2021.1913298
   Kramareva LS, 2019, PROCEDIA COMPUT SCI, V150, P368, DOI 10.1016/j.procs.2019.02.065
   Lee KS, 2017, J SENSORS, V2017, P0, DOI 10.1155/2017/4820905
   Lee S, 2022, GISCI REMOTE SENS, V59, P1078, DOI 10.1080/15481603.2022.2097395
   Li J, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9090902
   Li ZW, 2017, REMOTE SENS ENVIRON, V191, P342, DOI 10.1016/j.rse.2017.01.026
   Lievens H, 2022, CRYOSPHERE, V16, P159, DOI 10.5194/tc-16-159-2022
   Liu CC, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11020119
   Long J, 2015, PROC CVPR IEEE, V0, PP3431, DOI 10.1109/CVPR.2015.7298965
   Lucas R. M., 1990, REMOTE SENSING REVIEWS, V4, P285, DOI 10.1080/02757259009532109
   Miller SD, 2005, J APPL METEOROL, V44, P987, DOI 10.1175/JAM2252.1
   NELSON J., 2020, IMPORTANCE BLUR IMAG, V0, P0
   NITR C., 2020, COMPUTATION MIOU MUL, V0, P0
   OShea K., 2015, ARXIV, V0, P0
   PAL NR, 1993, PATTERN RECOGN, V26, P1277, DOI 10.1016/0031-3203(93)90135-J
   PELTO M., 2019, OFHIDRO GLACIER CHIL, V0, P0
   Raiyani K, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13020300
   Siljamo N, 2011, J APPL METEOROL CLIM, V50, P1275, DOI 10.1175/2010JAMC2568.1
   Sultana F, 2020, KNOWL-BASED SYST, V201, P0, DOI 10.1016/j.knosys.2020.106062
   Syrris V, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11080907
   Thompson JA, 2015, PHOTOGRAMM ENG REM S, V81, P119, DOI 10.14358/PERS.81.2.119
   Varade D, 2020, GISCI REMOTE SENS, V57, P107, DOI 10.1080/15481603.2019.1672365
   VARSHENY D., 2019, THESIS U TWENTE, V0, P0
   Vermote E., 2015, MYD09A1 MODIS AQUA S, V0, P0
   Wang J, 2003, INT J REMOTE SENS, V24, P4129, DOI 10.1080/0143116031000070409
   Yan DJ, 2020, WATER-SUI, V12, P0, DOI 10.3390/w12051339
   Yin DM, 2013, INT J REMOTE SENS, V34, P6529, DOI 10.1080/01431161.2013.803631
   Zhan YJ, 2017, IEEE GEOSCI REMOTE S, V14, P1785, DOI 10.1109/LGRS.2017.2735801
   Zhang XW, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12020221
   Zheng K, 2021, ISPRS INT J GEO-INF, V10, P0, DOI 10.3390/ijgi10070462
   Zhuge XY, 2017, IEEE T GEOSCI REMOTE, V55, P6111, DOI 10.1109/TGRS.2017.2720664
NR 49
TC 0
Z9 0
U1 8
U2 16
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 1548-1603
EI 1943-7226
J9 GISCI REMOTE SENS
JI GISci. Remote Sens.
PD DEC 31
PY 2022
VL 59
IS 1
BP 1321
EP 1343
DI 10.1080/15481603.2022.2112391
PG 23
WC Geography, Physical; Remote Sensing
SC Physical Geography; Remote Sensing
GA 3X1ZX
UT WOS:000842843000001
DA 2023-04-26
ER

PT J
AU Kojima, H
   Ikegami, T
AF Kojima, Hiroki
   Ikegami, Takashi
TI Organization of a Latent Space structure in VAE/GAN trained by navigation data
SO NEURAL NETWORKS
LA English
DT Article
DE Cognitive map; GAN; Place cell; Prediction; Latent space; Chaos
ID spatial periodicity; theta rhythm; hippocampus; memory; map; cells; representations; knowledge; replay; time
AB We present a novel artificial cognitive mapping system using generative deep neural networks, called variational autoencoder/generative adversarial network (VAE/GAN), which can map input images to latent vectors and generate temporal sequences internally. The results show that the distance of the predicted image is reflected in the distance of the corresponding latent vector after training. This indicates that the latent space is self-organized to reflect the proximity structure of the dataset and may provide a mechanism through which many aspects of cognition are spatially represented. The present study allows the network to internally generate temporal sequences that are analogous to the hippocampal replay/pre-play ability, where VAE produces only near-accurate replays of past experiences, but by introducing GANs, the generated sequences are coupled with instability and novelty. (C)& nbsp;2022 Elsevier Ltd. All rights reserved.
C1 [Kojima, Hiroki; Ikegami, Takashi] Univ Tokyo, Grad Sch Arts & Sci, 3-8-1 Komaba,Meguro Ku, Tokyo 1538902, Japan.
C3 University of Tokyo
RP Kojima, H (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, 3-8-1 Komaba,Meguro Ku, Tokyo 1538902, Japan.
EM kojima@sacral.c.u-tokyo.ac.jp; ikeg@sacral.c.u-tokyo.ac.jp
FU JSPS KAKENHI Grant [17H06024, 19H04979]; Fusion of AI and Brain [19H05306]; Chronogenesis; Grants-in-Aid for Scientific Research [17H06024, 19H04979] Funding Source: KAKEN
CR Aghajan ZM, 2015, NAT NEUROSCI, V18, P121, DOI 10.1038/nn.3884
   Aronov D, 2017, NATURE, V543, P719, DOI 10.1038/nature21692
   Babaeizadeh Mohammad, 2017, ARXIV171011252, V0, P0
   Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6
   Behrens TEJ, 2018, NEURON, V100, P490, DOI 10.1016/j.neuron.2018.10.002
   Bellmund JLS, 2018, SCIENCE, V362, P0, DOI 10.1126/science.aat6766
   Brandon MP, 2011, SCIENCE, V332, P595, DOI 10.1126/science.1201652
   BUZSAKI G, 1983, BRAIN RES REV, V6, P139, DOI 10.1016/0165-0173(83)90037-1
   Buzsaki G, 2013, NAT NEUROSCI, V16, P130, DOI 10.1038/nn.3304
   Constantinescu AO, 2016, SCIENCE, V352, P1464, DOI 10.1126/science.aaf0941
   Dragoi G, 2011, NATURE, V469, P397, DOI 10.1038/nature09633
   Foster DJ, 2017, ANNU REV NEUROSCI, V40, P581, DOI 10.1146/annurev-neuro-072116-031538
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gulrajani I., 2017, ADV NEURAL INFORM PR, V0, PP5769, DOI 10.5555/3295222.3295327
   Gupta AS, 2010, NEURON, V65, P695, DOI 10.1016/j.neuron.2010.01.034
   Ha D., 2018, ADV NEURAL INFORM PR, V0, P0
   Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721
   Hassabis D, 2007, J NEUROSCI, V27, P14365, DOI 10.1523/JNEUROSCI.4549-07.2007
   Hassabis D, 2007, TRENDS COGN SCI, V11, P299, DOI 10.1016/j.tics.2007.05.001
   Hassabis D, 2007, P NATL ACAD SCI USA, V104, P1726, DOI 10.1073/pnas.0610561104
   Kingma D. P., 2013, ARXIV13126114, V0, P0
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Koenig J, 2011, SCIENCE, V332, P592, DOI 10.1126/science.1201685
   Larsen ABL, 2016, PR MACH LEARN RES, V48, P0
   Lee Alex X, 2018, ARXIV180401523, V0, P0
   McNaughton BL, 1996, J EXP BIOL, V199, P173
   Moser EI, 2017, NAT NEUROSCI, V20, P1448, DOI 10.1038/nn.4653
   Nielson DM, 2015, P NATL ACAD SCI USA, V112, P11078, DOI 10.1073/pnas.1507104112
   Noguchi W, 2017, ADAPT BEHAV, V25, P129, DOI 10.1177/1059712317711487
   Nolfi S, 1999, CONNECT SCI, V11, P125, DOI 10.1080/095400999116313
   OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1
   OKEEFE J, 1979, BEHAV BRAIN SCI, V2, P487, DOI 10.1017/S0140525X00063949
   Oprea S, 2022, IEEE T PATTERN ANAL, V44, P2806, DOI 10.1109/TPAMI.2020.3045007
   pfnet-research, 2017, CHAIN GAN LIB, V0, P0
   QUIRK GJ, 1990, J NEUROSCI, V10, P2008
   Radford A., 2015, 4 INT C LEARNING REP, V0, P0
   Recanatesi S, 2021, NAT COMMUN, V12, P0, DOI 10.1038/s41467-021-21696-1
   Rikhye R. V., 2020, BIORXIV, V0, P0
   Rolls ET, 2006, REV NEUROSCIENCE, V17, P175
   ROSENSTEIN MT, 1993, PHYSICA D, V65, P117, DOI 10.1016/0167-2789(93)90009-P
   ROSSLER OE, 1981, BIOSYSTEMS, V13, P203, DOI 10.1016/0303-2647(81)90061-7
   Schiller D, 2015, J NEUROSCI, V35, P13904, DOI 10.1523/JNEUROSCI.2618-15.2015
   Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650
   Stackman RW, 2002, HIPPOCAMPUS, V12, P291, DOI 10.1002/hipo.1112
   Stella F, 2019, NEURON, V102, P450, DOI 10.1016/j.neuron.2019.01.052
   Tavares RM, 2015, NEURON, V87, P231, DOI 10.1016/j.neuron.2015.06.011
   Tokui S., 2015, P WORKSH MACH LEARN, V0, P0
   TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626
   Uria B., 2020, BIORXIV, V0, P0, DOI DOI 10.1101/2020.11.11.378141
   Whittington JCR, 2020, CELL, V183, P1249, DOI 10.1016/j.cell.2020.10.024
NR 50
TC 1
Z9 1
U1 2
U2 8
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD AUG 15
PY 2022
VL 152
IS 
BP 234
EP 243
DI 10.1016/j.neunet.2022.04.012
EA MAY 2022
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 1N1VQ
UT WOS:000800451200001
PM 35561527
DA 2023-04-26
ER
