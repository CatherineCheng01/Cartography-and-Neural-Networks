
PT J
AU Huang, G
   Ercetin, O
   Gokcesu, H
   Kalem, G
AF Huang, Gan
   Ercetin, Ozgur
   Gokcesu, Hakan
   Kalem, Gokhan
TI Deep Learning-Based QoE Prediction for Streaming Services in Mobile Networks
SO 2022 18TH INTERNATIONAL CONFERENCE ON WIRELESS AND MOBILE COMPUTING, NETWORKING AND COMMUNICATIONS (WIMOB)
LA English
DT Proceedings Paper
DE quality of experience; prediction; deep learning; video streaming; mobile networks; key performance indicators
ID path-loss prediction; neural-network; 5g
AB Video streaming accounts for the most of the global Internet traffic and providing a high user Quality of Experience (QoE) is considered an essential target for mobile network operators (MNOs). QoE strongly depends on network Quality of Service (QoS) parameters. In this work, we use real-world network traces obtained from a major cellular operator in Turkey to establish a mapping from network side parameters to the user QoE. To this end, we use a model-aided deep learning method for first predicting channel path loss, and then, employ this prediction for predicting video streaming MOS. The experimental results demonstrate that the proposed model-aided deep learning model can guarantee higher prediction accuracy compared to predictions only relying on mathematical models. We also demonstrate that even though a trained model cannot be directly transferred from one geographical area to another, they significantly reduce the volume of required training when used for prediction in a new area.
C1 [Huang, Gan; Ercetin, Ozgur] Sabanci Univ, Fac Engn & Nat Sci, Istanbul, Turkey.
   [Gokcesu, Hakan] Bilkent Univ, Dept Elect & Elect Engn, Ankara, Turkey.
   [Gokcesu, Hakan; Kalem, Gokhan] Turkcell Technol, 5G Res & Dev, Istanbul, Turkey.
C3 Sabanci University; Ihsan Dogramaci Bilkent University
RP Huang, G (corresponding author), Sabanci Univ, Fac Engn & Nat Sci, Istanbul, Turkey.
EM gan.huang@sabanciuniv.edu; oercetin@sabanciuniv.edu; hgokcesu@ee.bilkent.edu.tr; gokhan.kalem@turkcell.com.tr
FU Tubitak [119E198]; Turkcell Technology
CR 3GPP, 2021, TECHNICAL SPECIFICAT, V0, P0
   Alhammadi A., 2021, 2021 INT C ARTIFICIA, V0, P47
   [Anonymous], 2019, 38901 3GPP TR, V0, P0
   Aroussi S, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTING, V0, P200, DOI 10.1109/ComManTel.2014.6825604
   Bouraqia K, 2020, IEEE ACCESS, V8, P13341, DOI 10.1109/ACCESS.2020.2965099
   Gahbiche Msakni H, 2013, INT WIREL COMMUN, V0, PP538, DOI 10.1109/IWCMC.2013.6583615
   Gokcesu H, 2021, ARXIV, V0, P0
   Gomez-Barquero D, 2019, IEEE T BROADCAST, V65, P351, DOI 10.1109/TBC.2019.2914866
   ITU Telecommunication Standardization Sector, 2017, ITU T REC P 1203 MOD, V0, P0
   Jo HS, 2020, SENSORS-BASEL, V20, P0, DOI 10.3390/s20071927
   Kougioumtzidis G, 2022, IEEE ACCESS, V10, P19507, DOI 10.1109/ACCESS.2022.3149592
   Lee TH, 2022, APPL SCI-BASEL, V12, P0, DOI 10.3390/app12084049
   Mitra K, 2015, IEEE T MOBILE COMPUT, V14, P920, DOI 10.1109/TMC.2013.155
   Nightingale J, 2018, IEEE T BROADCAST, V64, P621, DOI 10.1109/TBC.2018.2816786
   Ostlin E, 2010, IEEE T VEH TECHNOL, V59, P2735, DOI 10.1109/TVT.2010.2050502
   Tao XM, 2019, IEEE J SEL AREA COMM, V37, P1337, DOI 10.1109/JSAC.2019.2904359
   Thrane J, 2020, IEEE ACCESS, V8, P7925, DOI 10.1109/ACCESS.2020.2964103
   Tsolkas D, 2017, J NETW COMPUT APPL, V77, P1, DOI 10.1016/j.jnca.2016.10.016
   Wen JX, 2019, IEEE ACCESS, V7, P159251, DOI 10.1109/ACCESS.2019.2950634
   Wu LN, 2020, IEEE ACCESS, V8, P199523, DOI 10.1109/ACCESS.2020.3035209
NR 20
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-4886
EI 
J9 IEEE CONF WIREL MOB
PD JUN 15
PY 2022
VL 0
IS 
BP 
EP 
DI 10.1109/WIMOB55322.2022.9941672
PG 6
WC Computer Science, Information Systems; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA BU7DW
UT WOS:000936188900043
DA 2023-04-26
ER

PT J
AU Moradi, F
   Javan, FD
   Samadzadegan, F
AF Moradi, Fatemeh
   Javan, Farzaneh Dadrass
   Samadzadegan, Farhad
TI Potential evaluation of visible-thermal UAV image fusion for individual tree detection based on convolutional neural network
SO INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION
LA English
DT Article
DE Individual Tree Crown Detection; UAV; Deep Neural Network; Thermal Image; Visible Image; Fusion
ID lidar data
AB Unmanned aerial vehicles (UAVs) outfitted with thermal and visible sensors are already a popular platform in precision agriculture thanks to recent advances in remote sensing. Many researchers have studied integrating data from sensors with different spectral characteristics to achieve higher-level properties and, consequently, detect the trees accurately. In this research, visible and thermal images, as well as normalized digital surface models resulting from UAVs with high spatial resolution, are employed to accurately extract trees from two studied urban areas with complex backgrounds. In the thermal image, trees can be detected in hidden areas based on their brightness temperature difference compared to other features. In contrast, the visible image has a higher spatial resolution, and fusing this data with thermal images can resolve the complexity of the problem. In the proposed method, first, a deep learning network based on visible-thermal data is evaluated in terms of detecting trees with various data approaches. These evaluations include comparison tests on four types of data input to the convolutional network of the visible images, thermal images, fusing visible-thermal images, and also fusing visible-thermal- normalized digital surface model images. Results of evaluation parameters indicate maximum precision in the fourth approach (intersection-over-union = 91.72, F-score = 95.67). Then, the output binary map with the highest accuracy approach and Canny edge detection operator is utilized to accurately identify tree boundaries, count, and estimate the area and diameter of the tree canopy. Finally, the findings revealed the root mean square error (RMSE) first and second areas are 0.21 m2, 0.08 m and 0.24 m2, 0.11 m respectively for the area and diameter of the tree crown.
C1 [Moradi, Fatemeh; Javan, Farzaneh Dadrass; Samadzadegan, Farhad] Univ Tehran, Coll Engn, Sch Surveying & Geospatial Engn, Tehran, Iran.
   [Javan, Farzaneh Dadrass] Univ Twente, Fac Geoinformat Sci & Earth Observat ITC, NL-7522 NB Enschede, Netherlands.
C3 University of Tehran; University of Twente
RP Javan, FD (corresponding author), Univ Tehran, Coll Engn, Sch Surveying & Geospatial Engn, Tehran, Iran.; Javan, FD (corresponding author), Univ Twente, Fac Geoinformat Sci & Earth Observat ITC, NL-7522 NB Enschede, Netherlands.
EM f.dadrassjavan@utwente.nl
CR Aghdami-Nia M, 2022, INT J APPL EARTH OBS, V109, P0, DOI 10.1016/j.jag.2022.102785
   Amani M, 2021, REMOTE SENS-BASEL, V13, P0, DOI 10.3390/rs13163315
   Ansari M, 2021, IEEE J-STARS, V14, P11240, DOI 10.1109/JSTARS.2021.3123087
   Ansari M, 2020, ADV SPACE RES, V65, P1490, DOI 10.1016/j.asr.2019.12.007
   Apolo-Apolo OE, 2020, FRONT PLANT SCI, V11, P0, DOI 10.3389/fpls.2020.01086
   Barron S, 2016, FORESTS, V7, P0, DOI 10.3390/f7090208
   Brandtberg T, 2007, ISPRS J PHOTOGRAMM, V61, P325, DOI 10.1016/j.isprsjprs.2006.10.006
   Buettner T, 2015, SPAT DEMOGR, V3, P91, DOI 10.1007/S40980-015-0004-2
   Chang HH, 2019, IEEE GEOSCI REMOTE S, V16, P1363, DOI 10.1109/LGRS.2019.2899123
   Chowdhury PN, 2022, PATTERN RECOGN LETT, V153, P1, DOI 10.1016/j.patrec.2021.11.016
   Colomina I, 2014, ISPRS J PHOTOGRAMM, V92, P79, DOI 10.1016/j.isprsjprs.2014.02.013
   Dadras Javan F., 2019, INT ARCH PHOTOGRAMM, VXLII-4/W18, P247, DOI 10.5194/isprs-archives-xlii-4-w18-247-2019
   DadrasJavan F, 2019, J PLANT DIS PROTECT, V126, P307, DOI 10.1007/s41348-019-00234-8
   Dong XY, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12010133
   Emami H, 2021, REMOTE SENS APPL, V23, P0, DOI 10.1016/j.rsase.2021.100594
   Espinoza CZ, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9090961
   Feng XX, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11171982
   Ganatra Nilay, 2021, MACHINE LEARNING FOR PREDICTIVE ANALYSIS. PROCEEDINGS OF ICTIS 2020. LECTURE NOTES IN NETWORKS AND SYSTEMS (LNNS 141), V0, PP515, DOI 10.1007/978-981-15-7106-0_51
   Guirado E, 2021, SENSORS-BASEL, V21, P0, DOI 10.3390/s21010320
   Guirado E, 2017, REMOTE SENS-BASEL, V9, P0, DOI 10.3390/rs9121220
   Han Y, 2018, IEEE T MED IMAGING, V37, P1418, DOI 10.1109/TMI.2018.2823768
   Hanapi S. N. H. Syed, 2019, IOP CONFERENCE SERIES: MATERIALS SCIENCE AND ENGINEERING, V705, P0, DOI 10.1088/1757-899X/705/1/012024
   Jalali J, 2021, HYDROLOG SCI J, V66, P2280, DOI 10.1080/02626667.2021.1985724
   Johansen K, 2020, FRONT ARTIF INTELL, V3, P0, DOI 10.3389/frai.2020.00028
   Kachamba DJ, 2016, REMOTE SENS-BASEL, V8, P0, DOI 10.3390/rs8110968
   Ke YH, 2011, INT J REMOTE SENS, V32, P4725, DOI 10.1080/01431161.2010.494184
   Li WJ, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11010011
   Liu XN, 2021, APPL ARTIF INTELL, V35, P13, DOI 10.1080/08839514.2020.1831226
   Liu YG, 2018, MULTIMED TOOLS APPL, V77, P22159, DOI 10.1007/s11042-018-5704-3
   Qin XF, 2020, SYMMETRY-BASEL, V12, P0, DOI 10.3390/sym12081230
   Ranjbar S, 2021, J APPL REMOTE SENS, V15, P0, DOI 10.1117/1.JRS.15.018503
   Renjie Song, 2017, PATTERN RECOGNITION AND IMAGE ANALYSIS (ADVANCES IN MATHEMATICAL THEORY AND APPLICATIONS) PATTERN RECOGNITION AND IMAGE ANALYSIS. (ADVANCES IN MATHEMATICAL THEORY AND APPLICATIONS), V27, P740, DOI 10.1134/S1054661817040162
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rostami A, 2022, REMOTE SENS-BASEL, V14, P0, DOI 10.3390/rs14040992
   Schiefer F, 2020, ISPRS J PHOTOGRAMM, V170, P205, DOI 10.1016/j.isprsjprs.2020.10.015
   Selvaraj MG, 2020, ISPRS J PHOTOGRAMM, V169, P110, DOI 10.1016/j.isprsjprs.2020.08.025
   Wang H, 2021, INT J APPL EARTH OBS, V101, P0, DOI 10.1016/j.jag.2021.102353
   Xu Z, 2020, INT J APPL EARTH OBS, V92, P0, DOI 10.1016/j.jag.2020.102173
   Yan WQ, 2020, REMOTE SENS-BASEL, V12, P0, DOI 10.3390/rs12030515
   Yin DM, 2019, REMOTE SENS ENVIRON, V223, P34, DOI 10.1016/j.rse.2018.12.034
   Yue K, 2019, ISPRS J PHOTOGRAMM, V156, P1, DOI 10.1016/j.isprsjprs.2019.07.007
   Zarei A., 2021, ISPRS ANN PHOTOGRAMM, V0, PP257263, DOI 10.5194/isprs-annals-V-3-2021-257-2021
   Zarei A, 2021, ADV SPACE RES, V67, P3979, DOI 10.1016/j.asr.2021.02.019
   Zhang CY, 2012, PHOTOGRAMM ENG REM S, V78, P1079, DOI 10.14358/PERS.78.10.1079
   Zhang KQ, 2003, IEEE T GEOSCI REMOTE, V41, P872, DOI 10.1109/TGRS.2003.810682
NR 46
TC 1
Z9 1
U1 5
U2 14
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1569-8432
EI 1872-826X
J9 INT J APPL EARTH OBS
JI Int. J. Appl. Earth Obs. Geoinf.
PD SEP 15
PY 2022
VL 113
IS 
BP 
EP 
DI 10.1016/j.jag.2022.103011
EA SEP 2022
PG 15
WC Remote Sensing
SC Remote Sensing
GA 4N0KD
UT WOS:000853705500001
DA 2023-04-26
ER

PT J
AU Stahl, N
   Weimann, L
AF Stahl, Niclas
   Weimann, Lisa
TI Identifying wetland areas in historical maps using deep convolutional neural networks
SO ECOLOGICAL INFORMATICS
LA English
DT Article
DE Analysis of historical maps; Convolutional neural networks; Wetland management; Wetland restoration
AB The local environment and land usages have changed a lot during the past one hundred years. Historical documents and materials are crucial in understanding and following these changes. Historical documents are, therefore, an important piece in the understanding of the impact and consequences of land usage change. This, in turn, is important in the search of restoration projects that can be conducted to turn and reduce harmful and unsustainable effects originating from changes in the land-usage.& nbsp;This work extracts information on the historical location and geographical distribution of wetlands, from hand-drawn maps. This is achieved by using deep learning (DL), and more specifically a convolutional neural network (CNN). The CNN model is trained on a manually pre-labelled dataset on historical wetlands in the area of Jonkoping county in Sweden. These are all extracted from the historical map called "Generalstabskartan ".& nbsp;The presented CNN performs well and achieves a F-1-score of 0.886 when evaluated using a 10-fold cross validation over the data.The trained models are additionally used to generate a GIS layer of the presumable historical geographical distribution of wetlands for the area that is depicted in the southern collection in Generalstabskartan, which covers the southern half of Sweden. This GIS layer is released as an open resource and can be freely used.& nbsp;To summarise, the presented results show that CNNs can be a useful tool in the extraction and digitalisation of non-textual information in historical documents, such as historical maps. A modern GIS material that can be used to further understand the past land-usage change is produced within this research. Previously, no material of this detail and extent have been available, due to the large effort needed to manually create such. However, with the presented resource better quantifications and estimations of historical wetlands that have been lost can be made.
C1 [Stahl, Niclas] Jonkoping Univ, Jonkoping Artificial Intelligence Lab, Jonkoping, Sweden.
   [Stahl, Niclas] Univ Skovde, Skovde Artificial Intelligence Lab, Kanikegrand 3, S-54134 Skovde, Sweden.
   [Weimann, Lisa] Cty Adm Board Jonkoping, Jonkoping, Sweden.
C3 Jonkoping University; University of Skovde
RP Stahl, N (corresponding author), Jonkoping Univ, Jonkoping Artificial Intelligence Lab, Jonkoping, Sweden.
EM niclas.stahl@ju.se
CR [Anonymous], 2017, PLANET DUMP, V0, P0
   [Anonymous], 1982, COMPETITION COOPERAT, V0, P0
   [Anonymous], 2021, SWED MAPP CAD LAND R, V0, P0
   Auffret AG, 2017, METHODS ECOL EVOL, V8, P1453, DOI 10.1111/2041-210X.12788
   Branson S, 2018, ISPRS J PHOTOGRAMM, V135, P13, DOI 10.1016/j.isprsjprs.2017.11.008
   Cousins SAO, 2009, BIOL CONSERV, V142, P2752, DOI 10.1016/j.biocon.2009.07.001
   Fang B, 2019, REMOTE SENS-BASEL, V11, P0, DOI 10.3390/rs11222631
   Gillies S., 2013, RASTERIO GEOSPATIAL, V0, P0
   Gray PC, 2019, METHODS ECOL EVOL, V10, P1490, DOI 10.1111/2041-210X.13246
   Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
   Herrault PA, 2013, LECT NOTES GEOINF CA, V0, PP95, DOI 10.1007/978-3-319-00615-4_6
   Huang B, 2018, REMOTE SENS ENVIRON, V214, P73, DOI 10.1016/j.rse.2018.04.050
   Kingma DP, 2015, 14126980 ARXIV, V0, P1
   Krizhevsky Alex, 2017, COMMUNICATIONS OF THE ACM, V60, P84, DOI 10.1145/3065386
   Le Bris A., 2020, ISPRS ANN PHOTOGRAMM, V2, P1013
   Li ZK, 2019, 27TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2019), V0, PP610, DOI 10.1145/3347146.3363463
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Paszke, 2019, ADV NEURAL INFORM PR, V0, P8024
   Saar L, 2012, DIVERS DISTRIB, V18, P808, DOI 10.1111/j.1472-4642.2012.00885.x
   Saeedimoghaddam M, 2020, INT J GEOGR INF SCI, V34, P947, DOI 10.1080/13658816.2019.1696968
   Shorten C, 2019, J BIG DATA-GER, V6, P0, DOI 10.1186/s40537-019-0197-0
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Weinman Jerod, 2019, 2019 INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION (ICDAR). PROCEEDINGS, V0, PP902, DOI 10.1109/ICDAR.2019.00149
   Xu B, 2015, ARXIV150500853, V0, P0
NR 24
TC 2
Z9 2
U1 1
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1574-9541
EI 1878-0512
J9 ECOL INFORM
JI Ecol. Inform.
PD MAY 15
PY 2022
VL 68
IS 
BP 
EP 
DI 10.1016/j.ecoinf.2022.101557
EA JAN 2022
PG 7
WC Ecology
SC Environmental Sciences & Ecology
GA 1B9SI
UT WOS:000792769800006
DA 2023-04-26
ER

PT J
AU Wang, Z
   Wang, YX
   Zhang, JP
   Hu, CF
   Yin, Z
   Song, Y
AF Wang, Zhe
   Wang, Yongxiong
   Zhang, Jiapeng
   Hu, Chuanfei
   Yin, Zhong
   Song, Yu
TI Spatial-Temporal Feature Fusion Neural Network for EEG-Based Emotion Recognition
SO IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT
LA English
DT Article
DE Electroencephalography; Feature extraction; Electrodes; Emotion recognition; Convolutional neural networks; Representation learning; Entropy; Dimensionality reduction; electroencephalogram (EEG); information fusion; spatial dependencies learning
AB The temporal and spatial information of electroencephalogram (EEG) are essential for the emotion recognition model to learn the discriminative features. Hence, we propose a novel hybrid spatial-temporal feature fusion neural network (STFFNN) to extract the discriminative features and integrate complementary information. The generated power topographic maps, which capture dependencies among the electrodes, are fed to convolutional neural network (CNN) for spatial feature learning. Furthermore, instance normalizations (INs) and batch normalizations (BNs) within the CNN are appropriately combined to alleviate the individual difference and preserve the domain-invariant information. Meanwhile, a feedforward network is adopted for temporal feature learning. Due to the high dimensionality of EEG features, we propose a grid-search-based configurational optimization method to robustly reduce the dimensionality. Finally, inspired by the multimodal fusion strategies that leverage the complementarity of data to obtain more robust predictions, we utilize a bidirectional long short-term memory (Bi-LSTM) network for temporal and spatial feature fusion. To validate the effectiveness of the proposed method, the tenfold cross-validation experiments and subject-dependent experiments are both conducted on the DEAP database. The experimental results demonstrate that the proposed method achieves outstanding performance in emotion recognition with arousal and valence level.
C1 [Wang, Zhe; Wang, Yongxiong; Zhang, Jiapeng; Yin, Zhong] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Hu, Chuanfei] Southeast Univ, Minist Educ, Sch Automat, Key Lab Measurement & Control CSE, Nanjing 214135, Peoples R China.
   [Song, Yu] Tianjin Univ Technol, Tianjin Key Lab Control Theory & Applicat Complic, Tianjin 300384, Peoples R China.
C3 University of Shanghai for Science & Technology; Southeast University - China; Tianjin University of Technology
RP Wang, YX (corresponding author), Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
EM 201440049@st.usst.edu.cn; wyxiong@usst.edu.cn; mnrsmzjp@126.com; cfhu@seu.edu.cn; yinzhong@usst.edu.cn; jasonsongrain@hotmail.com
FU Natural Science Foundation of Shanghai [22ZR1443700]; National Natural Science Foundation of China [62103299]; Key technologies R&D program of Tianjin [20YDTPJC00040]
CR Alhagry S, 2017, INT J ADV COMPUT SC, V8, P355
   Atkinson J, 2016, EXPERT SYST APPL, V47, P35, DOI 10.1016/j.eswa.2015.10.049
   Chao H, 2019, SENSORS-BASEL, V19, P0, DOI 10.3390/s19092212
   Chen JX, 2019, IEEE ACCESS, V7, P44317, DOI 10.1109/ACCESS.2019.2908285
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Diaz-Romero DJ, 2021, IEEE T INSTRUM MEAS, V70, P0, DOI 10.1109/TIM.2021.3059467
   Esslen M, 2004, NEUROIMAGE, V21, P1189, DOI 10.1016/j.neuroimage.2003.10.001
   Etkin A, 2011, TRENDS COGN SCI, V15, P85, DOI 10.1016/j.tics.2010.11.004
   Gao ZK, 2021, IEEE T INSTRUM MEAS, V70, P0, DOI 10.1109/TIM.2021.3090164
   Gonzalez HA, 2020, IEEE ACCESS, V8, P140896, DOI 10.1109/ACCESS.2020.3012900
   Gunes H, 2013, IMAGE VISION COMPUT, V31, P120, DOI 10.1016/j.imavis.2012.06.016
   Guo KL, 2019, 2019 IEEE MTT-S INTERNATIONAL MICROWAVE BIOMEDICAL CONFERENCE (IMBIOC 2019), V0, P0
   He KM, 2016, PROC CVPR IEEE, V0, PP770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, V0, PP7132, DOI 10.1109/CVPR.2018.00745
   Huang G, 2017, PROC CVPR IEEE, V0, PP2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jatupaiboon N, 2013, SCI WORLD J, V0, P0, DOI DOI 10.1155/2013/618649
   Khosrowabadi R, 2014, IEEE T NEUR NET LEAR, V25, P609, DOI 10.1109/TNNLS.2013.2280271
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kwon YH, 2018, SENSORS-BASEL, V18, P0, DOI 10.3390/s18051383
   Li RX, 2021, FRONT HUM NEUROSCI, V15, P0, DOI 10.3389/fnhum.2021.621493
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mehmood RM, 2021, IEEE T INSTRUM MEAS, V70, P0, DOI 10.1109/TIM.2020.3011817
   Moon SE, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, V0, P2556
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Pei Z, 2021, IEEE T INSTRUM MEAS, V70, P0, DOI 10.1109/TIM.2020.3019849
   Petrantonakis PC, 2010, IEEE T INF TECHNOL B, V14, P186, DOI 10.1109/TITB.2009.2034649
   Picard R. W., 1997, AFFECTIVE COMPUTING, V0, P247
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Rayatdoost S, 2018, IEEE INT WORKS MACH, V0, P0
   Tian ZK, 2021, IEEE T INSTRUM MEAS, V70, P0, DOI 10.1109/TIM.2021.3121473
   Ulyanov D, 2017, PROC CVPR IEEE, V0, PP4105, DOI 10.1109/CVPR.2017.437
   Ulyanov Dmitry, 2016, ABS160708022 CORR, V0, P0
   Val-Calvo M, 2020, IEEE ACCESS, V8, P134051, DOI 10.1109/ACCESS.2020.3007109
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang XW, 2014, NEUROCOMPUTING, V129, P94, DOI 10.1016/j.neucom.2013.06.046
   Xing XF, 2019, FRONT NEUROROBOTICS, V13, P0, DOI 10.3389/fnbot.2019.00037
   Yin Z, 2017, COGN TECHNOL WORK, V19, P667, DOI 10.1007/s10111-017-0450-2
   Yin Z, 2017, COMPUT METH PROG BIO, V140, P93, DOI 10.1016/j.cmpb.2016.12.005
   Yu Z, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), V0, PP338, DOI 10.1109/ASRU.2015.7404814
   Zhang Y., 2020, IEEE ACCESS, V9, P0
NR 41
TC 3
Z9 3
U1 29
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9456
EI 1557-9662
J9 IEEE T INSTRUM MEAS
JI IEEE Trans. Instrum. Meas.
PD JUN 15
PY 2022
VL 71
IS 
BP 
EP 
DI 10.1109/TIM.2022.3165280
PG 12
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA 0P4QO
UT WOS:000784206300010
DA 2023-04-26
ER

PT J
AU Fu, X
   Liu, YF
   Zhu, Q
   Ge, DQ
   Li, Y
   Zeng, HW
AF Fu, Xiao
   Liu, Yuefan
   Zhu, Qing
   Ge, Daqing
   Li, Yun
   Zeng, Haowei
TI Reliable assessment approach of landslide susceptibility in broad areas based on optimal slope units and negative samples involving priori knowledge
SO INTERNATIONAL JOURNAL OF DIGITAL EARTH
LA English
DT Article
DE Slope units; mapping units; landslide susceptibility assessment; digital elevation model; certainty factor; machine learning
ID fuzzy multicriteria; gis; hazard; models
AB Reliable assessment of landslide susceptibility in broad areas of terrain remains challenging due to complex topography and poor representation of randomly selected negative samples. Assessment in broad areas is now primarily based on grid units, which do not have a clear physical meaning like slope units, and their accuracy is not ideal. Nevertheless, the large amount of manual editing, due to the incorrectly generated horizontal and vertical lines during slope unit partitioning, limits using slope units for rapid assessment over large areas. Hence, this paper proposes a reliable susceptibility assessment approach to solve this problem based on optimal slope units and negative samples involving prior knowledge. Precisely, an algorithm to automatically extract slope units is designed to eliminate fragmented and erroneous units. Second, a samples labeling index (SLI) is defined based on the certainty factors model to select negative samples reasonably. Sichuan Province, China is selected for experimental analysis, with the results demonstrate that the optimized slope unit and the negative samples selection strategy consider prior knowledge achieve better results in the random forest model, support vector machine model, and artificial neural network model. In particular, the composite performance index AUC of artificial neural network model improved from 0.81 to 0.90.
C1 [Fu, Xiao; Liu, Yuefan; Zhu, Qing; Zeng, Haowei] Southwest Jiaotong Univ, Fac Geosci & Environm Engn, Chengdu, Peoples R China.
   [Ge, Daqing] China Aero Geophys Surveying & Remote Sensing Ctr, Beijing, Peoples R China.
   [Li, Yun] Lanzhou Jiaotong Univ, Fac Geomat, Lanzhou, Peoples R China.
   [Zhu, Qing] 999 Xian Rd, Chengdu, Sichuan, Peoples R China.
C3 Southwest Jiaotong University; Lanzhou Jiaotong University
RP Zhu, Q (corresponding author), 999 Xian Rd, Chengdu, Sichuan, Peoples R China.
EM zhuqing@swjtu.edu.cn
FU National Natural Science Foundation of China [41941019]; Identification of potential geohazards by integrated remote sensing technologies and applications [DD20211365]
CR Abu El-Magd SA, 2021, EARTH SCI INFORM, V14, P1227, DOI 10.1007/s12145-021-00653-y
   Akinci H, 2021, NAT HAZARDS, V108, P1515, DOI 10.1007/s11069-021-04743-4
   Ali SKA, 2022, NAT HAZARDS, V113, P1601, DOI 10.1007/s11069-022-05360-5
   Ali SA, 2021, GEOSCI FRONT, V12, P857, DOI 10.1016/j.gsf.2020.09.004
   Alvioli M, 2016, GEOSCI MODEL DEV, V9, P3975, DOI 10.5194/gmd-9-3975-2016
   CARRARA A, 1995, ADV NAT TECHNOL HAZ, V5, P135
   CARRARA A, 1991, EARTH SURF PROCESSES, V16, P427, DOI 10.1002/esp.3290160505
   Chefaoui RM, 2008, ECOL MODEL, V210, P478, DOI 10.1016/j.ecolmodel.2007.08.010
   Chen Z, 2020, GEOCARTO INT, V35, P1641, DOI 10.1080/10106049.2019.1582716
   Costache R, 2021, GEOCARTO INT, V0, P0, DOI DOI 10.1080/10106049.2021.1973115
   Dahal RK, 2012, GEOMAT NAT HAZ RISK, V3, P161, DOI 10.1080/19475705.2011.629007
   Erener A, 2012, ENVIRON EARTH SCI, V66, P859, DOI 10.1007/s12665-011-1297-0
   Giles PT, 1998, GEOMORPHOLOGY, V21, P251, DOI 10.1016/S0169-555X(97)00064-0
   Guzzetti F, 1999, GEOMORPHOLOGY, V31, P181, DOI 10.1016/S0169-555X(99)00078-1
   Heckerman David., 1986, MACHINE INTELLIGENCE, V0, P0
   Hu Q, 2020, GEOMORPHOLOGY, V351, P0, DOI 10.1016/j.geomorph.2019.106975
   Huang FM, 2021, LANDSLIDES, V18, P3715, DOI 10.1007/s10346-021-01756-9
   Jacobs L, 2020, GEOMORPHOLOGY, V356, P0, DOI 10.1016/j.geomorph.2020.107084
   Kavzoglu T, 2014, LANDSLIDES, V11, P425, DOI 10.1007/s10346-013-0391-7
   Lin CH, 2018, ENG GEOL, V246, P310, DOI 10.1016/j.enggeo.2018.10.004
   Liu LL, 2022, B ENG GEOL ENVIRON, V81, P0, DOI 10.1007/s10064-022-02836-3
   Martinello C, 2021, J MAPS, V17, P152, DOI 10.1080/17445647.2020.1805807
   Qi JD, 2020, J MANUF PROCESS, V59, P302, DOI 10.1016/j.jmapro.2020.09.061
   Pham QB, 2021, GEOMAT NAT HAZ RISK, V12, P1741, DOI 10.1080/19475705.2021.1944330
   Romstad B., 2009, P GEOMORPHOMETRY, V31, P55
   Romstad B, 2012, GEOMORPHOLOGY, V139, P293, DOI 10.1016/j.geomorph.2011.10.031
   Shortliffe E.H., 1975, MATH BIOSCI, V23, P351, DOI 10.1016/0025-5564(75)90047-4
   Thomas AV, 2021, J GEOVIS SPAT ANAL, V5, P0, DOI 10.1007/s41651-021-00090-x
   van Westen CJ, 2006, B ENG GEOL ENVIRON, V65, P167, DOI 10.1007/s10064-005-0023-0
   Wang K, 2019, B ENG GEOL ENVIRON, V78, P4139, DOI 10.1007/s10064-018-1389-0
   Xiao CC, 2010, SCI CHINA TECHNOL SC, V53, P75, DOI 10.1007/s11431-010-3219-x
   Yalcin A, 2007, NAT HAZARDS, V41, P201, DOI 10.1007/s11069-006-9030-0
   Zeng HW, 2022, INT J GEOGR INF SCI, V36, P2270, DOI 10.1080/13658816.2022.2103819
   Zhu AX, 2019, CATENA, V183, P0, DOI 10.1016/j.catena.2019.104188
NR 34
TC 0
Z9 0
U1 0
U2 0
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 1753-8947
EI 1753-8955
J9 INT J DIGIT EARTH
JI Int. J. Digit. Earth
PD DEC 31
PY 2022
VL 15
IS 1
BP 2495
EP 2510
DI 10.1080/17538947.2022.2159549
PG 16
WC Geography, Physical; Remote Sensing
SC Physical Geography; Remote Sensing
GA A3ZE2
UT WOS:000954537200005
DA 2023-04-26
ER
